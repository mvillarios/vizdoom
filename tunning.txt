Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 6.06     |
| time/              |          |
|    fps             | 102      |
|    iterations      | 1        |
|    time_elapsed    | 20       |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 127        |
|    ep_rew_mean          | 6.94       |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 2          |
|    time_elapsed         | 48         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.00901191 |
|    clip_fraction        | 0.0965     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | 0.0214     |
|    learning_rate        | 3.57e-05   |
|    loss                 | -0.000482  |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 0.133      |
----------------------------------------
Eval num_timesteps=5000, episode_reward=9.50 +/- 2.25
Episode length: 126.30 +/- 30.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 9.5         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009584801 |
|    clip_fraction        | 0.045       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.207       |
|    learning_rate        | 3.57e-05    |
|    loss                 | 0.0722      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00645    |
|    value_loss           | 0.156       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 7.89     |
| time/              |          |
|    fps             | 77       |
|    iterations      | 3        |
|    time_elapsed    | 78       |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 138         |
|    ep_rew_mean          | 8.32        |
| time/                   |             |
|    fps                  | 78          |
|    iterations           | 4           |
|    time_elapsed         | 104         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.009818949 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.245       |
|    learning_rate        | 3.57e-05    |
|    loss                 | 0.059       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00713    |
|    value_loss           | 0.185       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=9.80 +/- 1.25
Episode length: 130.80 +/- 17.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 131         |
|    mean_reward          | 9.8         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.013294411 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.407       |
|    learning_rate        | 3.57e-05    |
|    loss                 | 0.0281      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.15        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 8.47     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 5        |
|    time_elapsed    | 134      |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 143         |
|    ep_rew_mean          | 8.88        |
| time/                   |             |
|    fps                  | 77          |
|    iterations           | 6           |
|    time_elapsed         | 159         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.008521101 |
|    clip_fraction        | 0.0452      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.539       |
|    learning_rate        | 3.57e-05    |
|    loss                 | 0.116       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00665    |
|    value_loss           | 0.183       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 148         |
|    ep_rew_mean          | 9.41        |
| time/                   |             |
|    fps                  | 77          |
|    iterations           | 7           |
|    time_elapsed         | 183         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.009591291 |
|    clip_fraction        | 0.0598      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.568       |
|    learning_rate        | 3.57e-05    |
|    loss                 | 0.0443      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00935    |
|    value_loss           | 0.182       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=7.90 +/- 2.47
Episode length: 98.80 +/- 34.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.8         |
|    mean_reward          | 7.9          |
| time/                   |              |
|    total_timesteps      | 15000        |
| train/                  |              |
|    approx_kl            | 0.0065842713 |
|    clip_fraction        | 0.0692       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.584        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.0694       |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00965     |
|    value_loss           | 0.177        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 8        |
|    time_elapsed    | 212      |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 156          |
|    ep_rew_mean          | 11.1         |
| time/                   |              |
|    fps                  | 77           |
|    iterations           | 9            |
|    time_elapsed         | 237          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0058044707 |
|    clip_fraction        | 0.0617       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.626        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.0719       |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.0115      |
|    value_loss           | 0.21         |
------------------------------------------
Eval num_timesteps=20000, episode_reward=11.90 +/- 4.55
Episode length: 143.90 +/- 46.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 144          |
|    mean_reward          | 11.9         |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0048264647 |
|    clip_fraction        | 0.0465       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.07        |
|    explained_variance   | 0.69         |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.0467       |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 0.211        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 12       |
| time/              |          |
|    fps             | 76       |
|    iterations      | 10       |
|    time_elapsed    | 269      |
|    total_timesteps | 20480    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 165          |
|    ep_rew_mean          | 12.8         |
| time/                   |              |
|    fps                  | 76           |
|    iterations           | 11           |
|    time_elapsed         | 293          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0070948238 |
|    clip_fraction        | 0.0757       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.976       |
|    explained_variance   | 0.562        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.0425       |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.012       |
|    value_loss           | 0.221        |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 170          |
|    ep_rew_mean          | 13.7         |
| time/                   |              |
|    fps                  | 75           |
|    iterations           | 12           |
|    time_elapsed         | 326          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0036253622 |
|    clip_fraction        | 0.0407       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.985       |
|    explained_variance   | 0.597        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.0333       |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.0062      |
|    value_loss           | 0.21         |
------------------------------------------
Eval num_timesteps=25000, episode_reward=10.50 +/- 3.72
Episode length: 125.80 +/- 38.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 126          |
|    mean_reward          | 10.5         |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0033161337 |
|    clip_fraction        | 0.0315       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1           |
|    explained_variance   | 0.522        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.105        |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00598     |
|    value_loss           | 0.245        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 14.6     |
| time/              |          |
|    fps             | 70       |
|    iterations      | 13       |
|    time_elapsed    | 379      |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 182          |
|    ep_rew_mean          | 15.5         |
| time/                   |              |
|    fps                  | 67           |
|    iterations           | 14           |
|    time_elapsed         | 423          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0049795285 |
|    clip_fraction        | 0.0711       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.978       |
|    explained_variance   | 0.731        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.067        |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00682     |
|    value_loss           | 0.224        |
------------------------------------------
Eval num_timesteps=30000, episode_reward=11.80 +/- 3.31
Episode length: 132.80 +/- 36.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 133          |
|    mean_reward          | 11.8         |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0056161857 |
|    clip_fraction        | 0.0766       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.879       |
|    explained_variance   | 0.728        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.0978       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00962     |
|    value_loss           | 0.213        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 16       |
| time/              |          |
|    fps             | 67       |
|    iterations      | 15       |
|    time_elapsed    | 454      |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 181          |
|    ep_rew_mean          | 16.4         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 16           |
|    time_elapsed         | 480          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0060305656 |
|    clip_fraction        | 0.0652       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.922       |
|    explained_variance   | 0.75         |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.086        |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00789     |
|    value_loss           | 0.243        |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 185          |
|    ep_rew_mean          | 16.9         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 17           |
|    time_elapsed         | 506          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0054368316 |
|    clip_fraction        | 0.0653       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.976       |
|    explained_variance   | 0.768        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.121        |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.008       |
|    value_loss           | 0.235        |
------------------------------------------
Eval num_timesteps=35000, episode_reward=16.50 +/- 4.98
Episode length: 172.20 +/- 54.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 172          |
|    mean_reward          | 16.5         |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0057753944 |
|    clip_fraction        | 0.0682       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.952       |
|    explained_variance   | 0.836        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.0748       |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.0087      |
|    value_loss           | 0.219        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 17.2     |
| time/              |          |
|    fps             | 68       |
|    iterations      | 18       |
|    time_elapsed    | 539      |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 188          |
|    ep_rew_mean          | 17.4         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 19           |
|    time_elapsed         | 565          |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0073014665 |
|    clip_fraction        | 0.1          |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.973       |
|    explained_variance   | 0.819        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.101        |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.011       |
|    value_loss           | 0.224        |
------------------------------------------
Eval num_timesteps=40000, episode_reward=17.30 +/- 2.87
Episode length: 194.60 +/- 39.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 195          |
|    mean_reward          | 17.3         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0050079813 |
|    clip_fraction        | 0.0577       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.927       |
|    explained_variance   | 0.84         |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.11         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00947     |
|    value_loss           | 0.221        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 17.4     |
| time/              |          |
|    fps             | 68       |
|    iterations      | 20       |
|    time_elapsed    | 599      |
|    total_timesteps | 40960    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 185          |
|    ep_rew_mean          | 17.3         |
| time/                   |              |
|    fps                  | 68           |
|    iterations           | 21           |
|    time_elapsed         | 625          |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0067875157 |
|    clip_fraction        | 0.0685       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.942       |
|    explained_variance   | 0.842        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.227        |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00819     |
|    value_loss           | 0.245        |
------------------------------------------
Eval num_timesteps=45000, episode_reward=15.50 +/- 4.92
Episode length: 158.30 +/- 41.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 158         |
|    mean_reward          | 15.5        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.005957138 |
|    clip_fraction        | 0.0612      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.983      |
|    explained_variance   | 0.83        |
|    learning_rate        | 3.57e-05    |
|    loss                 | 0.126       |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00864    |
|    value_loss           | 0.226       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 17.8     |
| time/              |          |
|    fps             | 68       |
|    iterations      | 22       |
|    time_elapsed    | 658      |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 184         |
|    ep_rew_mean          | 17.6        |
| time/                   |             |
|    fps                  | 68          |
|    iterations           | 23          |
|    time_elapsed         | 684         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.004683488 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.963      |
|    explained_variance   | 0.854       |
|    learning_rate        | 3.57e-05    |
|    loss                 | 0.0806      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00846    |
|    value_loss           | 0.218       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 17.7        |
| time/                   |             |
|    fps                  | 69          |
|    iterations           | 24          |
|    time_elapsed         | 710         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.006668082 |
|    clip_fraction        | 0.0718      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.894      |
|    explained_variance   | 0.873       |
|    learning_rate        | 3.57e-05    |
|    loss                 | 0.0797      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.209       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=15.30 +/- 4.08
Episode length: 155.50 +/- 38.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 15.3        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.006042033 |
|    clip_fraction        | 0.0788      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.862      |
|    explained_variance   | 0.854       |
|    learning_rate        | 3.57e-05    |
|    loss                 | 0.118       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.239       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 69       |
|    iterations      | 25       |
|    time_elapsed    | 740      |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 188         |
|    ep_rew_mean          | 18.1        |
| time/                   |             |
|    fps                  | 67          |
|    iterations           | 26          |
|    time_elapsed         | 785         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.008208066 |
|    clip_fraction        | 0.0678      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.847      |
|    explained_variance   | 0.846       |
|    learning_rate        | 3.57e-05    |
|    loss                 | 0.199       |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.255       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=20.20 +/- 5.19
Episode length: 202.90 +/- 56.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | 20.2        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.008795537 |
|    clip_fraction        | 0.0996      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.824      |
|    explained_variance   | 0.852       |
|    learning_rate        | 3.57e-05    |
|    loss                 | 0.159       |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.24        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 18.2     |
| time/              |          |
|    fps             | 65       |
|    iterations      | 27       |
|    time_elapsed    | 844      |
|    total_timesteps | 55296    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 186          |
|    ep_rew_mean          | 18.1         |
| time/                   |              |
|    fps                  | 65           |
|    iterations           | 28           |
|    time_elapsed         | 875          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0059506074 |
|    clip_fraction        | 0.0806       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.892       |
|    explained_variance   | 0.898        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.0513       |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.011       |
|    value_loss           | 0.24         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | 18.2        |
| time/                   |             |
|    fps                  | 65          |
|    iterations           | 29          |
|    time_elapsed         | 900         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.009129835 |
|    clip_fraction        | 0.0747      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.854      |
|    explained_variance   | 0.882       |
|    learning_rate        | 3.57e-05    |
|    loss                 | 0.0563      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.247       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=20.80 +/- 3.28
Episode length: 204.80 +/- 39.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 205          |
|    mean_reward          | 20.8         |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0072351787 |
|    clip_fraction        | 0.081        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.867       |
|    explained_variance   | 0.862        |
|    learning_rate        | 3.57e-05     |
|    loss                 | 0.0807       |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.0125      |
|    value_loss           | 0.259        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 65       |
|    iterations      | 30       |
|    time_elapsed    | 933      |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 8.54     |
| time/              |          |
|    fps             | 124      |
|    iterations      | 1        |
|    time_elapsed    | 33       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=12.90 +/- 2.39
Episode length: 155.60 +/- 34.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 12.9        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.008461125 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.182      |
|    learning_rate        | 7.58e-05    |
|    loss                 | 0.0787      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0091     |
|    value_loss           | 0.165       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 8.39     |
| time/              |          |
|    fps             | 106      |
|    iterations      | 2        |
|    time_elapsed    | 76       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=14.60 +/- 6.79
Episode length: 177.40 +/- 73.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 177        |
|    mean_reward          | 14.6       |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.00631359 |
|    clip_fraction        | 0.0493     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.192      |
|    learning_rate        | 7.58e-05   |
|    loss                 | 0.0582     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0078    |
|    value_loss           | 0.145      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 8.69     |
| time/              |          |
|    fps             | 101      |
|    iterations      | 3        |
|    time_elapsed    | 121      |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=11.10 +/- 4.99
Episode length: 141.10 +/- 55.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 141         |
|    mean_reward          | 11.1        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.008718092 |
|    clip_fraction        | 0.0734      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.371       |
|    learning_rate        | 7.58e-05    |
|    loss                 | 0.0499      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.131       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 9.21     |
| time/              |          |
|    fps             | 99       |
|    iterations      | 4        |
|    time_elapsed    | 164      |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=11.50 +/- 3.96
Episode length: 145.70 +/- 51.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 146         |
|    mean_reward          | 11.5        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.009360684 |
|    clip_fraction        | 0.0942      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.377       |
|    learning_rate        | 7.58e-05    |
|    loss                 | 0.0731      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.149       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    fps             | 98       |
|    iterations      | 5        |
|    time_elapsed    | 208      |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | 12          |
| time/                   |             |
|    fps                  | 97          |
|    iterations           | 6           |
|    time_elapsed         | 252         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.010586293 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.431       |
|    learning_rate        | 7.58e-05    |
|    loss                 | 0.05        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.146       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=14.80 +/- 4.62
Episode length: 157.00 +/- 42.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 157         |
|    mean_reward          | 14.8        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.009832144 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.422       |
|    learning_rate        | 7.58e-05    |
|    loss                 | 0.048       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.155       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 7        |
|    time_elapsed    | 332      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=16.20 +/- 4.89
Episode length: 176.30 +/- 52.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 176         |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.011231592 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.466       |
|    learning_rate        | 7.58e-05    |
|    loss                 | 0.0888      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.15        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 14.8     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 8        |
|    time_elapsed    | 384      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=12.30 +/- 4.36
Episode length: 136.50 +/- 37.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 136        |
|    mean_reward          | 12.3       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.00720667 |
|    clip_fraction        | 0.09       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1         |
|    explained_variance   | 0.442      |
|    learning_rate        | 7.58e-05   |
|    loss                 | 0.0572     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 0.149      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 15.9     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 9        |
|    time_elapsed    | 427      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=14.60 +/- 6.17
Episode length: 168.80 +/- 76.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 169          |
|    mean_reward          | 14.6         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0066580176 |
|    clip_fraction        | 0.0842       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.973       |
|    explained_variance   | 0.535        |
|    learning_rate        | 7.58e-05     |
|    loss                 | 0.0432       |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.0108      |
|    value_loss           | 0.138        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 16.4     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 10       |
|    time_elapsed    | 473      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=18.30 +/- 4.38
Episode length: 198.20 +/- 50.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 198          |
|    mean_reward          | 18.3         |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0060792323 |
|    clip_fraction        | 0.0553       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.934       |
|    explained_variance   | 0.534        |
|    learning_rate        | 7.58e-05     |
|    loss                 | 0.0492       |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00999     |
|    value_loss           | 0.139        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 17       |
| time/              |          |
|    fps             | 86       |
|    iterations      | 11       |
|    time_elapsed    | 518      |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 187         |
|    ep_rew_mean          | 17.6        |
| time/                   |             |
|    fps                  | 88          |
|    iterations           | 12          |
|    time_elapsed         | 556         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.007084906 |
|    clip_fraction        | 0.0764      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.922      |
|    explained_variance   | 0.559       |
|    learning_rate        | 7.58e-05    |
|    loss                 | 0.062       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.136       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=16.60 +/- 4.57
Episode length: 167.00 +/- 41.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 167          |
|    mean_reward          | 16.6         |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0075962804 |
|    clip_fraction        | 0.0931       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.945       |
|    explained_variance   | 0.582        |
|    learning_rate        | 7.58e-05     |
|    loss                 | 0.0455       |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.0128      |
|    value_loss           | 0.142        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 18       |
| time/              |          |
|    fps             | 88       |
|    iterations      | 13       |
|    time_elapsed    | 601      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=16.00 +/- 3.41
Episode length: 165.30 +/- 38.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 165          |
|    mean_reward          | 16           |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0088296635 |
|    clip_fraction        | 0.0764       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.957       |
|    explained_variance   | 0.582        |
|    learning_rate        | 7.58e-05     |
|    loss                 | 0.0552       |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.0145      |
|    value_loss           | 0.143        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    fps             | 88       |
|    iterations      | 14       |
|    time_elapsed    | 647      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=17.20 +/- 3.54
Episode length: 177.20 +/- 36.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 177          |
|    mean_reward          | 17.2         |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0069119083 |
|    clip_fraction        | 0.0775       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.933       |
|    explained_variance   | 0.614        |
|    learning_rate        | 7.58e-05     |
|    loss                 | 0.0171       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.013       |
|    value_loss           | 0.137        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 18.5     |
| time/              |          |
|    fps             | 87       |
|    iterations      | 15       |
|    time_elapsed    | 701      |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=2.20 +/- 1.94
Episode length: 84.40 +/- 36.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.4     |
|    mean_reward     | 2.2      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 7.66     |
| time/              |          |
|    fps             | 81       |
|    iterations      | 1        |
|    time_elapsed    | 100      |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=16.20 +/- 4.60
Episode length: 179.80 +/- 56.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 180         |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.019119374 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0609     |
|    learning_rate        | 0.0006      |
|    loss                 | -0.00472    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.131       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=13.70 +/- 3.20
Episode length: 153.10 +/- 36.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 153      |
|    mean_reward     | 13.7     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 9.25     |
| time/              |          |
|    fps             | 76       |
|    iterations      | 2        |
|    time_elapsed    | 214      |
|    total_timesteps | 16384    |
---------------------------------
