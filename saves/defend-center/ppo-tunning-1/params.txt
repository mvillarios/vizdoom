Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.8     |
|    ep_rew_mean     | 0.1      |
| time/              |          |
|    fps             | 893      |
|    iterations      | 1        |
|    time_elapsed    | 4        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.30 +/- 0.90
Episode length: 74.70 +/- 14.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.7        |
|    mean_reward          | -0.3        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009685383 |
|    clip_fraction        | 0.0728      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0111     |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0133     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00804    |
|    value_loss           | 0.0412      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 796      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.40 +/- 0.49
Episode length: 75.50 +/- 16.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.5        |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009949598 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.362       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.00791    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.0404      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.8     |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 794      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=0.60 +/- 0.66
Episode length: 87.70 +/- 10.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 87.7       |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01771173 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.000183   |
|    loss                 | -0.0199    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0256    |
|    value_loss           | 0.0411     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.8     |
|    ep_rew_mean     | 0.8      |
| time/              |          |
|    fps             | 787      |
|    iterations      | 4        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=3.00 +/- 1.84
Episode length: 111.50 +/- 33.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 3           |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012525703 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.00116    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 0.0456      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.9     |
|    ep_rew_mean     | 1.26     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 5        |
|    time_elapsed    | 26       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 96.8        |
|    ep_rew_mean          | 2.05        |
| time/                   |             |
|    fps                  | 775         |
|    iterations           | 6           |
|    time_elapsed         | 31          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.015251009 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.00914    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0301     |
|    value_loss           | 0.0416      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=8.20 +/- 3.60
Episode length: 165.10 +/- 57.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 165         |
|    mean_reward          | 8.2         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.018653665 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0233     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0396     |
|    value_loss           | 0.0567      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 756      |
|    iterations      | 7        |
|    time_elapsed    | 37       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=10.80 +/- 4.53
Episode length: 202.40 +/- 78.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | 10.8        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.019206509 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0472     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0399     |
|    value_loss           | 0.0546      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 3.95     |
| time/              |          |
|    fps             | 738      |
|    iterations      | 8        |
|    time_elapsed    | 44       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=9.70 +/- 3.13
Episode length: 191.20 +/- 53.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 191         |
|    mean_reward          | 9.7         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.020110412 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0182     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0368     |
|    value_loss           | 0.0709      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 4.95     |
| time/              |          |
|    fps             | 729      |
|    iterations      | 9        |
|    time_elapsed    | 50       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=11.50 +/- 3.50
Episode length: 233.70 +/- 51.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 234         |
|    mean_reward          | 11.5        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.023657933 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0132     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0352     |
|    value_loss           | 0.0716      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 5.97     |
| time/              |          |
|    fps             | 716      |
|    iterations      | 10       |
|    time_elapsed    | 57       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=12.40 +/- 2.29
Episode length: 224.70 +/- 48.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 225         |
|    mean_reward          | 12.4        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.024088439 |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.996      |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.00989    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.082       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 6.9      |
| time/              |          |
|    fps             | 708      |
|    iterations      | 11       |
|    time_elapsed    | 63       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 162         |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 718         |
|    iterations           | 12          |
|    time_elapsed         | 68          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.028842852 |
|    clip_fraction        | 0.269       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.951      |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.000183    |
|    loss                 | 0.00195     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0288     |
|    value_loss           | 0.0805      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=13.80 +/- 4.07
Episode length: 241.10 +/- 58.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 241         |
|    mean_reward          | 13.8        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.027087156 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.908      |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.000183    |
|    loss                 | 0.00999     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.0805      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 8.7      |
| time/              |          |
|    fps             | 705      |
|    iterations      | 13       |
|    time_elapsed    | 75       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=14.60 +/- 4.45
Episode length: 255.60 +/- 54.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 14.6        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.032086544 |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.814      |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0402     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0306     |
|    value_loss           | 0.0899      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 9.3      |
| time/              |          |
|    fps             | 698      |
|    iterations      | 14       |
|    time_elapsed    | 82       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=15.90 +/- 2.39
Episode length: 275.30 +/- 45.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 275         |
|    mean_reward          | 15.9        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.030571168 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.785      |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.000183    |
|    loss                 | -0.0136     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0297     |
|    value_loss           | 0.0927      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 10       |
| time/              |          |
|    fps             | 692      |
|    iterations      | 15       |
|    time_elapsed    | 88       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=-1.00 +/- 0.00
Episode length: 71.20 +/- 8.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.2     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.6     |
|    ep_rew_mean     | 0.11     |
| time/              |          |
|    fps             | 1063     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=1.10 +/- 1.04
Episode length: 79.90 +/- 13.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.9        |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.015258251 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0395     |
|    learning_rate        | 0.000702    |
|    loss                 | -0.00655    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.0879      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=1.60 +/- 0.80
Episode length: 90.30 +/- 12.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.3     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.3     |
|    ep_rew_mean     | 0.71     |
| time/              |          |
|    fps             | 918      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=9.20 +/- 3.71
Episode length: 193.00 +/- 64.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 193         |
|    mean_reward          | 9.2         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.017531369 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.000702    |
|    loss                 | 0.00375     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.101       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.3     |
|    ep_rew_mean     | 1.12     |
| time/              |          |
|    fps             | 885      |
|    iterations      | 3        |
|    time_elapsed    | 27       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=4.80 +/- 4.14
Episode length: 127.90 +/- 71.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 4.8         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.022352219 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.000702    |
|    loss                 | -0.00228    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0267     |
|    value_loss           | 0.0869      |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=6.50 +/- 3.67
Episode length: 151.30 +/- 61.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 151      |
|    mean_reward     | 6.5      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 1.99     |
| time/              |          |
|    fps             | 858      |
|    iterations      | 4        |
|    time_elapsed    | 38       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=6.90 +/- 3.33
Episode length: 164.00 +/- 62.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 164        |
|    mean_reward          | 6.9        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.03153588 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.000702   |
|    loss                 | 0.00185    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0354    |
|    value_loss           | 0.115      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=8.30 +/- 4.84
Episode length: 185.20 +/- 69.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 8.3      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.39     |
| time/              |          |
|    fps             | 833      |
|    iterations      | 5        |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=14.60 +/- 5.66
Episode length: 289.50 +/- 104.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 290        |
|    mean_reward          | 14.6       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.03173881 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.2       |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.000702   |
|    loss                 | -0.00758   |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0351    |
|    value_loss           | 0.129      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 4.65     |
| time/              |          |
|    fps             | 830      |
|    iterations      | 6        |
|    time_elapsed    | 59       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=10.00 +/- 5.16
Episode length: 168.50 +/- 73.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 168        |
|    mean_reward          | 10         |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.03419461 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.000702   |
|    loss                 | 0.0156     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0323    |
|    value_loss           | 0.135      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=11.50 +/- 5.85
Episode length: 200.20 +/- 84.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 200      |
|    mean_reward     | 11.5     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 5.68     |
| time/              |          |
|    fps             | 824      |
|    iterations      | 7        |
|    time_elapsed    | 69       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=9.60 +/- 2.15
Episode length: 177.80 +/- 45.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 178       |
|    mean_reward          | 9.6       |
| time/                   |           |
|    total_timesteps      | 60000     |
| train/                  |           |
|    approx_kl            | 0.0373763 |
|    clip_fraction        | 0.313     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.08     |
|    explained_variance   | 0.71      |
|    learning_rate        | 0.000702  |
|    loss                 | 0.00721   |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.0333   |
|    value_loss           | 0.137     |
---------------------------------------
Eval num_timesteps=65000, episode_reward=10.30 +/- 3.90
Episode length: 183.30 +/- 57.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 183      |
|    mean_reward     | 10.3     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 6.96     |
| time/              |          |
|    fps             | 819      |
|    iterations      | 8        |
|    time_elapsed    | 79       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=-1.00 +/- 0.00
Episode length: 71.50 +/- 10.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.5     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81       |
|    ep_rew_mean     | 0.15     |
| time/              |          |
|    fps             | 1150     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=1.00 +/- 1.18
Episode length: 88.10 +/- 29.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 88.1       |
|    mean_reward          | 1          |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01395519 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | -0.0135    |
|    learning_rate        | 0.000286   |
|    loss                 | 0.0379     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.01      |
|    value_loss           | 0.0773     |
----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=1.20 +/- 0.87
Episode length: 81.70 +/- 13.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.7     |
|    mean_reward     | 1.2      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.5     |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 960      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=2.30 +/- 1.35
Episode length: 88.20 +/- 23.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88.2        |
|    mean_reward          | 2.3         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012391508 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.521       |
|    learning_rate        | 0.000286    |
|    loss                 | 0.00603     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.0537      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.5     |
|    ep_rew_mean     | 0.87     |
| time/              |          |
|    fps             | 916      |
|    iterations      | 3        |
|    time_elapsed    | 26       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=4.30 +/- 1.79
Episode length: 131.90 +/- 41.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 132        |
|    mean_reward          | 4.3        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01716723 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.000286   |
|    loss                 | -0.0326    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0263    |
|    value_loss           | 0.0534     |
----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=4.60 +/- 2.11
Episode length: 129.50 +/- 41.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 4.6      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 1.9      |
| time/              |          |
|    fps             | 876      |
|    iterations      | 4        |
|    time_elapsed    | 37       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=4.90 +/- 1.87
Episode length: 129.70 +/- 29.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 130         |
|    mean_reward          | 4.9         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.027479917 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.000286    |
|    loss                 | -0.0114     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0363     |
|    value_loss           | 0.062       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=5.00 +/- 1.84
Episode length: 133.30 +/- 38.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 5        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.72     |
| time/              |          |
|    fps             | 852      |
|    iterations      | 5        |
|    time_elapsed    | 48       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=15.20 +/- 2.32
Episode length: 282.00 +/- 29.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 282         |
|    mean_reward          | 15.2        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.025299683 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.000286    |
|    loss                 | -0.0284     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0379     |
|    value_loss           | 0.0552      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 4.41     |
| time/              |          |
|    fps             | 834      |
|    iterations      | 6        |
|    time_elapsed    | 58       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=14.40 +/- 3.01
Episode length: 241.00 +/- 53.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 241         |
|    mean_reward          | 14.4        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.031407155 |
|    clip_fraction        | 0.377       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.000286    |
|    loss                 | -0.045      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0434     |
|    value_loss           | 0.0599      |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=15.10 +/- 4.06
Episode length: 266.00 +/- 64.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 15.1     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 5.82     |
| time/              |          |
|    fps             | 807      |
|    iterations      | 7        |
|    time_elapsed    | 71       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=16.90 +/- 2.62
Episode length: 292.40 +/- 42.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 292         |
|    mean_reward          | 16.9        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.036573336 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.000286    |
|    loss                 | -0.0281     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0374     |
|    value_loss           | 0.0623      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=65000, episode_reward=16.50 +/- 2.38
Episode length: 286.30 +/- 44.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 286      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 7.68     |
| time/              |          |
|    fps             | 781      |
|    iterations      | 8        |
|    time_elapsed    | 83       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.8     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 1221     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=0.20 +/- 0.40
Episode length: 72.50 +/- 12.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.5        |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012330086 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.0984     |
|    learning_rate        | 0.000587    |
|    loss                 | -0.0222     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.0534      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.5     |
|    ep_rew_mean     | 0.343    |
| time/              |          |
|    fps             | 857      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=7.90 +/- 3.08
Episode length: 168.80 +/- 54.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 7.9         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.023907434 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.307       |
|    learning_rate        | 0.000587    |
|    loss                 | -0.0653     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.0514      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.8     |
|    ep_rew_mean     | 0.82     |
| time/              |          |
|    fps             | 753      |
|    iterations      | 3        |
|    time_elapsed    | 16       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=5.70 +/- 2.79
Episode length: 142.30 +/- 36.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 142        |
|    mean_reward          | 5.7        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.03203674 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.31      |
|    explained_variance   | 0.535      |
|    learning_rate        | 0.000587   |
|    loss                 | -0.0259    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.034     |
|    value_loss           | 0.0533     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.3     |
|    ep_rew_mean     | 1.54     |
| time/              |          |
|    fps             | 711      |
|    iterations      | 4        |
|    time_elapsed    | 23       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=5.10 +/- 2.30
Episode length: 131.20 +/- 45.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 131        |
|    mean_reward          | 5.1        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.03730788 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.26      |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.000587   |
|    loss                 | -0.0291    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0346    |
|    value_loss           | 0.0565     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 693      |
|    iterations      | 5        |
|    time_elapsed    | 29       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 119         |
|    ep_rew_mean          | 3.78        |
| time/                   |             |
|    fps                  | 699         |
|    iterations           | 6           |
|    time_elapsed         | 35          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.052053902 |
|    clip_fraction        | 0.355       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.000587    |
|    loss                 | -0.0103     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0424     |
|    value_loss           | 0.0569      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=14.70 +/- 4.05
Episode length: 266.20 +/- 59.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 266         |
|    mean_reward          | 14.7        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.060558476 |
|    clip_fraction        | 0.372       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.000587    |
|    loss                 | -0.0685     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0444     |
|    value_loss           | 0.0645      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 4.96     |
| time/              |          |
|    fps             | 674      |
|    iterations      | 7        |
|    time_elapsed    | 42       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=12.10 +/- 2.62
Episode length: 203.10 +/- 40.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | 12.1        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.061378174 |
|    clip_fraction        | 0.419       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.000587    |
|    loss                 | -0.0741     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0455     |
|    value_loss           | 0.0675      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 5.54     |
| time/              |          |
|    fps             | 662      |
|    iterations      | 8        |
|    time_elapsed    | 49       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=11.10 +/- 4.64
Episode length: 207.80 +/- 71.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 208        |
|    mean_reward          | 11.1       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.06568863 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.983     |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.000587   |
|    loss                 | -0.0751    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0421    |
|    value_loss           | 0.0706     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 6.2      |
| time/              |          |
|    fps             | 652      |
|    iterations      | 9        |
|    time_elapsed    | 56       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=16.10 +/- 2.88
Episode length: 278.70 +/- 58.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 279       |
|    mean_reward          | 16.1      |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0825435 |
|    clip_fraction        | 0.411     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.914    |
|    explained_variance   | 0.726     |
|    learning_rate        | 0.000587  |
|    loss                 | -0.0733   |
|    n_updates            | 90        |
|    policy_gradient_loss | -0.0463   |
|    value_loss           | 0.0706    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 6.78     |
| time/              |          |
|    fps             | 640      |
|    iterations      | 10       |
|    time_elapsed    | 63       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=17.20 +/- 3.37
Episode length: 299.10 +/- 47.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 299         |
|    mean_reward          | 17.2        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.083249584 |
|    clip_fraction        | 0.398       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.883      |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.000587    |
|    loss                 | -0.00992    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0412     |
|    value_loss           | 0.0715      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 7.39     |
| time/              |          |
|    fps             | 630      |
|    iterations      | 11       |
|    time_elapsed    | 71       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 172         |
|    ep_rew_mean          | 8.53        |
| time/                   |             |
|    fps                  | 638         |
|    iterations           | 12          |
|    time_elapsed         | 76          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.086558744 |
|    clip_fraction        | 0.39        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.795      |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.000587    |
|    loss                 | 0.000549    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0373     |
|    value_loss           | 0.0751      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=15.70 +/- 4.45
Episode length: 268.80 +/- 66.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 269        |
|    mean_reward          | 15.7       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.09365692 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.758     |
|    explained_variance   | 0.794      |
|    learning_rate        | 0.000587   |
|    loss                 | -0.0762    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0443    |
|    value_loss           | 0.0681     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 9.32     |
| time/              |          |
|    fps             | 632      |
|    iterations      | 13       |
|    time_elapsed    | 84       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=15.10 +/- 4.93
Episode length: 243.10 +/- 68.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 243        |
|    mean_reward          | 15.1       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.09734922 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.722     |
|    explained_variance   | 0.85       |
|    learning_rate        | 0.000587   |
|    loss                 | -0.075     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0359    |
|    value_loss           | 0.0666     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 10.1     |
| time/              |          |
|    fps             | 626      |
|    iterations      | 14       |
|    time_elapsed    | 91       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=12.50 +/- 4.86
Episode length: 213.60 +/- 65.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 214         |
|    mean_reward          | 12.5        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.098458886 |
|    clip_fraction        | 0.366       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.648      |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.000587    |
|    loss                 | -0.0518     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 0.0696      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 11       |
| time/              |          |
|    fps             | 622      |
|    iterations      | 15       |
|    time_elapsed    | 98       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=0.00 +/- 0.89
Episode length: 89.80 +/- 17.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.8     |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 0.172    |
| time/              |          |
|    fps             | 1116     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=4.40 +/- 1.85
Episode length: 126.50 +/- 16.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 126        |
|    mean_reward          | 4.4        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01042169 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | -0.0212    |
|    learning_rate        | 0.00033    |
|    loss                 | -0.00444   |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.011     |
|    value_loss           | 0.0764     |
----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=3.90 +/- 1.22
Episode length: 115.10 +/- 17.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 115      |
|    mean_reward     | 3.9      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.1     |
|    ep_rew_mean     | 0.67     |
| time/              |          |
|    fps             | 914      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=8.20 +/- 3.89
Episode length: 186.70 +/- 60.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 187         |
|    mean_reward          | 8.2         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.013710504 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.00033     |
|    loss                 | -0.00321    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.0739      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.8     |
|    ep_rew_mean     | 1.42     |
| time/              |          |
|    fps             | 871      |
|    iterations      | 3        |
|    time_elapsed    | 28       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=13.00 +/- 3.63
Episode length: 255.90 +/- 67.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 256         |
|    mean_reward          | 13          |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.019268602 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.531       |
|    learning_rate        | 0.00033     |
|    loss                 | 0.000956    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0323     |
|    value_loss           | 0.0801      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=14.00 +/- 2.68
Episode length: 270.40 +/- 51.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.97     |
| time/              |          |
|    fps             | 803      |
|    iterations      | 4        |
|    time_elapsed    | 40       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=16.50 +/- 2.42
Episode length: 307.00 +/- 39.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 307         |
|    mean_reward          | 16.5        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.025034957 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.498       |
|    learning_rate        | 0.00033     |
|    loss                 | -0.0272     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0394     |
|    value_loss           | 0.0971      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=13.60 +/- 5.02
Episode length: 250.80 +/- 71.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 251      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 4.05     |
| time/              |          |
|    fps             | 764      |
|    iterations      | 5        |
|    time_elapsed    | 53       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=15.30 +/- 4.31
Episode length: 284.90 +/- 73.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 285         |
|    mean_reward          | 15.3        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.030032016 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.564       |
|    learning_rate        | 0.00033     |
|    loss                 | -0.0243     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0418     |
|    value_loss           | 0.102       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 4.68     |
| time/              |          |
|    fps             | 760      |
|    iterations      | 6        |
|    time_elapsed    | 64       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=15.40 +/- 3.38
Episode length: 280.50 +/- 55.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 280         |
|    mean_reward          | 15.4        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.032784402 |
|    clip_fraction        | 0.32        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.00033     |
|    loss                 | -0.0142     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.039      |
|    value_loss           | 0.0959      |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=13.90 +/- 3.56
Episode length: 265.60 +/- 60.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 266      |
|    mean_reward     | 13.9     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 5.76     |
| time/              |          |
|    fps             | 743      |
|    iterations      | 7        |
|    time_elapsed    | 77       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=16.20 +/- 3.74
Episode length: 285.30 +/- 70.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 285         |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.038519442 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.00033     |
|    loss                 | -0.00616    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0397     |
|    value_loss           | 0.102       |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=15.30 +/- 1.55
Episode length: 256.10 +/- 39.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 256      |
|    mean_reward     | 15.3     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 6.83     |
| time/              |          |
|    fps             | 730      |
|    iterations      | 8        |
|    time_elapsed    | 89       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.9     |
|    ep_rew_mean     | -0.135   |
| time/              |          |
|    fps             | 1236     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=1.80 +/- 0.98
Episode length: 85.10 +/- 22.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 85.1        |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010967728 |
|    clip_fraction        | 0.054       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0145     |
|    learning_rate        | 0.000986    |
|    loss                 | -0.017      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00874    |
|    value_loss           | 0.0481      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.3     |
|    ep_rew_mean     | -0.02    |
| time/              |          |
|    fps             | 1028     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.80 +/- 0.98
Episode length: 79.90 +/- 14.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.9        |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.014279268 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.000986    |
|    loss                 | -0.0168     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.0419      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 981      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=2.70 +/- 1.49
Episode length: 107.30 +/- 23.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 107         |
|    mean_reward          | 2.7         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.021956244 |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.000986    |
|    loss                 | -0.00871    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0277     |
|    value_loss           | 0.0465      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.2     |
|    ep_rew_mean     | 0.74     |
| time/              |          |
|    fps             | 950      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=2.90 +/- 2.51
Episode length: 102.20 +/- 45.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | 2.9         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.025900818 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.529       |
|    learning_rate        | 0.000986    |
|    loss                 | -0.0184     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.03       |
|    value_loss           | 0.0431      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.2     |
|    ep_rew_mean     | 1.22     |
| time/              |          |
|    fps             | 934      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 98.6        |
|    ep_rew_mean          | 1.89        |
| time/                   |             |
|    fps                  | 948         |
|    iterations           | 6           |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.034144033 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.000986    |
|    loss                 | -0.0446     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.039      |
|    value_loss           | 0.0366      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=4.30 +/- 2.24
Episode length: 124.60 +/- 35.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | 4.3         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.037641596 |
|    clip_fraction        | 0.344       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.000986    |
|    loss                 | -0.048      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0426     |
|    value_loss           | 0.0396      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 933      |
|    iterations      | 7        |
|    time_elapsed    | 30       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=9.90 +/- 3.01
Episode length: 193.50 +/- 54.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 194         |
|    mean_reward          | 9.9         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.047600992 |
|    clip_fraction        | 0.353       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.000986    |
|    loss                 | -0.0382     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0454     |
|    value_loss           | 0.0497      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 4.27     |
| time/              |          |
|    fps             | 911      |
|    iterations      | 8        |
|    time_elapsed    | 35       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=14.00 +/- 1.55
Episode length: 267.70 +/- 24.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 268         |
|    mean_reward          | 14          |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.046673734 |
|    clip_fraction        | 0.386       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.000986    |
|    loss                 | -0.0457     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0426     |
|    value_loss           | 0.0574      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 5.46     |
| time/              |          |
|    fps             | 884      |
|    iterations      | 9        |
|    time_elapsed    | 41       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=13.40 +/- 3.35
Episode length: 247.90 +/- 47.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | 13.4        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.060195327 |
|    clip_fraction        | 0.37        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.989      |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.000986    |
|    loss                 | -0.0293     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0425     |
|    value_loss           | 0.0611      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 6.61     |
| time/              |          |
|    fps             | 867      |
|    iterations      | 10       |
|    time_elapsed    | 47       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=11.00 +/- 1.95
Episode length: 209.30 +/- 30.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 209        |
|    mean_reward          | 11         |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.06103303 |
|    clip_fraction        | 0.363      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.897     |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.000986   |
|    loss                 | -0.0355    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0382    |
|    value_loss           | 0.0605     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 7.79     |
| time/              |          |
|    fps             | 857      |
|    iterations      | 11       |
|    time_elapsed    | 52       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 170         |
|    ep_rew_mean          | 8.59        |
| time/                   |             |
|    fps                  | 870         |
|    iterations           | 12          |
|    time_elapsed         | 56          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.072237074 |
|    clip_fraction        | 0.364       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.814      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.000986    |
|    loss                 | -0.0518     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0421     |
|    value_loss           | 0.0553      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=15.10 +/- 4.09
Episode length: 258.30 +/- 65.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 258         |
|    mean_reward          | 15.1        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.086035125 |
|    clip_fraction        | 0.393       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.777      |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.000986    |
|    loss                 | -0.0606     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0438     |
|    value_loss           | 0.0576      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 9.42     |
| time/              |          |
|    fps             | 857      |
|    iterations      | 13       |
|    time_elapsed    | 62       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=12.10 +/- 3.59
Episode length: 211.70 +/- 53.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 212        |
|    mean_reward          | 12.1       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.08260477 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.789     |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.000986   |
|    loss                 | -0.0471    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0367    |
|    value_loss           | 0.0602     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 9.88     |
| time/              |          |
|    fps             | 850      |
|    iterations      | 14       |
|    time_elapsed    | 67       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=11.80 +/- 4.09
Episode length: 205.00 +/- 62.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 205        |
|    mean_reward          | 11.8       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.10189733 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.744     |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.000986   |
|    loss                 | -0.058     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0401    |
|    value_loss           | 0.0556     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 844      |
|    iterations      | 15       |
|    time_elapsed    | 72       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.70 +/- 0.46
Episode length: 72.60 +/- 11.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.6     |
|    mean_reward     | -0.7     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81       |
|    ep_rew_mean     | 0.19     |
| time/              |          |
|    fps             | 1155     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.60 +/- 0.80
Episode length: 84.90 +/- 15.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.9        |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010243179 |
|    clip_fraction        | 0.0841      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0241     |
|    learning_rate        | 0.000335    |
|    loss                 | 0.0255      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.0569      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=0.70 +/- 0.78
Episode length: 87.20 +/- 12.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.2     |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.9     |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 862      |
|    iterations      | 2        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=8.10 +/- 3.70
Episode length: 176.70 +/- 57.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 177        |
|    mean_reward          | 8.1        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.01878128 |
|    clip_fraction        | 0.191      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.427      |
|    learning_rate        | 0.000335   |
|    loss                 | 0.00959    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0265    |
|    value_loss           | 0.0569     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.8     |
|    ep_rew_mean     | 1.01     |
| time/              |          |
|    fps             | 780      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=4.00 +/- 1.55
Episode length: 114.00 +/- 14.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 4           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.026885483 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.000335    |
|    loss                 | 0.000574    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.0607      |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=6.30 +/- 2.61
Episode length: 138.40 +/- 40.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 6.3      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.2     |
|    ep_rew_mean     | 1.89     |
| time/              |          |
|    fps             | 737      |
|    iterations      | 4        |
|    time_elapsed    | 44       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=4.90 +/- 1.81
Episode length: 107.90 +/- 24.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 4.9         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.037073925 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.000335    |
|    loss                 | -0.03       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0396     |
|    value_loss           | 0.0653      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=5.10 +/- 3.30
Episode length: 117.10 +/- 45.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 5.1      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 3.56     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 5        |
|    time_elapsed    | 57       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=7.60 +/- 3.44
Episode length: 150.20 +/- 51.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 150         |
|    mean_reward          | 7.6         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.045237277 |
|    clip_fraction        | 0.369       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.000335    |
|    loss                 | -0.0377     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0423     |
|    value_loss           | 0.0784      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 5.2      |
| time/              |          |
|    fps             | 710      |
|    iterations      | 6        |
|    time_elapsed    | 69       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=12.50 +/- 2.69
Episode length: 223.60 +/- 45.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 224         |
|    mean_reward          | 12.5        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.051395405 |
|    clip_fraction        | 0.392       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.000335    |
|    loss                 | -0.0463     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0426     |
|    value_loss           | 0.0823      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=12.30 +/- 3.47
Episode length: 233.80 +/- 60.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 234      |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 6.19     |
| time/              |          |
|    fps             | 691      |
|    iterations      | 7        |
|    time_elapsed    | 82       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=12.20 +/- 4.42
Episode length: 215.30 +/- 64.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | 12.2       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.05718472 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.000335   |
|    loss                 | -0.0324    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 0.0917     |
----------------------------------------
Eval num_timesteps=65000, episode_reward=14.00 +/- 3.07
Episode length: 247.20 +/- 51.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 14       |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 6.85     |
| time/              |          |
|    fps             | 676      |
|    iterations      | 8        |
|    time_elapsed    | 96       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=2.90 +/- 1.14
Episode length: 117.80 +/- 12.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 2.9      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.8     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 1109     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.70 +/- 0.46
Episode length: 79.30 +/- 10.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.3        |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009541022 |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0104     |
|    learning_rate        | 1.22e-05    |
|    loss                 | 0.048       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00332    |
|    value_loss           | 0.0832      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=0.50 +/- 0.67
Episode length: 82.50 +/- 13.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.5     |
|    mean_reward     | 0.5      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.4     |
|    ep_rew_mean     | 0.28     |
| time/              |          |
|    fps             | 936      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=0.30 +/- 0.46
Episode length: 73.30 +/- 9.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73.3        |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.017144784 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.304       |
|    learning_rate        | 1.22e-05    |
|    loss                 | -0.0066     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.0803      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.5     |
|    ep_rew_mean     | 1.09     |
| time/              |          |
|    fps             | 914      |
|    iterations      | 3        |
|    time_elapsed    | 26       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=5.10 +/- 0.94
Episode length: 118.00 +/- 13.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 118          |
|    mean_reward          | 5.1          |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0132800415 |
|    clip_fraction        | 0.138        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.368        |
|    learning_rate        | 1.22e-05     |
|    loss                 | 0.0236       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0108      |
|    value_loss           | 0.11         |
------------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=5.40 +/- 1.11
Episode length: 125.10 +/- 20.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 125      |
|    mean_reward     | 5.4      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 1.94     |
| time/              |          |
|    fps             | 870      |
|    iterations      | 4        |
|    time_elapsed    | 37       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-1.00 +/- 0.00
Episode length: 70.70 +/- 11.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70.7         |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0121941995 |
|    clip_fraction        | 0.0945       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0.387        |
|    learning_rate        | 1.22e-05     |
|    loss                 | 0.0228       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00914     |
|    value_loss           | 0.123        |
------------------------------------------
Eval num_timesteps=40000, episode_reward=-1.00 +/- 0.00
Episode length: 68.70 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.7     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 1.89     |
| time/              |          |
|    fps             | 860      |
|    iterations      | 5        |
|    time_elapsed    | 47       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=0.30 +/- 0.64
Episode length: 92.70 +/- 15.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.7        |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.006271733 |
|    clip_fraction        | 0.0589      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.51        |
|    learning_rate        | 1.22e-05    |
|    loss                 | 0.0606      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00475    |
|    value_loss           | 0.113       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 2.01     |
| time/              |          |
|    fps             | 861      |
|    iterations      | 6        |
|    time_elapsed    | 57       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=1.10 +/- 1.22
Episode length: 82.20 +/- 23.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.2        |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.009158443 |
|    clip_fraction        | 0.0157      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.532       |
|    learning_rate        | 1.22e-05    |
|    loss                 | 0.0584      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00201    |
|    value_loss           | 0.119       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=1.60 +/- 0.80
Episode length: 85.70 +/- 17.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.7     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 854      |
|    iterations      | 7        |
|    time_elapsed    | 67       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=4.20 +/- 1.40
Episode length: 145.10 +/- 32.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 145         |
|    mean_reward          | 4.2         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.012299933 |
|    clip_fraction        | 0.0613      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.607       |
|    learning_rate        | 1.22e-05    |
|    loss                 | 0.0567      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00452    |
|    value_loss           | 0.116       |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=3.20 +/- 1.33
Episode length: 125.30 +/- 25.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 125      |
|    mean_reward     | 3.2      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 840      |
|    iterations      | 8        |
|    time_elapsed    | 77       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | 0.32     |
| time/              |          |
|    fps             | 1222     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 84.2        |
|    ep_rew_mean          | 0.375       |
| time/                   |             |
|    fps                  | 1033        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.007854817 |
|    clip_fraction        | 0.0516      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.01       |
|    learning_rate        | 0.000739    |
|    loss                 | -0.00569    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00934    |
|    value_loss           | 0.0845      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=5.70 +/- 2.19
Episode length: 137.40 +/- 38.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 137        |
|    mean_reward          | 5.7        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01617632 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.244      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0137    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0236    |
|    value_loss           | 0.0582     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.2     |
|    ep_rew_mean     | 0.681    |
| time/              |          |
|    fps             | 867      |
|    iterations      | 3        |
|    time_elapsed    | 7        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 91.8        |
|    ep_rew_mean          | 1.06        |
| time/                   |             |
|    fps                  | 873         |
|    iterations           | 4           |
|    time_elapsed         | 9           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.025909973 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.403       |
|    learning_rate        | 0.000739    |
|    loss                 | -0.0499     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0396     |
|    value_loss           | 0.0624      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=9.40 +/- 1.28
Episode length: 209.80 +/- 40.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 9.4        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.02896756 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.414      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0409    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0411    |
|    value_loss           | 0.0704     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.7     |
|    ep_rew_mean     | 1.39     |
| time/              |          |
|    fps             | 787      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 98         |
|    ep_rew_mean          | 1.7        |
| time/                   |            |
|    fps                  | 806        |
|    iterations           | 6          |
|    time_elapsed         | 15         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.03325416 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.405      |
|    learning_rate        | 0.000739   |
|    loss                 | 0.00352    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0386    |
|    value_loss           | 0.0812     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 2.23        |
| time/                   |             |
|    fps                  | 820         |
|    iterations           | 7           |
|    time_elapsed         | 17          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.041631684 |
|    clip_fraction        | 0.344       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.000739    |
|    loss                 | -0.0413     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0405     |
|    value_loss           | 0.0697      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=6.20 +/- 2.99
Episode length: 138.30 +/- 47.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 138         |
|    mean_reward          | 6.2         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.039349385 |
|    clip_fraction        | 0.35        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.000739    |
|    loss                 | -0.021      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0414     |
|    value_loss           | 0.0881      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.78     |
| time/              |          |
|    fps             | 794      |
|    iterations      | 8        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 114        |
|    ep_rew_mean          | 3.42       |
| time/                   |            |
|    fps                  | 805        |
|    iterations           | 9          |
|    time_elapsed         | 22         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.06385703 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.35       |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0549    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0487    |
|    value_loss           | 0.0917     |
----------------------------------------
Eval num_timesteps=20000, episode_reward=10.10 +/- 2.47
Episode length: 204.50 +/- 39.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 204        |
|    mean_reward          | 10.1       |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.06156319 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.01      |
|    explained_variance   | 0.461      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.00831   |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0421    |
|    value_loss           | 0.0906     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 3.99     |
| time/              |          |
|    fps             | 772      |
|    iterations      | 10       |
|    time_elapsed    | 26       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 4.63       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 11         |
|    time_elapsed         | 28         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.06457263 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.94      |
|    explained_variance   | 0.494      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0626    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.0707     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 137         |
|    ep_rew_mean          | 5.36        |
| time/                   |             |
|    fps                  | 791         |
|    iterations           | 12          |
|    time_elapsed         | 31          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.065911606 |
|    clip_fraction        | 0.355       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.904      |
|    explained_variance   | 0.508       |
|    learning_rate        | 0.000739    |
|    loss                 | -0.0531     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.0873      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=7.60 +/- 2.65
Episode length: 144.60 +/- 39.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 145        |
|    mean_reward          | 7.6        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.06816316 |
|    clip_fraction        | 0.355      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.84      |
|    explained_variance   | 0.533      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0454    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0328    |
|    value_loss           | 0.0832     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 6.2      |
| time/              |          |
|    fps             | 777      |
|    iterations      | 13       |
|    time_elapsed    | 34       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 156        |
|    ep_rew_mean          | 6.92       |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 14         |
|    time_elapsed         | 36         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.08081739 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.859     |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.051     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0395    |
|    value_loss           | 0.073      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=11.50 +/- 3.58
Episode length: 225.80 +/- 42.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 226         |
|    mean_reward          | 11.5        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.078714944 |
|    clip_fraction        | 0.387       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.848      |
|    explained_variance   | 0.515       |
|    learning_rate        | 0.000739    |
|    loss                 | -0.0519     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0361     |
|    value_loss           | 0.0882      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 165      |
|    ep_rew_mean     | 7.71     |
| time/              |          |
|    fps             | 762      |
|    iterations      | 15       |
|    time_elapsed    | 40       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 172        |
|    ep_rew_mean          | 8.28       |
| time/                   |            |
|    fps                  | 769        |
|    iterations           | 16         |
|    time_elapsed         | 42         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.07661264 |
|    clip_fraction        | 0.381      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.836     |
|    explained_variance   | 0.392      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0263    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0382    |
|    value_loss           | 0.101      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 179         |
|    ep_rew_mean          | 8.91        |
| time/                   |             |
|    fps                  | 777         |
|    iterations           | 17          |
|    time_elapsed         | 44          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.074848935 |
|    clip_fraction        | 0.376       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.822      |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.000739    |
|    loss                 | -0.0367     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0358     |
|    value_loss           | 0.08        |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=12.40 +/- 4.54
Episode length: 221.40 +/- 60.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 221        |
|    mean_reward          | 12.4       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.09389579 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.753     |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0515    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.039     |
|    value_loss           | 0.0718     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 9.26     |
| time/              |          |
|    fps             | 759      |
|    iterations      | 18       |
|    time_elapsed    | 48       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 9.81       |
| time/                   |            |
|    fps                  | 766        |
|    iterations           | 19         |
|    time_elapsed         | 50         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.10432743 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.759     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0867    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.043     |
|    value_loss           | 0.0785     |
----------------------------------------
Eval num_timesteps=40000, episode_reward=14.30 +/- 3.95
Episode length: 279.00 +/- 59.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 279         |
|    mean_reward          | 14.3        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.088858366 |
|    clip_fraction        | 0.355       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.706      |
|    explained_variance   | 0.52        |
|    learning_rate        | 0.000739    |
|    loss                 | -0.0508     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.0794      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 10.1     |
| time/              |          |
|    fps             | 746      |
|    iterations      | 20       |
|    time_elapsed    | 54       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 198        |
|    ep_rew_mean          | 10.4       |
| time/                   |            |
|    fps                  | 753        |
|    iterations           | 21         |
|    time_elapsed         | 57         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.09012939 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.651     |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0223    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0335    |
|    value_loss           | 0.0871     |
----------------------------------------
Eval num_timesteps=45000, episode_reward=16.80 +/- 3.19
Episode length: 288.10 +/- 46.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 288        |
|    mean_reward          | 16.8       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.10558217 |
|    clip_fraction        | 0.358      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.65      |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0412    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0352    |
|    value_loss           | 0.0753     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 203      |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 22       |
|    time_elapsed    | 61       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 209        |
|    ep_rew_mean          | 11.3       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 23         |
|    time_elapsed         | 63         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.10499253 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.594     |
|    explained_variance   | 0.647      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.021     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0295    |
|    value_loss           | 0.0724     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 214        |
|    ep_rew_mean          | 11.6       |
| time/                   |            |
|    fps                  | 748        |
|    iterations           | 24         |
|    time_elapsed         | 65         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.07702375 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.672     |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0257    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0363    |
|    value_loss           | 0.0675     |
----------------------------------------
Eval num_timesteps=50000, episode_reward=14.10 +/- 5.99
Episode length: 272.90 +/- 82.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 273        |
|    mean_reward          | 14.1       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.12304422 |
|    clip_fraction        | 0.374      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.694     |
|    explained_variance   | 0.648      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0168    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0419    |
|    value_loss           | 0.0672     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 11.9     |
| time/              |          |
|    fps             | 734      |
|    iterations      | 25       |
|    time_elapsed    | 69       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 222         |
|    ep_rew_mean          | 12.2        |
| time/                   |             |
|    fps                  | 740         |
|    iterations           | 26          |
|    time_elapsed         | 71          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.109829016 |
|    clip_fraction        | 0.363       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.622      |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.000739    |
|    loss                 | -0.0296     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0389     |
|    value_loss           | 0.0751      |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=16.80 +/- 3.57
Episode length: 255.10 +/- 48.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 255        |
|    mean_reward          | 16.8       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.10581715 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.614     |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0595    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0449    |
|    value_loss           | 0.0605     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 12.7     |
| time/              |          |
|    fps             | 729      |
|    iterations      | 27       |
|    time_elapsed    | 75       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 13         |
| time/                   |            |
|    fps                  | 734        |
|    iterations           | 28         |
|    time_elapsed         | 78         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.10126266 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.614     |
|    explained_variance   | 0.596      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0405    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0373    |
|    value_loss           | 0.068      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 13.1       |
| time/                   |            |
|    fps                  | 740        |
|    iterations           | 29         |
|    time_elapsed         | 80         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.12180502 |
|    clip_fraction        | 0.377      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.6       |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0427    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0361    |
|    value_loss           | 0.0656     |
----------------------------------------
Eval num_timesteps=60000, episode_reward=19.10 +/- 2.55
Episode length: 289.00 +/- 41.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 289        |
|    mean_reward          | 19.1       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.10312245 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.514     |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.000739   |
|    loss                 | -0.0373    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0359    |
|    value_loss           | 0.077      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 728      |
|    iterations      | 30       |
|    time_elapsed    | 84       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=-1.00 +/- 0.00
Episode length: 70.20 +/- 12.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.2     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.5     |
|    ep_rew_mean     | 0.214    |
| time/              |          |
|    fps             | 1144     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=4.10 +/- 1.30
Episode length: 118.50 +/- 21.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 118         |
|    mean_reward          | 4.1         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.008744653 |
|    clip_fraction        | 0.0752      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.00377    |
|    learning_rate        | 0.00137     |
|    loss                 | 0.00242     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00898    |
|    value_loss           | 0.0541      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=3.40 +/- 1.11
Episode length: 111.80 +/- 10.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 3.4      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.4     |
|    ep_rew_mean     | 0.63     |
| time/              |          |
|    fps             | 981      |
|    iterations      | 2        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=6.50 +/- 1.43
Episode length: 142.90 +/- 16.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | 6.5         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.017096195 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.338       |
|    learning_rate        | 0.00137     |
|    loss                 | -0.0205     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0217     |
|    value_loss           | 0.0517      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 1.57     |
| time/              |          |
|    fps             | 959      |
|    iterations      | 3        |
|    time_elapsed    | 25       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=2.30 +/- 2.00
Episode length: 98.60 +/- 26.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.6        |
|    mean_reward          | 2.3         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.027549792 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.00137     |
|    loss                 | -0.0127     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0318     |
|    value_loss           | 0.0591      |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=1.90 +/- 1.37
Episode length: 103.50 +/- 20.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3.04     |
| time/              |          |
|    fps             | 938      |
|    iterations      | 4        |
|    time_elapsed    | 34       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=7.70 +/- 2.72
Episode length: 177.60 +/- 46.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 178         |
|    mean_reward          | 7.7         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.038445517 |
|    clip_fraction        | 0.326       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.00137     |
|    loss                 | -0.0217     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.039      |
|    value_loss           | 0.0618      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=8.30 +/- 2.76
Episode length: 175.80 +/- 42.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 176      |
|    mean_reward     | 8.3      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 4.03     |
| time/              |          |
|    fps             | 905      |
|    iterations      | 5        |
|    time_elapsed    | 45       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=8.30 +/- 3.07
Episode length: 177.50 +/- 44.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 178        |
|    mean_reward          | 8.3        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.04881289 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.15      |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.00137    |
|    loss                 | -0.041     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.0625     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 5.18     |
| time/              |          |
|    fps             | 902      |
|    iterations      | 6        |
|    time_elapsed    | 54       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=9.80 +/- 3.28
Episode length: 192.30 +/- 51.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 192        |
|    mean_reward          | 9.8        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.06304281 |
|    clip_fraction        | 0.404      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.00137    |
|    loss                 | -0.0525    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.0652     |
----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=9.00 +/- 4.12
Episode length: 178.00 +/- 67.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 178      |
|    mean_reward     | 9        |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 6.14     |
| time/              |          |
|    fps             | 883      |
|    iterations      | 7        |
|    time_elapsed    | 64       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=9.50 +/- 3.77
Episode length: 172.80 +/- 59.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 173        |
|    mean_reward          | 9.5        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.06961641 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.997     |
|    explained_variance   | 0.679      |
|    learning_rate        | 0.00137    |
|    loss                 | -0.0258    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0437    |
|    value_loss           | 0.069      |
----------------------------------------
Eval num_timesteps=65000, episode_reward=13.60 +/- 2.76
Episode length: 247.00 +/- 54.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 247      |
|    mean_reward     | 13.6     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 7.44     |
| time/              |          |
|    fps             | 865      |
|    iterations      | 8        |
|    time_elapsed    | 75       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.5     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 1236     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 78.3        |
|    ep_rew_mean          | 0.0577      |
| time/                   |             |
|    fps                  | 1045        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.012264264 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.0482     |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0117      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00842    |
|    value_loss           | 0.334       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=0.70 +/- 0.78
Episode length: 75.20 +/- 13.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.2        |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.006573632 |
|    clip_fraction        | 0.0543      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.161       |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0392      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00405    |
|    value_loss           | 0.0799      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.6     |
|    ep_rew_mean     | 0.143    |
| time/              |          |
|    fps             | 919      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 80.7        |
|    ep_rew_mean          | 0.19        |
| time/                   |             |
|    fps                  | 917         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.010308396 |
|    clip_fraction        | 0.0983      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.0452      |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0149      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 0.103       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=0.50 +/- 0.50
Episode length: 77.80 +/- 13.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 77.8       |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01391606 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.139      |
|    learning_rate        | 0.00555    |
|    loss                 | 0.022      |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00485   |
|    value_loss           | 0.0971     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.6     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 874      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 83.6         |
|    ep_rew_mean          | 0.3          |
| time/                   |              |
|    fps                  | 878          |
|    iterations           | 6            |
|    time_elapsed         | 13           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0093633095 |
|    clip_fraction        | 0.1          |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.301        |
|    learning_rate        | 0.00555      |
|    loss                 | 0.0115       |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00453     |
|    value_loss           | 0.0789       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 84.5        |
|    ep_rew_mean          | 0.41        |
| time/                   |             |
|    fps                  | 882         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.011853386 |
|    clip_fraction        | 0.0745      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.241       |
|    learning_rate        | 0.00555     |
|    loss                 | 0.00528     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00279    |
|    value_loss           | 0.0923      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=0.40 +/- 0.66
Episode length: 71.40 +/- 8.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 71.4       |
|    mean_reward          | 0.4        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01585636 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.114      |
|    learning_rate        | 0.00555    |
|    loss                 | 0.0676     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.00586   |
|    value_loss           | 0.103      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.8     |
|    ep_rew_mean     | 0.48     |
| time/              |          |
|    fps             | 863      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 88        |
|    ep_rew_mean          | 0.65      |
| time/                   |           |
|    fps                  | 869       |
|    iterations           | 9         |
|    time_elapsed         | 21        |
|    total_timesteps      | 18432     |
| train/                  |           |
|    approx_kl            | 0.0151688 |
|    clip_fraction        | 0.168     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.28     |
|    explained_variance   | 0.0798    |
|    learning_rate        | 0.00555   |
|    loss                 | 0.0287    |
|    n_updates            | 80        |
|    policy_gradient_loss | -0.00761  |
|    value_loss           | 0.0965    |
---------------------------------------
Eval num_timesteps=20000, episode_reward=0.40 +/- 0.66
Episode length: 74.40 +/- 6.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.4        |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.013511241 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.124       |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0441      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00915    |
|    value_loss           | 0.123       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.2     |
|    ep_rew_mean     | 0.94     |
| time/              |          |
|    fps             | 854      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 92.5        |
|    ep_rew_mean          | 1.04        |
| time/                   |             |
|    fps                  | 857         |
|    iterations           | 11          |
|    time_elapsed         | 26          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.020550046 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.148       |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0526      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.13        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 95.6       |
|    ep_rew_mean          | 1.26       |
| time/                   |            |
|    fps                  | 861        |
|    iterations           | 12         |
|    time_elapsed         | 28         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.01832857 |
|    clip_fraction        | 0.172      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.0759     |
|    learning_rate        | 0.00555    |
|    loss                 | 0.0238     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0115    |
|    value_loss           | 0.107      |
----------------------------------------
Eval num_timesteps=25000, episode_reward=0.30 +/- 0.46
Episode length: 72.40 +/- 11.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.4        |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.022292588 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.143       |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0446      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.116       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.4     |
|    ep_rew_mean     | 1.36     |
| time/              |          |
|    fps             | 849      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 90.4        |
|    ep_rew_mean          | 1.27        |
| time/                   |             |
|    fps                  | 852         |
|    iterations           | 14          |
|    time_elapsed         | 33          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.022908662 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.964      |
|    explained_variance   | 0.213       |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0249      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00977    |
|    value_loss           | 0.109       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=0.40 +/- 0.49
Episode length: 76.80 +/- 14.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.8        |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.034101054 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.792      |
|    explained_variance   | 0.158       |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0339      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.102       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.2     |
|    ep_rew_mean     | 1.15     |
| time/              |          |
|    fps             | 843      |
|    iterations      | 15       |
|    time_elapsed    | 36       |
|    total_timesteps | 30720    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 83.4      |
|    ep_rew_mean          | 0.99      |
| time/                   |           |
|    fps                  | 847       |
|    iterations           | 16        |
|    time_elapsed         | 38        |
|    total_timesteps      | 32768     |
| train/                  |           |
|    approx_kl            | 0.0224882 |
|    clip_fraction        | 0.188     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.759    |
|    explained_variance   | 0.0885    |
|    learning_rate        | 0.00555   |
|    loss                 | 0.0209    |
|    n_updates            | 150       |
|    policy_gradient_loss | -0.0118   |
|    value_loss           | 0.111     |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 83.9        |
|    ep_rew_mean          | 1.02        |
| time/                   |             |
|    fps                  | 849         |
|    iterations           | 17          |
|    time_elapsed         | 40          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.023169776 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.637      |
|    explained_variance   | 0.105       |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0337      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.0988      |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=0.80 +/- 0.87
Episode length: 75.10 +/- 15.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.1        |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.048739973 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.132       |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0438      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00809    |
|    value_loss           | 0.119       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.1     |
|    ep_rew_mean     | 1.18     |
| time/              |          |
|    fps             | 841      |
|    iterations      | 18       |
|    time_elapsed    | 43       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 90.2       |
|    ep_rew_mean          | 1.36       |
| time/                   |            |
|    fps                  | 844        |
|    iterations           | 19         |
|    time_elapsed         | 46         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.05944385 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.806     |
|    explained_variance   | -0.0906    |
|    learning_rate        | 0.00555    |
|    loss                 | 0.0412     |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.00984   |
|    value_loss           | 0.12       |
----------------------------------------
Eval num_timesteps=40000, episode_reward=1.30 +/- 0.90
Episode length: 96.40 +/- 15.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.4        |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.019495288 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.93       |
|    explained_variance   | 0.125       |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0137      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00978    |
|    value_loss           | 0.135       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.8     |
|    ep_rew_mean     | 1.65     |
| time/              |          |
|    fps             | 835      |
|    iterations      | 20       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 99          |
|    ep_rew_mean          | 2.08        |
| time/                   |             |
|    fps                  | 839         |
|    iterations           | 21          |
|    time_elapsed         | 51          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.029339952 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.894      |
|    explained_variance   | 0.142       |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0386      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.138       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=0.50 +/- 0.50
Episode length: 67.00 +/- 13.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 67         |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.03422026 |
|    clip_fraction        | 0.23       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.888     |
|    explained_variance   | -0.0231    |
|    learning_rate        | 0.00555    |
|    loss                 | 0.0401     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.00973   |
|    value_loss           | 0.154      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 2.28     |
| time/              |          |
|    fps             | 835      |
|    iterations      | 22       |
|    time_elapsed    | 53       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 2.53        |
| time/                   |             |
|    fps                  | 838         |
|    iterations           | 23          |
|    time_elapsed         | 56          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.037937127 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.833      |
|    explained_variance   | 0.0951      |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0327      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.145       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | 2.62        |
| time/                   |             |
|    fps                  | 840         |
|    iterations           | 24          |
|    time_elapsed         | 58          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.028537443 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.83       |
|    explained_variance   | 0.106       |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0255      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.157       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=1.00 +/- 0.89
Episode length: 80.60 +/- 16.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 80.6       |
|    mean_reward          | 1          |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.02802961 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.743     |
|    explained_variance   | 0.00853    |
|    learning_rate        | 0.00555    |
|    loss                 | 0.0169     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 0.143      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 836      |
|    iterations      | 25       |
|    time_elapsed    | 61       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 107         |
|    ep_rew_mean          | 2.85        |
| time/                   |             |
|    fps                  | 838         |
|    iterations           | 26          |
|    time_elapsed         | 63          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.026223877 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.793      |
|    explained_variance   | 0.0848      |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0318      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.156       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=1.40 +/- 0.92
Episode length: 94.60 +/- 20.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 94.6       |
|    mean_reward          | 1.4        |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.02918276 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.815     |
|    explained_variance   | 0.015      |
|    learning_rate        | 0.00555    |
|    loss                 | 0.0125     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 0.174      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    fps             | 833      |
|    iterations      | 27       |
|    time_elapsed    | 66       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 2.81        |
| time/                   |             |
|    fps                  | 835         |
|    iterations           | 28          |
|    time_elapsed         | 68          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.028067082 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.697      |
|    explained_variance   | 0.0531      |
|    learning_rate        | 0.00555     |
|    loss                 | -0.00293    |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.14        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | 2.86        |
| time/                   |             |
|    fps                  | 837         |
|    iterations           | 29          |
|    time_elapsed         | 70          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.035456434 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.839      |
|    explained_variance   | 0.00956     |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0329      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.173       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=0.50 +/- 0.92
Episode length: 81.50 +/- 24.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.5        |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.029310737 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.0622      |
|    learning_rate        | 0.00555     |
|    loss                 | 0.0556      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00953    |
|    value_loss           | 0.138       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 833      |
|    iterations      | 30       |
|    time_elapsed    | 73       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.9     |
|    ep_rew_mean     | 0.08     |
| time/              |          |
|    fps             | 1237     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 79.9        |
|    ep_rew_mean          | 0.137       |
| time/                   |             |
|    fps                  | 1052        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.007363652 |
|    clip_fraction        | 0.0972      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.000262   |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.021       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0085     |
|    value_loss           | 0.0482      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=0.70 +/- 0.78
Episode length: 77.10 +/- 13.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 77.1         |
|    mean_reward          | 0.7          |
| time/                   |              |
|    total_timesteps      | 5000         |
| train/                  |              |
|    approx_kl            | 0.0077380748 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.37        |
|    explained_variance   | 0.23         |
|    learning_rate        | 8.44e-05     |
|    loss                 | -0.00325     |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00573     |
|    value_loss           | 0.0456       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.2     |
|    ep_rew_mean     | 0.173    |
| time/              |          |
|    fps             | 929      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 83.8        |
|    ep_rew_mean          | 0.392       |
| time/                   |             |
|    fps                  | 929         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.011233551 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.371       |
|    learning_rate        | 8.44e-05    |
|    loss                 | -0.0251     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.0391      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=3.10 +/- 1.76
Episode length: 111.50 +/- 21.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 3.1         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.008697785 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.447       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.0143      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.0454      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.5     |
|    ep_rew_mean     | 0.64     |
| time/              |          |
|    fps             | 870      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 92.4        |
|    ep_rew_mean          | 1.03        |
| time/                   |             |
|    fps                  | 879         |
|    iterations           | 6           |
|    time_elapsed         | 13          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.012772799 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.542       |
|    learning_rate        | 8.44e-05    |
|    loss                 | -0.00984    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.0485      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 98          |
|    ep_rew_mean          | 1.56        |
| time/                   |             |
|    fps                  | 886         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.009662941 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.502       |
|    learning_rate        | 8.44e-05    |
|    loss                 | -0.0119     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.055       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=3.20 +/- 1.17
Episode length: 111.90 +/- 9.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 112        |
|    mean_reward          | 3.2        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01566701 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.548      |
|    learning_rate        | 8.44e-05   |
|    loss                 | -0.0124    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0228    |
|    value_loss           | 0.0567     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 2.13     |
| time/              |          |
|    fps             | 855      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 107         |
|    ep_rew_mean          | 2.56        |
| time/                   |             |
|    fps                  | 861         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.014489686 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.617       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.01        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.056       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=10.30 +/- 1.73
Episode length: 207.10 +/- 30.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 207         |
|    mean_reward          | 10.3        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.009942434 |
|    clip_fraction        | 0.0929      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.606       |
|    learning_rate        | 8.44e-05    |
|    loss                 | -0.0163     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.0582      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 3.07     |
| time/              |          |
|    fps             | 820      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 115         |
|    ep_rew_mean          | 3.61        |
| time/                   |             |
|    fps                  | 828         |
|    iterations           | 11          |
|    time_elapsed         | 27          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.009217965 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.697       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.0138      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.0511      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | 4.04        |
| time/                   |             |
|    fps                  | 834         |
|    iterations           | 12          |
|    time_elapsed         | 29          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.014856574 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.937      |
|    explained_variance   | 0.59        |
|    learning_rate        | 8.44e-05    |
|    loss                 | -0.03       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0282     |
|    value_loss           | 0.0697      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=17.10 +/- 3.65
Episode length: 307.50 +/- 60.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 308        |
|    mean_reward          | 17.1       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.01384095 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.898     |
|    explained_variance   | 0.61       |
|    learning_rate        | 8.44e-05   |
|    loss                 | -0.00132   |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0237    |
|    value_loss           | 0.0652     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 4.52     |
| time/              |          |
|    fps             | 790      |
|    iterations      | 13       |
|    time_elapsed    | 33       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 132         |
|    ep_rew_mean          | 5.03        |
| time/                   |             |
|    fps                  | 797         |
|    iterations           | 14          |
|    time_elapsed         | 35          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.014072612 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.826      |
|    explained_variance   | 0.572       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.0135      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.0628      |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=16.10 +/- 3.70
Episode length: 293.30 +/- 55.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 293         |
|    mean_reward          | 16.1        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.016427478 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.786      |
|    explained_variance   | 0.604       |
|    learning_rate        | 8.44e-05    |
|    loss                 | -0.0158     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.0605      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 5.47     |
| time/              |          |
|    fps             | 765      |
|    iterations      | 15       |
|    time_elapsed    | 40       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 149         |
|    ep_rew_mean          | 6.22        |
| time/                   |             |
|    fps                  | 773         |
|    iterations           | 16          |
|    time_elapsed         | 42          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.014933869 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.744      |
|    explained_variance   | 0.577       |
|    learning_rate        | 8.44e-05    |
|    loss                 | -0.03       |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.0682      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 157         |
|    ep_rew_mean          | 6.82        |
| time/                   |             |
|    fps                  | 780         |
|    iterations           | 17          |
|    time_elapsed         | 44          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.013183737 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.554       |
|    learning_rate        | 8.44e-05    |
|    loss                 | -0.0129     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.0676      |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=12.60 +/- 3.14
Episode length: 237.50 +/- 57.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 238         |
|    mean_reward          | 12.6        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012778219 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.571       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.00726     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.0735      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    fps             | 761      |
|    iterations      | 18       |
|    time_elapsed    | 48       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 172         |
|    ep_rew_mean          | 7.88        |
| time/                   |             |
|    fps                  | 768         |
|    iterations           | 19          |
|    time_elapsed         | 50          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.011713941 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.576       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.0232      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.0755      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=11.80 +/- 3.31
Episode length: 233.30 +/- 52.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 233         |
|    mean_reward          | 11.8        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.013171779 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.597      |
|    explained_variance   | 0.552       |
|    learning_rate        | 8.44e-05    |
|    loss                 | -0.0112     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.0746      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 8.26     |
| time/              |          |
|    fps             | 753      |
|    iterations      | 20       |
|    time_elapsed    | 54       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 188         |
|    ep_rew_mean          | 8.83        |
| time/                   |             |
|    fps                  | 759         |
|    iterations           | 21          |
|    time_elapsed         | 56          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.011874187 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.604      |
|    explained_variance   | 0.591       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.00994     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.0698      |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=12.60 +/- 1.69
Episode length: 239.10 +/- 22.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 239         |
|    mean_reward          | 12.6        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.009042131 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.563      |
|    explained_variance   | 0.649       |
|    learning_rate        | 8.44e-05    |
|    loss                 | -0.00969    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.0665      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 9.22     |
| time/              |          |
|    fps             | 744      |
|    iterations      | 22       |
|    time_elapsed    | 60       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | 9.71        |
| time/                   |             |
|    fps                  | 750         |
|    iterations           | 23          |
|    time_elapsed         | 62          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.014392313 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.471      |
|    explained_variance   | 0.607       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.00341     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.0636      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 206         |
|    ep_rew_mean          | 10.2        |
| time/                   |             |
|    fps                  | 755         |
|    iterations           | 24          |
|    time_elapsed         | 65          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.012663654 |
|    clip_fraction        | 0.0939      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.39       |
|    explained_variance   | 0.571       |
|    learning_rate        | 8.44e-05    |
|    loss                 | -0.0087     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.0705      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=14.60 +/- 3.64
Episode length: 255.20 +/- 58.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | 14.6        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.009415749 |
|    clip_fraction        | 0.0868      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.417      |
|    explained_variance   | 0.638       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.02        |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.0661      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 213      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 743      |
|    iterations      | 25       |
|    time_elapsed    | 68       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 218         |
|    ep_rew_mean          | 11.2        |
| time/                   |             |
|    fps                  | 748         |
|    iterations           | 26          |
|    time_elapsed         | 71          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.013748978 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.411      |
|    explained_variance   | 0.641       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.00677     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.0645      |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=15.20 +/- 2.40
Episode length: 263.40 +/- 32.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 263         |
|    mean_reward          | 15.2        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.010020107 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.349      |
|    explained_variance   | 0.574       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.00185     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.0754      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 11.4     |
| time/              |          |
|    fps             | 737      |
|    iterations      | 27       |
|    time_elapsed    | 75       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 224         |
|    ep_rew_mean          | 11.7        |
| time/                   |             |
|    fps                  | 742         |
|    iterations           | 28          |
|    time_elapsed         | 77          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.011067353 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.385      |
|    explained_variance   | 0.614       |
|    learning_rate        | 8.44e-05    |
|    loss                 | -0.00327    |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.0687      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 226         |
|    ep_rew_mean          | 11.9        |
| time/                   |             |
|    fps                  | 747         |
|    iterations           | 29          |
|    time_elapsed         | 79          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.014580148 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.384      |
|    explained_variance   | 0.6         |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.0114      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.0732      |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=15.90 +/- 2.88
Episode length: 276.30 +/- 50.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 276         |
|    mean_reward          | 15.9        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.013297053 |
|    clip_fraction        | 0.0932      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.382      |
|    explained_variance   | 0.591       |
|    learning_rate        | 8.44e-05    |
|    loss                 | 0.0175      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.0721      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 230      |
|    ep_rew_mean     | 12.2     |
| time/              |          |
|    fps             | 736      |
|    iterations      | 30       |
|    time_elapsed    | 83       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.4     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 1234     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 81.4       |
|    ep_rew_mean          | -0.102     |
| time/                   |            |
|    fps                  | 1055       |
|    iterations           | 2          |
|    time_elapsed         | 3          |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.01615941 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | -0.0726    |
|    learning_rate        | 6.59e-05   |
|    loss                 | -0.0165    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0088    |
|    value_loss           | 0.0562     |
----------------------------------------
Eval num_timesteps=5000, episode_reward=0.30 +/- 0.46
Episode length: 72.10 +/- 12.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 72.1       |
|    mean_reward          | 0.3        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01122554 |
|    clip_fraction        | 0.049      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.407      |
|    learning_rate        | 6.59e-05   |
|    loss                 | 0.00876    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00432   |
|    value_loss           | 0.047      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.7     |
|    ep_rew_mean     | 0.205    |
| time/              |          |
|    fps             | 925      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 85.2        |
|    ep_rew_mean          | 0.406       |
| time/                   |             |
|    fps                  | 921         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.006400893 |
|    clip_fraction        | 0.0614      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.433       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0103      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00552    |
|    value_loss           | 0.0608      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=0.40 +/- 0.49
Episode length: 76.70 +/- 12.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.7        |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009292125 |
|    clip_fraction        | 0.0749      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.456       |
|    learning_rate        | 6.59e-05    |
|    loss                 | -0.00214    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00637    |
|    value_loss           | 0.0685      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.5     |
|    ep_rew_mean     | 0.8      |
| time/              |          |
|    fps             | 875      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 94.5        |
|    ep_rew_mean          | 1.4         |
| time/                   |             |
|    fps                  | 880         |
|    iterations           | 6           |
|    time_elapsed         | 13          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.010197108 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.456       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0217      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.073       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 98.7        |
|    ep_rew_mean          | 1.77        |
| time/                   |             |
|    fps                  | 883         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.010517988 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.394       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0325      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.0792      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=4.80 +/- 1.60
Episode length: 129.30 +/- 27.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 129         |
|    mean_reward          | 4.8         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.009833955 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.24        |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0169      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.0898      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.16     |
| time/              |          |
|    fps             | 845      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 108          |
|    ep_rew_mean          | 2.58         |
| time/                   |              |
|    fps                  | 850          |
|    iterations           | 9            |
|    time_elapsed         | 21           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0031957799 |
|    clip_fraction        | 0.046        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.13        |
|    explained_variance   | 0.446        |
|    learning_rate        | 6.59e-05     |
|    loss                 | 0.0229       |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00812     |
|    value_loss           | 0.08         |
------------------------------------------
Eval num_timesteps=20000, episode_reward=6.50 +/- 1.80
Episode length: 157.00 +/- 39.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 157        |
|    mean_reward          | 6.5        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.00949925 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.07      |
|    explained_variance   | 0.52       |
|    learning_rate        | 6.59e-05   |
|    loss                 | 0.0366     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0139    |
|    value_loss           | 0.0862     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 818      |
|    iterations      | 10       |
|    time_elapsed    | 25       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 114         |
|    ep_rew_mean          | 3.15        |
| time/                   |             |
|    fps                  | 825         |
|    iterations           | 11          |
|    time_elapsed         | 27          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.007416426 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.99       |
|    explained_variance   | 0.436       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0406      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.0926      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 114         |
|    ep_rew_mean          | 3.33        |
| time/                   |             |
|    fps                  | 830         |
|    iterations           | 12          |
|    time_elapsed         | 29          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.007238283 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.905      |
|    explained_variance   | 0.486       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0261      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.0929      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=13.70 +/- 1.42
Episode length: 259.80 +/- 19.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 260         |
|    mean_reward          | 13.7        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.008328442 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.845      |
|    explained_variance   | 0.594       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0144      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.0847      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 3.52     |
| time/              |          |
|    fps             | 792      |
|    iterations      | 13       |
|    time_elapsed    | 33       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 119         |
|    ep_rew_mean          | 3.91        |
| time/                   |             |
|    fps                  | 799         |
|    iterations           | 14          |
|    time_elapsed         | 35          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.007965425 |
|    clip_fraction        | 0.0988      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.773      |
|    explained_variance   | 0.445       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0349      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.102       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=18.20 +/- 2.23
Episode length: 332.30 +/- 22.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 332          |
|    mean_reward          | 18.2         |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0072566275 |
|    clip_fraction        | 0.097        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.742       |
|    explained_variance   | 0.444        |
|    learning_rate        | 6.59e-05     |
|    loss                 | 0.0251       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0104      |
|    value_loss           | 0.0933       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 4.27     |
| time/              |          |
|    fps             | 763      |
|    iterations      | 15       |
|    time_elapsed    | 40       |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 129          |
|    ep_rew_mean          | 4.8          |
| time/                   |              |
|    fps                  | 771          |
|    iterations           | 16           |
|    time_elapsed         | 42           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0059247236 |
|    clip_fraction        | 0.0804       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.704       |
|    explained_variance   | 0.432        |
|    learning_rate        | 6.59e-05     |
|    loss                 | 0.039        |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.0132      |
|    value_loss           | 0.109        |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 134          |
|    ep_rew_mean          | 5.23         |
| time/                   |              |
|    fps                  | 778          |
|    iterations           | 17           |
|    time_elapsed         | 44           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0070333113 |
|    clip_fraction        | 0.0702       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.653       |
|    explained_variance   | 0.419        |
|    learning_rate        | 6.59e-05     |
|    loss                 | 0.0331       |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.0105      |
|    value_loss           | 0.115        |
------------------------------------------
Eval num_timesteps=35000, episode_reward=18.50 +/- 3.77
Episode length: 322.10 +/- 64.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 322         |
|    mean_reward          | 18.5        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.009939165 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.655      |
|    explained_variance   | 0.495       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0559      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.101       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 5.63     |
| time/              |          |
|    fps             | 750      |
|    iterations      | 18       |
|    time_elapsed    | 49       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 149         |
|    ep_rew_mean          | 6.22        |
| time/                   |             |
|    fps                  | 757         |
|    iterations           | 19          |
|    time_elapsed         | 51          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.007440773 |
|    clip_fraction        | 0.0815      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.56       |
|    explained_variance   | 0.484       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0294      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.103       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=19.70 +/- 1.10
Episode length: 345.10 +/- 30.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 345          |
|    mean_reward          | 19.7         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0076114233 |
|    clip_fraction        | 0.0755       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.577       |
|    explained_variance   | 0.539        |
|    learning_rate        | 6.59e-05     |
|    loss                 | 0.032        |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.0148      |
|    value_loss           | 0.103        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 6.76     |
| time/              |          |
|    fps             | 732      |
|    iterations      | 20       |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 162         |
|    ep_rew_mean          | 7.29        |
| time/                   |             |
|    fps                  | 739         |
|    iterations           | 21          |
|    time_elapsed         | 58          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.009116649 |
|    clip_fraction        | 0.0723      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.479      |
|    explained_variance   | 0.549       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0079      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.0967      |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=17.60 +/- 2.33
Episode length: 312.10 +/- 40.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 312         |
|    mean_reward          | 17.6        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.010004196 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.497      |
|    explained_variance   | 0.615       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.02        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.0896      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 7.68     |
| time/              |          |
|    fps             | 721      |
|    iterations      | 22       |
|    time_elapsed    | 62       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 172        |
|    ep_rew_mean          | 7.93       |
| time/                   |            |
|    fps                  | 728        |
|    iterations           | 23         |
|    time_elapsed         | 64         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.00770783 |
|    clip_fraction        | 0.0778     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.446     |
|    explained_variance   | 0.621      |
|    learning_rate        | 6.59e-05   |
|    loss                 | 0.0367     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.00878   |
|    value_loss           | 0.0959     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 177         |
|    ep_rew_mean          | 8.27        |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 24          |
|    time_elapsed         | 66          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.006409471 |
|    clip_fraction        | 0.0884      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.461      |
|    explained_variance   | 0.553       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0371      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.0978      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=12.40 +/- 4.34
Episode length: 231.00 +/- 61.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 231        |
|    mean_reward          | 12.4       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.00888478 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.459     |
|    explained_variance   | 0.563      |
|    learning_rate        | 6.59e-05   |
|    loss                 | 0.0363     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0139    |
|    value_loss           | 0.0967     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 8.58     |
| time/              |          |
|    fps             | 724      |
|    iterations      | 25       |
|    time_elapsed    | 70       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 8.92        |
| time/                   |             |
|    fps                  | 730         |
|    iterations           | 26          |
|    time_elapsed         | 72          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.009297922 |
|    clip_fraction        | 0.0991      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.442      |
|    explained_variance   | 0.608       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0193      |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0095     |
|    value_loss           | 0.0966      |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=15.60 +/- 2.46
Episode length: 257.30 +/- 36.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 257         |
|    mean_reward          | 15.6        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.009513196 |
|    clip_fraction        | 0.0855      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.395      |
|    explained_variance   | 0.62        |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0192      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.104       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 9.31     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 27       |
|    time_elapsed    | 76       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 9.55        |
| time/                   |             |
|    fps                  | 724         |
|    iterations           | 28          |
|    time_elapsed         | 79          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.009748796 |
|    clip_fraction        | 0.0843      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.395      |
|    explained_variance   | 0.542       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0505      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.105       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 197         |
|    ep_rew_mean          | 9.86        |
| time/                   |             |
|    fps                  | 730         |
|    iterations           | 29          |
|    time_elapsed         | 81          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.008355106 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.354      |
|    explained_variance   | 0.666       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0203      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00903    |
|    value_loss           | 0.095       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=16.50 +/- 2.29
Episode length: 287.00 +/- 34.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 287         |
|    mean_reward          | 16.5        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.009813967 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.345      |
|    explained_variance   | 0.633       |
|    learning_rate        | 6.59e-05    |
|    loss                 | 0.0112      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00853    |
|    value_loss           | 0.107       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 30       |
|    time_elapsed    | 85       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.7     |
|    ep_rew_mean     | 0.208    |
| time/              |          |
|    fps             | 1243     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 88.4        |
|    ep_rew_mean          | 0.696       |
| time/                   |             |
|    fps                  | 1058        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.018855605 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.0183     |
|    learning_rate        | 0.00349     |
|    loss                 | 0.159       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.505       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=0.30 +/- 0.78
Episode length: 83.60 +/- 15.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83.6        |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.016748063 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.255       |
|    learning_rate        | 0.00349     |
|    loss                 | 0.207       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00962    |
|    value_loss           | 0.464       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.9     |
|    ep_rew_mean     | 0.909    |
| time/              |          |
|    fps             | 916      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 93.7        |
|    ep_rew_mean          | 1.01        |
| time/                   |             |
|    fps                  | 915         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.028054632 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.00349     |
|    loss                 | 0.0435      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.419       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=2.80 +/- 1.25
Episode length: 110.50 +/- 8.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 110       |
|    mean_reward          | 2.8       |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 0.0394105 |
|    clip_fraction        | 0.279     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.3      |
|    explained_variance   | 0.315     |
|    learning_rate        | 0.00349   |
|    loss                 | 0.086     |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0205   |
|    value_loss           | 0.39      |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.6     |
|    ep_rew_mean     | 1.27     |
| time/              |          |
|    fps             | 857      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 102        |
|    ep_rew_mean          | 1.65       |
| time/                   |            |
|    fps                  | 866        |
|    iterations           | 6          |
|    time_elapsed         | 14         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.05520954 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.431      |
|    learning_rate        | 0.00349    |
|    loss                 | 0.114      |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0277    |
|    value_loss           | 0.493      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 104         |
|    ep_rew_mean          | 1.85        |
| time/                   |             |
|    fps                  | 871         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.056374565 |
|    clip_fraction        | 0.342       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.00349     |
|    loss                 | 0.0647      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0309     |
|    value_loss           | 0.429       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=4.60 +/- 2.11
Episode length: 134.90 +/- 28.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 135        |
|    mean_reward          | 4.6        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.08381712 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.23      |
|    explained_variance   | 0.369      |
|    learning_rate        | 0.00349    |
|    loss                 | 0.0552     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0347    |
|    value_loss           | 0.556      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.17     |
| time/              |          |
|    fps             | 837      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 111         |
|    ep_rew_mean          | 2.49        |
| time/                   |             |
|    fps                  | 845         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.092457175 |
|    clip_fraction        | 0.457       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.45        |
|    learning_rate        | 0.00349     |
|    loss                 | 0.0318      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0447     |
|    value_loss           | 0.37        |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=4.40 +/- 2.24
Episode length: 126.20 +/- 30.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 4.4         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.092408806 |
|    clip_fraction        | 0.512       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.453       |
|    learning_rate        | 0.00349     |
|    loss                 | 0.0513      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0383     |
|    value_loss           | 0.472       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    fps             | 823      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 113       |
|    ep_rew_mean          | 2.89      |
| time/                   |           |
|    fps                  | 832       |
|    iterations           | 11        |
|    time_elapsed         | 27        |
|    total_timesteps      | 22528     |
| train/                  |           |
|    approx_kl            | 0.1119152 |
|    clip_fraction        | 0.503     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.18     |
|    explained_variance   | 0.489     |
|    learning_rate        | 0.00349   |
|    loss                 | 0.175     |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.0362   |
|    value_loss           | 0.684     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 115        |
|    ep_rew_mean          | 3.01       |
| time/                   |            |
|    fps                  | 838        |
|    iterations           | 12         |
|    time_elapsed         | 29         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.15035631 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | 0.426      |
|    learning_rate        | 0.00349    |
|    loss                 | 0.0457     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0479    |
|    value_loss           | 0.557      |
----------------------------------------
Eval num_timesteps=25000, episode_reward=4.40 +/- 1.20
Episode length: 126.60 +/- 12.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 127       |
|    mean_reward          | 4.4       |
| time/                   |           |
|    total_timesteps      | 25000     |
| train/                  |           |
|    approx_kl            | 0.1583156 |
|    clip_fraction        | 0.596     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.08     |
|    explained_variance   | 0.515     |
|    learning_rate        | 0.00349   |
|    loss                 | 0.0175    |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.029    |
|    value_loss           | 0.373     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.27     |
| time/              |          |
|    fps             | 821      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 119        |
|    ep_rew_mean          | 3.49       |
| time/                   |            |
|    fps                  | 827        |
|    iterations           | 14         |
|    time_elapsed         | 34         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.18424627 |
|    clip_fraction        | 0.565      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.00349    |
|    loss                 | 0.00231    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0528    |
|    value_loss           | 0.315      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=3.30 +/- 1.73
Episode length: 115.40 +/- 21.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 115        |
|    mean_reward          | 3.3        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.21926174 |
|    clip_fraction        | 0.557      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.00349    |
|    loss                 | -0.0406    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0636    |
|    value_loss           | 0.467      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 3.72     |
| time/              |          |
|    fps             | 815      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 118        |
|    ep_rew_mean          | 3.69       |
| time/                   |            |
|    fps                  | 820        |
|    iterations           | 16         |
|    time_elapsed         | 39         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.23010106 |
|    clip_fraction        | 0.59       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.992     |
|    explained_variance   | 0.507      |
|    learning_rate        | 0.00349    |
|    loss                 | 0.0276     |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0564    |
|    value_loss           | 0.465      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 117        |
|    ep_rew_mean          | 3.6        |
| time/                   |            |
|    fps                  | 825        |
|    iterations           | 17         |
|    time_elapsed         | 42         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.22413877 |
|    clip_fraction        | 0.601      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.993     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.00349    |
|    loss                 | -0.0446    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0525    |
|    value_loss           | 0.361      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=4.20 +/- 2.04
Episode length: 121.10 +/- 29.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 121        |
|    mean_reward          | 4.2        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.25630352 |
|    clip_fraction        | 0.612      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.996     |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.00349    |
|    loss                 | 0.112      |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.498      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 3.5      |
| time/              |          |
|    fps             | 815      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 113        |
|    ep_rew_mean          | 3.27       |
| time/                   |            |
|    fps                  | 819        |
|    iterations           | 19         |
|    time_elapsed         | 47         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.25398767 |
|    clip_fraction        | 0.635      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.963     |
|    explained_variance   | 0.165      |
|    learning_rate        | 0.00349    |
|    loss                 | -0.0406    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0546    |
|    value_loss           | 0.386      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=1.00 +/- 0.89
Episode length: 87.30 +/- 17.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 87.3      |
|    mean_reward          | 1         |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.2969603 |
|    clip_fraction        | 0.613     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.965    |
|    explained_variance   | 0.306     |
|    learning_rate        | 0.00349   |
|    loss                 | -0.023    |
|    n_updates            | 190       |
|    policy_gradient_loss | -0.0607   |
|    value_loss           | 0.447     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    fps             | 814      |
|    iterations      | 20       |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 110        |
|    ep_rew_mean          | 2.78       |
| time/                   |            |
|    fps                  | 817        |
|    iterations           | 21         |
|    time_elapsed         | 52         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.30010885 |
|    clip_fraction        | 0.637      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.947     |
|    explained_variance   | 0.288      |
|    learning_rate        | 0.00349    |
|    loss                 | 0.00344    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0425    |
|    value_loss           | 0.467      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=2.00 +/- 1.61
Episode length: 97.50 +/- 15.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.5       |
|    mean_reward          | 2          |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.28516462 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.951     |
|    explained_variance   | 0.5        |
|    learning_rate        | 0.00349    |
|    loss                 | 0.0338     |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0572    |
|    value_loss           | 0.399      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 812      |
|    iterations      | 22       |
|    time_elapsed    | 55       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 111       |
|    ep_rew_mean          | 2.8       |
| time/                   |           |
|    fps                  | 817       |
|    iterations           | 23        |
|    time_elapsed         | 57        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.3461389 |
|    clip_fraction        | 0.633     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.899    |
|    explained_variance   | 0.367     |
|    learning_rate        | 0.00349   |
|    loss                 | 0.0397    |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0528   |
|    value_loss           | 0.555     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 113       |
|    ep_rew_mean          | 2.89      |
| time/                   |           |
|    fps                  | 820       |
|    iterations           | 24        |
|    time_elapsed         | 59        |
|    total_timesteps      | 49152     |
| train/                  |           |
|    approx_kl            | 0.3118983 |
|    clip_fraction        | 0.639     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.922    |
|    explained_variance   | 0.424     |
|    learning_rate        | 0.00349   |
|    loss                 | 0.0425    |
|    n_updates            | 230       |
|    policy_gradient_loss | -0.0431   |
|    value_loss           | 0.413     |
---------------------------------------
Eval num_timesteps=50000, episode_reward=5.20 +/- 1.54
Episode length: 130.20 +/- 28.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 130        |
|    mean_reward          | 5.2        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.30891973 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.929     |
|    explained_variance   | 0.376      |
|    learning_rate        | 0.00349    |
|    loss                 | 0.025      |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0471    |
|    value_loss           | 0.449      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 813      |
|    iterations      | 25       |
|    time_elapsed    | 62       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 110        |
|    ep_rew_mean          | 2.94       |
| time/                   |            |
|    fps                  | 815        |
|    iterations           | 26         |
|    time_elapsed         | 65         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.32449162 |
|    clip_fraction        | 0.622      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.938     |
|    explained_variance   | 0.551      |
|    learning_rate        | 0.00349    |
|    loss                 | -0.0526    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.057     |
|    value_loss           | 0.33       |
----------------------------------------
Eval num_timesteps=55000, episode_reward=3.80 +/- 1.08
Episode length: 118.10 +/- 12.85
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 118      |
|    mean_reward          | 3.8      |
| time/                   |          |
|    total_timesteps      | 55000    |
| train/                  |          |
|    approx_kl            | 0.375826 |
|    clip_fraction        | 0.63     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.894   |
|    explained_variance   | 0.433    |
|    learning_rate        | 0.00349  |
|    loss                 | -0.0165  |
|    n_updates            | 260      |
|    policy_gradient_loss | -0.0566  |
|    value_loss           | 0.347    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.96     |
| time/              |          |
|    fps             | 809      |
|    iterations      | 27       |
|    time_elapsed    | 68       |
|    total_timesteps | 55296    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 106       |
|    ep_rew_mean          | 2.66      |
| time/                   |           |
|    fps                  | 813       |
|    iterations           | 28        |
|    time_elapsed         | 70        |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.2916067 |
|    clip_fraction        | 0.654     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.96     |
|    explained_variance   | 0.331     |
|    learning_rate        | 0.00349   |
|    loss                 | -0.00102  |
|    n_updates            | 270       |
|    policy_gradient_loss | -0.0491   |
|    value_loss           | 0.287     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 100        |
|    ep_rew_mean          | 2.21       |
| time/                   |            |
|    fps                  | 816        |
|    iterations           | 29         |
|    time_elapsed         | 72         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.34890378 |
|    clip_fraction        | 0.648      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.953     |
|    explained_variance   | 0.276      |
|    learning_rate        | 0.00349    |
|    loss                 | 0.0967     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0287    |
|    value_loss           | 0.503      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=0.50 +/- 1.02
Episode length: 92.20 +/- 15.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 92.2       |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.31843886 |
|    clip_fraction        | 0.64       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.973     |
|    explained_variance   | 0.203      |
|    learning_rate        | 0.00349    |
|    loss                 | 0.00725    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0279    |
|    value_loss           | 0.425      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.6     |
|    ep_rew_mean     | 1.96     |
| time/              |          |
|    fps             | 812      |
|    iterations      | 30       |
|    time_elapsed    | 75       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.1     |
|    ep_rew_mean     | 0.24     |
| time/              |          |
|    fps             | 1238     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 82.4         |
|    ep_rew_mean          | 0.367        |
| time/                   |              |
|    fps                  | 1042         |
|    iterations           | 2            |
|    time_elapsed         | 3            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0088402275 |
|    clip_fraction        | 0.0748       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.1         |
|    learning_rate        | 0.00213      |
|    loss                 | -0.00935     |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00447     |
|    value_loss           | 0.0695       |
------------------------------------------
Eval num_timesteps=5000, episode_reward=1.80 +/- 0.75
Episode length: 96.00 +/- 19.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96          |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.014750135 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.00213     |
|    loss                 | -0.0313     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.0493      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.6     |
|    ep_rew_mean     | 0.643    |
| time/              |          |
|    fps             | 902      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 88.5       |
|    ep_rew_mean          | 0.804      |
| time/                   |            |
|    fps                  | 905        |
|    iterations           | 4          |
|    time_elapsed         | 9          |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.02245038 |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.3        |
|    learning_rate        | 0.00213    |
|    loss                 | 0.000958   |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0255    |
|    value_loss           | 0.0619     |
----------------------------------------
Eval num_timesteps=10000, episode_reward=0.80 +/- 0.75
Episode length: 79.60 +/- 13.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.6        |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.027599791 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.237       |
|    learning_rate        | 0.00213     |
|    loss                 | -0.0431     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0279     |
|    value_loss           | 0.0667      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.2     |
|    ep_rew_mean     | 1        |
| time/              |          |
|    fps             | 867      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 91.8       |
|    ep_rew_mean          | 1.37       |
| time/                   |            |
|    fps                  | 874        |
|    iterations           | 6          |
|    time_elapsed         | 14         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.04069151 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.315      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0454    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0352    |
|    value_loss           | 0.0642     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 99.2        |
|    ep_rew_mean          | 2.01        |
| time/                   |             |
|    fps                  | 881         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.055573687 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.00213     |
|    loss                 | -0.0572     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0397     |
|    value_loss           | 0.0582      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=8.00 +/- 3.07
Episode length: 193.00 +/- 63.41
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 193      |
|    mean_reward          | 8        |
| time/                   |          |
|    total_timesteps      | 15000    |
| train/                  |          |
|    approx_kl            | 0.05274  |
|    clip_fraction        | 0.359    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.16    |
|    explained_variance   | 0.556    |
|    learning_rate        | 0.00213  |
|    loss                 | -0.0674  |
|    n_updates            | 70       |
|    policy_gradient_loss | -0.0485  |
|    value_loss           | 0.0557   |
--------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.48     |
| time/              |          |
|    fps             | 828      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | 3           |
| time/                   |             |
|    fps                  | 835         |
|    iterations           | 9           |
|    time_elapsed         | 22          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.068644375 |
|    clip_fraction        | 0.384       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.00213     |
|    loss                 | -0.0504     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0475     |
|    value_loss           | 0.0639      |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=8.60 +/- 3.72
Episode length: 188.70 +/- 64.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 189         |
|    mean_reward          | 8.6         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.072549745 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.49        |
|    learning_rate        | 0.00213     |
|    loss                 | -0.0545     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0493     |
|    value_loss           | 0.0607      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 3.64     |
| time/              |          |
|    fps             | 799      |
|    iterations      | 10       |
|    time_elapsed    | 25       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 123        |
|    ep_rew_mean          | 4.36       |
| time/                   |            |
|    fps                  | 808        |
|    iterations           | 11         |
|    time_elapsed         | 27         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.07703471 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.975     |
|    explained_variance   | 0.385      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0254    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0478    |
|    value_loss           | 0.0637     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 130        |
|    ep_rew_mean          | 4.96       |
| time/                   |            |
|    fps                  | 817        |
|    iterations           | 12         |
|    time_elapsed         | 30         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.08972371 |
|    clip_fraction        | 0.388      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.969     |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0547    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.0585     |
----------------------------------------
Eval num_timesteps=25000, episode_reward=10.10 +/- 3.18
Episode length: 195.00 +/- 52.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 195        |
|    mean_reward          | 10.1       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.13880622 |
|    clip_fraction        | 0.439      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.858     |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0555    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 0.0736     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 5.48     |
| time/              |          |
|    fps             | 791      |
|    iterations      | 13       |
|    time_elapsed    | 33       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 139        |
|    ep_rew_mean          | 5.98       |
| time/                   |            |
|    fps                  | 799        |
|    iterations           | 14         |
|    time_elapsed         | 35         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.13189901 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.844     |
|    explained_variance   | 0.472      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0606    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0525    |
|    value_loss           | 0.07       |
----------------------------------------
Eval num_timesteps=30000, episode_reward=13.80 +/- 2.64
Episode length: 237.80 +/- 43.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 238        |
|    mean_reward          | 13.8       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.16903244 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.752     |
|    explained_variance   | 0.374      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0442    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0466    |
|    value_loss           | 0.0789     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 6.46     |
| time/              |          |
|    fps             | 774      |
|    iterations      | 15       |
|    time_elapsed    | 39       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 151        |
|    ep_rew_mean          | 6.94       |
| time/                   |            |
|    fps                  | 781        |
|    iterations           | 16         |
|    time_elapsed         | 41         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.12758723 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.793     |
|    explained_variance   | 0.533      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0361    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0429    |
|    value_loss           | 0.0682     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 159        |
|    ep_rew_mean          | 7.59       |
| time/                   |            |
|    fps                  | 787        |
|    iterations           | 17         |
|    time_elapsed         | 44         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.16160247 |
|    clip_fraction        | 0.433      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.714     |
|    explained_variance   | 0.428      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0734    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.05      |
|    value_loss           | 0.0766     |
----------------------------------------
Eval num_timesteps=35000, episode_reward=11.30 +/- 2.53
Episode length: 210.80 +/- 43.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 211        |
|    mean_reward          | 11.3       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.13388357 |
|    clip_fraction        | 0.415      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.708     |
|    explained_variance   | 0.529      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.044     |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.0693     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 8.16     |
| time/              |          |
|    fps             | 770      |
|    iterations      | 18       |
|    time_elapsed    | 47       |
|    total_timesteps | 36864    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 173       |
|    ep_rew_mean          | 8.66      |
| time/                   |           |
|    fps                  | 777       |
|    iterations           | 19        |
|    time_elapsed         | 50        |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.2143968 |
|    clip_fraction        | 0.4       |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.583    |
|    explained_variance   | 0.331     |
|    learning_rate        | 0.00213   |
|    loss                 | -0.0452   |
|    n_updates            | 180       |
|    policy_gradient_loss | -0.05     |
|    value_loss           | 0.0802    |
---------------------------------------
Eval num_timesteps=40000, episode_reward=11.40 +/- 1.96
Episode length: 211.50 +/- 29.13
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 212        |
|    mean_reward          | 11.4       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.20289269 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.57      |
|    explained_variance   | 0.44       |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0741    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0432    |
|    value_loss           | 0.0776     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 9.2      |
| time/              |          |
|    fps             | 762      |
|    iterations      | 20       |
|    time_elapsed    | 53       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 9.53       |
| time/                   |            |
|    fps                  | 768        |
|    iterations           | 21         |
|    time_elapsed         | 55         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.21268824 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.558     |
|    explained_variance   | 0.439      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0579    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0439    |
|    value_loss           | 0.0765     |
----------------------------------------
Eval num_timesteps=45000, episode_reward=11.50 +/- 1.28
Episode length: 210.40 +/- 17.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 11.5       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.18616876 |
|    clip_fraction        | 0.395      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.567     |
|    explained_variance   | 0.541      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0459    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0434    |
|    value_loss           | 0.0787     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 10.1     |
| time/              |          |
|    fps             | 756      |
|    iterations      | 22       |
|    time_elapsed    | 59       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 195       |
|    ep_rew_mean          | 10.3      |
| time/                   |           |
|    fps                  | 762       |
|    iterations           | 23        |
|    time_elapsed         | 61        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.1957654 |
|    clip_fraction        | 0.383     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.539    |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.00213   |
|    loss                 | -0.0438   |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0371   |
|    value_loss           | 0.078     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 197        |
|    ep_rew_mean          | 10.5       |
| time/                   |            |
|    fps                  | 768        |
|    iterations           | 24         |
|    time_elapsed         | 63         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.23045897 |
|    clip_fraction        | 0.399      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.564     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0495    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0412    |
|    value_loss           | 0.076      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=12.70 +/- 2.24
Episode length: 220.90 +/- 34.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 221        |
|    mean_reward          | 12.7       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.20217414 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.565     |
|    explained_variance   | 0.596      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.062     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0482    |
|    value_loss           | 0.0737     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 756      |
|    iterations      | 25       |
|    time_elapsed    | 67       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 204        |
|    ep_rew_mean          | 10.9       |
| time/                   |            |
|    fps                  | 761        |
|    iterations           | 26         |
|    time_elapsed         | 69         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.28607944 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.58      |
|    explained_variance   | 0.556      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0757    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0433    |
|    value_loss           | 0.0769     |
----------------------------------------
Eval num_timesteps=55000, episode_reward=12.00 +/- 2.10
Episode length: 203.80 +/- 28.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 204        |
|    mean_reward          | 12         |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.23111157 |
|    clip_fraction        | 0.391      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.517     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.06      |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0447    |
|    value_loss           | 0.0727     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 751      |
|    iterations      | 27       |
|    time_elapsed    | 73       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 202        |
|    ep_rew_mean          | 10.8       |
| time/                   |            |
|    fps                  | 756        |
|    iterations           | 28         |
|    time_elapsed         | 75         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.22958477 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.548     |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0772    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0466    |
|    value_loss           | 0.0698     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 201        |
|    ep_rew_mean          | 10.9       |
| time/                   |            |
|    fps                  | 760        |
|    iterations           | 29         |
|    time_elapsed         | 78         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.20249042 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.533     |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.027     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0334    |
|    value_loss           | 0.0764     |
----------------------------------------
Eval num_timesteps=60000, episode_reward=10.80 +/- 2.71
Episode length: 204.10 +/- 36.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 204        |
|    mean_reward          | 10.8       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.23682934 |
|    clip_fraction        | 0.41       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.545     |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.00213    |
|    loss                 | -0.0572    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0458    |
|    value_loss           | 0.0737     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 752      |
|    iterations      | 30       |
|    time_elapsed    | 81       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=-1.00 +/- 0.00
Episode length: 72.40 +/- 9.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.4     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.9     |
|    ep_rew_mean     | 0.296    |
| time/              |          |
|    fps             | 1145     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=4.40 +/- 1.96
Episode length: 136.00 +/- 44.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 136          |
|    mean_reward          | 4.4          |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0103444755 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.37        |
|    explained_variance   | 0.0246       |
|    learning_rate        | 9.28e-05     |
|    loss                 | 0.0165       |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00952     |
|    value_loss           | 0.079        |
------------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=3.50 +/- 0.92
Episode length: 127.40 +/- 14.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 127      |
|    mean_reward     | 3.5      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.9     |
|    ep_rew_mean     | 0.54     |
| time/              |          |
|    fps             | 806      |
|    iterations      | 2        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=9.10 +/- 1.76
Episode length: 190.60 +/- 40.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 191         |
|    mean_reward          | 9.1         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010425757 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.437       |
|    learning_rate        | 9.28e-05    |
|    loss                 | 0.0202      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.0742      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93       |
|    ep_rew_mean     | 0.98     |
| time/              |          |
|    fps             | 758      |
|    iterations      | 3        |
|    time_elapsed    | 32       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=7.20 +/- 2.89
Episode length: 171.60 +/- 49.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 172         |
|    mean_reward          | 7.2         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.010135415 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.496       |
|    learning_rate        | 9.28e-05    |
|    loss                 | -0.00595    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.0802      |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=9.30 +/- 3.63
Episode length: 204.70 +/- 67.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 9.3      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.01     |
| time/              |          |
|    fps             | 716      |
|    iterations      | 4        |
|    time_elapsed    | 45       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=9.30 +/- 1.42
Episode length: 197.40 +/- 32.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 197         |
|    mean_reward          | 9.3         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012219051 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.482       |
|    learning_rate        | 9.28e-05    |
|    loss                 | -0.0243     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 0.101       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=10.50 +/- 1.91
Episode length: 218.90 +/- 42.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 10.5     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 3.16     |
| time/              |          |
|    fps             | 691      |
|    iterations      | 5        |
|    time_elapsed    | 59       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=7.90 +/- 2.07
Episode length: 194.90 +/- 55.98
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 195      |
|    mean_reward          | 7.9      |
| time/                   |          |
|    total_timesteps      | 45000    |
| train/                  |          |
|    approx_kl            | 0.015705 |
|    clip_fraction        | 0.186    |
|    clip_range           | 0.2      |
|    entropy_loss         | -1.23    |
|    explained_variance   | 0.501    |
|    learning_rate        | 9.28e-05 |
|    loss                 | 0.0168   |
|    n_updates            | 50       |
|    policy_gradient_loss | -0.0293  |
|    value_loss           | 0.108    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 4.23     |
| time/              |          |
|    fps             | 684      |
|    iterations      | 6        |
|    time_elapsed    | 71       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=15.90 +/- 2.02
Episode length: 303.30 +/- 25.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 303       |
|    mean_reward          | 15.9      |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0163452 |
|    clip_fraction        | 0.209     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.16     |
|    explained_variance   | 0.575     |
|    learning_rate        | 9.28e-05  |
|    loss                 | -0.011    |
|    n_updates            | 60        |
|    policy_gradient_loss | -0.0273   |
|    value_loss           | 0.11      |
---------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=14.30 +/- 2.00
Episode length: 283.40 +/- 31.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 283      |
|    mean_reward     | 14.3     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 4.99     |
| time/              |          |
|    fps             | 661      |
|    iterations      | 7        |
|    time_elapsed    | 86       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=8.90 +/- 2.55
Episode length: 187.60 +/- 43.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 188         |
|    mean_reward          | 8.9         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.016300958 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.519       |
|    learning_rate        | 9.28e-05    |
|    loss                 | 0.0523      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 0.112       |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=9.20 +/- 1.47
Episode length: 204.70 +/- 25.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 205      |
|    mean_reward     | 9.2      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 5.71     |
| time/              |          |
|    fps             | 652      |
|    iterations      | 8        |
|    time_elapsed    | 100      |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.6     |
|    ep_rew_mean     | 0.0741   |
| time/              |          |
|    fps             | 1218     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 83.3        |
|    ep_rew_mean          | 0.449       |
| time/                   |             |
|    fps                  | 1038        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.017430935 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0175     |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.019       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.0464      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=3.50 +/- 1.20
Episode length: 119.00 +/- 8.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 119         |
|    mean_reward          | 3.5         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.014493974 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.226       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.00321     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00924    |
|    value_loss           | 0.0497      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.6     |
|    ep_rew_mean     | 0.623    |
| time/              |          |
|    fps             | 880      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 92.6        |
|    ep_rew_mean          | 0.966       |
| time/                   |             |
|    fps                  | 886         |
|    iterations           | 4           |
|    time_elapsed         | 9           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.011022763 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.295       |
|    learning_rate        | 3.69e-05    |
|    loss                 | -0.00776    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.0528      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=3.30 +/- 1.27
Episode length: 108.10 +/- 21.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 3.3         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010489685 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.348       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.00367     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.0626      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.8     |
|    ep_rew_mean     | 1.36     |
| time/              |          |
|    fps             | 840      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 1.82        |
| time/                   |             |
|    fps                  | 851         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.011254583 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.426       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.0279      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0068     |
|    value_loss           | 0.0652      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | 2.11        |
| time/                   |             |
|    fps                  | 860         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.013484903 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.407       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.0241      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00653    |
|    value_loss           | 0.0647      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=5.60 +/- 0.66
Episode length: 128.40 +/- 12.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 5.6         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.004254366 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.424       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.00912     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00233    |
|    value_loss           | 0.0624      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 826      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 111          |
|    ep_rew_mean          | 2.56         |
| time/                   |              |
|    fps                  | 833          |
|    iterations           | 9            |
|    time_elapsed         | 22           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0064546326 |
|    clip_fraction        | 0.0382       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.973       |
|    explained_variance   | 0.365        |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.0195       |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00585     |
|    value_loss           | 0.0697       |
------------------------------------------
Eval num_timesteps=20000, episode_reward=5.40 +/- 1.20
Episode length: 130.70 +/- 18.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 131          |
|    mean_reward          | 5.4          |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0071510077 |
|    clip_fraction        | 0.0629       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.912       |
|    explained_variance   | 0.445        |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.0122       |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00726     |
|    value_loss           | 0.0707       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 812      |
|    iterations      | 10       |
|    time_elapsed    | 25       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 108        |
|    ep_rew_mean          | 2.69       |
| time/                   |            |
|    fps                  | 821        |
|    iterations           | 11         |
|    time_elapsed         | 27         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.01093184 |
|    clip_fraction        | 0.0797     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.841     |
|    explained_variance   | 0.419      |
|    learning_rate        | 3.69e-05   |
|    loss                 | 0.0108     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.00964   |
|    value_loss           | 0.0706     |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 107          |
|    ep_rew_mean          | 2.82         |
| time/                   |              |
|    fps                  | 827          |
|    iterations           | 12           |
|    time_elapsed         | 29           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0038238505 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.801       |
|    explained_variance   | 0.526        |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.0613       |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00284     |
|    value_loss           | 0.0649       |
------------------------------------------
Eval num_timesteps=25000, episode_reward=3.30 +/- 1.42
Episode length: 101.80 +/- 19.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 102          |
|    mean_reward          | 3.3          |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0030276645 |
|    clip_fraction        | 0.0526       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.748       |
|    explained_variance   | 0.586        |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.0352       |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00364     |
|    value_loss           | 0.0649       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 814      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 111         |
|    ep_rew_mean          | 3.13        |
| time/                   |             |
|    fps                  | 820         |
|    iterations           | 14          |
|    time_elapsed         | 34          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.005696268 |
|    clip_fraction        | 0.0826      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.705      |
|    explained_variance   | 0.559       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.0242      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00808    |
|    value_loss           | 0.066       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=5.90 +/- 0.70
Episode length: 133.80 +/- 19.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 134          |
|    mean_reward          | 5.9          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0033286468 |
|    clip_fraction        | 0.0486       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.671       |
|    explained_variance   | 0.56         |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.0295       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00521     |
|    value_loss           | 0.0703       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 3.31     |
| time/              |          |
|    fps             | 807      |
|    iterations      | 15       |
|    time_elapsed    | 38       |
|    total_timesteps | 30720    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 116          |
|    ep_rew_mean          | 3.58         |
| time/                   |              |
|    fps                  | 813          |
|    iterations           | 16           |
|    time_elapsed         | 40           |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0062093413 |
|    clip_fraction        | 0.0676       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.64        |
|    explained_variance   | 0.55         |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.0108       |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00626     |
|    value_loss           | 0.0644       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 118          |
|    ep_rew_mean          | 3.75         |
| time/                   |              |
|    fps                  | 818          |
|    iterations           | 17           |
|    time_elapsed         | 42           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0026530088 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.635       |
|    explained_variance   | 0.546        |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.0338       |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00468     |
|    value_loss           | 0.0684       |
------------------------------------------
Eval num_timesteps=35000, episode_reward=6.00 +/- 1.48
Episode length: 149.80 +/- 29.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 150         |
|    mean_reward          | 6           |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.007709536 |
|    clip_fraction        | 0.0545      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.604      |
|    explained_variance   | 0.547       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.0402      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00539    |
|    value_loss           | 0.0715      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 3.96     |
| time/              |          |
|    fps             | 806      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 126         |
|    ep_rew_mean          | 4.33        |
| time/                   |             |
|    fps                  | 811         |
|    iterations           | 19          |
|    time_elapsed         | 47          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.005420519 |
|    clip_fraction        | 0.0591      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.498       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.0098      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00514    |
|    value_loss           | 0.0775      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=5.40 +/- 1.20
Episode length: 132.40 +/- 15.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 132         |
|    mean_reward          | 5.4         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.003050286 |
|    clip_fraction        | 0.044       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.58       |
|    explained_variance   | 0.477       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.022       |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00328    |
|    value_loss           | 0.0703      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 4.51     |
| time/              |          |
|    fps             | 802      |
|    iterations      | 20       |
|    time_elapsed    | 51       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | 4.61        |
| time/                   |             |
|    fps                  | 806         |
|    iterations           | 21          |
|    time_elapsed         | 53          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.005535707 |
|    clip_fraction        | 0.0606      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.548       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.0226      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00784    |
|    value_loss           | 0.0693      |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=6.30 +/- 1.00
Episode length: 152.10 +/- 22.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 152          |
|    mean_reward          | 6.3          |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0062195817 |
|    clip_fraction        | 0.0963       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.615       |
|    explained_variance   | 0.496        |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.0267       |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00489     |
|    value_loss           | 0.0758       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 4.71     |
| time/              |          |
|    fps             | 795      |
|    iterations      | 22       |
|    time_elapsed    | 56       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 132         |
|    ep_rew_mean          | 4.84        |
| time/                   |             |
|    fps                  | 799         |
|    iterations           | 23          |
|    time_elapsed         | 58          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.005042338 |
|    clip_fraction        | 0.0589      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.594      |
|    explained_variance   | 0.494       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.00689     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00508    |
|    value_loss           | 0.0735      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 137         |
|    ep_rew_mean          | 5.09        |
| time/                   |             |
|    fps                  | 803         |
|    iterations           | 24          |
|    time_elapsed         | 61          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.010975387 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.576      |
|    explained_variance   | 0.499       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.0137      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00618    |
|    value_loss           | 0.0775      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=7.30 +/- 1.10
Episode length: 164.90 +/- 27.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 165         |
|    mean_reward          | 7.3         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.006495255 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.557      |
|    explained_variance   | 0.481       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.0278      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00854    |
|    value_loss           | 0.0678      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 5.27     |
| time/              |          |
|    fps             | 793      |
|    iterations      | 25       |
|    time_elapsed    | 64       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 140          |
|    ep_rew_mean          | 5.4          |
| time/                   |              |
|    fps                  | 797          |
|    iterations           | 26           |
|    time_elapsed         | 66           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0051126936 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.529       |
|    explained_variance   | 0.6          |
|    learning_rate        | 3.69e-05     |
|    loss                 | 0.013        |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00627     |
|    value_loss           | 0.0692       |
------------------------------------------
Eval num_timesteps=55000, episode_reward=7.20 +/- 1.54
Episode length: 166.40 +/- 37.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 7.2         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.009104935 |
|    clip_fraction        | 0.0754      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.499      |
|    explained_variance   | 0.519       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.0184      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00906    |
|    value_loss           | 0.08        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 5.66     |
| time/              |          |
|    fps             | 789      |
|    iterations      | 27       |
|    time_elapsed    | 70       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 146         |
|    ep_rew_mean          | 5.9         |
| time/                   |             |
|    fps                  | 793         |
|    iterations           | 28          |
|    time_elapsed         | 72          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.005948561 |
|    clip_fraction        | 0.0847      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.481      |
|    explained_variance   | 0.527       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.00306     |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 0.0824      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 151         |
|    ep_rew_mean          | 6.23        |
| time/                   |             |
|    fps                  | 796         |
|    iterations           | 29          |
|    time_elapsed         | 74          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.006211455 |
|    clip_fraction        | 0.0912      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.493      |
|    explained_variance   | 0.593       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.00825     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00589    |
|    value_loss           | 0.0713      |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=12.90 +/- 2.91
Episode length: 252.70 +/- 37.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 253         |
|    mean_reward          | 12.9        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.010449757 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.454      |
|    explained_variance   | 0.536       |
|    learning_rate        | 3.69e-05    |
|    loss                 | 0.0525      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00978    |
|    value_loss           | 0.076       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 6.56     |
| time/              |          |
|    fps             | 782      |
|    iterations      | 30       |
|    time_elapsed    | 78       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=-1.00 +/- 0.00
Episode length: 67.20 +/- 11.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.2     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.8     |
|    ep_rew_mean     | 0.05     |
| time/              |          |
|    fps             | 1141     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=1.10 +/- 0.70
Episode length: 90.00 +/- 16.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 90          |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.008322277 |
|    clip_fraction        | 0.0968      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0437     |
|    learning_rate        | 0.000223    |
|    loss                 | -0.0084     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00838    |
|    value_loss           | 0.0571      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=1.60 +/- 0.80
Episode length: 97.60 +/- 13.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.6     |
|    mean_reward     | 1.6      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.1     |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 930      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=6.00 +/- 3.38
Episode length: 154.20 +/- 63.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 154         |
|    mean_reward          | 6           |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010163524 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.000223    |
|    loss                 | -0.0143     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.0445      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.5     |
|    ep_rew_mean     | 0.79     |
| time/              |          |
|    fps             | 889      |
|    iterations      | 3        |
|    time_elapsed    | 27       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=6.00 +/- 3.79
Episode length: 150.20 +/- 65.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 150         |
|    mean_reward          | 6           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.016256377 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.000223    |
|    loss                 | 0.00456     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0292     |
|    value_loss           | 0.053       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=8.10 +/- 6.20
Episode length: 189.30 +/- 94.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 8.1      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.9     |
|    ep_rew_mean     | 1.71     |
| time/              |          |
|    fps             | 845      |
|    iterations      | 4        |
|    time_elapsed    | 38       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=4.40 +/- 2.01
Episode length: 125.30 +/- 28.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 125        |
|    mean_reward          | 4.4        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.01849744 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.000223   |
|    loss                 | -0.031     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.03      |
|    value_loss           | 0.058      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=2.80 +/- 1.60
Episode length: 98.40 +/- 26.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.4     |
|    mean_reward     | 2.8      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 833      |
|    iterations      | 5        |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=5.00 +/- 3.16
Episode length: 129.90 +/- 57.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 130         |
|    mean_reward          | 5           |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.020685114 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.000223    |
|    loss                 | -0.0561     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0363     |
|    value_loss           | 0.0615      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 3.65     |
| time/              |          |
|    fps             | 830      |
|    iterations      | 6        |
|    time_elapsed    | 59       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=5.20 +/- 2.60
Episode length: 132.40 +/- 40.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 132         |
|    mean_reward          | 5.2         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.026476732 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.000223    |
|    loss                 | -0.00314    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0416     |
|    value_loss           | 0.0645      |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=5.10 +/- 3.08
Episode length: 128.80 +/- 41.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 129      |
|    mean_reward     | 5.1      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 4.97     |
| time/              |          |
|    fps             | 819      |
|    iterations      | 7        |
|    time_elapsed    | 69       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=12.90 +/- 2.77
Episode length: 224.90 +/- 51.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 225         |
|    mean_reward          | 12.9        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.030578226 |
|    clip_fraction        | 0.346       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.000223    |
|    loss                 | -0.0635     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0411     |
|    value_loss           | 0.0713      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=65000, episode_reward=11.20 +/- 3.99
Episode length: 196.80 +/- 66.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 197      |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 6.04     |
| time/              |          |
|    fps             | 800      |
|    iterations      | 8        |
|    time_elapsed    | 81       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.3     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 1236     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 85.7        |
|    ep_rew_mean          | 0.319       |
| time/                   |             |
|    fps                  | 928         |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011828933 |
|    clip_fraction        | 0.0524      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0485     |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0241      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.000952   |
|    value_loss           | 0.331       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=0.50 +/- 0.67
Episode length: 75.70 +/- 14.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 75.7       |
|    mean_reward          | 0.5        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01198252 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.119      |
|    learning_rate        | 0.00656    |
|    loss                 | 0.0329     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0061    |
|    value_loss           | 0.0803     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.8     |
|    ep_rew_mean     | 0.588    |
| time/              |          |
|    fps             | 797      |
|    iterations      | 3        |
|    time_elapsed    | 7        |
|    total_timesteps | 6144     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 90.7         |
|    ep_rew_mean          | 0.667        |
| time/                   |              |
|    fps                  | 785          |
|    iterations           | 4            |
|    time_elapsed         | 10           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0095255235 |
|    clip_fraction        | 0.0855       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.108        |
|    learning_rate        | 0.00656      |
|    loss                 | 0.0667       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00241     |
|    value_loss           | 0.0989       |
------------------------------------------
Eval num_timesteps=10000, episode_reward=0.60 +/- 0.80
Episode length: 68.70 +/- 14.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.7        |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.025662731 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.161       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0289      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.0913      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93       |
|    ep_rew_mean     | 0.89     |
| time/              |          |
|    fps             | 752      |
|    iterations      | 5        |
|    time_elapsed    | 13       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 94.8        |
|    ep_rew_mean          | 1.09        |
| time/                   |             |
|    fps                  | 748         |
|    iterations           | 6           |
|    time_elapsed         | 16          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.015393486 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.102       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.117       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.000248   |
|    value_loss           | 0.108       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 95.2        |
|    ep_rew_mean          | 1.2         |
| time/                   |             |
|    fps                  | 749         |
|    iterations           | 7           |
|    time_elapsed         | 19          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.012723636 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.0739      |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0609      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0073     |
|    value_loss           | 0.0978      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-0.60 +/- 0.49
Episode length: 77.80 +/- 10.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.8        |
|    mean_reward          | -0.6        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.012026664 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.228       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0574      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.000368   |
|    value_loss           | 0.0971      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 1.38     |
| time/              |          |
|    fps             | 731      |
|    iterations      | 8        |
|    time_elapsed    | 22       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 98          |
|    ep_rew_mean          | 1.57        |
| time/                   |             |
|    fps                  | 731         |
|    iterations           | 9           |
|    time_elapsed         | 25          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.014768293 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0169      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00918    |
|    value_loss           | 0.102       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=0.40 +/- 0.66
Episode length: 80.50 +/- 15.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.5        |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.019790696 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.233       |
|    learning_rate        | 0.00656     |
|    loss                 | -0.0158     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00819    |
|    value_loss           | 0.122       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.9     |
|    ep_rew_mean     | 1.71     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 10       |
|    time_elapsed    | 28       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 100        |
|    ep_rew_mean          | 1.84       |
| time/                   |            |
|    fps                  | 722        |
|    iterations           | 11         |
|    time_elapsed         | 31         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.02088959 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.114      |
|    learning_rate        | 0.00656    |
|    loss                 | 0.0575     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.00833   |
|    value_loss           | 0.117      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 2.06        |
| time/                   |             |
|    fps                  | 726         |
|    iterations           | 12          |
|    time_elapsed         | 33          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.022197694 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.241       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.029       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00558    |
|    value_loss           | 0.0983      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=0.40 +/- 0.66
Episode length: 77.00 +/- 15.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77          |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.013351662 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.137       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0421      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00649    |
|    value_loss           | 0.132       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.2      |
| time/              |          |
|    fps             | 718      |
|    iterations      | 13       |
|    time_elapsed    | 37       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 104         |
|    ep_rew_mean          | 2.24        |
| time/                   |             |
|    fps                  | 720         |
|    iterations           | 14          |
|    time_elapsed         | 39          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.018762078 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.126       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0453      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 0.123       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=0.60 +/- 0.66
Episode length: 76.30 +/- 10.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.3        |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.026214456 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0121      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0008     |
|    value_loss           | 0.117       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.33     |
| time/              |          |
|    fps             | 714      |
|    iterations      | 15       |
|    time_elapsed    | 42       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 107         |
|    ep_rew_mean          | 2.53        |
| time/                   |             |
|    fps                  | 717         |
|    iterations           | 16          |
|    time_elapsed         | 45          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.030798499 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.985      |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0584      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 0.104       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 107         |
|    ep_rew_mean          | 2.52        |
| time/                   |             |
|    fps                  | 719         |
|    iterations           | 17          |
|    time_elapsed         | 48          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.023637105 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.99       |
|    explained_variance   | 0.236       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0649      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00389    |
|    value_loss           | 0.124       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=0.70 +/- 1.00
Episode length: 70.70 +/- 25.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 70.7        |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.033345565 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.911      |
|    explained_variance   | 0.232       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0534      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00336    |
|    value_loss           | 0.102       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.68     |
| time/              |          |
|    fps             | 714      |
|    iterations      | 18       |
|    time_elapsed    | 51       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 109         |
|    ep_rew_mean          | 2.74        |
| time/                   |             |
|    fps                  | 716         |
|    iterations           | 19          |
|    time_elapsed         | 54          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.034550868 |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.905      |
|    explained_variance   | 0.185       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0549      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00177    |
|    value_loss           | 0.13        |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=1.10 +/- 1.14
Episode length: 80.40 +/- 19.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.4        |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.039425552 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.848      |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0403      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00439    |
|    value_loss           | 0.107       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 712      |
|    iterations      | 20       |
|    time_elapsed    | 57       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 111        |
|    ep_rew_mean          | 2.97       |
| time/                   |            |
|    fps                  | 714        |
|    iterations           | 21         |
|    time_elapsed         | 60         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.03805688 |
|    clip_fraction        | 0.274      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.853     |
|    explained_variance   | 0.254      |
|    learning_rate        | 0.00656    |
|    loss                 | 0.0567     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.00312   |
|    value_loss           | 0.129      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=1.00 +/- 0.77
Episode length: 82.80 +/- 13.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.8        |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.031269442 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.782      |
|    explained_variance   | 0.18        |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0424      |
|    n_updates            | 210         |
|    policy_gradient_loss | 0.0014      |
|    value_loss           | 0.131       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 709      |
|    iterations      | 22       |
|    time_elapsed    | 63       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 110       |
|    ep_rew_mean          | 3.03      |
| time/                   |           |
|    fps                  | 710       |
|    iterations           | 23        |
|    time_elapsed         | 66        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.0318581 |
|    clip_fraction        | 0.222     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.768    |
|    explained_variance   | 0.197     |
|    learning_rate        | 0.00656   |
|    loss                 | 0.0149    |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.00712  |
|    value_loss           | 0.121     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 109        |
|    ep_rew_mean          | 2.98       |
| time/                   |            |
|    fps                  | 712        |
|    iterations           | 24         |
|    time_elapsed         | 68         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.02851059 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.84      |
|    explained_variance   | 0.187      |
|    learning_rate        | 0.00656    |
|    loss                 | 0.0593     |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.00236   |
|    value_loss           | 0.12       |
----------------------------------------
Eval num_timesteps=50000, episode_reward=1.70 +/- 1.19
Episode length: 97.10 +/- 19.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.1       |
|    mean_reward          | 1.7        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.03738623 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.783     |
|    explained_variance   | 0.142      |
|    learning_rate        | 0.00656    |
|    loss                 | 0.0879     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.00372   |
|    value_loss           | 0.131      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.89     |
| time/              |          |
|    fps             | 707      |
|    iterations      | 25       |
|    time_elapsed    | 72       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | 2.97        |
| time/                   |             |
|    fps                  | 708         |
|    iterations           | 26          |
|    time_elapsed         | 75          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.042013906 |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.85       |
|    explained_variance   | 0.0365      |
|    learning_rate        | 0.00656     |
|    loss                 | 0.101       |
|    n_updates            | 250         |
|    policy_gradient_loss | -6.82e-06   |
|    value_loss           | 0.127       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=1.70 +/- 0.46
Episode length: 95.60 +/- 20.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.6        |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.061537188 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.863      |
|    explained_variance   | 0.196       |
|    learning_rate        | 0.00656     |
|    loss                 | 0.0628      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00366    |
|    value_loss           | 0.128       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 703      |
|    iterations      | 27       |
|    time_elapsed    | 78       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 109        |
|    ep_rew_mean          | 2.82       |
| time/                   |            |
|    fps                  | 705        |
|    iterations           | 28         |
|    time_elapsed         | 81         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.03304487 |
|    clip_fraction        | 0.226      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.766     |
|    explained_variance   | 0.157      |
|    learning_rate        | 0.00656    |
|    loss                 | 0.0137     |
|    n_updates            | 270        |
|    policy_gradient_loss | 0.00449    |
|    value_loss           | 0.123      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 109        |
|    ep_rew_mean          | 2.89       |
| time/                   |            |
|    fps                  | 706        |
|    iterations           | 29         |
|    time_elapsed         | 84         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.08934547 |
|    clip_fraction        | 0.248      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.709     |
|    explained_variance   | 0.158      |
|    learning_rate        | 0.00656    |
|    loss                 | 0.0617     |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.00612   |
|    value_loss           | 0.116      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=1.40 +/- 1.20
Episode length: 92.80 +/- 21.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 92.8       |
|    mean_reward          | 1.4        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.06345841 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.767     |
|    explained_variance   | 0.117      |
|    learning_rate        | 0.00656    |
|    loss                 | 0.143      |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.00107   |
|    value_loss           | 0.12       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.91     |
| time/              |          |
|    fps             | 702      |
|    iterations      | 30       |
|    time_elapsed    | 87       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.4     |
|    ep_rew_mean     | 0        |
| time/              |          |
|    fps             | 1213     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-1.00 +/- 0.00
Episode length: 70.10 +/- 7.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70.1         |
|    mean_reward          | -1           |
| time/                   |              |
|    total_timesteps      | 5000         |
| train/                  |              |
|    approx_kl            | 0.0092911795 |
|    clip_fraction        | 0.00657      |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.0458      |
|    learning_rate        | 2.31e-05     |
|    loss                 | 0.0263       |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.0029      |
|    value_loss           | 0.0895       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.5     |
|    ep_rew_mean     | 0.299    |
| time/              |          |
|    fps             | 1041     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-1.00 +/- 0.00
Episode length: 64.60 +/- 7.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 64.6        |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.016142944 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.274       |
|    learning_rate        | 2.31e-05    |
|    loss                 | 0.0429      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.0815      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.6     |
|    ep_rew_mean     | 0.73     |
| time/              |          |
|    fps             | 1001     |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=3.00 +/- 0.63
Episode length: 109.90 +/- 14.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 3           |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.012421515 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.286       |
|    learning_rate        | 2.31e-05    |
|    loss                 | 0.0266      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00996    |
|    value_loss           | 0.0929      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 1.52     |
| time/              |          |
|    fps             | 960      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=0.70 +/- 0.46
Episode length: 98.80 +/- 12.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.8        |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012044188 |
|    clip_fraction        | 0.0863      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.312       |
|    learning_rate        | 2.31e-05    |
|    loss                 | 0.0595      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00703    |
|    value_loss           | 0.113       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.07     |
| time/              |          |
|    fps             | 940      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | 2.51        |
| time/                   |             |
|    fps                  | 953         |
|    iterations           | 6           |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009873811 |
|    clip_fraction        | 0.0884      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.372       |
|    learning_rate        | 2.31e-05    |
|    loss                 | 0.0344      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00682    |
|    value_loss           | 0.103       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-0.60 +/- 0.66
Episode length: 75.50 +/- 12.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.5        |
|    mean_reward          | -0.6        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.004798305 |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.325       |
|    learning_rate        | 2.31e-05    |
|    loss                 | 0.0586      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00518    |
|    value_loss           | 0.125       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 946      |
|    iterations      | 7        |
|    time_elapsed    | 30       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=5.90 +/- 1.22
Episode length: 136.10 +/- 26.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 136         |
|    mean_reward          | 5.9         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.008979378 |
|    clip_fraction        | 0.0699      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.358       |
|    learning_rate        | 2.31e-05    |
|    loss                 | 0.035       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00532    |
|    value_loss           | 0.106       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 2.19     |
| time/              |          |
|    fps             | 929      |
|    iterations      | 8        |
|    time_elapsed    | 35       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=3.80 +/- 0.60
Episode length: 115.40 +/- 15.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 115         |
|    mean_reward          | 3.8         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.003909111 |
|    clip_fraction        | 0.0121      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.305       |
|    learning_rate        | 2.31e-05    |
|    loss                 | 0.0798      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00122    |
|    value_loss           | 0.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.17     |
| time/              |          |
|    fps             | 920      |
|    iterations      | 9        |
|    time_elapsed    | 40       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=3.10 +/- 1.22
Episode length: 110.10 +/- 15.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 3.1         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.013891753 |
|    clip_fraction        | 0.0608      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.967      |
|    explained_variance   | 0.409       |
|    learning_rate        | 2.31e-05    |
|    loss                 | 0.0375      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00544    |
|    value_loss           | 0.104       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.39     |
| time/              |          |
|    fps             | 913      |
|    iterations      | 10       |
|    time_elapsed    | 44       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=3.80 +/- 0.40
Episode length: 117.10 +/- 13.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 117         |
|    mean_reward          | 3.8         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.009947035 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.935      |
|    explained_variance   | 0.362       |
|    learning_rate        | 2.31e-05    |
|    loss                 | 0.0548      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00203    |
|    value_loss           | 0.119       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.67     |
| time/              |          |
|    fps             | 907      |
|    iterations      | 11       |
|    time_elapsed    | 49       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | 2.74        |
| time/                   |             |
|    fps                  | 916         |
|    iterations           | 12          |
|    time_elapsed         | 53          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.010028566 |
|    clip_fraction        | 0.0517      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.882      |
|    explained_variance   | 0.438       |
|    learning_rate        | 2.31e-05    |
|    loss                 | 0.0437      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00355    |
|    value_loss           | 0.118       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=3.40 +/- 1.02
Episode length: 114.40 +/- 16.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 3.4         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.010592436 |
|    clip_fraction        | 0.0463      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.863      |
|    explained_variance   | 0.419       |
|    learning_rate        | 2.31e-05    |
|    loss                 | 0.0415      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00435    |
|    value_loss           | 0.106       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 911      |
|    iterations      | 13       |
|    time_elapsed    | 58       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=3.50 +/- 0.81
Episode length: 109.60 +/- 15.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 3.5         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.003098351 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.827      |
|    explained_variance   | 0.395       |
|    learning_rate        | 2.31e-05    |
|    loss                 | 0.0468      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00315    |
|    value_loss           | 0.11        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 907      |
|    iterations      | 14       |
|    time_elapsed    | 63       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=3.30 +/- 0.90
Episode length: 114.50 +/- 9.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 114          |
|    mean_reward          | 3.3          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0045564724 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.782       |
|    explained_variance   | 0.397        |
|    learning_rate        | 2.31e-05     |
|    loss                 | 0.0501       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00351     |
|    value_loss           | 0.113        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.49     |
| time/              |          |
|    fps             | 904      |
|    iterations      | 15       |
|    time_elapsed    | 67       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.6     |
|    ep_rew_mean     | 0.115    |
| time/              |          |
|    fps             | 1234     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 78.3        |
|    ep_rew_mean          | 0.137       |
| time/                   |             |
|    fps                  | 1037        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009613351 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0997     |
|    learning_rate        | 0.00178     |
|    loss                 | -0.00119    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.0592      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=0.90 +/- 0.70
Episode length: 75.80 +/- 13.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.8        |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.022798236 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.00178     |
|    loss                 | -0.00224    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0228     |
|    value_loss           | 0.038       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.9     |
|    ep_rew_mean     | 0.187    |
| time/              |          |
|    fps             | 921      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 84.3        |
|    ep_rew_mean          | 0.458       |
| time/                   |             |
|    fps                  | 921         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.030984942 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.00178     |
|    loss                 | -0.0392     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0245     |
|    value_loss           | 0.0326      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=1.30 +/- 1.00
Episode length: 86.00 +/- 23.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86          |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.039764397 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.00178     |
|    loss                 | -0.0261     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0398     |
|    value_loss           | 0.0406      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.4     |
|    ep_rew_mean     | 0.83     |
| time/              |          |
|    fps             | 875      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 94.2        |
|    ep_rew_mean          | 1.34        |
| time/                   |             |
|    fps                  | 880         |
|    iterations           | 6           |
|    time_elapsed         | 13          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.049037926 |
|    clip_fraction        | 0.357       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.00178     |
|    loss                 | -0.0703     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0413     |
|    value_loss           | 0.038       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 101        |
|    ep_rew_mean          | 1.95       |
| time/                   |            |
|    fps                  | 882        |
|    iterations           | 7          |
|    time_elapsed         | 16         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.07542456 |
|    clip_fraction        | 0.392      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0677    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0475    |
|    value_loss           | 0.0442     |
----------------------------------------
Eval num_timesteps=15000, episode_reward=2.80 +/- 1.40
Episode length: 96.70 +/- 23.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96.7       |
|    mean_reward          | 2.8        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.06938999 |
|    clip_fraction        | 0.418      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.11      |
|    explained_variance   | 0.525      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0455    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0541    |
|    value_loss           | 0.0543     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 854      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 114        |
|    ep_rew_mean          | 3.31       |
| time/                   |            |
|    fps                  | 859        |
|    iterations           | 9          |
|    time_elapsed         | 21         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.07192197 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0712    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0554    |
|    value_loss           | 0.0486     |
----------------------------------------
Eval num_timesteps=20000, episode_reward=6.80 +/- 2.14
Episode length: 167.80 +/- 36.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 168        |
|    mean_reward          | 6.8        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.06938406 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.072     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.062      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 3.98     |
| time/              |          |
|    fps             | 824      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 4.68       |
| time/                   |            |
|    fps                  | 830        |
|    iterations           | 11         |
|    time_elapsed         | 27         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.09374354 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.971     |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0741    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0586    |
|    value_loss           | 0.0551     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 136         |
|    ep_rew_mean          | 5.27        |
| time/                   |             |
|    fps                  | 836         |
|    iterations           | 12          |
|    time_elapsed         | 29          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.103905305 |
|    clip_fraction        | 0.451       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.851      |
|    explained_variance   | 0.623       |
|    learning_rate        | 0.00178     |
|    loss                 | -0.067      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0487     |
|    value_loss           | 0.0598      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=12.70 +/- 2.28
Episode length: 228.20 +/- 28.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 12.7       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.13322143 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.821     |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0772    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0498    |
|    value_loss           | 0.057      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 5.96     |
| time/              |          |
|    fps             | 803      |
|    iterations      | 13       |
|    time_elapsed    | 33       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 150        |
|    ep_rew_mean          | 6.46       |
| time/                   |            |
|    fps                  | 811        |
|    iterations           | 14         |
|    time_elapsed         | 35         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.13344395 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.817     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0849    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0554    |
|    value_loss           | 0.0651     |
----------------------------------------
Eval num_timesteps=30000, episode_reward=11.80 +/- 4.53
Episode length: 221.70 +/- 69.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 222        |
|    mean_reward          | 11.8       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.14945376 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.802     |
|    explained_variance   | 0.686      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0262    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0587    |
|    value_loss           | 0.0588     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 6.93     |
| time/              |          |
|    fps             | 787      |
|    iterations      | 15       |
|    time_elapsed    | 39       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 161        |
|    ep_rew_mean          | 7.47       |
| time/                   |            |
|    fps                  | 793        |
|    iterations           | 16         |
|    time_elapsed         | 41         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.16158473 |
|    clip_fraction        | 0.398      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.687     |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0256    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0473    |
|    value_loss           | 0.0581     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 165        |
|    ep_rew_mean          | 7.81       |
| time/                   |            |
|    fps                  | 799        |
|    iterations           | 17         |
|    time_elapsed         | 43         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.19658674 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.71      |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.1       |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0612    |
|    value_loss           | 0.0529     |
----------------------------------------
Eval num_timesteps=35000, episode_reward=13.40 +/- 2.37
Episode length: 240.90 +/- 38.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 241       |
|    mean_reward          | 13.4      |
| time/                   |           |
|    total_timesteps      | 35000     |
| train/                  |           |
|    approx_kl            | 0.1473484 |
|    clip_fraction        | 0.436     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.75     |
|    explained_variance   | 0.742     |
|    learning_rate        | 0.00178   |
|    loss                 | -0.0509   |
|    n_updates            | 170       |
|    policy_gradient_loss | -0.057    |
|    value_loss           | 0.0581    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 7.97     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 18       |
|    time_elapsed    | 47       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 167        |
|    ep_rew_mean          | 7.95       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 19         |
|    time_elapsed         | 49         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.18078403 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.731     |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0567    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0453    |
|    value_loss           | 0.059      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=10.80 +/- 5.15
Episode length: 202.40 +/- 74.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | 10.8       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.16224003 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.727     |
|    explained_variance   | 0.795      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0759    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0467    |
|    value_loss           | 0.0463     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 8.08     |
| time/              |          |
|    fps             | 768      |
|    iterations      | 20       |
|    time_elapsed    | 53       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 164        |
|    ep_rew_mean          | 7.96       |
| time/                   |            |
|    fps                  | 774        |
|    iterations           | 21         |
|    time_elapsed         | 55         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.16367987 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.65      |
|    explained_variance   | 0.761      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.105     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0524    |
|    value_loss           | 0.055      |
----------------------------------------
Eval num_timesteps=45000, episode_reward=5.90 +/- 3.70
Episode length: 124.80 +/- 51.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 125        |
|    mean_reward          | 5.9        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.18830696 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.744     |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0815    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0599    |
|    value_loss           | 0.06       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 768      |
|    iterations      | 22       |
|    time_elapsed    | 58       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 153        |
|    ep_rew_mean          | 7.48       |
| time/                   |            |
|    fps                  | 772        |
|    iterations           | 23         |
|    time_elapsed         | 60         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.17103644 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.743     |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0694    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0557    |
|    value_loss           | 0.0535     |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 145      |
|    ep_rew_mean          | 7.08     |
| time/                   |          |
|    fps                  | 777      |
|    iterations           | 24       |
|    time_elapsed         | 63       |
|    total_timesteps      | 49152    |
| train/                  |          |
|    approx_kl            | 0.168478 |
|    clip_fraction        | 0.445    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.724   |
|    explained_variance   | 0.794    |
|    learning_rate        | 0.00178  |
|    loss                 | -0.0633  |
|    n_updates            | 230      |
|    policy_gradient_loss | -0.0551  |
|    value_loss           | 0.0575   |
--------------------------------------
Eval num_timesteps=50000, episode_reward=6.50 +/- 2.84
Episode length: 121.40 +/- 50.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 121        |
|    mean_reward          | 6.5        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.20048305 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.75      |
|    explained_variance   | 0.86       |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0904    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0621    |
|    value_loss           | 0.0468     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 6.75     |
| time/              |          |
|    fps             | 772      |
|    iterations      | 25       |
|    time_elapsed    | 66       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 135        |
|    ep_rew_mean          | 6.59       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 26         |
|    time_elapsed         | 68         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.25137973 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.757     |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0685    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0619    |
|    value_loss           | 0.0464     |
----------------------------------------
Eval num_timesteps=55000, episode_reward=9.10 +/- 2.95
Episode length: 184.70 +/- 44.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 185        |
|    mean_reward          | 9.1        |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.25062525 |
|    clip_fraction        | 0.494      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.749     |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0809    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0595    |
|    value_loss           | 0.0583     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 6.46     |
| time/              |          |
|    fps             | 769      |
|    iterations      | 27       |
|    time_elapsed    | 71       |
|    total_timesteps | 55296    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 134       |
|    ep_rew_mean          | 6.46      |
| time/                   |           |
|    fps                  | 773       |
|    iterations           | 28        |
|    time_elapsed         | 74        |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.2283974 |
|    clip_fraction        | 0.518     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.779    |
|    explained_variance   | 0.772     |
|    learning_rate        | 0.00178   |
|    loss                 | -0.097    |
|    n_updates            | 270       |
|    policy_gradient_loss | -0.0573   |
|    value_loss           | 0.0539    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 137        |
|    ep_rew_mean          | 6.63       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 29         |
|    time_elapsed         | 76         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.30794933 |
|    clip_fraction        | 0.546      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.785     |
|    explained_variance   | 0.687      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0552    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0586    |
|    value_loss           | 0.0581     |
----------------------------------------
Eval num_timesteps=60000, episode_reward=8.60 +/- 2.73
Episode length: 163.40 +/- 46.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 163        |
|    mean_reward          | 8.6        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.31383914 |
|    clip_fraction        | 0.538      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.739     |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.00178    |
|    loss                 | -0.0686    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0617    |
|    value_loss           | 0.0563     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 6.89     |
| time/              |          |
|    fps             | 771      |
|    iterations      | 30       |
|    time_elapsed    | 79       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 0.125    |
| time/              |          |
|    fps             | 1220     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 85.2         |
|    ep_rew_mean          | 0.271        |
| time/                   |              |
|    fps                  | 1031         |
|    iterations           | 2            |
|    time_elapsed         | 3            |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0138956625 |
|    clip_fraction        | 0.0708       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.38        |
|    explained_variance   | -0.00378     |
|    learning_rate        | 9.4e-05      |
|    loss                 | 0.0473       |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00606     |
|    value_loss           | 0.0562       |
------------------------------------------
Eval num_timesteps=5000, episode_reward=1.80 +/- 1.17
Episode length: 100.20 +/- 14.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012881543 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.324       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0113      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.0497      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.7     |
|    ep_rew_mean     | 0.691    |
| time/              |          |
|    fps             | 888      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 92.9         |
|    ep_rew_mean          | 0.943        |
| time/                   |              |
|    fps                  | 893          |
|    iterations           | 4            |
|    time_elapsed         | 9            |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0083470885 |
|    clip_fraction        | 0.0986       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.328        |
|    learning_rate        | 9.4e-05      |
|    loss                 | 0.00634      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0113      |
|    value_loss           | 0.0691       |
------------------------------------------
Eval num_timesteps=10000, episode_reward=3.20 +/- 1.54
Episode length: 133.10 +/- 37.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 133         |
|    mean_reward          | 3.2         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011798913 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.354       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0472      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.0745      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.6     |
|    ep_rew_mean     | 1.23     |
| time/              |          |
|    fps             | 830      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 101          |
|    ep_rew_mean          | 1.58         |
| time/                   |              |
|    fps                  | 842          |
|    iterations           | 6            |
|    time_elapsed         | 14           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0075244494 |
|    clip_fraction        | 0.0762       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.386        |
|    learning_rate        | 9.4e-05      |
|    loss                 | 0.0215       |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.0103      |
|    value_loss           | 0.0777       |
------------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 105          |
|    ep_rew_mean          | 2.02         |
| time/                   |              |
|    fps                  | 853          |
|    iterations           | 7            |
|    time_elapsed         | 16           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0048777917 |
|    clip_fraction        | 0.0939       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.458        |
|    learning_rate        | 9.4e-05      |
|    loss                 | 0.0227       |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.011       |
|    value_loss           | 0.0789       |
------------------------------------------
Eval num_timesteps=15000, episode_reward=4.90 +/- 2.30
Episode length: 157.40 +/- 55.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 157         |
|    mean_reward          | 4.9         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.007085845 |
|    clip_fraction        | 0.0786      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.54        |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.00292     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.0681      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.16     |
| time/              |          |
|    fps             | 817      |
|    iterations      | 8        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 111         |
|    ep_rew_mean          | 2.51        |
| time/                   |             |
|    fps                  | 826         |
|    iterations           | 9           |
|    time_elapsed         | 22          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.008986545 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.46        |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0243      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.076       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=9.80 +/- 3.84
Episode length: 239.90 +/- 74.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 240        |
|    mean_reward          | 9.8        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.00786987 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.989     |
|    explained_variance   | 0.479      |
|    learning_rate        | 9.4e-05    |
|    loss                 | 0.0115     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.0915     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 785      |
|    iterations      | 10       |
|    time_elapsed    | 26       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 115         |
|    ep_rew_mean          | 3.06        |
| time/                   |             |
|    fps                  | 794         |
|    iterations           | 11          |
|    time_elapsed         | 28          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.012919664 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.964      |
|    explained_variance   | 0.461       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.015       |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.0843      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 121        |
|    ep_rew_mean          | 3.56       |
| time/                   |            |
|    fps                  | 803        |
|    iterations           | 12         |
|    time_elapsed         | 30         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.01227776 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.908     |
|    explained_variance   | 0.467      |
|    learning_rate        | 9.4e-05    |
|    loss                 | 0.0284     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0185    |
|    value_loss           | 0.0908     |
----------------------------------------
Eval num_timesteps=25000, episode_reward=22.40 +/- 2.11
Episode length: 414.90 +/- 40.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 415         |
|    mean_reward          | 22.4        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.010037311 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.819      |
|    explained_variance   | 0.44        |
|    learning_rate        | 9.4e-05     |
|    loss                 | -0.0143     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.0994      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 4.1      |
| time/              |          |
|    fps             | 748      |
|    iterations      | 13       |
|    time_elapsed    | 35       |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 138          |
|    ep_rew_mean          | 4.69         |
| time/                   |              |
|    fps                  | 759          |
|    iterations           | 14           |
|    time_elapsed         | 37           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0104824295 |
|    clip_fraction        | 0.111        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.71        |
|    explained_variance   | 0.451        |
|    learning_rate        | 9.4e-05      |
|    loss                 | 0.000552     |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.016       |
|    value_loss           | 0.102        |
------------------------------------------
Eval num_timesteps=30000, episode_reward=18.80 +/- 2.82
Episode length: 340.20 +/- 48.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 340         |
|    mean_reward          | 18.8        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.011054559 |
|    clip_fraction        | 0.0983      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.617      |
|    explained_variance   | 0.421       |
|    learning_rate        | 9.4e-05     |
|    loss                 | -0.0053     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.0955      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 5.45     |
| time/              |          |
|    fps             | 727      |
|    iterations      | 15       |
|    time_elapsed    | 42       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | 6.23        |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 16          |
|    time_elapsed         | 44          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.015400381 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.376       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0638      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.106       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 168         |
|    ep_rew_mean          | 6.96        |
| time/                   |             |
|    fps                  | 745         |
|    iterations           | 17          |
|    time_elapsed         | 46          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.010926435 |
|    clip_fraction        | 0.0942      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.474      |
|    explained_variance   | 0.49        |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0347      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.0955      |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=20.90 +/- 1.64
Episode length: 381.60 +/- 39.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 382         |
|    mean_reward          | 20.9        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.009833053 |
|    clip_fraction        | 0.0753      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.389      |
|    explained_variance   | 0.387       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0153      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.0957      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 7.7      |
| time/              |          |
|    fps             | 716      |
|    iterations      | 18       |
|    time_elapsed    | 51       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 192         |
|    ep_rew_mean          | 8.59        |
| time/                   |             |
|    fps                  | 724         |
|    iterations           | 19          |
|    time_elapsed         | 53          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.013529297 |
|    clip_fraction        | 0.0947      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.415      |
|    explained_variance   | 0.49        |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0534      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.0933      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=18.50 +/- 6.30
Episode length: 324.50 +/- 102.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 324         |
|    mean_reward          | 18.5        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.008460052 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.306      |
|    explained_variance   | 0.444       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0317      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.0944      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 9.52     |
| time/              |          |
|    fps             | 705      |
|    iterations      | 20       |
|    time_elapsed    | 58       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 215         |
|    ep_rew_mean          | 10.3        |
| time/                   |             |
|    fps                  | 713         |
|    iterations           | 21          |
|    time_elapsed         | 60          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.008173663 |
|    clip_fraction        | 0.071       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.321      |
|    explained_variance   | 0.399       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0223      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.103       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=20.90 +/- 3.91
Episode length: 369.40 +/- 76.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 369         |
|    mean_reward          | 20.9        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.013444681 |
|    clip_fraction        | 0.0654      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.276      |
|    explained_variance   | 0.46        |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0223      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.101       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    fps             | 694      |
|    iterations      | 22       |
|    time_elapsed    | 64       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 237         |
|    ep_rew_mean          | 11.8        |
| time/                   |             |
|    fps                  | 701         |
|    iterations           | 23          |
|    time_elapsed         | 67          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.010627719 |
|    clip_fraction        | 0.0726      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.26       |
|    explained_variance   | 0.5         |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0194      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.0838      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 250         |
|    ep_rew_mean          | 12.7        |
| time/                   |             |
|    fps                  | 708         |
|    iterations           | 24          |
|    time_elapsed         | 69          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.009856572 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.252      |
|    explained_variance   | 0.521       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0219      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.0923      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=18.40 +/- 2.91
Episode length: 326.40 +/- 38.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 326         |
|    mean_reward          | 18.4        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.008783096 |
|    clip_fraction        | 0.0588      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.257      |
|    explained_variance   | 0.556       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0176      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.0926      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 257      |
|    ep_rew_mean     | 13.2     |
| time/              |          |
|    fps             | 694      |
|    iterations      | 25       |
|    time_elapsed    | 73       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 263         |
|    ep_rew_mean          | 13.7        |
| time/                   |             |
|    fps                  | 700         |
|    iterations           | 26          |
|    time_elapsed         | 75          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.010558085 |
|    clip_fraction        | 0.0808      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.301      |
|    explained_variance   | 0.522       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.00771     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.101       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=18.40 +/- 2.65
Episode length: 322.70 +/- 47.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 323         |
|    mean_reward          | 18.4        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.011653349 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.322      |
|    explained_variance   | 0.563       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0313      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.0903      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 268      |
|    ep_rew_mean     | 14.2     |
| time/              |          |
|    fps             | 688      |
|    iterations      | 27       |
|    time_elapsed    | 80       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 274         |
|    ep_rew_mean          | 14.7        |
| time/                   |             |
|    fps                  | 694         |
|    iterations           | 28          |
|    time_elapsed         | 82          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.012099995 |
|    clip_fraction        | 0.0707      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.258      |
|    explained_variance   | 0.562       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0127      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.101       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 279         |
|    ep_rew_mean          | 15.2        |
| time/                   |             |
|    fps                  | 700         |
|    iterations           | 29          |
|    time_elapsed         | 84          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.012654113 |
|    clip_fraction        | 0.086       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.275      |
|    explained_variance   | 0.617       |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0062      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.0831      |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=18.00 +/- 2.10
Episode length: 307.40 +/- 40.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 307         |
|    mean_reward          | 18          |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.009444994 |
|    clip_fraction        | 0.0727      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.274      |
|    explained_variance   | 0.64        |
|    learning_rate        | 9.4e-05     |
|    loss                 | 0.0404      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.0932      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 279      |
|    ep_rew_mean     | 15.3     |
| time/              |          |
|    fps             | 690      |
|    iterations      | 30       |
|    time_elapsed    | 89       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 0.0769   |
| time/              |          |
|    fps             | 1216     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 82.9        |
|    ep_rew_mean          | 0.388       |
| time/                   |             |
|    fps                  | 1045        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.008910142 |
|    clip_fraction        | 0.0564      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.0177      |
|    learning_rate        | 0.000148    |
|    loss                 | -0.0119     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00615    |
|    value_loss           | 0.0571      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=2.80 +/- 0.98
Episode length: 108.90 +/- 18.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 2.8         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.013490658 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.000116   |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.0481      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84       |
|    ep_rew_mean     | 0.389    |
| time/              |          |
|    fps             | 888      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 89.9        |
|    ep_rew_mean          | 0.846       |
| time/                   |             |
|    fps                  | 890         |
|    iterations           | 4           |
|    time_elapsed         | 9           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.011237875 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.449       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.00234    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.0455      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=3.80 +/- 1.17
Episode length: 115.70 +/- 22.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | 3.8         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011907651 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.394       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0204      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.0636      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.5     |
|    ep_rew_mean     | 1.28     |
| time/              |          |
|    fps             | 840      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | 1.83        |
| time/                   |             |
|    fps                  | 853         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.009175282 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.394       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.0114     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.0621      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | 2.37       |
| time/                   |            |
|    fps                  | 861        |
|    iterations           | 7          |
|    time_elapsed         | 16         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.01062801 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.13      |
|    explained_variance   | 0.361      |
|    learning_rate        | 0.000148   |
|    loss                 | 0.000996   |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0197    |
|    value_loss           | 0.0774     |
----------------------------------------
Eval num_timesteps=15000, episode_reward=16.60 +/- 4.34
Episode length: 301.00 +/- 69.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 301         |
|    mean_reward          | 16.6        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.015146984 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.0324     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.0718      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3.09     |
| time/              |          |
|    fps             | 784      |
|    iterations      | 8        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | 3.7         |
| time/                   |             |
|    fps                  | 798         |
|    iterations           | 9           |
|    time_elapsed         | 23          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.016823456 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.945      |
|    explained_variance   | 0.476       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.00768    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 0.0746      |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=17.70 +/- 2.24
Episode length: 304.90 +/- 39.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 305         |
|    mean_reward          | 17.7        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.017966643 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.853      |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.00693    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.0763      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 4.19     |
| time/              |          |
|    fps             | 747      |
|    iterations      | 10       |
|    time_elapsed    | 27       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | 4.58        |
| time/                   |             |
|    fps                  | 759         |
|    iterations           | 11          |
|    time_elapsed         | 29          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.016957557 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.842      |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.00121    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.026      |
|    value_loss           | 0.0827      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 134         |
|    ep_rew_mean          | 5.11        |
| time/                   |             |
|    fps                  | 768         |
|    iterations           | 12          |
|    time_elapsed         | 31          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.019070704 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.773      |
|    explained_variance   | 0.387       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.0224     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0288     |
|    value_loss           | 0.0814      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=13.90 +/- 3.01
Episode length: 247.70 +/- 56.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 248         |
|    mean_reward          | 13.9        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.018898448 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.762      |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.000148    |
|    loss                 | -0.0128     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0279     |
|    value_loss           | 0.0752      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 5.73     |
| time/              |          |
|    fps             | 742      |
|    iterations      | 13       |
|    time_elapsed    | 35       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 148         |
|    ep_rew_mean          | 6.27        |
| time/                   |             |
|    fps                  | 752         |
|    iterations           | 14          |
|    time_elapsed         | 38          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.015965441 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.711      |
|    explained_variance   | 0.481       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.0196     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.0848      |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=17.80 +/- 2.09
Episode length: 309.40 +/- 29.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 309         |
|    mean_reward          | 17.8        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.016485121 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.687      |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.000148    |
|    loss                 | -0.0302     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.0856      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 153      |
|    ep_rew_mean     | 6.67     |
| time/              |          |
|    fps             | 724      |
|    iterations      | 15       |
|    time_elapsed    | 42       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 162         |
|    ep_rew_mean          | 7.21        |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 16          |
|    time_elapsed         | 44          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.018160807 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.634      |
|    explained_variance   | 0.475       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.00417    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 0.079       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 168         |
|    ep_rew_mean          | 7.69        |
| time/                   |             |
|    fps                  | 743         |
|    iterations           | 17          |
|    time_elapsed         | 46          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.029059906 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.663      |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.00223     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.0773      |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=9.50 +/- 3.41
Episode length: 200.90 +/- 48.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 201        |
|    mean_reward          | 9.5        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.01956297 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.591     |
|    explained_variance   | 0.505      |
|    learning_rate        | 0.000148   |
|    loss                 | -0.0304    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0218    |
|    value_loss           | 0.0805     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 8.11     |
| time/              |          |
|    fps             | 731      |
|    iterations      | 18       |
|    time_elapsed    | 50       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 177         |
|    ep_rew_mean          | 8.39        |
| time/                   |             |
|    fps                  | 740         |
|    iterations           | 19          |
|    time_elapsed         | 52          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.026040081 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.636      |
|    explained_variance   | 0.586       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.00461    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0217     |
|    value_loss           | 0.0718      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=10.20 +/- 2.75
Episode length: 200.90 +/- 39.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 201         |
|    mean_reward          | 10.2        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.024794014 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.606      |
|    explained_variance   | 0.432       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.0265     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.079       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 8.63     |
| time/              |          |
|    fps             | 730      |
|    iterations      | 20       |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 186         |
|    ep_rew_mean          | 9.08        |
| time/                   |             |
|    fps                  | 737         |
|    iterations           | 21          |
|    time_elapsed         | 58          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.025235796 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.534       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.0106     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.0735      |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=10.20 +/- 3.84
Episode length: 214.30 +/- 62.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 214         |
|    mean_reward          | 10.2        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.025654107 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.572      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0146      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.0719      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 9.38     |
| time/              |          |
|    fps             | 728      |
|    iterations      | 22       |
|    time_elapsed    | 61       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 193         |
|    ep_rew_mean          | 9.56        |
| time/                   |             |
|    fps                  | 734         |
|    iterations           | 23          |
|    time_elapsed         | 64          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.023359135 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.572      |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.00507     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.0748      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 9.72       |
| time/                   |            |
|    fps                  | 740        |
|    iterations           | 24         |
|    time_elapsed         | 66         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.02044804 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.477     |
|    explained_variance   | 0.573      |
|    learning_rate        | 0.000148   |
|    loss                 | 0.0197     |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0191    |
|    value_loss           | 0.0747     |
----------------------------------------
Eval num_timesteps=50000, episode_reward=14.20 +/- 2.64
Episode length: 266.60 +/- 28.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 267         |
|    mean_reward          | 14.2        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.025962787 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.522      |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.000148    |
|    loss                 | -0.023      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.0628      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 9.96     |
| time/              |          |
|    fps             | 727      |
|    iterations      | 25       |
|    time_elapsed    | 70       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 202         |
|    ep_rew_mean          | 10.3        |
| time/                   |             |
|    fps                  | 732         |
|    iterations           | 26          |
|    time_elapsed         | 72          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.021419076 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.499      |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.00159     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.0833      |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=16.10 +/- 2.95
Episode length: 280.30 +/- 52.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 280         |
|    mean_reward          | 16.1        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.021459676 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.448      |
|    explained_variance   | 0.594       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.0211     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.0674      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 720      |
|    iterations      | 27       |
|    time_elapsed    | 76       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 209         |
|    ep_rew_mean          | 10.9        |
| time/                   |             |
|    fps                  | 726         |
|    iterations           | 28          |
|    time_elapsed         | 78          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.020643227 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.405      |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.0141      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.0731      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 215         |
|    ep_rew_mean          | 11.3        |
| time/                   |             |
|    fps                  | 731         |
|    iterations           | 29          |
|    time_elapsed         | 81          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.024130166 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.366      |
|    explained_variance   | 0.541       |
|    learning_rate        | 0.000148    |
|    loss                 | 0.00806     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.0651      |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=14.30 +/- 2.45
Episode length: 247.00 +/- 38.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 247         |
|    mean_reward          | 14.3        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.024715059 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.345      |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.000148    |
|    loss                 | -0.00054    |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.0774      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 11.7     |
| time/              |          |
|    fps             | 722      |
|    iterations      | 30       |
|    time_elapsed    | 85       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 0.115    |
| time/              |          |
|    fps             | 1210     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 79.3        |
|    ep_rew_mean          | 0.196       |
| time/                   |             |
|    fps                  | 1027        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.008121625 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.000314    |
|    learning_rate        | 0.000456    |
|    loss                 | 0.0158      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00932    |
|    value_loss           | 0.0557      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=0.60 +/- 0.66
Episode length: 78.30 +/- 11.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.3        |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010012734 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.000456    |
|    loss                 | -0.00718    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.0438      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.9     |
|    ep_rew_mean     | 0.276    |
| time/              |          |
|    fps             | 903      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 82          |
|    ep_rew_mean          | 0.414       |
| time/                   |             |
|    fps                  | 904         |
|    iterations           | 4           |
|    time_elapsed         | 9           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.016178697 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.403       |
|    learning_rate        | 0.000456    |
|    loss                 | -0.045      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 0.05        |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=2.00 +/- 1.41
Episode length: 96.30 +/- 20.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.3        |
|    mean_reward          | 2           |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.018849134 |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.509       |
|    learning_rate        | 0.000456    |
|    loss                 | -0.0446     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.05        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.2     |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 855      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 88.8        |
|    ep_rew_mean          | 1.18        |
| time/                   |             |
|    fps                  | 865         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.020065587 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.000456    |
|    loss                 | -0.0291     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0342     |
|    value_loss           | 0.0458      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 93.9        |
|    ep_rew_mean          | 1.68        |
| time/                   |             |
|    fps                  | 872         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.024189677 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.000456    |
|    loss                 | -0.0655     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0411     |
|    value_loss           | 0.0484      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=1.80 +/- 1.33
Episode length: 93.60 +/- 26.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 93.6       |
|    mean_reward          | 1.8        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.02750627 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.525      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.0363    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.0635     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | 2        |
| time/              |          |
|    fps             | 848      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 98.5        |
|    ep_rew_mean          | 2.29        |
| time/                   |             |
|    fps                  | 853         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.027768882 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.000456    |
|    loss                 | 0.0451      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0415     |
|    value_loss           | 0.0499      |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=6.90 +/- 1.37
Episode length: 156.00 +/- 21.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 6.9         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.028449224 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.000456    |
|    loss                 | -0.0288     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0359     |
|    value_loss           | 0.0511      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 2.5      |
| time/              |          |
|    fps             | 821      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 105        |
|    ep_rew_mean          | 2.86       |
| time/                   |            |
|    fps                  | 828        |
|    iterations           | 11         |
|    time_elapsed         | 27         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.03866475 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.19      |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.0264    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0432    |
|    value_loss           | 0.0561     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | 3.35        |
| time/                   |             |
|    fps                  | 832         |
|    iterations           | 12          |
|    time_elapsed         | 29          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.041301753 |
|    clip_fraction        | 0.379       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.000456    |
|    loss                 | -0.0733     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0518     |
|    value_loss           | 0.0584      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=4.30 +/- 4.00
Episode length: 112.40 +/- 59.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 4.3         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.055504493 |
|    clip_fraction        | 0.374       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.000456    |
|    loss                 | -0.0674     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0479     |
|    value_loss           | 0.0699      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.83     |
| time/              |          |
|    fps             | 818      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 124        |
|    ep_rew_mean          | 4.39       |
| time/                   |            |
|    fps                  | 823        |
|    iterations           | 14         |
|    time_elapsed         | 34         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.05221367 |
|    clip_fraction        | 0.374      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.06      |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.0657    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0508    |
|    value_loss           | 0.0621     |
----------------------------------------
Eval num_timesteps=30000, episode_reward=10.40 +/- 1.62
Episode length: 194.80 +/- 36.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 195        |
|    mean_reward          | 10.4       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.06502322 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.957     |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.000456   |
|    loss                 | 0.0146     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0395    |
|    value_loss           | 0.0757     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 5.07     |
| time/              |          |
|    fps             | 800      |
|    iterations      | 15       |
|    time_elapsed    | 38       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 141        |
|    ep_rew_mean          | 5.81       |
| time/                   |            |
|    fps                  | 806        |
|    iterations           | 16         |
|    time_elapsed         | 40         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.06988324 |
|    clip_fraction        | 0.389      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.94      |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.0608    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0467    |
|    value_loss           | 0.0678     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 146        |
|    ep_rew_mean          | 6.37       |
| time/                   |            |
|    fps                  | 810        |
|    iterations           | 17         |
|    time_elapsed         | 42         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.08119201 |
|    clip_fraction        | 0.396      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.896     |
|    explained_variance   | 0.711      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.045     |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0335    |
|    value_loss           | 0.0637     |
----------------------------------------
Eval num_timesteps=35000, episode_reward=7.90 +/- 2.95
Episode length: 168.40 +/- 52.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 168         |
|    mean_reward          | 7.9         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.107947856 |
|    clip_fraction        | 0.392       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.807      |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.000456    |
|    loss                 | 0.0115      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0371     |
|    value_loss           | 0.067       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 6.8      |
| time/              |          |
|    fps             | 795      |
|    iterations      | 18       |
|    time_elapsed    | 46       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 154        |
|    ep_rew_mean          | 7.11       |
| time/                   |            |
|    fps                  | 800        |
|    iterations           | 19         |
|    time_elapsed         | 48         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.07419127 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.829     |
|    explained_variance   | 0.549      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.0325    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0358    |
|    value_loss           | 0.0812     |
----------------------------------------
Eval num_timesteps=40000, episode_reward=11.60 +/- 3.14
Episode length: 202.70 +/- 48.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 203        |
|    mean_reward          | 11.6       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.07917158 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.778     |
|    explained_variance   | 0.693      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.0538    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0371    |
|    value_loss           | 0.0682     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 7.58     |
| time/              |          |
|    fps             | 784      |
|    iterations      | 20       |
|    time_elapsed    | 52       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 164        |
|    ep_rew_mean          | 7.91       |
| time/                   |            |
|    fps                  | 789        |
|    iterations           | 21         |
|    time_elapsed         | 54         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.08175653 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.748     |
|    explained_variance   | 0.701      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.0274    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.038     |
|    value_loss           | 0.0678     |
----------------------------------------
Eval num_timesteps=45000, episode_reward=9.60 +/- 2.33
Episode length: 161.30 +/- 31.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 161        |
|    mean_reward          | 9.6        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.10136637 |
|    clip_fraction        | 0.381      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.725     |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.0442    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0344    |
|    value_loss           | 0.0651     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 8.22     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 22       |
|    time_elapsed    | 57       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | 8.34       |
| time/                   |            |
|    fps                  | 784        |
|    iterations           | 23         |
|    time_elapsed         | 60         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.09724173 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.696     |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.000456   |
|    loss                 | 0.0226     |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0326    |
|    value_loss           | 0.0761     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | 8.44        |
| time/                   |             |
|    fps                  | 788         |
|    iterations           | 24          |
|    time_elapsed         | 62          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.082610875 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.64       |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.000456    |
|    loss                 | 0.00532     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0326     |
|    value_loss           | 0.0655      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=9.90 +/- 4.23
Episode length: 181.50 +/- 75.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 182        |
|    mean_reward          | 9.9        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.07398939 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.66      |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.016     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0366    |
|    value_loss           | 0.0646     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 8.61     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 25       |
|    time_elapsed    | 65       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 170         |
|    ep_rew_mean          | 8.89        |
| time/                   |             |
|    fps                  | 782         |
|    iterations           | 26          |
|    time_elapsed         | 68          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.079952985 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.000456    |
|    loss                 | -0.0551     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.036      |
|    value_loss           | 0.0646      |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=11.10 +/- 3.70
Episode length: 190.00 +/- 49.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 190        |
|    mean_reward          | 11.1       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.07828196 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.678     |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.026     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0326    |
|    value_loss           | 0.0618     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 9.27     |
| time/              |          |
|    fps             | 772      |
|    iterations      | 27       |
|    time_elapsed    | 71       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 172        |
|    ep_rew_mean          | 9.15       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 28         |
|    time_elapsed         | 73         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.07960451 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.628     |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.0417    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0346    |
|    value_loss           | 0.0705     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 175        |
|    ep_rew_mean          | 9.42       |
| time/                   |            |
|    fps                  | 781        |
|    iterations           | 29         |
|    time_elapsed         | 76         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.08113262 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.633     |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.000456   |
|    loss                 | -0.00826   |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0307    |
|    value_loss           | 0.0729     |
----------------------------------------
Eval num_timesteps=60000, episode_reward=13.10 +/- 2.77
Episode length: 217.70 +/- 49.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 218        |
|    mean_reward          | 13.1       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.09310725 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.518     |
|    explained_variance   | 0.735      |
|    learning_rate        | 0.000456   |
|    loss                 | -0.0424    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0366    |
|    value_loss           | 0.0623     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 9.69     |
| time/              |          |
|    fps             | 770      |
|    iterations      | 30       |
|    time_elapsed    | 79       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.4     |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 1225     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 85.7        |
|    ep_rew_mean          | 0.511       |
| time/                   |             |
|    fps                  | 1034        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.015119792 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.11       |
|    learning_rate        | 4.13e-05    |
|    loss                 | -0.0342     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00848    |
|    value_loss           | 0.0443      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=0.60 +/- 0.66
Episode length: 76.80 +/- 16.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.8        |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010098724 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.294       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0153      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00699    |
|    value_loss           | 0.0439      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.7     |
|    ep_rew_mean     | 0.629    |
| time/              |          |
|    fps             | 916      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 91.4        |
|    ep_rew_mean          | 0.854       |
| time/                   |             |
|    fps                  | 916         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.012434367 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.299       |
|    learning_rate        | 4.13e-05    |
|    loss                 | -0.0293     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.0574      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=3.30 +/- 1.19
Episode length: 126.10 +/- 20.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 3.3         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.009674989 |
|    clip_fraction        | 0.098       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.365       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0261      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00756    |
|    value_loss           | 0.0557      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.6     |
|    ep_rew_mean     | 1.1      |
| time/              |          |
|    fps             | 849      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | 1.44        |
| time/                   |             |
|    fps                  | 857         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.012726506 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.446       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0259      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00617    |
|    value_loss           | 0.0616      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 1.88        |
| time/                   |             |
|    fps                  | 864         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.008437932 |
|    clip_fraction        | 0.0589      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.424       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0118      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00423    |
|    value_loss           | 0.0687      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=3.20 +/- 0.87
Episode length: 108.40 +/- 16.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 108          |
|    mean_reward          | 3.2          |
| time/                   |              |
|    total_timesteps      | 15000        |
| train/                  |              |
|    approx_kl            | 0.0071030385 |
|    clip_fraction        | 0.0598       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.1         |
|    explained_variance   | 0.523        |
|    learning_rate        | 4.13e-05     |
|    loss                 | 0.0169       |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00548     |
|    value_loss           | 0.0679       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.11     |
| time/              |          |
|    fps             | 839      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 109         |
|    ep_rew_mean          | 2.41        |
| time/                   |             |
|    fps                  | 848         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.012064411 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.516       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0403      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00817    |
|    value_loss           | 0.0615      |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=3.50 +/- 1.20
Episode length: 124.30 +/- 19.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 124         |
|    mean_reward          | 3.5         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.008362027 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.974      |
|    explained_variance   | 0.453       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0476      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00765    |
|    value_loss           | 0.0773      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.55     |
| time/              |          |
|    fps             | 826      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 111         |
|    ep_rew_mean          | 2.7         |
| time/                   |             |
|    fps                  | 833         |
|    iterations           | 11          |
|    time_elapsed         | 27          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.008489562 |
|    clip_fraction        | 0.0979      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.927      |
|    explained_variance   | 0.6         |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.00309     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00759    |
|    value_loss           | 0.0589      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 111          |
|    ep_rew_mean          | 2.74         |
| time/                   |              |
|    fps                  | 838          |
|    iterations           | 12           |
|    time_elapsed         | 29           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0067068315 |
|    clip_fraction        | 0.0581       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.925       |
|    explained_variance   | 0.569        |
|    learning_rate        | 4.13e-05     |
|    loss                 | 0.0591       |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00587     |
|    value_loss           | 0.0683       |
------------------------------------------
Eval num_timesteps=25000, episode_reward=3.50 +/- 0.92
Episode length: 110.10 +/- 20.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 3.5         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.005210637 |
|    clip_fraction        | 0.0809      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.88       |
|    explained_variance   | 0.473       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.026       |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00433    |
|    value_loss           | 0.0707      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 824      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 115          |
|    ep_rew_mean          | 3.08         |
| time/                   |              |
|    fps                  | 829          |
|    iterations           | 14           |
|    time_elapsed         | 34           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0027698642 |
|    clip_fraction        | 0.0403       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.834       |
|    explained_variance   | 0.561        |
|    learning_rate        | 4.13e-05     |
|    loss                 | 0.024        |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00333     |
|    value_loss           | 0.0724       |
------------------------------------------
Eval num_timesteps=30000, episode_reward=3.50 +/- 1.28
Episode length: 113.00 +/- 21.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 113         |
|    mean_reward          | 3.5         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.008438213 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.592       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0305      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00185    |
|    value_loss           | 0.0676      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 3.2      |
| time/              |          |
|    fps             | 817      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 116         |
|    ep_rew_mean          | 3.41        |
| time/                   |             |
|    fps                  | 821         |
|    iterations           | 16          |
|    time_elapsed         | 39          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.006495118 |
|    clip_fraction        | 0.0705      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.8        |
|    explained_variance   | 0.584       |
|    learning_rate        | 4.13e-05    |
|    loss                 | -0.00541    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00716    |
|    value_loss           | 0.0697      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | 3.53        |
| time/                   |             |
|    fps                  | 825         |
|    iterations           | 17          |
|    time_elapsed         | 42          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.006950576 |
|    clip_fraction        | 0.0779      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.742      |
|    explained_variance   | 0.482       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0322      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00805    |
|    value_loss           | 0.0903      |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=5.70 +/- 2.15
Episode length: 143.10 +/- 35.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | 5.7         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.011275256 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.703      |
|    explained_variance   | 0.52        |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.00623     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0075     |
|    value_loss           | 0.0818      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 3.74     |
| time/              |          |
|    fps             | 812      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 3.82         |
| time/                   |              |
|    fps                  | 816          |
|    iterations           | 19           |
|    time_elapsed         | 47           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0066858074 |
|    clip_fraction        | 0.089        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.662       |
|    explained_variance   | 0.503        |
|    learning_rate        | 4.13e-05     |
|    loss                 | 0.0121       |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00862     |
|    value_loss           | 0.0783       |
------------------------------------------
Eval num_timesteps=40000, episode_reward=8.40 +/- 2.50
Episode length: 178.50 +/- 50.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 178         |
|    mean_reward          | 8.4         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.007049628 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.68       |
|    explained_variance   | 0.402       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0254      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00824    |
|    value_loss           | 0.0794      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 4.15     |
| time/              |          |
|    fps             | 801      |
|    iterations      | 20       |
|    time_elapsed    | 51       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 127         |
|    ep_rew_mean          | 4.48        |
| time/                   |             |
|    fps                  | 806         |
|    iterations           | 21          |
|    time_elapsed         | 53          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.005804993 |
|    clip_fraction        | 0.0624      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.64       |
|    explained_variance   | 0.426       |
|    learning_rate        | 4.13e-05    |
|    loss                 | -0.00387    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00627    |
|    value_loss           | 0.081       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=10.60 +/- 1.96
Episode length: 217.10 +/- 32.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 217         |
|    mean_reward          | 10.6        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.010920409 |
|    clip_fraction        | 0.0848      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.582      |
|    explained_variance   | 0.431       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0259      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00684    |
|    value_loss           | 0.0864      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 4.78     |
| time/              |          |
|    fps             | 789      |
|    iterations      | 22       |
|    time_elapsed    | 57       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 134         |
|    ep_rew_mean          | 4.95        |
| time/                   |             |
|    fps                  | 793         |
|    iterations           | 23          |
|    time_elapsed         | 59          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.004245002 |
|    clip_fraction        | 0.0725      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.585      |
|    explained_variance   | 0.433       |
|    learning_rate        | 4.13e-05    |
|    loss                 | -0.00579    |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00526    |
|    value_loss           | 0.0849      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 136          |
|    ep_rew_mean          | 5.19         |
| time/                   |              |
|    fps                  | 798          |
|    iterations           | 24           |
|    time_elapsed         | 61           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0045504672 |
|    clip_fraction        | 0.0645       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.556       |
|    explained_variance   | 0.47         |
|    learning_rate        | 4.13e-05     |
|    loss                 | 0.0167       |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00663     |
|    value_loss           | 0.0795       |
------------------------------------------
Eval num_timesteps=50000, episode_reward=12.40 +/- 2.29
Episode length: 243.90 +/- 41.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 244         |
|    mean_reward          | 12.4        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.005498426 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.532      |
|    explained_variance   | 0.527       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0316      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00661    |
|    value_loss           | 0.0729      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 5.39     |
| time/              |          |
|    fps             | 782      |
|    iterations      | 25       |
|    time_elapsed    | 65       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 141        |
|    ep_rew_mean          | 5.49       |
| time/                   |            |
|    fps                  | 786        |
|    iterations           | 26         |
|    time_elapsed         | 67         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.00611095 |
|    clip_fraction        | 0.0876     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.532     |
|    explained_variance   | 0.453      |
|    learning_rate        | 4.13e-05   |
|    loss                 | 0.0182     |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0074    |
|    value_loss           | 0.0822     |
----------------------------------------
Eval num_timesteps=55000, episode_reward=12.70 +/- 3.03
Episode length: 246.20 +/- 55.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 246         |
|    mean_reward          | 12.7        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.007388023 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.512      |
|    explained_variance   | 0.471       |
|    learning_rate        | 4.13e-05    |
|    loss                 | 0.0181      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00497    |
|    value_loss           | 0.0808      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 5.68     |
| time/              |          |
|    fps             | 772      |
|    iterations      | 27       |
|    time_elapsed    | 71       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 145        |
|    ep_rew_mean          | 5.77       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 28         |
|    time_elapsed         | 73         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.00517629 |
|    clip_fraction        | 0.0691     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.498     |
|    explained_variance   | 0.503      |
|    learning_rate        | 4.13e-05   |
|    loss                 | 0.0423     |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.00447   |
|    value_loss           | 0.0829     |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 147          |
|    ep_rew_mean          | 5.94         |
| time/                   |              |
|    fps                  | 779          |
|    iterations           | 29           |
|    time_elapsed         | 76           |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0065582385 |
|    clip_fraction        | 0.095        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.485       |
|    explained_variance   | 0.497        |
|    learning_rate        | 4.13e-05     |
|    loss                 | 0.0368       |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00624     |
|    value_loss           | 0.0743       |
------------------------------------------
Eval num_timesteps=60000, episode_reward=8.80 +/- 2.75
Episode length: 192.60 +/- 55.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 193        |
|    mean_reward          | 8.8        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.00890727 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.464     |
|    explained_variance   | 0.591      |
|    learning_rate        | 4.13e-05   |
|    loss                 | 0.0424     |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.00495   |
|    value_loss           | 0.0751     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 6.1      |
| time/              |          |
|    fps             | 770      |
|    iterations      | 30       |
|    time_elapsed    | 79       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=-1.00 +/- 0.00
Episode length: 74.60 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.6     |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.4     |
|    ep_rew_mean     | 0.224    |
| time/              |          |
|    fps             | 1146     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.40 +/- 0.66
Episode length: 70.00 +/- 13.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 70          |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.007498767 |
|    clip_fraction        | 0.0592      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0734     |
|    learning_rate        | 0.000147    |
|    loss                 | 0.015       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00665    |
|    value_loss           | 0.0686      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=0.30 +/- 0.46
Episode length: 75.80 +/- 7.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.8     |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.1     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 954      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=3.20 +/- 1.60
Episode length: 100.00 +/- 26.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 100        |
|    mean_reward          | 3.2        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.00981192 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.447      |
|    learning_rate        | 0.000147   |
|    loss                 | 0.00517    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0133    |
|    value_loss           | 0.0551     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.9     |
|    ep_rew_mean     | 0.9      |
| time/              |          |
|    fps             | 915      |
|    iterations      | 3        |
|    time_elapsed    | 26       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=5.80 +/- 2.04
Episode length: 157.20 +/- 47.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 157         |
|    mean_reward          | 5.8         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.012908749 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.614       |
|    learning_rate        | 0.000147    |
|    loss                 | -0.0345     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.025      |
|    value_loss           | 0.0547      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=5.30 +/- 2.33
Episode length: 140.70 +/- 45.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 141      |
|    mean_reward     | 5.3      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.15     |
| time/              |          |
|    fps             | 866      |
|    iterations      | 4        |
|    time_elapsed    | 37       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=6.20 +/- 3.31
Episode length: 144.60 +/- 60.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 145         |
|    mean_reward          | 6.2         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.013972196 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.000147    |
|    loss                 | -0.0197     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0279     |
|    value_loss           | 0.0735      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=8.00 +/- 1.55
Episode length: 184.80 +/- 35.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 185      |
|    mean_reward     | 8        |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.23     |
| time/              |          |
|    fps             | 834      |
|    iterations      | 5        |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=8.70 +/- 1.68
Episode length: 196.40 +/- 39.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 196         |
|    mean_reward          | 8.7         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.015725272 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.000147    |
|    loss                 | -0.0189     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.035      |
|    value_loss           | 0.0792      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 4.4      |
| time/              |          |
|    fps             | 828      |
|    iterations      | 6        |
|    time_elapsed    | 59       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=6.80 +/- 1.89
Episode length: 162.30 +/- 50.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 162         |
|    mean_reward          | 6.8         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.017598182 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.000147    |
|    loss                 | -0.0152     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0344     |
|    value_loss           | 0.0806      |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=6.80 +/- 1.83
Episode length: 166.70 +/- 41.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 167      |
|    mean_reward     | 6.8      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 5.36     |
| time/              |          |
|    fps             | 813      |
|    iterations      | 7        |
|    time_elapsed    | 70       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=16.20 +/- 3.09
Episode length: 283.30 +/- 49.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 283         |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.020188678 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.000147    |
|    loss                 | -0.00727    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0313     |
|    value_loss           | 0.0846      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=65000, episode_reward=16.50 +/- 2.87
Episode length: 294.20 +/- 49.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 294      |
|    mean_reward     | 16.5     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 5.99     |
| time/              |          |
|    fps             | 786      |
|    iterations      | 8        |
|    time_elapsed    | 83       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.3     |
|    ep_rew_mean     | 0.04     |
| time/              |          |
|    fps             | 1237     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 82.6        |
|    ep_rew_mean          | 0.306       |
| time/                   |             |
|    fps                  | 1047        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009642595 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0276     |
|    learning_rate        | 0.000845    |
|    loss                 | 0.00175     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.0626      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=1.30 +/- 1.35
Episode length: 76.70 +/- 19.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.7        |
|    mean_reward          | 1.3         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012972778 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.000845    |
|    loss                 | -0.00914    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0611      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.8     |
|    ep_rew_mean     | 0.427    |
| time/              |          |
|    fps             | 926      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 82.7        |
|    ep_rew_mean          | 0.566       |
| time/                   |             |
|    fps                  | 923         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.018545534 |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.000845    |
|    loss                 | 0.035       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.0693      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=1.60 +/- 2.29
Episode length: 89.60 +/- 23.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 89.6        |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.030756693 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.000845    |
|    loss                 | -0.0513     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.033      |
|    value_loss           | 0.0693      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.4     |
|    ep_rew_mean     | 0.77     |
| time/              |          |
|    fps             | 876      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 83.3       |
|    ep_rew_mean          | 0.88       |
| time/                   |            |
|    fps                  | 880        |
|    iterations           | 6          |
|    time_elapsed         | 13         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.02654357 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.527      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0412    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0295    |
|    value_loss           | 0.0508     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 88          |
|    ep_rew_mean          | 1.26        |
| time/                   |             |
|    fps                  | 884         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.023359813 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.000845    |
|    loss                 | -0.0164     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 0.0475      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=2.70 +/- 1.85
Episode length: 92.40 +/- 26.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 92.4       |
|    mean_reward          | 2.7        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.03679228 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | 0.384      |
|    learning_rate        | 0.000845   |
|    loss                 | 0.000919   |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.035     |
|    value_loss           | 0.0681     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.8     |
|    ep_rew_mean     | 1.73     |
| time/              |          |
|    fps             | 859      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 98.3        |
|    ep_rew_mean          | 2.11        |
| time/                   |             |
|    fps                  | 864         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.049275685 |
|    clip_fraction        | 0.352       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.453       |
|    learning_rate        | 0.000845    |
|    loss                 | -0.0299     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0428     |
|    value_loss           | 0.0776      |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=3.40 +/- 1.96
Episode length: 107.70 +/- 25.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 108        |
|    mean_reward          | 3.4        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.05010184 |
|    clip_fraction        | 0.37       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0149    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.037     |
|    value_loss           | 0.0654     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 842      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 112         |
|    ep_rew_mean          | 3.31        |
| time/                   |             |
|    fps                  | 848         |
|    iterations           | 11          |
|    time_elapsed         | 26          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.057539795 |
|    clip_fraction        | 0.413       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.505       |
|    learning_rate        | 0.000845    |
|    loss                 | -0.0517     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0408     |
|    value_loss           | 0.0654      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 3.94        |
| time/                   |             |
|    fps                  | 852         |
|    iterations           | 12          |
|    time_elapsed         | 28          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.058363862 |
|    clip_fraction        | 0.387       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.000845    |
|    loss                 | -0.0515     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0427     |
|    value_loss           | 0.0641      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=7.70 +/- 2.05
Episode length: 164.90 +/- 36.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 165        |
|    mean_reward          | 7.7        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.06417647 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.998     |
|    explained_variance   | 0.499      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0381    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0408    |
|    value_loss           | 0.0624     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 4.46     |
| time/              |          |
|    fps             | 827      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 133        |
|    ep_rew_mean          | 5.06       |
| time/                   |            |
|    fps                  | 832        |
|    iterations           | 14         |
|    time_elapsed         | 34         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.08333371 |
|    clip_fraction        | 0.385      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.924     |
|    explained_variance   | 0.468      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.027     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0442    |
|    value_loss           | 0.067      |
----------------------------------------
Eval num_timesteps=30000, episode_reward=6.50 +/- 2.33
Episode length: 145.50 +/- 24.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 146         |
|    mean_reward          | 6.5         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.078154445 |
|    clip_fraction        | 0.399       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.897      |
|    explained_variance   | 0.542       |
|    learning_rate        | 0.000845    |
|    loss                 | -0.0707     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0428     |
|    value_loss           | 0.0673      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 5.6      |
| time/              |          |
|    fps             | 816      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 146        |
|    ep_rew_mean          | 6.13       |
| time/                   |            |
|    fps                  | 821        |
|    iterations           | 16         |
|    time_elapsed         | 39         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.07024638 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.884     |
|    explained_variance   | 0.41       |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0385    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0387    |
|    value_loss           | 0.0681     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 154        |
|    ep_rew_mean          | 6.81       |
| time/                   |            |
|    fps                  | 825        |
|    iterations           | 17         |
|    time_elapsed         | 42         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.07473515 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.809     |
|    explained_variance   | 0.405      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0715    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0368    |
|    value_loss           | 0.0699     |
----------------------------------------
Eval num_timesteps=35000, episode_reward=13.90 +/- 2.30
Episode length: 251.80 +/- 38.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 252        |
|    mean_reward          | 13.9       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.07789865 |
|    clip_fraction        | 0.356      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.747     |
|    explained_variance   | 0.521      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0551    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0312    |
|    value_loss           | 0.0827     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 7.29     |
| time/              |          |
|    fps             | 800      |
|    iterations      | 18       |
|    time_elapsed    | 46       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | 7.79       |
| time/                   |            |
|    fps                  | 805        |
|    iterations           | 19         |
|    time_elapsed         | 48         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.10993922 |
|    clip_fraction        | 0.372      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.679     |
|    explained_variance   | 0.332      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0617    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0403    |
|    value_loss           | 0.0754     |
----------------------------------------
Eval num_timesteps=40000, episode_reward=15.00 +/- 3.90
Episode length: 270.20 +/- 61.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 270        |
|    mean_reward          | 15         |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.10447808 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.598     |
|    explained_variance   | 0.301      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0532    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0384    |
|    value_loss           | 0.0795     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 8.64     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 20       |
|    time_elapsed    | 52       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 9.15       |
| time/                   |            |
|    fps                  | 788        |
|    iterations           | 21         |
|    time_elapsed         | 54         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.09043359 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.565     |
|    explained_variance   | 0.474      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0575    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0384    |
|    value_loss           | 0.0747     |
----------------------------------------
Eval num_timesteps=45000, episode_reward=16.90 +/- 2.17
Episode length: 296.50 +/- 44.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 296        |
|    mean_reward          | 16.9       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.07778369 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.507     |
|    explained_variance   | 0.461      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0205    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0349    |
|    value_loss           | 0.0761     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 9.75     |
| time/              |          |
|    fps             | 767      |
|    iterations      | 22       |
|    time_elapsed    | 58       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 198        |
|    ep_rew_mean          | 10.2       |
| time/                   |            |
|    fps                  | 773        |
|    iterations           | 23         |
|    time_elapsed         | 60         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.11592214 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.447     |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.000845   |
|    loss                 | 0.00645    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0315    |
|    value_loss           | 0.0674     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 204         |
|    ep_rew_mean          | 10.7        |
| time/                   |             |
|    fps                  | 778         |
|    iterations           | 24          |
|    time_elapsed         | 63          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.118763864 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.494      |
|    explained_variance   | 0.558       |
|    learning_rate        | 0.000845    |
|    loss                 | -0.0408     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 0.0633      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=17.20 +/- 4.38
Episode length: 296.00 +/- 67.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 296       |
|    mean_reward          | 17.2      |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.1256857 |
|    clip_fraction        | 0.315     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.479    |
|    explained_variance   | 0.52      |
|    learning_rate        | 0.000845  |
|    loss                 | -0.0223   |
|    n_updates            | 240       |
|    policy_gradient_loss | -0.0399   |
|    value_loss           | 0.0668    |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 11.2     |
| time/              |          |
|    fps             | 760      |
|    iterations      | 25       |
|    time_elapsed    | 67       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 220        |
|    ep_rew_mean          | 11.9       |
| time/                   |            |
|    fps                  | 766        |
|    iterations           | 26         |
|    time_elapsed         | 69         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.11257847 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.446     |
|    explained_variance   | 0.522      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.00116   |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0325    |
|    value_loss           | 0.0678     |
----------------------------------------
Eval num_timesteps=55000, episode_reward=15.90 +/- 4.25
Episode length: 271.70 +/- 58.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 272        |
|    mean_reward          | 15.9       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.10879469 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.571     |
|    explained_variance   | 0.573      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0586    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0424    |
|    value_loss           | 0.0759     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 12.4     |
| time/              |          |
|    fps             | 752      |
|    iterations      | 27       |
|    time_elapsed    | 73       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 12.7       |
| time/                   |            |
|    fps                  | 758        |
|    iterations           | 28         |
|    time_elapsed         | 75         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.13396752 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.584     |
|    explained_variance   | 0.508      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0493    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0421    |
|    value_loss           | 0.0713     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 12.8       |
| time/                   |            |
|    fps                  | 762        |
|    iterations           | 29         |
|    time_elapsed         | 77         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.13379955 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.626     |
|    explained_variance   | 0.586      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0601    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.0594     |
----------------------------------------
Eval num_timesteps=60000, episode_reward=17.50 +/- 1.43
Episode length: 283.50 +/- 21.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 284        |
|    mean_reward          | 17.5       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.13706261 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.625     |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.000845   |
|    loss                 | -0.0722    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.067      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 13       |
| time/              |          |
|    fps             | 749      |
|    iterations      | 30       |
|    time_elapsed    | 81       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.1     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 1226     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 82.8        |
|    ep_rew_mean          | 0.306       |
| time/                   |             |
|    fps                  | 1043        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.008874567 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.011      |
|    learning_rate        | 0.000256    |
|    loss                 | -0.00193    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.0588      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=2.30 +/- 1.19
Episode length: 103.60 +/- 12.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 2.3         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010874695 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.394       |
|    learning_rate        | 0.000256    |
|    loss                 | 0.000485    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.0437      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.3     |
|    ep_rew_mean     | 0.514    |
| time/              |          |
|    fps             | 894      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 90.2         |
|    ep_rew_mean          | 0.811        |
| time/                   |              |
|    fps                  | 897          |
|    iterations           | 4            |
|    time_elapsed         | 9            |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0140773915 |
|    clip_fraction        | 0.179        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.34        |
|    explained_variance   | 0.583        |
|    learning_rate        | 0.000256     |
|    loss                 | -0.0411      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0315      |
|    value_loss           | 0.0375       |
------------------------------------------
Eval num_timesteps=10000, episode_reward=7.10 +/- 2.55
Episode length: 159.20 +/- 39.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 159         |
|    mean_reward          | 7.1         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.015882894 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.0412     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0328     |
|    value_loss           | 0.0555      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94       |
|    ep_rew_mean     | 1.13     |
| time/              |          |
|    fps             | 822      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 100        |
|    ep_rew_mean          | 1.72       |
| time/                   |            |
|    fps                  | 834        |
|    iterations           | 6          |
|    time_elapsed         | 14         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.01702666 |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.28      |
|    explained_variance   | 0.578      |
|    learning_rate        | 0.000256   |
|    loss                 | -0.0136    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.037     |
|    value_loss           | 0.0557     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | 2.36       |
| time/                   |            |
|    fps                  | 843        |
|    iterations           | 7          |
|    time_elapsed         | 16         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.01937909 |
|    clip_fraction        | 0.262      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.000256   |
|    loss                 | -0.0252    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0409    |
|    value_loss           | 0.0592     |
----------------------------------------
Eval num_timesteps=15000, episode_reward=3.10 +/- 1.81
Episode length: 106.20 +/- 36.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 106         |
|    mean_reward          | 3.1         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.021183996 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.0459     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0401     |
|    value_loss           | 0.059       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3.01     |
| time/              |          |
|    fps             | 821      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 116        |
|    ep_rew_mean          | 3.61       |
| time/                   |            |
|    fps                  | 831        |
|    iterations           | 9          |
|    time_elapsed         | 22         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.02455109 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.16      |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.000256   |
|    loss                 | -0.0404    |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0395    |
|    value_loss           | 0.0656     |
----------------------------------------
Eval num_timesteps=20000, episode_reward=2.80 +/- 1.08
Episode length: 114.40 +/- 17.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 2.8         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.028403994 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.0285     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0409     |
|    value_loss           | 0.0693      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 4.25     |
| time/              |          |
|    fps             | 814      |
|    iterations      | 10       |
|    time_elapsed    | 25       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 129         |
|    ep_rew_mean          | 4.82        |
| time/                   |             |
|    fps                  | 823         |
|    iterations           | 11          |
|    time_elapsed         | 27          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.027075507 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.0189     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0442     |
|    value_loss           | 0.0669      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 138         |
|    ep_rew_mean          | 5.54        |
| time/                   |             |
|    fps                  | 831         |
|    iterations           | 12          |
|    time_elapsed         | 29          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.037309945 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.0288     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0419     |
|    value_loss           | 0.0716      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=4.70 +/- 2.28
Episode length: 126.10 +/- 35.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 4.7         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.032308213 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.987      |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.0466     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0395     |
|    value_loss           | 0.0642      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 5.96     |
| time/              |          |
|    fps             | 816      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 146       |
|    ep_rew_mean          | 6.38      |
| time/                   |           |
|    fps                  | 822       |
|    iterations           | 14        |
|    time_elapsed         | 34        |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.0371572 |
|    clip_fraction        | 0.29      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.921    |
|    explained_variance   | 0.694     |
|    learning_rate        | 0.000256  |
|    loss                 | -0.0372   |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.0388   |
|    value_loss           | 0.0764    |
---------------------------------------
Eval num_timesteps=30000, episode_reward=7.80 +/- 2.75
Episode length: 158.40 +/- 36.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 158         |
|    mean_reward          | 7.8         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.036979854 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.923      |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.000256    |
|    loss                 | 0.000129    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0327     |
|    value_loss           | 0.0756      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 6.93     |
| time/              |          |
|    fps             | 804      |
|    iterations      | 15       |
|    time_elapsed    | 38       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | 7.32        |
| time/                   |             |
|    fps                  | 809         |
|    iterations           | 16          |
|    time_elapsed         | 40          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.041393064 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.877      |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.0313     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.0756      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 165         |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 815         |
|    iterations           | 17          |
|    time_elapsed         | 42          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.045734312 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.896      |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.00567    |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0381     |
|    value_loss           | 0.0728      |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=11.40 +/- 2.37
Episode length: 202.70 +/- 39.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 203         |
|    mean_reward          | 11.4        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.046602294 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.0115     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0312     |
|    value_loss           | 0.079       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 8.31     |
| time/              |          |
|    fps             | 797      |
|    iterations      | 18       |
|    time_elapsed    | 46       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 174         |
|    ep_rew_mean          | 8.75        |
| time/                   |             |
|    fps                  | 802         |
|    iterations           | 19          |
|    time_elapsed         | 48          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.049050815 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.76       |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.0126     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 0.0743      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=13.00 +/- 3.13
Episode length: 227.70 +/- 33.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 228         |
|    mean_reward          | 13          |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.049026117 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.0239     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0295     |
|    value_loss           | 0.0833      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 9.16     |
| time/              |          |
|    fps             | 784      |
|    iterations      | 20       |
|    time_elapsed    | 52       |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 185        |
|    ep_rew_mean          | 9.64       |
| time/                   |            |
|    fps                  | 789        |
|    iterations           | 21         |
|    time_elapsed         | 54         |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.04531482 |
|    clip_fraction        | 0.264      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.698     |
|    explained_variance   | 0.764      |
|    learning_rate        | 0.000256   |
|    loss                 | 0.0138     |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0262    |
|    value_loss           | 0.0715     |
----------------------------------------
Eval num_timesteps=45000, episode_reward=9.90 +/- 4.55
Episode length: 180.20 +/- 66.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 180        |
|    mean_reward          | 9.9        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.04828808 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.655     |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.000256   |
|    loss                 | -0.0622    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0304    |
|    value_loss           | 0.0741     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 22       |
|    time_elapsed    | 57       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 202        |
|    ep_rew_mean          | 10.9       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 23         |
|    time_elapsed         | 60         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.05664523 |
|    clip_fraction        | 0.246      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.56      |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.000256   |
|    loss                 | -0.0255    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0277    |
|    value_loss           | 0.0672     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 210        |
|    ep_rew_mean          | 11.4       |
| time/                   |            |
|    fps                  | 787        |
|    iterations           | 24         |
|    time_elapsed         | 62         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.04619412 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.596     |
|    explained_variance   | 0.802      |
|    learning_rate        | 0.000256   |
|    loss                 | -0.0318    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0196    |
|    value_loss           | 0.081      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=11.40 +/- 3.26
Episode length: 207.10 +/- 50.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 207         |
|    mean_reward          | 11.4        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.053703003 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.533      |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.0136     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0224     |
|    value_loss           | 0.0761      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 25       |
|    time_elapsed    | 66       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 216         |
|    ep_rew_mean          | 12          |
| time/                   |             |
|    fps                  | 779         |
|    iterations           | 26          |
|    time_elapsed         | 68          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.053171836 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.562      |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.0328     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.0853      |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=14.70 +/- 2.49
Episode length: 249.40 +/- 43.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 249        |
|    mean_reward          | 14.7       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.12088236 |
|    clip_fraction        | 0.228      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.515     |
|    explained_variance   | 0.777      |
|    learning_rate        | 0.000256   |
|    loss                 | -0.0187    |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0257    |
|    value_loss           | 0.0898     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 222      |
|    ep_rew_mean     | 12.4     |
| time/              |          |
|    fps             | 766      |
|    iterations      | 27       |
|    time_elapsed    | 72       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 226         |
|    ep_rew_mean          | 12.7        |
| time/                   |             |
|    fps                  | 770         |
|    iterations           | 28          |
|    time_elapsed         | 74          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.054064877 |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.487      |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.00638    |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.0768      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 13.1       |
| time/                   |            |
|    fps                  | 774        |
|    iterations           | 29         |
|    time_elapsed         | 76         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.07415601 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.506     |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.000256   |
|    loss                 | -0.0172    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0268    |
|    value_loss           | 0.0827     |
----------------------------------------
Eval num_timesteps=60000, episode_reward=15.20 +/- 3.22
Episode length: 249.00 +/- 44.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 249         |
|    mean_reward          | 15.2        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.057363212 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.454      |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.000256    |
|    loss                 | -0.028      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.0722      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 13.4     |
| time/              |          |
|    fps             | 762      |
|    iterations      | 30       |
|    time_elapsed    | 80       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.7     |
|    ep_rew_mean     | 0.0784   |
| time/              |          |
|    fps             | 1230     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=3.50 +/- 2.91
Episode length: 122.40 +/- 45.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 122         |
|    mean_reward          | 3.5         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010614994 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.000944   |
|    learning_rate        | 0.000109    |
|    loss                 | 0.0165      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0033     |
|    value_loss           | 0.0985      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.2     |
|    ep_rew_mean     | 0.222    |
| time/              |          |
|    fps             | 1002     |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=2.90 +/- 1.14
Episode length: 117.40 +/- 11.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 117         |
|    mean_reward          | 2.9         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.014247155 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.39        |
|    learning_rate        | 0.000109    |
|    loss                 | 0.0357      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.0828      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.1     |
|    ep_rew_mean     | 0.92     |
| time/              |          |
|    fps             | 949      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=4.10 +/- 2.17
Episode length: 142.30 +/- 45.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 142         |
|    mean_reward          | 4.1         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.010997208 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.000109    |
|    loss                 | 0.0212      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.105       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.8     |
|    ep_rew_mean     | 1.55     |
| time/              |          |
|    fps             | 917      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=5.80 +/- 3.37
Episode length: 167.60 +/- 57.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 168         |
|    mean_reward          | 5.8         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010814274 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.406       |
|    learning_rate        | 0.000109    |
|    loss                 | 0.0462      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.119       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.01     |
| time/              |          |
|    fps             | 891      |
|    iterations      | 5        |
|    time_elapsed    | 22       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 109         |
|    ep_rew_mean          | 2.42        |
| time/                   |             |
|    fps                  | 909         |
|    iterations           | 6           |
|    time_elapsed         | 27          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009067954 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.468       |
|    learning_rate        | 0.000109    |
|    loss                 | 0.0385      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.0958      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=6.10 +/- 0.94
Episode length: 151.10 +/- 14.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 151         |
|    mean_reward          | 6.1         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.009441285 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.433       |
|    learning_rate        | 0.000109    |
|    loss                 | 0.0214      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.106       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 893      |
|    iterations      | 7        |
|    time_elapsed    | 32       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=6.40 +/- 1.43
Episode length: 153.20 +/- 15.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 153        |
|    mean_reward          | 6.4        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.01185012 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.12      |
|    explained_variance   | 0.428      |
|    learning_rate        | 0.000109   |
|    loss                 | 0.0563     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0115    |
|    value_loss           | 0.119      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.21     |
| time/              |          |
|    fps             | 883      |
|    iterations      | 8        |
|    time_elapsed    | 37       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=5.20 +/- 0.98
Episode length: 137.50 +/- 14.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 138         |
|    mean_reward          | 5.2         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.010933183 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.000109    |
|    loss                 | 0.0315      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.132       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 3.45     |
| time/              |          |
|    fps             | 877      |
|    iterations      | 9        |
|    time_elapsed    | 42       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=6.50 +/- 0.92
Episode length: 160.40 +/- 19.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 160         |
|    mean_reward          | 6.5         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.012779832 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.996      |
|    explained_variance   | 0.481       |
|    learning_rate        | 0.000109    |
|    loss                 | 0.0239      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.119       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 4.08     |
| time/              |          |
|    fps             | 869      |
|    iterations      | 10       |
|    time_elapsed    | 47       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=10.10 +/- 1.81
Episode length: 221.20 +/- 29.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 221         |
|    mean_reward          | 10.1        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.011280352 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.936      |
|    explained_variance   | 0.441       |
|    learning_rate        | 0.000109    |
|    loss                 | 0.0536      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.152       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 4.53     |
| time/              |          |
|    fps             | 856      |
|    iterations      | 11       |
|    time_elapsed    | 52       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 138         |
|    ep_rew_mean          | 5.23        |
| time/                   |             |
|    fps                  | 868         |
|    iterations           | 12          |
|    time_elapsed         | 56          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.012037088 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.875      |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.000109    |
|    loss                 | 0.0309      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.145       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=12.50 +/- 2.20
Episode length: 248.70 +/- 28.81
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 249        |
|    mean_reward          | 12.5       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.01230379 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.818     |
|    explained_variance   | 0.358      |
|    learning_rate        | 0.000109   |
|    loss                 | 0.0403     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0184    |
|    value_loss           | 0.173      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 5.77     |
| time/              |          |
|    fps             | 855      |
|    iterations      | 13       |
|    time_elapsed    | 62       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=13.70 +/- 2.05
Episode length: 263.20 +/- 30.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 263         |
|    mean_reward          | 13.7        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.009703357 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.785      |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.000109    |
|    loss                 | 0.0479      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.149       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 156      |
|    ep_rew_mean     | 6.43     |
| time/              |          |
|    fps             | 843      |
|    iterations      | 14       |
|    time_elapsed    | 68       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=14.80 +/- 1.94
Episode length: 281.00 +/- 32.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | 14.8        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.010789542 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.000109    |
|    loss                 | 0.0446      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.155       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 6.89     |
| time/              |          |
|    fps             | 831      |
|    iterations      | 15       |
|    time_elapsed    | 73       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.8     |
|    ep_rew_mean     | -0.0196  |
| time/              |          |
|    fps             | 1214     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=0.50 +/- 0.67
Episode length: 78.40 +/- 12.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.4        |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.011072443 |
|    clip_fraction        | 0.0515      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.00686    |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.0195      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 0.083       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80       |
|    ep_rew_mean     | -0.01    |
| time/              |          |
|    fps             | 1033     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.20 +/- 0.40
Episode length: 74.70 +/- 9.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.7        |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011320995 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.259       |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.0261      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00575    |
|    value_loss           | 0.0754      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.8     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 987      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=0.70 +/- 0.64
Episode length: 74.60 +/- 14.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.6        |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.013185382 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.367       |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.0462      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00317    |
|    value_loss           | 0.0789      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.9     |
|    ep_rew_mean     | 0.41     |
| time/              |          |
|    fps             | 964      |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=0.30 +/- 0.46
Episode length: 71.10 +/- 11.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 71.1         |
|    mean_reward          | 0.3          |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0060826372 |
|    clip_fraction        | 0.0478       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.27        |
|    explained_variance   | 0.454        |
|    learning_rate        | 5.2e-05      |
|    loss                 | 0.0415       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00388     |
|    value_loss           | 0.077        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87       |
|    ep_rew_mean     | 0.67     |
| time/              |          |
|    fps             | 953      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 92.6        |
|    ep_rew_mean          | 1.1         |
| time/                   |             |
|    fps                  | 963         |
|    iterations           | 6           |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.011433541 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.429       |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.0155      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0096     |
|    value_loss           | 0.0869      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=0.20 +/- 0.40
Episode length: 72.50 +/- 10.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.5        |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.014347754 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.417       |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.0295      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.102       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 1.79     |
| time/              |          |
|    fps             | 957      |
|    iterations      | 7        |
|    time_elapsed    | 29       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=9.80 +/- 3.97
Episode length: 210.40 +/- 70.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | 9.8         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.008946223 |
|    clip_fraction        | 0.0693      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.438       |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.0227      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00716    |
|    value_loss           | 0.101       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.45     |
| time/              |          |
|    fps             | 927      |
|    iterations      | 8        |
|    time_elapsed    | 35       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=9.20 +/- 3.12
Episode length: 210.40 +/- 53.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 210          |
|    mean_reward          | 9.2          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0038076933 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.18        |
|    explained_variance   | 0.446        |
|    learning_rate        | 5.2e-05      |
|    loss                 | 0.0322       |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00409     |
|    value_loss           | 0.105        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.56     |
| time/              |          |
|    fps             | 904      |
|    iterations      | 9        |
|    time_elapsed    | 40       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=4.90 +/- 1.37
Episode length: 131.90 +/- 17.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 132         |
|    mean_reward          | 4.9         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.007336882 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.456       |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.0518      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00471    |
|    value_loss           | 0.111       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 896      |
|    iterations      | 10       |
|    time_elapsed    | 45       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=3.90 +/- 1.04
Episode length: 117.70 +/- 20.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 118        |
|    mean_reward          | 3.9        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.00938511 |
|    clip_fraction        | 0.0667     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.1       |
|    explained_variance   | 0.509      |
|    learning_rate        | 5.2e-05    |
|    loss                 | 0.0345     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0078    |
|    value_loss           | 0.102      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 892      |
|    iterations      | 11       |
|    time_elapsed    | 50       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 116          |
|    ep_rew_mean          | 3.19         |
| time/                   |              |
|    fps                  | 902          |
|    iterations           | 12           |
|    time_elapsed         | 54           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0058009517 |
|    clip_fraction        | 0.0363       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.516        |
|    learning_rate        | 5.2e-05      |
|    loss                 | 0.0552       |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00513     |
|    value_loss           | 0.103        |
------------------------------------------
Eval num_timesteps=50000, episode_reward=3.80 +/- 1.40
Episode length: 101.70 +/- 19.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 102        |
|    mean_reward          | 3.8        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.00655593 |
|    clip_fraction        | 0.0607     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.995     |
|    explained_variance   | 0.487      |
|    learning_rate        | 5.2e-05    |
|    loss                 | 0.0357     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.00781   |
|    value_loss           | 0.11       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 3.45     |
| time/              |          |
|    fps             | 901      |
|    iterations      | 13       |
|    time_elapsed    | 59       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=5.30 +/- 1.10
Episode length: 131.80 +/- 11.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 132         |
|    mean_reward          | 5.3         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.004981501 |
|    clip_fraction        | 0.0681      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.958      |
|    explained_variance   | 0.376       |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.0385      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00772    |
|    value_loss           | 0.126       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 3.64     |
| time/              |          |
|    fps             | 896      |
|    iterations      | 14       |
|    time_elapsed    | 63       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=5.60 +/- 0.80
Episode length: 137.10 +/- 16.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 137         |
|    mean_reward          | 5.6         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.006883579 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.915      |
|    explained_variance   | 0.4         |
|    learning_rate        | 5.2e-05     |
|    loss                 | 0.0486      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00654    |
|    value_loss           | 0.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.75     |
| time/              |          |
|    fps             | 892      |
|    iterations      | 15       |
|    time_elapsed    | 68       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.7     |
|    ep_rew_mean     | 0.245    |
| time/              |          |
|    fps             | 1161     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=0.50 +/- 0.67
Episode length: 79.00 +/- 14.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79          |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.015069932 |
|    clip_fraction        | 0.0412      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0557     |
|    learning_rate        | 2.59e-05    |
|    loss                 | 0.0876      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00425    |
|    value_loss           | 0.0983      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.2     |
|    ep_rew_mean     | 0.265    |
| time/              |          |
|    fps             | 1017     |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.50 +/- 0.67
Episode length: 77.50 +/- 7.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.5        |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.015061609 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.207       |
|    learning_rate        | 2.59e-05    |
|    loss                 | 0.0168      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0067     |
|    value_loss           | 0.0751      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.8     |
|    ep_rew_mean     | 0.6      |
| time/              |          |
|    fps             | 976      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=3.70 +/- 0.90
Episode length: 114.30 +/- 14.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 3.7         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.011524489 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.308       |
|    learning_rate        | 2.59e-05    |
|    loss                 | 0.0346      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00981    |
|    value_loss           | 0.0838      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.3     |
|    ep_rew_mean     | 1.15     |
| time/              |          |
|    fps             | 941      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-1.00 +/- 0.00
Episode length: 74.90 +/- 8.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.9        |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012870611 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.334       |
|    learning_rate        | 2.59e-05    |
|    loss                 | 0.0494      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00883    |
|    value_loss           | 0.0906      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.8     |
|    ep_rew_mean     | 1.64     |
| time/              |          |
|    fps             | 933      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 103         |
|    ep_rew_mean          | 1.94        |
| time/                   |             |
|    fps                  | 948         |
|    iterations           | 6           |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.010700421 |
|    clip_fraction        | 0.0956      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.383       |
|    learning_rate        | 2.59e-05    |
|    loss                 | 0.0235      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00845    |
|    value_loss           | 0.0951      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-1.00 +/- 0.00
Episode length: 79.10 +/- 11.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.1        |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.008790379 |
|    clip_fraction        | 0.0614      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.407       |
|    learning_rate        | 2.59e-05    |
|    loss                 | 0.0561      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00547    |
|    value_loss           | 0.0952      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.11     |
| time/              |          |
|    fps             | 940      |
|    iterations      | 7        |
|    time_elapsed    | 30       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=5.90 +/- 0.83
Episode length: 144.50 +/- 17.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 144         |
|    mean_reward          | 5.9         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.009780308 |
|    clip_fraction        | 0.0675      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.413       |
|    learning_rate        | 2.59e-05    |
|    loss                 | 0.0342      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00604    |
|    value_loss           | 0.0869      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 2        |
| time/              |          |
|    fps             | 922      |
|    iterations      | 8        |
|    time_elapsed    | 35       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=3.60 +/- 0.92
Episode length: 115.50 +/- 13.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 116          |
|    mean_reward          | 3.6          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0077586174 |
|    clip_fraction        | 0.0358       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.05        |
|    explained_variance   | 0.427        |
|    learning_rate        | 2.59e-05     |
|    loss                 | 0.0302       |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00367     |
|    value_loss           | 0.0924       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.32     |
| time/              |          |
|    fps             | 913      |
|    iterations      | 9        |
|    time_elapsed    | 40       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=3.80 +/- 1.47
Episode length: 115.70 +/- 18.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | 3.8         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.006113632 |
|    clip_fraction        | 0.0232      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.453       |
|    learning_rate        | 2.59e-05    |
|    loss                 | 0.0408      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 0.0966      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 907      |
|    iterations      | 10       |
|    time_elapsed    | 45       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=5.10 +/- 1.14
Episode length: 125.00 +/- 18.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | 5.1         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.010757441 |
|    clip_fraction        | 0.035       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.976      |
|    explained_variance   | 0.484       |
|    learning_rate        | 2.59e-05    |
|    loss                 | 0.048       |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0026     |
|    value_loss           | 0.0869      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.32     |
| time/              |          |
|    fps             | 900      |
|    iterations      | 11       |
|    time_elapsed    | 50       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 2.32        |
| time/                   |             |
|    fps                  | 909         |
|    iterations           | 12          |
|    time_elapsed         | 54          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.011349803 |
|    clip_fraction        | 0.0543      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.922      |
|    explained_variance   | 0.389       |
|    learning_rate        | 2.59e-05    |
|    loss                 | 0.0468      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00562    |
|    value_loss           | 0.0944      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=3.30 +/- 1.27
Episode length: 116.10 +/- 24.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 116          |
|    mean_reward          | 3.3          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0022602018 |
|    clip_fraction        | 0.022        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.881       |
|    explained_variance   | 0.467        |
|    learning_rate        | 2.59e-05     |
|    loss                 | 0.0365       |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00239     |
|    value_loss           | 0.0867       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 905      |
|    iterations      | 13       |
|    time_elapsed    | 58       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=3.50 +/- 0.67
Episode length: 108.70 +/- 10.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 109          |
|    mean_reward          | 3.5          |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0039609633 |
|    clip_fraction        | 0.0291       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.835       |
|    explained_variance   | 0.456        |
|    learning_rate        | 2.59e-05     |
|    loss                 | 0.0373       |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.0039      |
|    value_loss           | 0.0918       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.62     |
| time/              |          |
|    fps             | 902      |
|    iterations      | 14       |
|    time_elapsed    | 63       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=3.30 +/- 1.10
Episode length: 112.70 +/- 20.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 113          |
|    mean_reward          | 3.3          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0063030296 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.781       |
|    explained_variance   | 0.436        |
|    learning_rate        | 2.59e-05     |
|    loss                 | 0.0556       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00443     |
|    value_loss           | 0.102        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    fps             | 899      |
|    iterations      | 15       |
|    time_elapsed    | 68       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.5     |
|    ep_rew_mean     | 0.118    |
| time/              |          |
|    fps             | 1239     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=0.20 +/- 0.40
Episode length: 67.50 +/- 12.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 67.5       |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 5000       |
| train/                  |            |
|    approx_kl            | 0.01492615 |
|    clip_fraction        | 0.0799     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.38      |
|    explained_variance   | -0.0226    |
|    learning_rate        | 0.000103   |
|    loss                 | 0.0262     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0061    |
|    value_loss           | 0.0912     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.6     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 1059     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.20 +/- 0.40
Episode length: 72.20 +/- 11.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 72.2       |
|    mean_reward          | 0.2        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01313814 |
|    clip_fraction        | 0.0587     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.35      |
|    explained_variance   | 0.325      |
|    learning_rate        | 0.000103   |
|    loss                 | 0.00879    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00643   |
|    value_loss           | 0.0657     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.3     |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 1006     |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=2.70 +/- 0.64
Episode length: 110.60 +/- 10.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 111         |
|    mean_reward          | 2.7         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.009288295 |
|    clip_fraction        | 0.0475      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.000103    |
|    loss                 | 0.0274      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00688    |
|    value_loss           | 0.0722      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.3     |
|    ep_rew_mean     | 0.57     |
| time/              |          |
|    fps             | 966      |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=3.60 +/- 1.02
Episode length: 110.70 +/- 19.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 111        |
|    mean_reward          | 3.6        |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.01102612 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.29      |
|    explained_variance   | 0.494      |
|    learning_rate        | 0.000103   |
|    loss                 | 0.0258     |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 0.0726     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.1     |
|    ep_rew_mean     | 1.13     |
| time/              |          |
|    fps             | 942      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 98.6        |
|    ep_rew_mean          | 1.76        |
| time/                   |             |
|    fps                  | 955         |
|    iterations           | 6           |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.009719249 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.000103    |
|    loss                 | 0.0505      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.102       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=3.70 +/- 1.27
Episode length: 118.00 +/- 18.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 118         |
|    mean_reward          | 3.7         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.011045812 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.000103    |
|    loss                 | 0.039       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.113       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 938      |
|    iterations      | 7        |
|    time_elapsed    | 30       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=4.90 +/- 1.30
Episode length: 127.60 +/- 23.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 4.9         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.010466063 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.000103    |
|    loss                 | 0.0262      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.0931      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.15     |
| time/              |          |
|    fps             | 924      |
|    iterations      | 8        |
|    time_elapsed    | 35       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=6.90 +/- 0.94
Episode length: 162.00 +/- 20.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 162         |
|    mean_reward          | 6.9         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.010058045 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.578       |
|    learning_rate        | 0.000103    |
|    loss                 | 0.0126      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.0947      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 3.48     |
| time/              |          |
|    fps             | 909      |
|    iterations      | 9        |
|    time_elapsed    | 40       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=13.70 +/- 1.90
Episode length: 258.00 +/- 42.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 258        |
|    mean_reward          | 13.7       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.01296876 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.527      |
|    learning_rate        | 0.000103   |
|    loss                 | 0.0367     |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.11       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 3.9      |
| time/              |          |
|    fps             | 884      |
|    iterations      | 10       |
|    time_elapsed    | 46       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=16.60 +/- 1.91
Episode length: 286.80 +/- 23.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 287       |
|    mean_reward          | 16.6      |
| time/                   |           |
|    total_timesteps      | 45000     |
| train/                  |           |
|    approx_kl            | 0.0116811 |
|    clip_fraction        | 0.138     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.01     |
|    explained_variance   | 0.49      |
|    learning_rate        | 0.000103  |
|    loss                 | 0.0106    |
|    n_updates            | 100       |
|    policy_gradient_loss | -0.0174   |
|    value_loss           | 0.118     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 4.53     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 11       |
|    time_elapsed    | 52       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 138         |
|    ep_rew_mean          | 5.42        |
| time/                   |             |
|    fps                  | 874         |
|    iterations           | 12          |
|    time_elapsed         | 56          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.009025438 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.905      |
|    explained_variance   | 0.502       |
|    learning_rate        | 0.000103    |
|    loss                 | 0.028       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.116       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=15.70 +/- 1.79
Episode length: 280.00 +/- 32.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 280          |
|    mean_reward          | 15.7         |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0072465544 |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.807       |
|    explained_variance   | 0.432        |
|    learning_rate        | 0.000103     |
|    loss                 | 0.0323       |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.0144      |
|    value_loss           | 0.127        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 6.24     |
| time/              |          |
|    fps             | 856      |
|    iterations      | 13       |
|    time_elapsed    | 62       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=15.60 +/- 3.47
Episode length: 305.60 +/- 61.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 306         |
|    mean_reward          | 15.6        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.009035569 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.521       |
|    learning_rate        | 0.000103    |
|    loss                 | 0.0404      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 7.03     |
| time/              |          |
|    fps             | 841      |
|    iterations      | 14       |
|    time_elapsed    | 68       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=17.50 +/- 1.75
Episode length: 318.90 +/- 37.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 319         |
|    mean_reward          | 17.5        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.009946051 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.698      |
|    explained_variance   | 0.447       |
|    learning_rate        | 0.000103    |
|    loss                 | 0.0271      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.137       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 7.88     |
| time/              |          |
|    fps             | 827      |
|    iterations      | 15       |
|    time_elapsed    | 74       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.3     |
|    ep_rew_mean     | -0.0566  |
| time/              |          |
|    fps             | 1219     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=2.10 +/- 0.83
Episode length: 116.00 +/- 11.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.008126341 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.0451      |
|    learning_rate        | 0.000142    |
|    loss                 | 0.0361      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00604    |
|    value_loss           | 0.0778      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.9     |
|    ep_rew_mean     | 0.276    |
| time/              |          |
|    fps             | 1004     |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=5.80 +/- 2.56
Episode length: 157.40 +/- 55.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 157        |
|    mean_reward          | 5.8        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01056967 |
|    clip_fraction        | 0.0852     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | 0.426      |
|    learning_rate        | 0.000142   |
|    loss                 | 0.0222     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00827   |
|    value_loss           | 0.0769     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.8     |
|    ep_rew_mean     | 0.92     |
| time/              |          |
|    fps             | 931      |
|    iterations      | 3        |
|    time_elapsed    | 13       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=8.60 +/- 1.91
Episode length: 190.00 +/- 40.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 190        |
|    mean_reward          | 8.6        |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01133712 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.3       |
|    explained_variance   | 0.375      |
|    learning_rate        | 0.000142   |
|    loss                 | 0.035      |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0128    |
|    value_loss           | 0.0941     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 1.83     |
| time/              |          |
|    fps             | 888      |
|    iterations      | 4        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=8.60 +/- 2.73
Episode length: 192.90 +/- 53.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 193          |
|    mean_reward          | 8.6          |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0106918225 |
|    clip_fraction        | 0.133        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | 0.375        |
|    learning_rate        | 0.000142     |
|    loss                 | 0.0386       |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.0147      |
|    value_loss           | 0.122        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.35     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 5        |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | 2.78        |
| time/                   |             |
|    fps                  | 886         |
|    iterations           | 6           |
|    time_elapsed         | 27          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.010789367 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.508       |
|    learning_rate        | 0.000142    |
|    loss                 | 0.0534      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.0975      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=11.00 +/- 2.49
Episode length: 226.90 +/- 43.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 227          |
|    mean_reward          | 11           |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0124171935 |
|    clip_fraction        | 0.143        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.404        |
|    learning_rate        | 0.000142     |
|    loss                 | 0.0219       |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0222      |
|    value_loss           | 0.125        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3.16     |
| time/              |          |
|    fps             | 855      |
|    iterations      | 7        |
|    time_elapsed    | 33       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=8.40 +/- 3.01
Episode length: 194.80 +/- 49.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 195        |
|    mean_reward          | 8.4        |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.01614345 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.452      |
|    learning_rate        | 0.000142   |
|    loss                 | 0.0101     |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0211    |
|    value_loss           | 0.131      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 3.99     |
| time/              |          |
|    fps             | 844      |
|    iterations      | 8        |
|    time_elapsed    | 38       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=8.00 +/- 2.19
Episode length: 178.20 +/- 44.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 178         |
|    mean_reward          | 8           |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.011483336 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.000142    |
|    loss                 | 0.018       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 0.139       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 4.71     |
| time/              |          |
|    fps             | 837      |
|    iterations      | 9        |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=8.50 +/- 2.20
Episode length: 186.00 +/- 39.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 186         |
|    mean_reward          | 8.5         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.009628396 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.975      |
|    explained_variance   | 0.42        |
|    learning_rate        | 0.000142    |
|    loss                 | 0.0525      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.144       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 5.41     |
| time/              |          |
|    fps             | 831      |
|    iterations      | 10       |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=10.30 +/- 4.10
Episode length: 205.40 +/- 55.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 205         |
|    mean_reward          | 10.3        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.010406675 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.908      |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.000142    |
|    loss                 | 0.0474      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.179       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 6.01     |
| time/              |          |
|    fps             | 824      |
|    iterations      | 11       |
|    time_elapsed    | 54       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 154         |
|    ep_rew_mean          | 6.69        |
| time/                   |             |
|    fps                  | 837         |
|    iterations           | 12          |
|    time_elapsed         | 58          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.010095524 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.854      |
|    explained_variance   | 0.441       |
|    learning_rate        | 0.000142    |
|    loss                 | 0.0504      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.158       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=8.70 +/- 2.79
Episode length: 190.20 +/- 53.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 190         |
|    mean_reward          | 8.7         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.013168165 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.803      |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.000142    |
|    loss                 | 0.0384      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.156       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 7.35     |
| time/              |          |
|    fps             | 833      |
|    iterations      | 13       |
|    time_elapsed    | 63       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=8.20 +/- 3.89
Episode length: 180.20 +/- 63.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 180         |
|    mean_reward          | 8.2         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.011397234 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.784      |
|    explained_variance   | 0.521       |
|    learning_rate        | 0.000142    |
|    loss                 | 0.0373      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.144       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 7.79     |
| time/              |          |
|    fps             | 830      |
|    iterations      | 14       |
|    time_elapsed    | 69       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=7.30 +/- 3.03
Episode length: 174.10 +/- 52.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 174         |
|    mean_reward          | 7.3         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.010929498 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.724      |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.000142    |
|    loss                 | 0.0551      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.149       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 8.44     |
| time/              |          |
|    fps             | 827      |
|    iterations      | 15       |
|    time_elapsed    | 74       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.9     |
|    ep_rew_mean     | 0.2      |
| time/              |          |
|    fps             | 1239     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=0.30 +/- 0.46
Episode length: 77.80 +/- 14.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.8        |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.008533787 |
|    clip_fraction        | 0.062       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0166     |
|    learning_rate        | 0.000144    |
|    loss                 | 0.0465      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00655    |
|    value_loss           | 0.0909      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.3     |
|    ep_rew_mean     | 0.184    |
| time/              |          |
|    fps             | 1045     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=1.90 +/- 0.54
Episode length: 104.10 +/- 17.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 104         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011736844 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.441       |
|    learning_rate        | 0.000144    |
|    loss                 | 0.00799     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.0649      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.9     |
|    ep_rew_mean     | 0.74     |
| time/              |          |
|    fps             | 979      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=3.60 +/- 1.28
Episode length: 113.70 +/- 20.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 3.6         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.009802077 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.381       |
|    learning_rate        | 0.000144    |
|    loss                 | 0.0198      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.103       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.7     |
|    ep_rew_mean     | 1.15     |
| time/              |          |
|    fps             | 943      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=5.10 +/- 1.04
Episode length: 128.00 +/- 12.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 5.1         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.010791285 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.000144    |
|    loss                 | -0.00508    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.081       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 1.77     |
| time/              |          |
|    fps             | 920      |
|    iterations      | 5        |
|    time_elapsed    | 22       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 106        |
|    ep_rew_mean          | 2.37       |
| time/                   |            |
|    fps                  | 935        |
|    iterations           | 6          |
|    time_elapsed         | 26         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.01126945 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.21      |
|    explained_variance   | 0.425      |
|    learning_rate        | 0.000144   |
|    loss                 | 0.0177     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0189    |
|    value_loss           | 0.1        |
----------------------------------------
Eval num_timesteps=25000, episode_reward=5.40 +/- 1.74
Episode length: 144.40 +/- 38.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 144         |
|    mean_reward          | 5.4         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.012683825 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.000144    |
|    loss                 | 0.00254     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0245     |
|    value_loss           | 0.102       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.17     |
| time/              |          |
|    fps             | 918      |
|    iterations      | 7        |
|    time_elapsed    | 31       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=6.20 +/- 1.83
Episode length: 152.90 +/- 38.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 153         |
|    mean_reward          | 6.2         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.014154211 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.456       |
|    learning_rate        | 0.000144    |
|    loss                 | 0.0309      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.112       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 3.79     |
| time/              |          |
|    fps             | 904      |
|    iterations      | 8        |
|    time_elapsed    | 36       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=13.10 +/- 1.45
Episode length: 237.20 +/- 15.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 237         |
|    mean_reward          | 13.1        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.013437433 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.000144    |
|    loss                 | -0.00905    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 0.118       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 4.4      |
| time/              |          |
|    fps             | 882      |
|    iterations      | 9        |
|    time_elapsed    | 41       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=7.00 +/- 1.61
Episode length: 166.40 +/- 36.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 7           |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.012860345 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.000144    |
|    loss                 | -6.29e-05   |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0215     |
|    value_loss           | 0.131       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 4.97     |
| time/              |          |
|    fps             | 875      |
|    iterations      | 10       |
|    time_elapsed    | 46       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=11.90 +/- 2.95
Episode length: 228.60 +/- 41.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 229         |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.014801484 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.966      |
|    explained_variance   | 0.417       |
|    learning_rate        | 0.000144    |
|    loss                 | -0.000389   |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0243     |
|    value_loss           | 0.12        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 5.54     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 11       |
|    time_elapsed    | 52       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 145         |
|    ep_rew_mean          | 6.25        |
| time/                   |             |
|    fps                  | 874         |
|    iterations           | 12          |
|    time_elapsed         | 56          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.011185915 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.92       |
|    explained_variance   | 0.504       |
|    learning_rate        | 0.000144    |
|    loss                 | 0.021       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.131       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=5.90 +/- 2.02
Episode length: 140.10 +/- 34.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 140         |
|    mean_reward          | 5.9         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.012765723 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.892      |
|    explained_variance   | 0.527       |
|    learning_rate        | 0.000144    |
|    loss                 | 0.0188      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0208     |
|    value_loss           | 0.127       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 6.55     |
| time/              |          |
|    fps             | 871      |
|    iterations      | 13       |
|    time_elapsed    | 61       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=12.10 +/- 1.64
Episode length: 224.70 +/- 30.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 225          |
|    mean_reward          | 12.1         |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0131381545 |
|    clip_fraction        | 0.171        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.861       |
|    explained_variance   | 0.515        |
|    learning_rate        | 0.000144     |
|    loss                 | 0.0187       |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.0198      |
|    value_loss           | 0.139        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 7.13     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 14       |
|    time_elapsed    | 66       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=12.10 +/- 1.87
Episode length: 222.90 +/- 40.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 223        |
|    mean_reward          | 12.1       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.01433299 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.793     |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.000144   |
|    loss                 | 0.0245     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0197    |
|    value_loss           | 0.128      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 7.62     |
| time/              |          |
|    fps             | 854      |
|    iterations      | 15       |
|    time_elapsed    | 71       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.9     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 1226     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=3.80 +/- 1.08
Episode length: 124.90 +/- 11.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | 3.8         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.017444048 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.0157     |
|    learning_rate        | 7.54e-05    |
|    loss                 | 0.0111      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00857    |
|    value_loss           | 0.0843      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.2     |
|    ep_rew_mean     | 0.568    |
| time/              |          |
|    fps             | 1009     |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=2.40 +/- 1.20
Episode length: 112.30 +/- 24.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 2.4         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.013335712 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.295       |
|    learning_rate        | 7.54e-05    |
|    loss                 | 0.0206      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00884    |
|    value_loss           | 0.0866      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.4     |
|    ep_rew_mean     | 1.1      |
| time/              |          |
|    fps             | 958      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=3.80 +/- 2.68
Episode length: 150.50 +/- 62.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 150         |
|    mean_reward          | 3.8         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.011820922 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.423       |
|    learning_rate        | 7.54e-05    |
|    loss                 | 0.011       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.0742      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.7     |
|    ep_rew_mean     | 1.5      |
| time/              |          |
|    fps             | 917      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=4.40 +/- 1.85
Episode length: 152.50 +/- 51.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 152         |
|    mean_reward          | 4.4         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011065885 |
|    clip_fraction        | 0.0792      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.495       |
|    learning_rate        | 7.54e-05    |
|    loss                 | 0.0055      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00879    |
|    value_loss           | 0.0746      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 1.96     |
| time/              |          |
|    fps             | 898      |
|    iterations      | 5        |
|    time_elapsed    | 22       |
|    total_timesteps | 20480    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 107          |
|    ep_rew_mean          | 2.25         |
| time/                   |              |
|    fps                  | 917          |
|    iterations           | 6            |
|    time_elapsed         | 26           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0064634746 |
|    clip_fraction        | 0.0641       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | 0.437        |
|    learning_rate        | 7.54e-05     |
|    loss                 | 0.0256       |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00722     |
|    value_loss           | 0.0856       |
------------------------------------------
Eval num_timesteps=25000, episode_reward=13.20 +/- 3.84
Episode length: 251.20 +/- 74.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 251        |
|    mean_reward          | 13.2       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.00865267 |
|    clip_fraction        | 0.0959     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.543      |
|    learning_rate        | 7.54e-05   |
|    loss                 | 0.0209     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.00943   |
|    value_loss           | 0.0802     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.37     |
| time/              |          |
|    fps             | 883      |
|    iterations      | 7        |
|    time_elapsed    | 32       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=17.20 +/- 3.09
Episode length: 339.20 +/- 59.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 339         |
|    mean_reward          | 17.2        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.007391371 |
|    clip_fraction        | 0.0919      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.532       |
|    learning_rate        | 7.54e-05    |
|    loss                 | 0.0152      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00891    |
|    value_loss           | 0.0777      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.52     |
| time/              |          |
|    fps             | 846      |
|    iterations      | 8        |
|    time_elapsed    | 38       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=16.20 +/- 3.89
Episode length: 315.00 +/- 74.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 315         |
|    mean_reward          | 16.2        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.010608204 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.528       |
|    learning_rate        | 7.54e-05    |
|    loss                 | 0.0105      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.0865      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 3.02     |
| time/              |          |
|    fps             | 823      |
|    iterations      | 9        |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=18.10 +/- 1.81
Episode length: 347.40 +/- 23.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 347         |
|    mean_reward          | 18.1        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.009546162 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.915      |
|    explained_variance   | 0.51        |
|    learning_rate        | 7.54e-05    |
|    loss                 | 0.0122      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.095       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 3.7      |
| time/              |          |
|    fps             | 802      |
|    iterations      | 10       |
|    time_elapsed    | 51       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=15.80 +/- 5.10
Episode length: 295.80 +/- 78.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 296         |
|    mean_reward          | 15.8        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.008605627 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.832      |
|    explained_variance   | 0.383       |
|    learning_rate        | 7.54e-05    |
|    loss                 | 0.0321      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.117       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 4.62     |
| time/              |          |
|    fps             | 791      |
|    iterations      | 11       |
|    time_elapsed    | 56       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 144          |
|    ep_rew_mean          | 5.34         |
| time/                   |              |
|    fps                  | 806          |
|    iterations           | 12           |
|    time_elapsed         | 60           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0065008374 |
|    clip_fraction        | 0.0956       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.715       |
|    explained_variance   | 0.356        |
|    learning_rate        | 7.54e-05     |
|    loss                 | 0.055        |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.0108      |
|    value_loss           | 0.12         |
------------------------------------------
Eval num_timesteps=50000, episode_reward=15.80 +/- 4.21
Episode length: 288.30 +/- 78.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 288         |
|    mean_reward          | 15.8        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.008827964 |
|    clip_fraction        | 0.097       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.692      |
|    explained_variance   | 0.447       |
|    learning_rate        | 7.54e-05    |
|    loss                 | 0.0201      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00756    |
|    value_loss           | 0.113       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 6.12     |
| time/              |          |
|    fps             | 796      |
|    iterations      | 13       |
|    time_elapsed    | 66       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=16.10 +/- 4.23
Episode length: 293.20 +/- 79.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 293         |
|    mean_reward          | 16.1        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.007758944 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.393       |
|    learning_rate        | 7.54e-05    |
|    loss                 | 0.0498      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00843    |
|    value_loss           | 0.124       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 6.86     |
| time/              |          |
|    fps             | 787      |
|    iterations      | 14       |
|    time_elapsed    | 72       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=17.60 +/- 1.96
Episode length: 305.20 +/- 33.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 305          |
|    mean_reward          | 17.6         |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0069938116 |
|    clip_fraction        | 0.0758       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.598       |
|    explained_variance   | 0.452        |
|    learning_rate        | 7.54e-05     |
|    loss                 | 0.037        |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0112      |
|    value_loss           | 0.119        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 7.34     |
| time/              |          |
|    fps             | 780      |
|    iterations      | 15       |
|    time_elapsed    | 78       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.1     |
|    ep_rew_mean     | 0.235    |
| time/              |          |
|    fps             | 1230     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=0.90 +/- 1.22
Episode length: 79.60 +/- 19.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.6        |
|    mean_reward          | 0.9         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.011213326 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0724     |
|    learning_rate        | 0.000442    |
|    loss                 | 0.017       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00882    |
|    value_loss           | 0.0805      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.7     |
|    ep_rew_mean     | 0.5      |
| time/              |          |
|    fps             | 1043     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=2.00 +/- 1.84
Episode length: 94.00 +/- 27.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94           |
|    mean_reward          | 2            |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0122881215 |
|    clip_fraction        | 0.148        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | 0.36         |
|    learning_rate        | 0.000442     |
|    loss                 | -0.00639     |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.0219      |
|    value_loss           | 0.077        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.4     |
|    ep_rew_mean     | 0.98     |
| time/              |          |
|    fps             | 984      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=4.60 +/- 2.87
Episode length: 125.50 +/- 43.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 4.6         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.016860915 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.43        |
|    learning_rate        | 0.000442    |
|    loss                 | -0.00551    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0326     |
|    value_loss           | 0.0826      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.2     |
|    ep_rew_mean     | 1.38     |
| time/              |          |
|    fps             | 947      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=4.90 +/- 2.74
Episode length: 127.70 +/- 46.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 4.9         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.019738283 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.000442    |
|    loss                 | 0.0024      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 0.0789      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.9     |
|    ep_rew_mean     | 1.5      |
| time/              |          |
|    fps             | 922      |
|    iterations      | 5        |
|    time_elapsed    | 22       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 99.8        |
|    ep_rew_mean          | 2.1         |
| time/                   |             |
|    fps                  | 938         |
|    iterations           | 6           |
|    time_elapsed         | 26          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.023340274 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.000442    |
|    loss                 | -0.0251     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0356     |
|    value_loss           | 0.0748      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=5.70 +/- 1.85
Episode length: 138.80 +/- 24.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 139         |
|    mean_reward          | 5.7         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.026303178 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.565       |
|    learning_rate        | 0.000442    |
|    loss                 | -0.0193     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0395     |
|    value_loss           | 0.0775      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 3.27     |
| time/              |          |
|    fps             | 922      |
|    iterations      | 7        |
|    time_elapsed    | 31       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=3.20 +/- 1.25
Episode length: 111.30 +/- 17.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 111         |
|    mean_reward          | 3.2         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.027021863 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.584       |
|    learning_rate        | 0.000442    |
|    loss                 | -0.0397     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 0.0967      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 4.33     |
| time/              |          |
|    fps             | 915      |
|    iterations      | 8        |
|    time_elapsed    | 35       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=4.90 +/- 3.56
Episode length: 125.80 +/- 53.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 4.9         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.030713579 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.000442    |
|    loss                 | -0.0375     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0379     |
|    value_loss           | 0.081       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 5.36     |
| time/              |          |
|    fps             | 907      |
|    iterations      | 9        |
|    time_elapsed    | 40       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=12.00 +/- 5.14
Episode length: 215.90 +/- 78.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 12         |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.04019349 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.000442   |
|    loss                 | -0.0163    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0381    |
|    value_loss           | 0.0976     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 6.1      |
| time/              |          |
|    fps             | 889      |
|    iterations      | 10       |
|    time_elapsed    | 46       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=4.00 +/- 2.53
Episode length: 119.20 +/- 40.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 119        |
|    mean_reward          | 4          |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.03879958 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.946     |
|    explained_variance   | 0.57       |
|    learning_rate        | 0.000442   |
|    loss                 | -0.00523   |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0294    |
|    value_loss           | 0.0953     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 6.71     |
| time/              |          |
|    fps             | 886      |
|    iterations      | 11       |
|    time_elapsed    | 50       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 158        |
|    ep_rew_mean          | 7.36       |
| time/                   |            |
|    fps                  | 896        |
|    iterations           | 12         |
|    time_elapsed         | 54         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.04210294 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.967     |
|    explained_variance   | 0.579      |
|    learning_rate        | 0.000442   |
|    loss                 | -0.0116    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0358    |
|    value_loss           | 0.104      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=14.40 +/- 5.12
Episode length: 265.10 +/- 85.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 265         |
|    mean_reward          | 14.4        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.041658163 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.911      |
|    explained_variance   | 0.596       |
|    learning_rate        | 0.000442    |
|    loss                 | -0.0085     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0299     |
|    value_loss           | 0.101       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 8.28     |
| time/              |          |
|    fps             | 879      |
|    iterations      | 13       |
|    time_elapsed    | 60       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=14.00 +/- 4.20
Episode length: 250.90 +/- 73.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 251         |
|    mean_reward          | 14          |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.043462947 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.831      |
|    explained_variance   | 0.384       |
|    learning_rate        | 0.000442    |
|    loss                 | -0.0207     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0353     |
|    value_loss           | 0.104       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 9.22     |
| time/              |          |
|    fps             | 866      |
|    iterations      | 14       |
|    time_elapsed    | 66       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=16.90 +/- 2.12
Episode length: 289.10 +/- 30.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 289         |
|    mean_reward          | 16.9        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.038987614 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.799      |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.000442    |
|    loss                 | -0.0116     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.0974      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    fps             | 852      |
|    iterations      | 15       |
|    time_elapsed    | 72       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.6     |
|    ep_rew_mean     | 0.22     |
| time/              |          |
|    fps             | 1237     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=0.50 +/- 0.50
Episode length: 76.40 +/- 13.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.4        |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.011559624 |
|    clip_fraction        | 0.0723      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.0291      |
|    learning_rate        | 9.61e-05    |
|    loss                 | 0.0169      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00596    |
|    value_loss           | 0.084       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.4     |
|    ep_rew_mean     | 0.162    |
| time/              |          |
|    fps             | 1047     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.50 +/- 0.50
Episode length: 75.50 +/- 10.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.5        |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.016764477 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.357       |
|    learning_rate        | 9.61e-05    |
|    loss                 | 0.0435      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00986    |
|    value_loss           | 0.0696      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.8     |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 996      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=0.30 +/- 0.46
Episode length: 72.60 +/- 14.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.6        |
|    mean_reward          | 0.3         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.012416175 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.405       |
|    learning_rate        | 9.61e-05    |
|    loss                 | 0.0417      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00718    |
|    value_loss           | 0.0854      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.4     |
|    ep_rew_mean     | 0.96     |
| time/              |          |
|    fps             | 974      |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=2.30 +/- 1.35
Episode length: 94.20 +/- 9.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.2        |
|    mean_reward          | 2.3         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.009622926 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.387       |
|    learning_rate        | 9.61e-05    |
|    loss                 | 0.0305      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.0984      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.5     |
|    ep_rew_mean     | 1.57     |
| time/              |          |
|    fps             | 954      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 108          |
|    ep_rew_mean          | 2.33         |
| time/                   |              |
|    fps                  | 965          |
|    iterations           | 6            |
|    time_elapsed         | 25           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0068361764 |
|    clip_fraction        | 0.093        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.2         |
|    explained_variance   | 0.521        |
|    learning_rate        | 9.61e-05     |
|    loss                 | 0.0371       |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00972     |
|    value_loss           | 0.0967       |
------------------------------------------
Eval num_timesteps=25000, episode_reward=6.60 +/- 1.56
Episode length: 159.50 +/- 33.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 160         |
|    mean_reward          | 6.6         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.009694342 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.433       |
|    learning_rate        | 9.61e-05    |
|    loss                 | 0.0603      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.128       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 938      |
|    iterations      | 7        |
|    time_elapsed    | 30       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=5.40 +/- 1.36
Episode length: 135.40 +/- 29.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 135         |
|    mean_reward          | 5.4         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.005531477 |
|    clip_fraction        | 0.0519      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.511       |
|    learning_rate        | 9.61e-05    |
|    loss                 | 0.0501      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00796    |
|    value_loss           | 0.123       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.79     |
| time/              |          |
|    fps             | 922      |
|    iterations      | 8        |
|    time_elapsed    | 35       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=6.10 +/- 1.70
Episode length: 142.70 +/- 32.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 143         |
|    mean_reward          | 6.1         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012167543 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.525       |
|    learning_rate        | 9.61e-05    |
|    loss                 | 0.0457      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.122       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 3.19     |
| time/              |          |
|    fps             | 911      |
|    iterations      | 9        |
|    time_elapsed    | 40       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=6.20 +/- 1.60
Episode length: 156.50 +/- 26.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 156         |
|    mean_reward          | 6.2         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.008280498 |
|    clip_fraction        | 0.098       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.999      |
|    explained_variance   | 0.541       |
|    learning_rate        | 9.61e-05    |
|    loss                 | 0.0401      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.119       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.75     |
| time/              |          |
|    fps             | 899      |
|    iterations      | 10       |
|    time_elapsed    | 45       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=4.00 +/- 1.55
Episode length: 133.40 +/- 31.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 133         |
|    mean_reward          | 4           |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.008849032 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.956      |
|    explained_variance   | 0.511       |
|    learning_rate        | 9.61e-05    |
|    loss                 | 0.0373      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00938    |
|    value_loss           | 0.141       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 4.27     |
| time/              |          |
|    fps             | 892      |
|    iterations      | 11       |
|    time_elapsed    | 50       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 130         |
|    ep_rew_mean          | 4.72        |
| time/                   |             |
|    fps                  | 902         |
|    iterations           | 12          |
|    time_elapsed         | 54          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.011730354 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.932      |
|    explained_variance   | 0.555       |
|    learning_rate        | 9.61e-05    |
|    loss                 | 0.0217      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.118       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=14.10 +/- 1.76
Episode length: 263.60 +/- 32.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 264         |
|    mean_reward          | 14.1        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.010743519 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.877      |
|    explained_variance   | 0.52        |
|    learning_rate        | 9.61e-05    |
|    loss                 | 0.0353      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.145       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 5.19     |
| time/              |          |
|    fps             | 884      |
|    iterations      | 13       |
|    time_elapsed    | 60       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=14.70 +/- 1.49
Episode length: 282.40 +/- 25.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 282         |
|    mean_reward          | 14.7        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.008129356 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.773      |
|    explained_variance   | 0.477       |
|    learning_rate        | 9.61e-05    |
|    loss                 | 0.0627      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.152       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 5.68     |
| time/              |          |
|    fps             | 867      |
|    iterations      | 14       |
|    time_elapsed    | 66       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=12.60 +/- 1.11
Episode length: 254.90 +/- 15.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 255          |
|    mean_reward          | 12.6         |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0129286125 |
|    clip_fraction        | 0.11         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.763       |
|    explained_variance   | 0.56         |
|    learning_rate        | 9.61e-05     |
|    loss                 | 0.0454       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0109      |
|    value_loss           | 0.142        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 6.21     |
| time/              |          |
|    fps             | 856      |
|    iterations      | 15       |
|    time_elapsed    | 71       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.4     |
|    ep_rew_mean     | 0.0196   |
| time/              |          |
|    fps             | 1217     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=0.40 +/- 0.49
Episode length: 75.30 +/- 11.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.3        |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010293665 |
|    clip_fraction        | 0.0309      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.00467     |
|    learning_rate        | 2.14e-05    |
|    loss                 | 0.0541      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00347    |
|    value_loss           | 0.0823      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.5     |
|    ep_rew_mean     | 0.245    |
| time/              |          |
|    fps             | 1040     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.60 +/- 0.80
Episode length: 79.70 +/- 14.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 79.7       |
|    mean_reward          | 0.6        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01571775 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.34      |
|    explained_variance   | 0.223      |
|    learning_rate        | 2.14e-05   |
|    loss                 | 0.00907    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00894   |
|    value_loss           | 0.091      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.2     |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 988      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=0.10 +/- 0.30
Episode length: 68.10 +/- 13.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 68.1         |
|    mean_reward          | 0.1          |
| time/                   |              |
|    total_timesteps      | 15000        |
| train/                  |              |
|    approx_kl            | 0.0116476985 |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.29        |
|    explained_variance   | 0.292        |
|    learning_rate        | 2.14e-05     |
|    loss                 | 0.0346       |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00909     |
|    value_loss           | 0.0973       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.4     |
|    ep_rew_mean     | 1.27     |
| time/              |          |
|    fps             | 972      |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-1.00 +/- 0.00
Episode length: 74.80 +/- 10.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.8        |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011904176 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.361       |
|    learning_rate        | 2.14e-05    |
|    loss                 | 0.038       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0097     |
|    value_loss           | 0.108       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 1.91     |
| time/              |          |
|    fps             | 959      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 105         |
|    ep_rew_mean          | 2.04        |
| time/                   |             |
|    fps                  | 968         |
|    iterations           | 6           |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.011457361 |
|    clip_fraction        | 0.0689      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.299       |
|    learning_rate        | 2.14e-05    |
|    loss                 | 0.0448      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00656    |
|    value_loss           | 0.124       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=4.10 +/- 2.51
Episode length: 146.80 +/- 50.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 147         |
|    mean_reward          | 4.1         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.008119293 |
|    clip_fraction        | 0.0558      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.451       |
|    learning_rate        | 2.14e-05    |
|    loss                 | 0.0294      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00448    |
|    value_loss           | 0.0946      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.19     |
| time/              |          |
|    fps             | 940      |
|    iterations      | 7        |
|    time_elapsed    | 30       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=0.60 +/- 0.66
Episode length: 80.40 +/- 17.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 80.4      |
|    mean_reward          | 0.6       |
| time/                   |           |
|    total_timesteps      | 30000     |
| train/                  |           |
|    approx_kl            | 0.0109403 |
|    clip_fraction        | 0.0121    |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.07     |
|    explained_variance   | 0.462     |
|    learning_rate        | 2.14e-05  |
|    loss                 | 0.0519    |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.0013   |
|    value_loss           | 0.103     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 935      |
|    iterations      | 8        |
|    time_elapsed    | 35       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=5.40 +/- 1.91
Episode length: 160.70 +/- 44.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 161         |
|    mean_reward          | 5.4         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.011582867 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.481       |
|    learning_rate        | 2.14e-05    |
|    loss                 | 0.0158      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00402    |
|    value_loss           | 0.0981      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.34     |
| time/              |          |
|    fps             | 919      |
|    iterations      | 9        |
|    time_elapsed    | 40       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=0.70 +/- 0.46
Episode length: 76.60 +/- 15.15
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 76.6     |
|    mean_reward          | 0.7      |
| time/                   |          |
|    total_timesteps      | 40000    |
| train/                  |          |
|    approx_kl            | 0.008703 |
|    clip_fraction        | 0.0219   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.99    |
|    explained_variance   | 0.464    |
|    learning_rate        | 2.14e-05 |
|    loss                 | 0.0398   |
|    n_updates            | 90       |
|    policy_gradient_loss | -0.00242 |
|    value_loss           | 0.109    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.43     |
| time/              |          |
|    fps             | 917      |
|    iterations      | 10       |
|    time_elapsed    | 44       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=0.70 +/- 0.78
Episode length: 80.90 +/- 17.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.9        |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.012049997 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.932      |
|    explained_variance   | 0.542       |
|    learning_rate        | 2.14e-05    |
|    loss                 | 0.0299      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00373    |
|    value_loss           | 0.103       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 915      |
|    iterations      | 11       |
|    time_elapsed    | 49       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 106         |
|    ep_rew_mean          | 2.49        |
| time/                   |             |
|    fps                  | 922         |
|    iterations           | 12          |
|    time_elapsed         | 53          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.010021824 |
|    clip_fraction        | 0.0901      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.91       |
|    explained_variance   | 0.458       |
|    learning_rate        | 2.14e-05    |
|    loss                 | 0.0855      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00706    |
|    value_loss           | 0.114       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=7.60 +/- 1.80
Episode length: 178.70 +/- 31.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 179         |
|    mean_reward          | 7.6         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.009055915 |
|    clip_fraction        | 0.0198      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.883      |
|    explained_variance   | 0.517       |
|    learning_rate        | 2.14e-05    |
|    loss                 | 0.0211      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00262    |
|    value_loss           | 0.101       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.42     |
| time/              |          |
|    fps             | 910      |
|    iterations      | 13       |
|    time_elapsed    | 58       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=2.10 +/- 0.94
Episode length: 109.00 +/- 7.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.016616266 |
|    clip_fraction        | 0.0871      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.838      |
|    explained_variance   | 0.524       |
|    learning_rate        | 2.14e-05    |
|    loss                 | 0.0417      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.006      |
|    value_loss           | 0.0997      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.6      |
| time/              |          |
|    fps             | 907      |
|    iterations      | 14       |
|    time_elapsed    | 63       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=2.20 +/- 0.87
Episode length: 115.80 +/- 8.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | 2.2         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.004323774 |
|    clip_fraction        | 0.0127      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.81       |
|    explained_variance   | 0.389       |
|    learning_rate        | 2.14e-05    |
|    loss                 | 0.0637      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00192    |
|    value_loss           | 0.125       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 3.05     |
| time/              |          |
|    fps             | 904      |
|    iterations      | 15       |
|    time_elapsed    | 67       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.4     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 1221     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=0.70 +/- 0.64
Episode length: 78.80 +/- 16.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78.8        |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009766911 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.025      |
|    learning_rate        | 0.000191    |
|    loss                 | 0.038       |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00724    |
|    value_loss           | 0.187       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83       |
|    ep_rew_mean     | 0.255    |
| time/              |          |
|    fps             | 879      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.90 +/- 0.70
Episode length: 84.60 +/- 16.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 84.6         |
|    mean_reward          | 0.9          |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0110775735 |
|    clip_fraction        | 0.131        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | 0.507        |
|    learning_rate        | 0.000191     |
|    loss                 | 0.0558       |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.0124      |
|    value_loss           | 0.143        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87       |
|    ep_rew_mean     | 0.58     |
| time/              |          |
|    fps             | 790      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=3.40 +/- 1.20
Episode length: 106.80 +/- 25.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 107         |
|    mean_reward          | 3.4         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.009560319 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.465       |
|    learning_rate        | 0.000191    |
|    loss                 | 0.04        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.183       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.5     |
|    ep_rew_mean     | 1.14     |
| time/              |          |
|    fps             | 751      |
|    iterations      | 4        |
|    time_elapsed    | 21       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=6.10 +/- 3.30
Episode length: 159.00 +/- 48.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 159         |
|    mean_reward          | 6.1         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.012387553 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.000191    |
|    loss                 | 0.11        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.242       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 103      |
|    ep_rew_mean     | 1.83     |
| time/              |          |
|    fps             | 718      |
|    iterations      | 5        |
|    time_elapsed    | 28       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 111         |
|    ep_rew_mean          | 2.7         |
| time/                   |             |
|    fps                  | 722         |
|    iterations           | 6           |
|    time_elapsed         | 34          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.013955065 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.000191    |
|    loss                 | 0.121       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0224     |
|    value_loss           | 0.281       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=5.00 +/- 2.93
Episode length: 131.30 +/- 48.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 131         |
|    mean_reward          | 5           |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.017104734 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.505       |
|    learning_rate        | 0.000191    |
|    loss                 | 0.0971      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.363       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.32     |
| time/              |          |
|    fps             | 707      |
|    iterations      | 7        |
|    time_elapsed    | 40       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=4.90 +/- 2.91
Episode length: 126.10 +/- 48.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 126         |
|    mean_reward          | 4.9         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.019786913 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.000191    |
|    loss                 | 0.0535      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0282     |
|    value_loss           | 0.279       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 3.84     |
| time/              |          |
|    fps             | 698      |
|    iterations      | 8        |
|    time_elapsed    | 46       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=4.10 +/- 2.62
Episode length: 117.40 +/- 47.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 117         |
|    mean_reward          | 4.1         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.021298228 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.000191    |
|    loss                 | 0.0627      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0339     |
|    value_loss           | 0.282       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 4.23     |
| time/              |          |
|    fps             | 692      |
|    iterations      | 9        |
|    time_elapsed    | 53       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=4.90 +/- 2.77
Episode length: 128.80 +/- 43.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 129         |
|    mean_reward          | 4.9         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.022253148 |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.000191    |
|    loss                 | 0.0422      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0308     |
|    value_loss           | 0.338       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 4.49     |
| time/              |          |
|    fps             | 687      |
|    iterations      | 10       |
|    time_elapsed    | 59       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=9.20 +/- 3.52
Episode length: 175.40 +/- 51.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 175        |
|    mean_reward          | 9.2        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.02459681 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.000191   |
|    loss                 | 0.0383     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0337    |
|    value_loss           | 0.31       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 5.13     |
| time/              |          |
|    fps             | 681      |
|    iterations      | 11       |
|    time_elapsed    | 66       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 140         |
|    ep_rew_mean          | 5.54        |
| time/                   |             |
|    fps                  | 687         |
|    iterations           | 12          |
|    time_elapsed         | 71          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.026473913 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.000191    |
|    loss                 | 0.0711      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.031      |
|    value_loss           | 0.324       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=10.20 +/- 3.03
Episode length: 215.00 +/- 48.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 215         |
|    mean_reward          | 10.2        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.028355073 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.000191    |
|    loss                 | 0.0559      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.348       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 5.93     |
| time/              |          |
|    fps             | 678      |
|    iterations      | 13       |
|    time_elapsed    | 78       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=9.40 +/- 1.91
Episode length: 208.10 +/- 20.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 208         |
|    mean_reward          | 9.4         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.030971922 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.000191    |
|    loss                 | 0.00634     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0321     |
|    value_loss           | 0.367       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 6.32     |
| time/              |          |
|    fps             | 671      |
|    iterations      | 14       |
|    time_elapsed    | 85       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=8.80 +/- 3.52
Episode length: 188.00 +/- 62.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 188        |
|    mean_reward          | 8.8        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.02835077 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.695      |
|    learning_rate        | 0.000191   |
|    loss                 | 0.148      |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0342    |
|    value_loss           | 0.377      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 6.59     |
| time/              |          |
|    fps             | 666      |
|    iterations      | 15       |
|    time_elapsed    | 92       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.1     |
|    ep_rew_mean     | 0.098    |
| time/              |          |
|    fps             | 1226     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=1.90 +/- 1.37
Episode length: 110.50 +/- 17.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 1.9         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.008235471 |
|    clip_fraction        | 0.053       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.00251     |
|    learning_rate        | 0.000631    |
|    loss                 | 0.0344      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00571    |
|    value_loss           | 0.118       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.9     |
|    ep_rew_mean     | 0.21     |
| time/              |          |
|    fps             | 1014     |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=4.20 +/- 1.54
Episode length: 123.00 +/- 26.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 123         |
|    mean_reward          | 4.2         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010602078 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.000631    |
|    loss                 | 0.0859      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.119       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.9     |
|    ep_rew_mean     | 0.34     |
| time/              |          |
|    fps             | 952      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=3.40 +/- 2.29
Episode length: 109.90 +/- 34.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 3.4         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.013912059 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.411       |
|    learning_rate        | 0.000631    |
|    loss                 | 0.0228      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.122       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.2     |
|    ep_rew_mean     | 0.59     |
| time/              |          |
|    fps             | 928      |
|    iterations      | 4        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=4.40 +/- 1.74
Episode length: 124.70 +/- 18.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | 4.4         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.018738635 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.454       |
|    learning_rate        | 0.000631    |
|    loss                 | 0.0327      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 0.147       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.1     |
|    ep_rew_mean     | 0.89     |
| time/              |          |
|    fps             | 911      |
|    iterations      | 5        |
|    time_elapsed    | 22       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 91.2        |
|    ep_rew_mean          | 1.4         |
| time/                   |             |
|    fps                  | 927         |
|    iterations           | 6           |
|    time_elapsed         | 26          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.020282386 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.000631    |
|    loss                 | 0.0386      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.148       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=4.20 +/- 1.89
Episode length: 123.30 +/- 36.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 123         |
|    mean_reward          | 4.2         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.026791861 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.407       |
|    learning_rate        | 0.000631    |
|    loss                 | 0.0211      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.035      |
|    value_loss           | 0.173       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.7     |
|    ep_rew_mean     | 2        |
| time/              |          |
|    fps             | 916      |
|    iterations      | 7        |
|    time_elapsed    | 31       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=10.70 +/- 4.27
Episode length: 228.20 +/- 68.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 228         |
|    mean_reward          | 10.7        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.030859988 |
|    clip_fraction        | 0.309       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.446       |
|    learning_rate        | 0.000631    |
|    loss                 | 0.018       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0375     |
|    value_loss           | 0.174       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 3        |
| time/              |          |
|    fps             | 891      |
|    iterations      | 8        |
|    time_elapsed    | 36       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=6.30 +/- 2.72
Episode length: 156.50 +/- 42.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 156        |
|    mean_reward          | 6.3        |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.04149299 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.14      |
|    explained_variance   | 0.442      |
|    learning_rate        | 0.000631   |
|    loss                 | 0.0281     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0397    |
|    value_loss           | 0.185      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 4.09     |
| time/              |          |
|    fps             | 883      |
|    iterations      | 9        |
|    time_elapsed    | 41       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=8.40 +/- 5.78
Episode length: 175.20 +/- 104.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 175         |
|    mean_reward          | 8.4         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.036295168 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.000631    |
|    loss                 | -0.00256    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0384     |
|    value_loss           | 0.182       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 4.86     |
| time/              |          |
|    fps             | 873      |
|    iterations      | 10       |
|    time_elapsed    | 46       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=11.30 +/- 5.92
Episode length: 214.30 +/- 89.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 214        |
|    mean_reward          | 11.3       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.04201597 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.000631   |
|    loss                 | -0.0138    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.166      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 5.75     |
| time/              |          |
|    fps             | 860      |
|    iterations      | 11       |
|    time_elapsed    | 52       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 146         |
|    ep_rew_mean          | 6.34        |
| time/                   |             |
|    fps                  | 872         |
|    iterations           | 12          |
|    time_elapsed         | 56          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.047510006 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.000631    |
|    loss                 | -0.00684    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0369     |
|    value_loss           | 0.172       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=12.90 +/- 3.91
Episode length: 234.90 +/- 52.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 235        |
|    mean_reward          | 12.9       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.04493214 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.952     |
|    explained_variance   | 0.572      |
|    learning_rate        | 0.000631   |
|    loss                 | -0.002     |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.036     |
|    value_loss           | 0.167      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 6.93     |
| time/              |          |
|    fps             | 860      |
|    iterations      | 13       |
|    time_elapsed    | 61       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=13.80 +/- 2.96
Episode length: 240.00 +/- 41.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 240       |
|    mean_reward          | 13.8      |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.0512579 |
|    clip_fraction        | 0.297     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.884    |
|    explained_variance   | 0.596     |
|    learning_rate        | 0.000631  |
|    loss                 | 0.00577   |
|    n_updates            | 130       |
|    policy_gradient_loss | -0.0375   |
|    value_loss           | 0.171     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 7.92     |
| time/              |          |
|    fps             | 850      |
|    iterations      | 14       |
|    time_elapsed    | 67       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=13.30 +/- 3.55
Episode length: 251.30 +/- 61.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 251         |
|    mean_reward          | 13.3        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.047677774 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.852      |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.000631    |
|    loss                 | -0.025      |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0412     |
|    value_loss           | 0.18        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 8.66     |
| time/              |          |
|    fps             | 840      |
|    iterations      | 15       |
|    time_elapsed    | 73       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.8     |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 1221     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=0.20 +/- 0.40
Episode length: 68.70 +/- 12.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.7        |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009975578 |
|    clip_fraction        | 0.0294      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0569     |
|    learning_rate        | 1.37e-05    |
|    loss                 | 0.0365      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00436    |
|    value_loss           | 0.0736      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.5     |
|    ep_rew_mean     | 0.411    |
| time/              |          |
|    fps             | 1043     |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.50 +/- 0.50
Episode length: 77.70 +/- 12.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.7        |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010476874 |
|    clip_fraction        | 0.0688      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.201       |
|    learning_rate        | 1.37e-05    |
|    loss                 | 0.0428      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0059     |
|    value_loss           | 0.0711      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91       |
|    ep_rew_mean     | 0.72     |
| time/              |          |
|    fps             | 989      |
|    iterations      | 3        |
|    time_elapsed    | 12       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=0.40 +/- 0.49
Episode length: 75.10 +/- 16.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.1        |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.008059146 |
|    clip_fraction        | 0.0674      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.35        |
|    learning_rate        | 1.37e-05    |
|    loss                 | 0.0191      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00473    |
|    value_loss           | 0.0676      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.8     |
|    ep_rew_mean     | 1.16     |
| time/              |          |
|    fps             | 966      |
|    iterations      | 4        |
|    time_elapsed    | 16       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=0.70 +/- 0.64
Episode length: 85.10 +/- 14.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 85.1        |
|    mean_reward          | 0.7         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011758927 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.347       |
|    learning_rate        | 1.37e-05    |
|    loss                 | 0.0474      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00507    |
|    value_loss           | 0.0818      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | 1.7      |
| time/              |          |
|    fps             | 951      |
|    iterations      | 5        |
|    time_elapsed    | 21       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 104         |
|    ep_rew_mean          | 2.12        |
| time/                   |             |
|    fps                  | 962         |
|    iterations           | 6           |
|    time_elapsed         | 25          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.011484236 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.373       |
|    learning_rate        | 1.37e-05    |
|    loss                 | 0.00515     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.0775      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=3.40 +/- 1.36
Episode length: 110.90 +/- 22.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 111          |
|    mean_reward          | 3.4          |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0072613265 |
|    clip_fraction        | 0.0941       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.16        |
|    explained_variance   | 0.344        |
|    learning_rate        | 1.37e-05     |
|    loss                 | 0.0236       |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00555     |
|    value_loss           | 0.0883       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.38     |
| time/              |          |
|    fps             | 947      |
|    iterations      | 7        |
|    time_elapsed    | 30       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-1.00 +/- 0.00
Episode length: 77.00 +/- 9.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77          |
|    mean_reward          | -1          |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.009431982 |
|    clip_fraction        | 0.011       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.436       |
|    learning_rate        | 1.37e-05    |
|    loss                 | 0.0501      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00139    |
|    value_loss           | 0.0837      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.26     |
| time/              |          |
|    fps             | 942      |
|    iterations      | 8        |
|    time_elapsed    | 34       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=3.70 +/- 1.10
Episode length: 110.50 +/- 15.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 110         |
|    mean_reward          | 3.7         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.009653851 |
|    clip_fraction        | 0.00417     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.367       |
|    learning_rate        | 1.37e-05    |
|    loss                 | 0.0194      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 0.0885      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.29     |
| time/              |          |
|    fps             | 933      |
|    iterations      | 9        |
|    time_elapsed    | 39       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=0.40 +/- 0.49
Episode length: 72.50 +/- 12.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.5        |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.014226899 |
|    clip_fraction        | 0.0181      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.416       |
|    learning_rate        | 1.37e-05    |
|    loss                 | 0.039       |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 0.0907      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 931      |
|    iterations      | 10       |
|    time_elapsed    | 43       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=0.80 +/- 0.87
Episode length: 75.60 +/- 14.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.6        |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.010390772 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.395       |
|    learning_rate        | 1.37e-05    |
|    loss                 | 0.0415      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00532    |
|    value_loss           | 0.093       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.57     |
| time/              |          |
|    fps             | 929      |
|    iterations      | 11       |
|    time_elapsed    | 48       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 109         |
|    ep_rew_mean          | 2.52        |
| time/                   |             |
|    fps                  | 936         |
|    iterations           | 12          |
|    time_elapsed         | 52          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.016341869 |
|    clip_fraction        | 0.0631      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.472       |
|    learning_rate        | 1.37e-05    |
|    loss                 | 0.0475      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00486    |
|    value_loss           | 0.0799      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=0.60 +/- 0.49
Episode length: 82.90 +/- 13.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.9        |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.009646751 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.975      |
|    explained_variance   | 0.445       |
|    learning_rate        | 1.37e-05    |
|    loss                 | 0.0527      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00258    |
|    value_loss           | 0.0804      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.47     |
| time/              |          |
|    fps             | 932      |
|    iterations      | 13       |
|    time_elapsed    | 57       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=5.20 +/- 1.54
Episode length: 134.50 +/- 32.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 134         |
|    mean_reward          | 5.2         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.012483422 |
|    clip_fraction        | 0.072       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.959      |
|    explained_variance   | 0.506       |
|    learning_rate        | 1.37e-05    |
|    loss                 | 0.03        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00478    |
|    value_loss           | 0.0751      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 107      |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 924      |
|    iterations      | 14       |
|    time_elapsed    | 61       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=3.60 +/- 1.56
Episode length: 115.20 +/- 23.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 115          |
|    mean_reward          | 3.6          |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0028205113 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.908       |
|    explained_variance   | 0.466        |
|    learning_rate        | 1.37e-05     |
|    loss                 | 0.0298       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00137     |
|    value_loss           | 0.0807       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 2.36     |
| time/              |          |
|    fps             | 920      |
|    iterations      | 15       |
|    time_elapsed    | 66       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=0.00 +/- 1.00
Episode length: 83.70 +/- 13.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.7     |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 0.23     |
| time/              |          |
|    fps             | 1122     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=2.90 +/- 1.22
Episode length: 92.20 +/- 21.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.2        |
|    mean_reward          | 2.9         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.007343294 |
|    clip_fraction        | 0.0577      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0235     |
|    learning_rate        | 0.000311    |
|    loss                 | 0.000296    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00822    |
|    value_loss           | 0.0403      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=4.50 +/- 1.80
Episode length: 118.80 +/- 34.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 4.5      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.8     |
|    ep_rew_mean     | 0.27     |
| time/              |          |
|    fps             | 923      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=3.70 +/- 0.90
Episode length: 105.00 +/- 22.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 105         |
|    mean_reward          | 3.7         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.014788077 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.000311    |
|    loss                 | -0.0463     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.0346      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.6     |
|    ep_rew_mean     | 0.91     |
| time/              |          |
|    fps             | 892      |
|    iterations      | 3        |
|    time_elapsed    | 27       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=6.60 +/- 1.74
Episode length: 146.20 +/- 29.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 146         |
|    mean_reward          | 6.6         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.021802315 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.489       |
|    learning_rate        | 0.000311    |
|    loss                 | -0.00865    |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0326     |
|    value_loss           | 0.043       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=5.40 +/- 1.56
Episode length: 132.90 +/- 26.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 5.4      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.1     |
|    ep_rew_mean     | 1.65     |
| time/              |          |
|    fps             | 850      |
|    iterations      | 4        |
|    time_elapsed    | 38       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=4.10 +/- 2.47
Episode length: 122.90 +/- 31.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 123         |
|    mean_reward          | 4.1         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.025183424 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.000311    |
|    loss                 | -0.0504     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0378     |
|    value_loss           | 0.0412      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=4.80 +/- 1.72
Episode length: 128.90 +/- 29.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 129      |
|    mean_reward     | 4.8      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.66     |
| time/              |          |
|    fps             | 833      |
|    iterations      | 5        |
|    time_elapsed    | 49       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=8.70 +/- 4.22
Episode length: 186.50 +/- 61.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 186        |
|    mean_reward          | 8.7        |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.02703657 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.000311   |
|    loss                 | -0.073     |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0388    |
|    value_loss           | 0.0469     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 3.89     |
| time/              |          |
|    fps             | 828      |
|    iterations      | 6        |
|    time_elapsed    | 59       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=9.50 +/- 3.67
Episode length: 198.30 +/- 58.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 198         |
|    mean_reward          | 9.5         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.032504983 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.000311    |
|    loss                 | -0.0364     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0397     |
|    value_loss           | 0.0542      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=9.50 +/- 4.90
Episode length: 188.90 +/- 74.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 189      |
|    mean_reward     | 9.5      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 5.36     |
| time/              |          |
|    fps             | 810      |
|    iterations      | 7        |
|    time_elapsed    | 70       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=7.30 +/- 4.29
Episode length: 149.40 +/- 55.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 149         |
|    mean_reward          | 7.3         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.038263977 |
|    clip_fraction        | 0.37        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.000311    |
|    loss                 | -0.00388    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.036      |
|    value_loss           | 0.0567      |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=6.00 +/- 2.86
Episode length: 142.50 +/- 46.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | 6        |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 7.23     |
| time/              |          |
|    fps             | 804      |
|    iterations      | 8        |
|    time_elapsed    | 81       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=0.20 +/- 0.40
Episode length: 70.20 +/- 13.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.2     |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.5     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 1148     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=1.20 +/- 0.40
Episode length: 86.50 +/- 20.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.5        |
|    mean_reward          | 1.2         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.008238055 |
|    clip_fraction        | 0.0792      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.049      |
|    learning_rate        | 0.00109     |
|    loss                 | -0.013      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00818    |
|    value_loss           | 0.0414      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=2.00 +/- 1.26
Episode length: 94.00 +/- 27.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94       |
|    mean_reward     | 2        |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.5     |
|    ep_rew_mean     | 0.42     |
| time/              |          |
|    fps             | 835      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=2.50 +/- 0.92
Episode length: 107.80 +/- 16.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 108         |
|    mean_reward          | 2.5         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.019557199 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.405       |
|    learning_rate        | 0.00109     |
|    loss                 | -0.018      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 0.0353      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.5     |
|    ep_rew_mean     | 0.95     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=2.80 +/- 0.87
Episode length: 111.40 +/- 11.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 111         |
|    mean_reward          | 2.8         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.033092894 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.469       |
|    learning_rate        | 0.00109     |
|    loss                 | -0.0586     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0347     |
|    value_loss           | 0.0381      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=3.50 +/- 2.16
Episode length: 118.00 +/- 32.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 3.5      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95       |
|    ep_rew_mean     | 1.75     |
| time/              |          |
|    fps             | 743      |
|    iterations      | 4        |
|    time_elapsed    | 44       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=3.00 +/- 1.18
Episode length: 113.50 +/- 20.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 3           |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.051779643 |
|    clip_fraction        | 0.34        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.00109     |
|    loss                 | -0.0118     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0385     |
|    value_loss           | 0.0419      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=3.20 +/- 1.17
Episode length: 103.40 +/- 15.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 3.2      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 2.71     |
| time/              |          |
|    fps             | 721      |
|    iterations      | 5        |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=4.30 +/- 1.19
Episode length: 118.90 +/- 28.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 119         |
|    mean_reward          | 4.3         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.061380047 |
|    clip_fraction        | 0.388       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.00109     |
|    loss                 | -0.0104     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0464     |
|    value_loss           | 0.0388      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 4.29     |
| time/              |          |
|    fps             | 714      |
|    iterations      | 6        |
|    time_elapsed    | 68       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=11.90 +/- 2.34
Episode length: 234.20 +/- 46.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 234         |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.077323094 |
|    clip_fraction        | 0.443       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.00109     |
|    loss                 | -0.0422     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.049      |
|    value_loss           | 0.0461      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=11.10 +/- 4.18
Episode length: 219.00 +/- 63.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 219      |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 5.8      |
| time/              |          |
|    fps             | 692      |
|    iterations      | 7        |
|    time_elapsed    | 82       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=11.90 +/- 2.51
Episode length: 214.50 +/- 38.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 214         |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.110259816 |
|    clip_fraction        | 0.472       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.00109     |
|    loss                 | -0.0906     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0528     |
|    value_loss           | 0.0479      |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=13.20 +/- 1.66
Episode length: 236.80 +/- 21.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 237      |
|    mean_reward     | 13.2     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 6.74     |
| time/              |          |
|    fps             | 676      |
|    iterations      | 8        |
|    time_elapsed    | 96       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=4.30 +/- 1.10
Episode length: 124.60 +/- 19.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 125      |
|    mean_reward     | 4.3      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81       |
|    ep_rew_mean     | 0.06     |
| time/              |          |
|    fps             | 1097     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.60 +/- 0.92
Episode length: 76.40 +/- 16.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.4        |
|    mean_reward          | 0.6         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.008666145 |
|    clip_fraction        | 0.056       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.165      |
|    learning_rate        | 0.000415    |
|    loss                 | 0.0118      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00693    |
|    value_loss           | 0.0813      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=0.30 +/- 0.46
Episode length: 75.90 +/- 10.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.9     |
|    mean_reward     | 0.3      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79       |
|    ep_rew_mean     | 0.14     |
| time/              |          |
|    fps             | 930      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=0.40 +/- 0.66
Episode length: 70.80 +/- 13.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 70.8        |
|    mean_reward          | 0.4         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.011224285 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.451       |
|    learning_rate        | 0.000415    |
|    loss                 | -0.00506    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.0678      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.4     |
|    ep_rew_mean     | 0.49     |
| time/              |          |
|    fps             | 902      |
|    iterations      | 3        |
|    time_elapsed    | 27       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=0.80 +/- 0.75
Episode length: 74.30 +/- 14.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.3        |
|    mean_reward          | 0.8         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.021392405 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.000415    |
|    loss                 | -0.0105     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0281     |
|    value_loss           | 0.057       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=0.80 +/- 0.98
Episode length: 80.10 +/- 12.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.1     |
|    mean_reward     | 0.8      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.5     |
|    ep_rew_mean     | 0.72     |
| time/              |          |
|    fps             | 876      |
|    iterations      | 4        |
|    time_elapsed    | 37       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=1.70 +/- 0.90
Episode length: 84.00 +/- 20.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84          |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.030637126 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.000415    |
|    loss                 | -0.0128     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 0.0494      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=1.90 +/- 1.92
Episode length: 92.00 +/- 28.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92       |
|    mean_reward     | 1.9      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.5     |
|    ep_rew_mean     | 1.36     |
| time/              |          |
|    fps             | 862      |
|    iterations      | 5        |
|    time_elapsed    | 47       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=2.10 +/- 2.07
Episode length: 98.10 +/- 36.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.1        |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.025066894 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.000415    |
|    loss                 | -0.0408     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.0452      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.7     |
|    ep_rew_mean     | 1.99     |
| time/              |          |
|    fps             | 859      |
|    iterations      | 6        |
|    time_elapsed    | 57       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=7.90 +/- 1.76
Episode length: 162.40 +/- 33.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 162         |
|    mean_reward          | 7.9         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.027506927 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.000415    |
|    loss                 | -0.0295     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0301     |
|    value_loss           | 0.0571      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=9.90 +/- 2.34
Episode length: 194.20 +/- 39.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 194      |
|    mean_reward     | 9.9      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 3.14     |
| time/              |          |
|    fps             | 838      |
|    iterations      | 7        |
|    time_elapsed    | 68       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=8.70 +/- 3.72
Episode length: 180.60 +/- 52.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 181         |
|    mean_reward          | 8.7         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.032975115 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.000415    |
|    loss                 | -0.0222     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 0.061       |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=9.20 +/- 5.27
Episode length: 174.50 +/- 82.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 9.2      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 4.86     |
| time/              |          |
|    fps             | 822      |
|    iterations      | 8        |
|    time_elapsed    | 79       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=0.70 +/- 0.64
Episode length: 76.10 +/- 10.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.1     |
|    mean_reward     | 0.7      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.8     |
|    ep_rew_mean     | 0.17     |
| time/              |          |
|    fps             | 1148     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=4.80 +/- 0.87
Episode length: 129.60 +/- 12.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 130          |
|    mean_reward          | 4.8          |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0111957295 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.37        |
|    explained_variance   | 0.00542      |
|    learning_rate        | 0.00011      |
|    loss                 | 0.0199       |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00717     |
|    value_loss           | 0.0806       |
------------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=5.70 +/- 1.10
Episode length: 133.40 +/- 19.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 5.7      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.8     |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 916      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=4.30 +/- 1.10
Episode length: 129.40 +/- 13.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 129         |
|    mean_reward          | 4.3         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.009814093 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.444       |
|    learning_rate        | 0.00011     |
|    loss                 | 0.0232      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.0757      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.2     |
|    ep_rew_mean     | 1.37     |
| time/              |          |
|    fps             | 883      |
|    iterations      | 3        |
|    time_elapsed    | 27       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=13.40 +/- 2.11
Episode length: 270.20 +/- 35.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 270         |
|    mean_reward          | 13.4        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.011282118 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.515       |
|    learning_rate        | 0.00011     |
|    loss                 | -0.018      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.0815      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=12.30 +/- 1.90
Episode length: 246.00 +/- 39.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 246      |
|    mean_reward     | 12.3     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.21     |
| time/              |          |
|    fps             | 811      |
|    iterations      | 4        |
|    time_elapsed    | 40       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=7.20 +/- 1.72
Episode length: 165.60 +/- 40.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 7.2         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.012548897 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.527       |
|    learning_rate        | 0.00011     |
|    loss                 | 0.0195      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0244     |
|    value_loss           | 0.0961      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=8.50 +/- 2.11
Episode length: 186.60 +/- 34.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 187      |
|    mean_reward     | 8.5      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 3.41     |
| time/              |          |
|    fps             | 790      |
|    iterations      | 5        |
|    time_elapsed    | 51       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=8.00 +/- 2.53
Episode length: 194.10 +/- 50.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 194         |
|    mean_reward          | 8           |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.012732686 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.00011     |
|    loss                 | -0.00875    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.104       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 3.99     |
| time/              |          |
|    fps             | 790      |
|    iterations      | 6        |
|    time_elapsed    | 62       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=9.70 +/- 1.10
Episode length: 208.70 +/- 9.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 209         |
|    mean_reward          | 9.7         |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.014520071 |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.00011     |
|    loss                 | 0.0283      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.026      |
|    value_loss           | 0.11        |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=9.50 +/- 1.20
Episode length: 212.70 +/- 24.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 213      |
|    mean_reward     | 9.5      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 4.85     |
| time/              |          |
|    fps             | 774      |
|    iterations      | 7        |
|    time_elapsed    | 74       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=12.60 +/- 2.91
Episode length: 239.00 +/- 41.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 239        |
|    mean_reward          | 12.6       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.01472763 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.00011    |
|    loss                 | -0.0065    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0259    |
|    value_loss           | 0.119      |
----------------------------------------
Eval num_timesteps=65000, episode_reward=14.90 +/- 2.26
Episode length: 269.70 +/- 23.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 270      |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 5.93     |
| time/              |          |
|    fps             | 757      |
|    iterations      | 8        |
|    time_elapsed    | 86       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.5     |
|    ep_rew_mean     | 0.0385   |
| time/              |          |
|    fps             | 1236     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 80.2        |
|    ep_rew_mean          | 0.157       |
| time/                   |             |
|    fps                  | 1120        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.012649642 |
|    clip_fraction        | 0.0519      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0361     |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0143      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00618    |
|    value_loss           | 0.061       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=0.20 +/- 0.40
Episode length: 72.20 +/- 11.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.2        |
|    mean_reward          | 0.2         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.013074414 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.196       |
|    learning_rate        | 5.5e-05     |
|    loss                 | -0.0158     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0058     |
|    value_loss           | 0.0508      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81       |
|    ep_rew_mean     | 0.16     |
| time/              |          |
|    fps             | 998      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 81.4        |
|    ep_rew_mean          | 0.18        |
| time/                   |             |
|    fps                  | 1005        |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.016301557 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.425       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0148      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00849    |
|    value_loss           | 0.0437      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=0.50 +/- 0.67
Episode length: 74.60 +/- 15.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.6        |
|    mean_reward          | 0.5         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.007516084 |
|    clip_fraction        | 0.0709      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.499       |
|    learning_rate        | 5.5e-05     |
|    loss                 | -0.00996    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00544    |
|    value_loss           | 0.0424      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.1     |
|    ep_rew_mean     | 0.37     |
| time/              |          |
|    fps             | 960      |
|    iterations      | 5        |
|    time_elapsed    | 10       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 87.4        |
|    ep_rew_mean          | 0.7         |
| time/                   |             |
|    fps                  | 969         |
|    iterations           | 6           |
|    time_elapsed         | 12          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.011356879 |
|    clip_fraction        | 0.0956      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.483       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0346      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00741    |
|    value_loss           | 0.0551      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 91.8        |
|    ep_rew_mean          | 1.04        |
| time/                   |             |
|    fps                  | 975         |
|    iterations           | 7           |
|    time_elapsed         | 14          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.008109315 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.394       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0256      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00725    |
|    value_loss           | 0.0811      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=1.10 +/- 0.83
Episode length: 86.40 +/- 21.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.4        |
|    mean_reward          | 1.1         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.009971764 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.474       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0144      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.0641      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97       |
|    ep_rew_mean     | 1.52     |
| time/              |          |
|    fps             | 945      |
|    iterations      | 8        |
|    time_elapsed    | 17       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 1.94        |
| time/                   |             |
|    fps                  | 952         |
|    iterations           | 9           |
|    time_elapsed         | 19          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.009560581 |
|    clip_fraction        | 0.0495      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.45        |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0254      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00426    |
|    value_loss           | 0.0709      |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=9.90 +/- 3.91
Episode length: 210.20 +/- 64.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 210         |
|    mean_reward          | 9.9         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.013463499 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.548       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0206      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.0741      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    fps             | 898      |
|    iterations      | 10       |
|    time_elapsed    | 22       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 107         |
|    ep_rew_mean          | 2.44        |
| time/                   |             |
|    fps                  | 907         |
|    iterations           | 11          |
|    time_elapsed         | 24          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.011813901 |
|    clip_fraction        | 0.0534      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.527       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0458      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 0.068       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 109         |
|    ep_rew_mean          | 2.5         |
| time/                   |             |
|    fps                  | 916         |
|    iterations           | 12          |
|    time_elapsed         | 26          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.006745152 |
|    clip_fraction        | 0.0828      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.389       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.00935     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00746    |
|    value_loss           | 0.078       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=5.80 +/- 1.83
Episode length: 157.80 +/- 36.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 158          |
|    mean_reward          | 5.8          |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0071028606 |
|    clip_fraction        | 0.0642       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.941       |
|    explained_variance   | 0.396        |
|    learning_rate        | 5.5e-05      |
|    loss                 | 0.0299       |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00773     |
|    value_loss           | 0.0834       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 2.61     |
| time/              |          |
|    fps             | 891      |
|    iterations      | 13       |
|    time_elapsed    | 29       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | 2.71        |
| time/                   |             |
|    fps                  | 899         |
|    iterations           | 14          |
|    time_elapsed         | 31          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.006524558 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.894      |
|    explained_variance   | 0.348       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0203      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00752    |
|    value_loss           | 0.0858      |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=5.60 +/- 1.20
Episode length: 131.80 +/- 22.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 132          |
|    mean_reward          | 5.6          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0031461534 |
|    clip_fraction        | 0.0567       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.858       |
|    explained_variance   | 0.385        |
|    learning_rate        | 5.5e-05      |
|    loss                 | 0.0311       |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00574     |
|    value_loss           | 0.0968       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 2.84     |
| time/              |          |
|    fps             | 883      |
|    iterations      | 15       |
|    time_elapsed    | 34       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | 3.04        |
| time/                   |             |
|    fps                  | 890         |
|    iterations           | 16          |
|    time_elapsed         | 36          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.011060725 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.814      |
|    explained_variance   | 0.433       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0422      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.0904      |
-----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 114          |
|    ep_rew_mean          | 3.24         |
| time/                   |              |
|    fps                  | 897          |
|    iterations           | 17           |
|    time_elapsed         | 38           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0077178376 |
|    clip_fraction        | 0.107        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.778       |
|    explained_variance   | 0.453        |
|    learning_rate        | 5.5e-05      |
|    loss                 | 0.0459       |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00537     |
|    value_loss           | 0.092        |
------------------------------------------
Eval num_timesteps=35000, episode_reward=5.30 +/- 0.90
Episode length: 125.80 +/- 19.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 126          |
|    mean_reward          | 5.3          |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0057886625 |
|    clip_fraction        | 0.0729       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.727       |
|    explained_variance   | 0.5          |
|    learning_rate        | 5.5e-05      |
|    loss                 | 0.0134       |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00586     |
|    value_loss           | 0.081        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.48     |
| time/              |          |
|    fps             | 885      |
|    iterations      | 18       |
|    time_elapsed    | 41       |
|    total_timesteps | 36864    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 117          |
|    ep_rew_mean          | 3.66         |
| time/                   |              |
|    fps                  | 891          |
|    iterations           | 19           |
|    time_elapsed         | 43           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0039447453 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.703       |
|    explained_variance   | 0.463        |
|    learning_rate        | 5.5e-05      |
|    loss                 | 0.0368       |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00777     |
|    value_loss           | 0.0923       |
------------------------------------------
Eval num_timesteps=40000, episode_reward=6.40 +/- 1.11
Episode length: 143.80 +/- 17.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 144         |
|    mean_reward          | 6.4         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.009655437 |
|    clip_fraction        | 0.0687      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.679      |
|    explained_variance   | 0.441       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0516      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00429    |
|    value_loss           | 0.0914      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 118      |
|    ep_rew_mean     | 3.84     |
| time/              |          |
|    fps             | 878      |
|    iterations      | 20       |
|    time_elapsed    | 46       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 119         |
|    ep_rew_mean          | 3.97        |
| time/                   |             |
|    fps                  | 884         |
|    iterations           | 21          |
|    time_elapsed         | 48          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.006439801 |
|    clip_fraction        | 0.0739      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.491       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0408      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00572    |
|    value_loss           | 0.0833      |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=6.40 +/- 0.92
Episode length: 164.60 +/- 31.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 165         |
|    mean_reward          | 6.4         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.006765471 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.612      |
|    explained_variance   | 0.527       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0355      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0071     |
|    value_loss           | 0.0837      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 4.15     |
| time/              |          |
|    fps             | 870      |
|    iterations      | 22       |
|    time_elapsed    | 51       |
|    total_timesteps | 45056    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | 4.44         |
| time/                   |              |
|    fps                  | 875          |
|    iterations           | 23           |
|    time_elapsed         | 53           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0034730658 |
|    clip_fraction        | 0.0857       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.607       |
|    explained_variance   | 0.534        |
|    learning_rate        | 5.5e-05      |
|    loss                 | 0.027        |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00281     |
|    value_loss           | 0.0817       |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 127         |
|    ep_rew_mean          | 4.71        |
| time/                   |             |
|    fps                  | 880         |
|    iterations           | 24          |
|    time_elapsed         | 55          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.004753283 |
|    clip_fraction        | 0.0834      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.608      |
|    explained_variance   | 0.588       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0399      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00483    |
|    value_loss           | 0.0766      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=6.40 +/- 1.36
Episode length: 157.00 +/- 27.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 157          |
|    mean_reward          | 6.4          |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0050066323 |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.574       |
|    explained_variance   | 0.491        |
|    learning_rate        | 5.5e-05      |
|    loss                 | 0.0563       |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00346     |
|    value_loss           | 0.0965       |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 4.79     |
| time/              |          |
|    fps             | 868      |
|    iterations      | 25       |
|    time_elapsed    | 58       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 130          |
|    ep_rew_mean          | 4.95         |
| time/                   |              |
|    fps                  | 873          |
|    iterations           | 26           |
|    time_elapsed         | 60           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0038191057 |
|    clip_fraction        | 0.0546       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.566       |
|    explained_variance   | 0.499        |
|    learning_rate        | 5.5e-05      |
|    loss                 | 0.0247       |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00574     |
|    value_loss           | 0.0922       |
------------------------------------------
Eval num_timesteps=55000, episode_reward=9.30 +/- 1.73
Episode length: 197.40 +/- 20.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 197         |
|    mean_reward          | 9.3         |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.006062621 |
|    clip_fraction        | 0.0793      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.484       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0297      |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00937    |
|    value_loss           | 0.0861      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 5.2      |
| time/              |          |
|    fps             | 860      |
|    iterations      | 27       |
|    time_elapsed    | 64       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 138         |
|    ep_rew_mean          | 5.45        |
| time/                   |             |
|    fps                  | 864         |
|    iterations           | 28          |
|    time_elapsed         | 66          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.004976677 |
|    clip_fraction        | 0.073       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.553      |
|    explained_variance   | 0.548       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0266      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00965    |
|    value_loss           | 0.0823      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 138         |
|    ep_rew_mean          | 5.53        |
| time/                   |             |
|    fps                  | 869         |
|    iterations           | 29          |
|    time_elapsed         | 68          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.005116343 |
|    clip_fraction        | 0.072       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.532      |
|    explained_variance   | 0.516       |
|    learning_rate        | 5.5e-05     |
|    loss                 | 0.0362      |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00708    |
|    value_loss           | 0.091       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=12.90 +/- 3.53
Episode length: 244.00 +/- 61.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 244          |
|    mean_reward          | 12.9         |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0049108164 |
|    clip_fraction        | 0.055        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.497       |
|    explained_variance   | 0.491        |
|    learning_rate        | 5.5e-05      |
|    loss                 | 0.0206       |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00612     |
|    value_loss           | 0.095        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 5.79     |
| time/              |          |
|    fps             | 853      |
|    iterations      | 30       |
|    time_elapsed    | 72       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.6     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 1206     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 82.7        |
|    ep_rew_mean          | 0.204       |
| time/                   |             |
|    fps                  | 1039        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.008365913 |
|    clip_fraction        | 0.0707      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.0685      |
|    learning_rate        | 0.000339    |
|    loss                 | 0.00951     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00525    |
|    value_loss           | 0.0776      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=5.80 +/- 2.09
Episode length: 144.00 +/- 31.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 144         |
|    mean_reward          | 5.8         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.009684973 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.311       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0132     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.0584      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.2     |
|    ep_rew_mean     | 0.329    |
| time/              |          |
|    fps             | 853      |
|    iterations      | 3        |
|    time_elapsed    | 7        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 86          |
|    ep_rew_mean          | 0.537       |
| time/                   |             |
|    fps                  | 867         |
|    iterations           | 4           |
|    time_elapsed         | 9           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.009697459 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0319     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.0579      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=2.20 +/- 1.47
Episode length: 103.40 +/- 26.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | 2.2         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.013859144 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.534       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.00647    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0331     |
|    value_loss           | 0.0555      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.9     |
|    ep_rew_mean     | 0.7      |
| time/              |          |
|    fps             | 823      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 92.6        |
|    ep_rew_mean          | 1.1         |
| time/                   |             |
|    fps                  | 836         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.018579163 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.000339    |
|    loss                 | 0.00744     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0362     |
|    value_loss           | 0.057       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 97.8        |
|    ep_rew_mean          | 1.73        |
| time/                   |             |
|    fps                  | 844         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.020160895 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0393     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0419     |
|    value_loss           | 0.0697      |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=7.70 +/- 4.24
Episode length: 167.30 +/- 76.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 167         |
|    mean_reward          | 7.7         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.023927698 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0442     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0425     |
|    value_loss           | 0.0809      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.26     |
| time/              |          |
|    fps             | 802      |
|    iterations      | 8        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 109         |
|    ep_rew_mean          | 2.92        |
| time/                   |             |
|    fps                  | 812         |
|    iterations           | 9           |
|    time_elapsed         | 22          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.027397878 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.18       |
|    explained_variance   | 0.531       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0587     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0452     |
|    value_loss           | 0.0723      |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=11.60 +/- 5.52
Episode length: 235.90 +/- 86.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 236         |
|    mean_reward          | 11.6        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.033210408 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0423     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0416     |
|    value_loss           | 0.0888      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 3.62     |
| time/              |          |
|    fps             | 772      |
|    iterations      | 10       |
|    time_elapsed    | 26       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | 4.37        |
| time/                   |             |
|    fps                  | 784         |
|    iterations           | 11          |
|    time_elapsed         | 28          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.034263477 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.449       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0497     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0398     |
|    value_loss           | 0.0896      |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 131        |
|    ep_rew_mean          | 5.05       |
| time/                   |            |
|    fps                  | 794        |
|    iterations           | 12         |
|    time_elapsed         | 30         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.03779938 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.981     |
|    explained_variance   | 0.405      |
|    learning_rate        | 0.000339   |
|    loss                 | -0.0383    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.039     |
|    value_loss           | 0.106      |
----------------------------------------
Eval num_timesteps=25000, episode_reward=15.30 +/- 3.55
Episode length: 291.50 +/- 65.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 292         |
|    mean_reward          | 15.3        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.039420504 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.985      |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0375     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0422     |
|    value_loss           | 0.0819      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 5.66     |
| time/              |          |
|    fps             | 759      |
|    iterations      | 13       |
|    time_elapsed    | 35       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 143         |
|    ep_rew_mean          | 6.18        |
| time/                   |             |
|    fps                  | 769         |
|    iterations           | 14          |
|    time_elapsed         | 37          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.046207905 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.91       |
|    explained_variance   | 0.491       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0263     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0419     |
|    value_loss           | 0.0917      |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=15.60 +/- 5.44
Episode length: 281.90 +/- 87.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 282         |
|    mean_reward          | 15.6        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.043866955 |
|    clip_fraction        | 0.311       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.843      |
|    explained_variance   | 0.495       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0249     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0387     |
|    value_loss           | 0.0923      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 6.9      |
| time/              |          |
|    fps             | 742      |
|    iterations      | 15       |
|    time_elapsed    | 41       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 159        |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 751        |
|    iterations           | 16         |
|    time_elapsed         | 43         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.05911559 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.784     |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.000339   |
|    loss                 | -0.0474    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.0359    |
|    value_loss           | 0.089      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 167        |
|    ep_rew_mean          | 8.16       |
| time/                   |            |
|    fps                  | 759        |
|    iterations           | 17         |
|    time_elapsed         | 45         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.05279714 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.784     |
|    explained_variance   | 0.459      |
|    learning_rate        | 0.000339   |
|    loss                 | -0.0388    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0336    |
|    value_loss           | 0.102      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=16.10 +/- 3.11
Episode length: 274.60 +/- 50.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 275        |
|    mean_reward          | 16.1       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.04907623 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.778     |
|    explained_variance   | 0.609      |
|    learning_rate        | 0.000339   |
|    loss                 | -0.0367    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0315    |
|    value_loss           | 0.0812     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 8.67     |
| time/              |          |
|    fps             | 738      |
|    iterations      | 18       |
|    time_elapsed    | 49       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 9.31       |
| time/                   |            |
|    fps                  | 745        |
|    iterations           | 19         |
|    time_elapsed         | 52         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.05780831 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.674     |
|    explained_variance   | 0.479      |
|    learning_rate        | 0.000339   |
|    loss                 | -0.0186    |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0318    |
|    value_loss           | 0.0951     |
----------------------------------------
Eval num_timesteps=40000, episode_reward=15.50 +/- 2.33
Episode length: 274.90 +/- 34.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 275        |
|    mean_reward          | 15.5       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.04776305 |
|    clip_fraction        | 0.278      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.627     |
|    explained_variance   | 0.487      |
|    learning_rate        | 0.000339   |
|    loss                 | -0.0364    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0318    |
|    value_loss           | 0.093      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 9.66     |
| time/              |          |
|    fps             | 728      |
|    iterations      | 20       |
|    time_elapsed    | 56       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 198         |
|    ep_rew_mean          | 10.2        |
| time/                   |             |
|    fps                  | 736         |
|    iterations           | 21          |
|    time_elapsed         | 58          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.059778944 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.666      |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.000339    |
|    loss                 | 0.0164      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0331     |
|    value_loss           | 0.0856      |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=18.10 +/- 2.81
Episode length: 315.60 +/- 43.04
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 316        |
|    mean_reward          | 18.1       |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.06286669 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.59      |
|    explained_variance   | 0.535      |
|    learning_rate        | 0.000339   |
|    loss                 | -0.0318    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0314    |
|    value_loss           | 0.0835     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 718      |
|    iterations      | 22       |
|    time_elapsed    | 62       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 214         |
|    ep_rew_mean          | 11.5        |
| time/                   |             |
|    fps                  | 724         |
|    iterations           | 23          |
|    time_elapsed         | 65          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.062253732 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.57       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.035      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.083       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 221         |
|    ep_rew_mean          | 12          |
| time/                   |             |
|    fps                  | 730         |
|    iterations           | 24          |
|    time_elapsed         | 67          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.059478175 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.589      |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.000339    |
|    loss                 | -0.019      |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0323     |
|    value_loss           | 0.0851      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=15.50 +/- 2.94
Episode length: 257.00 +/- 42.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 257         |
|    mean_reward          | 15.5        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.056277167 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.625      |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0122     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0318     |
|    value_loss           | 0.0849      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 12.5     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 25       |
|    time_elapsed    | 71       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 229         |
|    ep_rew_mean          | 12.6        |
| time/                   |             |
|    fps                  | 724         |
|    iterations           | 26          |
|    time_elapsed         | 73          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.064239755 |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.578      |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0446     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 0.083       |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=16.10 +/- 1.64
Episode length: 280.70 +/- 32.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 281         |
|    mean_reward          | 16.1        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.066997245 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.529      |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0116     |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.0303     |
|    value_loss           | 0.0953      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 13       |
| time/              |          |
|    fps             | 712      |
|    iterations      | 27       |
|    time_elapsed    | 77       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 13.1       |
| time/                   |            |
|    fps                  | 718        |
|    iterations           | 28         |
|    time_elapsed         | 79         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.07146342 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.518     |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.000339   |
|    loss                 | -0.0447    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.027     |
|    value_loss           | 0.0782     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 238         |
|    ep_rew_mean          | 13.4        |
| time/                   |             |
|    fps                  | 723         |
|    iterations           | 29          |
|    time_elapsed         | 82          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.071762055 |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.511      |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.000339    |
|    loss                 | -0.0247     |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.03       |
|    value_loss           | 0.0816      |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=17.80 +/- 2.04
Episode length: 294.30 +/- 34.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 294        |
|    mean_reward          | 17.8       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.07532168 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.543     |
|    explained_variance   | 0.688      |
|    learning_rate        | 0.000339   |
|    loss                 | 0.0041     |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0255    |
|    value_loss           | 0.085      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 241      |
|    ep_rew_mean     | 13.7     |
| time/              |          |
|    fps             | 711      |
|    iterations      | 30       |
|    time_elapsed    | 86       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
Eval num_timesteps=5000, episode_reward=-0.40 +/- 0.66
Episode length: 75.80 +/- 9.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.8     |
|    mean_reward     | -0.4     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.7     |
|    ep_rew_mean     | -0.12    |
| time/              |          |
|    fps             | 1140     |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=0.00 +/- 1.00
Episode length: 77.30 +/- 16.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77.3        |
|    mean_reward          | 0           |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010563183 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0241     |
|    learning_rate        | 0.000258    |
|    loss                 | -0.00662    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.0438      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=0.10 +/- 0.70
Episode length: 72.60 +/- 8.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.6     |
|    mean_reward     | 0.1      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.5     |
|    ep_rew_mean     | 0.18     |
| time/              |          |
|    fps             | 845      |
|    iterations      | 2        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=0.50 +/- 1.02
Episode length: 79.90 +/- 14.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 79.9         |
|    mean_reward          | 0.5          |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0151668675 |
|    clip_fraction        | 0.149        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | 0.336        |
|    learning_rate        | 0.000258     |
|    loss                 | -0.00991     |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.0253      |
|    value_loss           | 0.0414       |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.2     |
|    ep_rew_mean     | 0.66     |
| time/              |          |
|    fps             | 789      |
|    iterations      | 3        |
|    time_elapsed    | 31       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=1.80 +/- 2.44
Episode length: 93.50 +/- 47.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 93.5       |
|    mean_reward          | 1.8        |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.02634921 |
|    clip_fraction        | 0.215      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.487      |
|    learning_rate        | 0.000258   |
|    loss                 | -0.0372    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0297    |
|    value_loss           | 0.0446     |
----------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=0.20 +/- 0.75
Episode length: 70.80 +/- 11.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.8     |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80       |
|    ep_rew_mean     | 0.86     |
| time/              |          |
|    fps             | 758      |
|    iterations      | 4        |
|    time_elapsed    | 43       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=1.00 +/- 0.63
Episode length: 75.10 +/- 10.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.1        |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.024307206 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.000258    |
|    loss                 | -0.025      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.0388      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=1.30 +/- 1.35
Episode length: 74.80 +/- 12.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.8     |
|    mean_reward     | 1.3      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.2     |
|    ep_rew_mean     | 1.15     |
| time/              |          |
|    fps             | 744      |
|    iterations      | 5        |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=1.00 +/- 0.89
Episode length: 78.00 +/- 15.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 78          |
|    mean_reward          | 1           |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.022972142 |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.000258    |
|    loss                 | -0.0666     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0282     |
|    value_loss           | 0.0306      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.2     |
|    ep_rew_mean     | 1.54     |
| time/              |          |
|    fps             | 734      |
|    iterations      | 6        |
|    time_elapsed    | 66       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=50000, episode_reward=2.10 +/- 1.51
Episode length: 85.80 +/- 21.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 85.8       |
|    mean_reward          | 2.1        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.03199448 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.24      |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.000258   |
|    loss                 | -0.063     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0325    |
|    value_loss           | 0.036      |
----------------------------------------
New best mean reward!
Eval num_timesteps=55000, episode_reward=2.10 +/- 1.97
Episode length: 87.40 +/- 25.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.4     |
|    mean_reward     | 2.1      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.1     |
|    ep_rew_mean     | 1.79     |
| time/              |          |
|    fps             | 723      |
|    iterations      | 7        |
|    time_elapsed    | 79       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=3.00 +/- 2.19
Episode length: 100.70 +/- 46.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | 3           |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.036752336 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.000258    |
|    loss                 | -0.0396     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0343     |
|    value_loss           | 0.0428      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=65000, episode_reward=3.00 +/- 2.24
Episode length: 97.00 +/- 35.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97       |
|    mean_reward     | 3        |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 2.64     |
| time/              |          |
|    fps             | 716      |
|    iterations      | 8        |
|    time_elapsed    | 91       |
|    total_timesteps | 65536    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.8     |
|    ep_rew_mean     | 0.0392   |
| time/              |          |
|    fps             | 1227     |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=5000, episode_reward=3.20 +/- 1.54
Episode length: 108.70 +/- 18.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 3.2         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.015395002 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.0227     |
|    learning_rate        | 0.000545    |
|    loss                 | -0.0305     |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.0699      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.9     |
|    ep_rew_mean     | 0.07     |
| time/              |          |
|    fps             | 950      |
|    iterations      | 2        |
|    time_elapsed    | 8        |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=10000, episode_reward=2.80 +/- 1.40
Episode length: 97.80 +/- 25.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.8        |
|    mean_reward          | 2.8         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.010377552 |
|    clip_fraction        | 0.0919      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.215       |
|    learning_rate        | 0.000545    |
|    loss                 | -0.0224     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.0716      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.5     |
|    ep_rew_mean     | 0.55     |
| time/              |          |
|    fps             | 895      |
|    iterations      | 3        |
|    time_elapsed    | 13       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=15000, episode_reward=3.10 +/- 1.81
Episode length: 93.10 +/- 27.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.1        |
|    mean_reward          | 3.1         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.016653378 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.205       |
|    learning_rate        | 0.000545    |
|    loss                 | -0.0187     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.0879      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.7     |
|    ep_rew_mean     | 0.82     |
| time/              |          |
|    fps             | 873      |
|    iterations      | 4        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=20000, episode_reward=1.80 +/- 0.98
Episode length: 82.40 +/- 21.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.4        |
|    mean_reward          | 1.8         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.026962409 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.289       |
|    learning_rate        | 0.000545    |
|    loss                 | -0.0125     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 0.0834      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.5     |
|    ep_rew_mean     | 0.79     |
| time/              |          |
|    fps             | 857      |
|    iterations      | 5        |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 85.4        |
|    ep_rew_mean          | 1.21        |
| time/                   |             |
|    fps                  | 864         |
|    iterations           | 6           |
|    time_elapsed         | 28          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.026764885 |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.464       |
|    learning_rate        | 0.000545    |
|    loss                 | -0.0129     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0314     |
|    value_loss           | 0.0665      |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=4.60 +/- 1.96
Episode length: 115.90 +/- 25.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 116         |
|    mean_reward          | 4.6         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.026413811 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.413       |
|    learning_rate        | 0.000545    |
|    loss                 | -0.00495    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0304     |
|    value_loss           | 0.0719      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.4     |
|    ep_rew_mean     | 1.99     |
| time/              |          |
|    fps             | 848      |
|    iterations      | 7        |
|    time_elapsed    | 33       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=30000, episode_reward=12.90 +/- 2.77
Episode length: 236.40 +/- 63.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 236        |
|    mean_reward          | 12.9       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.03679832 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.513      |
|    learning_rate        | 0.000545   |
|    loss                 | -0.0169    |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0406    |
|    value_loss           | 0.0801     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 3.31     |
| time/              |          |
|    fps             | 821      |
|    iterations      | 8        |
|    time_elapsed    | 39       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=35000, episode_reward=7.20 +/- 3.25
Episode length: 162.60 +/- 55.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 163         |
|    mean_reward          | 7.2         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.043867737 |
|    clip_fraction        | 0.348       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.419       |
|    learning_rate        | 0.000545    |
|    loss                 | -0.0478     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0437     |
|    value_loss           | 0.0933      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 4.68     |
| time/              |          |
|    fps             | 811      |
|    iterations      | 9        |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=40000, episode_reward=11.00 +/- 2.49
Episode length: 211.90 +/- 46.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 212       |
|    mean_reward          | 11        |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0432988 |
|    clip_fraction        | 0.354     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.07     |
|    explained_variance   | 0.442     |
|    learning_rate        | 0.000545  |
|    loss                 | -0.0551   |
|    n_updates            | 90        |
|    policy_gradient_loss | -0.0407   |
|    value_loss           | 0.0929    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 5.67     |
| time/              |          |
|    fps             | 797      |
|    iterations      | 10       |
|    time_elapsed    | 51       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=45000, episode_reward=8.90 +/- 4.64
Episode length: 175.80 +/- 71.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 176         |
|    mean_reward          | 8.9         |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.050347473 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.523       |
|    learning_rate        | 0.000545    |
|    loss                 | -0.00913    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0377     |
|    value_loss           | 0.0876      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 6.32     |
| time/              |          |
|    fps             | 791      |
|    iterations      | 11       |
|    time_elapsed    | 56       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 150         |
|    ep_rew_mean          | 6.84        |
| time/                   |             |
|    fps                  | 800         |
|    iterations           | 12          |
|    time_elapsed         | 61          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.060482018 |
|    clip_fraction        | 0.368       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.512       |
|    learning_rate        | 0.000545    |
|    loss                 | -0.0414     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0414     |
|    value_loss           | 0.082       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=6.10 +/- 2.12
Episode length: 137.00 +/- 34.29
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 137        |
|    mean_reward          | 6.1        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.06613959 |
|    clip_fraction        | 0.384      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.949     |
|    explained_variance   | 0.505      |
|    learning_rate        | 0.000545   |
|    loss                 | 0.000128   |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0375    |
|    value_loss           | 0.0886     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    fps             | 797      |
|    iterations      | 13       |
|    time_elapsed    | 66       |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=55000, episode_reward=9.20 +/- 2.89
Episode length: 161.40 +/- 45.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 161        |
|    mean_reward          | 9.2        |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.06770477 |
|    clip_fraction        | 0.381      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.921     |
|    explained_variance   | 0.434      |
|    learning_rate        | 0.000545   |
|    loss                 | -0.0225    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.04      |
|    value_loss           | 0.0898     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 7.99     |
| time/              |          |
|    fps             | 792      |
|    iterations      | 14       |
|    time_elapsed    | 72       |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=60000, episode_reward=8.10 +/- 3.94
Episode length: 163.40 +/- 53.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 163        |
|    mean_reward          | 8.1        |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.06674487 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.881     |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.000545   |
|    loss                 | -0.0492    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.043     |
|    value_loss           | 0.0864     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 8.57     |
| time/              |          |
|    fps             | 787      |
|    iterations      | 15       |
|    time_elapsed    | 77       |
|    total_timesteps | 61440    |
---------------------------------
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.6     |
|    ep_rew_mean     | 0.36     |
| time/              |          |
|    fps             | 1232     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 81.6        |
|    ep_rew_mean          | 0.24        |
| time/                   |             |
|    fps                  | 1110        |
|    iterations           | 2           |
|    time_elapsed         | 3           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009688191 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0557     |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0469      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00705    |
|    value_loss           | 0.141       |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=1.70 +/- 1.27
Episode length: 92.40 +/- 21.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.4        |
|    mean_reward          | 1.7         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.007490633 |
|    clip_fraction        | 0.08        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0332      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0065     |
|    value_loss           | 0.0862      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.7     |
|    ep_rew_mean     | 0.315    |
| time/              |          |
|    fps             | 969      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 87          |
|    ep_rew_mean          | 0.617       |
| time/                   |             |
|    fps                  | 983         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.010741967 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.541       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0321      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.0868      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=4.40 +/- 1.11
Episode length: 107.20 +/- 20.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 107         |
|    mean_reward          | 4.4         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.013609538 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0428      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.143       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.7     |
|    ep_rew_mean     | 0.94     |
| time/              |          |
|    fps             | 922      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 96.9         |
|    ep_rew_mean          | 1.39         |
| time/                   |              |
|    fps                  | 937          |
|    iterations           | 6            |
|    time_elapsed         | 13           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0063642566 |
|    clip_fraction        | 0.0771       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.27        |
|    explained_variance   | 0.338        |
|    learning_rate        | 0.000113     |
|    loss                 | 0.0597       |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00778     |
|    value_loss           | 0.181        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 99.6        |
|    ep_rew_mean          | 1.71        |
| time/                   |             |
|    fps                  | 946         |
|    iterations           | 7           |
|    time_elapsed         | 15          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.010383534 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.455       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0493      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.152       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=8.20 +/- 4.26
Episode length: 184.20 +/- 70.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 184         |
|    mean_reward          | 8.2         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.009254316 |
|    clip_fraction        | 0.0794      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.15       |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0554      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00887    |
|    value_loss           | 0.17        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | 2.02     |
| time/              |          |
|    fps             | 890      |
|    iterations      | 8        |
|    time_elapsed    | 18       |
|    total_timesteps | 16384    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 104          |
|    ep_rew_mean          | 2.21         |
| time/                   |              |
|    fps                  | 902          |
|    iterations           | 9            |
|    time_elapsed         | 20           |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0074299695 |
|    clip_fraction        | 0.0711       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.14        |
|    explained_variance   | 0.564        |
|    learning_rate        | 0.000113     |
|    loss                 | 0.0455       |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.0103      |
|    value_loss           | 0.143        |
------------------------------------------
Eval num_timesteps=20000, episode_reward=6.30 +/- 1.27
Episode length: 137.60 +/- 22.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 138          |
|    mean_reward          | 6.3          |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0063188826 |
|    clip_fraction        | 0.0763       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.517        |
|    learning_rate        | 0.000113     |
|    loss                 | 0.0671       |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.0115      |
|    value_loss           | 0.144        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    fps             | 878      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 20480    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 108          |
|    ep_rew_mean          | 2.52         |
| time/                   |              |
|    fps                  | 889          |
|    iterations           | 11           |
|    time_elapsed         | 25           |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0054043164 |
|    clip_fraction        | 0.0808       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.03        |
|    explained_variance   | 0.562        |
|    learning_rate        | 0.000113     |
|    loss                 | 0.0522       |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00994     |
|    value_loss           | 0.164        |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | 2.84        |
| time/                   |             |
|    fps                  | 899         |
|    iterations           | 12          |
|    time_elapsed         | 27          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.005849325 |
|    clip_fraction        | 0.0839      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.472       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.064       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.159       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=5.70 +/- 1.85
Episode length: 138.60 +/- 27.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 139          |
|    mean_reward          | 5.7          |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0057065384 |
|    clip_fraction        | 0.0734       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.997       |
|    explained_variance   | 0.463        |
|    learning_rate        | 0.000113     |
|    loss                 | 0.0505       |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.0142      |
|    value_loss           | 0.192        |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 3.13     |
| time/              |          |
|    fps             | 878      |
|    iterations      | 13       |
|    time_elapsed    | 30       |
|    total_timesteps | 26624    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | 3.43        |
| time/                   |             |
|    fps                  | 887         |
|    iterations           | 14          |
|    time_elapsed         | 32          |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.009265302 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.962      |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0494      |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.143       |
-----------------------------------------
Eval num_timesteps=30000, episode_reward=6.80 +/- 1.33
Episode length: 143.70 +/- 31.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 144         |
|    mean_reward          | 6.8         |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.008463166 |
|    clip_fraction        | 0.0932      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.895      |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.065       |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.174       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 4        |
| time/              |          |
|    fps             | 871      |
|    iterations      | 15       |
|    time_elapsed    | 35       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | 4.4         |
| time/                   |             |
|    fps                  | 879         |
|    iterations           | 16          |
|    time_elapsed         | 37          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.007770471 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.896      |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0525      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.189       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 126         |
|    ep_rew_mean          | 4.72        |
| time/                   |             |
|    fps                  | 886         |
|    iterations           | 17          |
|    time_elapsed         | 39          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.007610743 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.839      |
|    explained_variance   | 0.491       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0713      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.202       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=13.30 +/- 3.16
Episode length: 248.40 +/- 54.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 248          |
|    mean_reward          | 13.3         |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0077873627 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.795       |
|    explained_variance   | 0.563        |
|    learning_rate        | 0.000113     |
|    loss                 | 0.0682       |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.014       |
|    value_loss           | 0.187        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 5.1      |
| time/              |          |
|    fps             | 859      |
|    iterations      | 18       |
|    time_elapsed    | 42       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 135         |
|    ep_rew_mean          | 5.38        |
| time/                   |             |
|    fps                  | 866         |
|    iterations           | 19          |
|    time_elapsed         | 44          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.008211132 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.738      |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0496      |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.206       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=13.30 +/- 3.29
Episode length: 255.20 +/- 48.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 255         |
|    mean_reward          | 13.3        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.006638051 |
|    clip_fraction        | 0.0642      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.693      |
|    explained_variance   | 0.394       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0625      |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.202       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 5.98     |
| time/              |          |
|    fps             | 843      |
|    iterations      | 20       |
|    time_elapsed    | 48       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 152         |
|    ep_rew_mean          | 6.47        |
| time/                   |             |
|    fps                  | 850         |
|    iterations           | 21          |
|    time_elapsed         | 50          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.005453468 |
|    clip_fraction        | 0.0705      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.671      |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0992      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.23        |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=17.30 +/- 1.27
Episode length: 313.00 +/- 34.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 313         |
|    mean_reward          | 17.3        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.007329329 |
|    clip_fraction        | 0.069       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.637      |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0826      |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.212       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 6.72     |
| time/              |          |
|    fps             | 823      |
|    iterations      | 22       |
|    time_elapsed    | 54       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 160         |
|    ep_rew_mean          | 6.9         |
| time/                   |             |
|    fps                  | 830         |
|    iterations           | 23          |
|    time_elapsed         | 56          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.007601404 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.669      |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0911      |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.232       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 168         |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 837         |
|    iterations           | 24          |
|    time_elapsed         | 58          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.006882654 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.587      |
|    explained_variance   | 0.57        |
|    learning_rate        | 0.000113    |
|    loss                 | 0.064       |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.212       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=17.60 +/- 4.29
Episode length: 312.40 +/- 78.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 312         |
|    mean_reward          | 17.6        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.006061705 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.525      |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.112       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00864    |
|    value_loss           | 0.229       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 815      |
|    iterations      | 25       |
|    time_elapsed    | 62       |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 180          |
|    ep_rew_mean          | 8.23         |
| time/                   |              |
|    fps                  | 821          |
|    iterations           | 26           |
|    time_elapsed         | 64           |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0067748968 |
|    clip_fraction        | 0.071        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.518       |
|    explained_variance   | 0.577        |
|    learning_rate        | 0.000113     |
|    loss                 | 0.0931       |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.013       |
|    value_loss           | 0.201        |
------------------------------------------
Eval num_timesteps=55000, episode_reward=17.90 +/- 2.95
Episode length: 316.70 +/- 54.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 317          |
|    mean_reward          | 17.9         |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0075268145 |
|    clip_fraction        | 0.0764       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.509       |
|    explained_variance   | 0.598        |
|    learning_rate        | 0.000113     |
|    loss                 | 0.0977       |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.0109      |
|    value_loss           | 0.209        |
------------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 8.59     |
| time/              |          |
|    fps             | 803      |
|    iterations      | 27       |
|    time_elapsed    | 68       |
|    total_timesteps | 55296    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 191         |
|    ep_rew_mean          | 9.01        |
| time/                   |             |
|    fps                  | 809         |
|    iterations           | 28          |
|    time_elapsed         | 70          |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.006929163 |
|    clip_fraction        | 0.0776      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.518      |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0651      |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.2         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 196         |
|    ep_rew_mean          | 9.36        |
| time/                   |             |
|    fps                  | 814         |
|    iterations           | 29          |
|    time_elapsed         | 72          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.008442653 |
|    clip_fraction        | 0.0819      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.492      |
|    explained_variance   | 0.508       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.116       |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.268       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=17.70 +/- 1.95
Episode length: 308.10 +/- 39.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 308         |
|    mean_reward          | 17.7        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.013269904 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.508      |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.000113    |
|    loss                 | 0.0518      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.192       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 9.6      |
| time/              |          |
|    fps             | 799      |
|    iterations      | 30       |
|    time_elapsed    | 76       |
|    total_timesteps | 61440    |
---------------------------------
PPO Best trial: 17.9
PPO Best hyperparameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0007385318370212941, 'gamma': 0.916648824764591, 'gae_lambda': 0.9529500719661653}
