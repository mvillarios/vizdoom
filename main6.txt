/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-526.38 +/- 72.36
Episode length: 26.18 +/- 10.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-531.79 +/- 57.86
Episode length: 25.14 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-515.37 +/- 75.22
Episode length: 25.38 +/- 9.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.4     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-514.27 +/- 75.01
Episode length: 25.80 +/- 8.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.8     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | -455     |
| time/              |          |
|    fps             | 365      |
|    iterations      | 1        |
|    time_elapsed    | 5        |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=-499.34 +/- 96.43
Episode length: 21.96 +/- 7.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22          |
|    mean_reward          | -499        |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.010560852 |
|    clip_fraction        | 0.066       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 7.21e-05    |
|    learning_rate        | 0.0001      |
|    loss                 | 9.8e+03     |
|    n_updates            | 3           |
|    policy_gradient_loss | -0.0066     |
|    value_loss           | 1.48e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=-531.14 +/- 67.45
Episode length: 26.18 +/- 10.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26.2     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-512.53 +/- 63.87
Episode length: 23.82 +/- 8.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.8     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=-522.75 +/- 76.17
Episode length: 25.14 +/- 10.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | -472     |
| time/              |          |
|    fps             | 375      |
|    iterations      | 2        |
|    time_elapsed    | 10       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=-502.34 +/- 73.40
Episode length: 24.56 +/- 9.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.6        |
|    mean_reward          | -502        |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.013178278 |
|    clip_fraction        | 0.0574      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | -0.000291   |
|    learning_rate        | 0.0001      |
|    loss                 | 7.74e+03    |
|    n_updates            | 7           |
|    policy_gradient_loss | -0.00514    |
|    value_loss           | 1.71e+04    |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-514.10 +/- 86.25
Episode length: 25.64 +/- 10.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.6     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-522.74 +/- 77.11
Episode length: 24.32 +/- 9.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-509.54 +/- 75.10
Episode length: 22.12 +/- 8.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.1     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.5     |
|    ep_rew_mean     | -440     |
| time/              |          |
|    fps             | 380      |
|    iterations      | 3        |
|    time_elapsed    | 16       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=6500, episode_reward=-197.99 +/- 130.69
Episode length: 15.04 +/- 3.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | -198        |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.005953366 |
|    clip_fraction        | 0.0928      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | -0.00151    |
|    learning_rate        | 0.0001      |
|    loss                 | 6.21e+03    |
|    n_updates            | 8           |
|    policy_gradient_loss | -0.00605    |
|    value_loss           | 1.34e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=7000, episode_reward=-143.80 +/- 160.02
Episode length: 15.74 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
New best mean reward!
Eval num_timesteps=7500, episode_reward=-131.48 +/- 98.19
Episode length: 16.54 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
New best mean reward!
Eval num_timesteps=8000, episode_reward=-96.05 +/- 175.44
Episode length: 17.92 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | -96.1    |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.5     |
|    ep_rew_mean     | -412     |
| time/              |          |
|    fps             | 405      |
|    iterations      | 4        |
|    time_elapsed    | 20       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=8500, episode_reward=-523.94 +/- 71.75
Episode length: 25.52 +/- 9.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 25.5         |
|    mean_reward          | -524         |
| time/                   |              |
|    total_timesteps      | 8500         |
| train/                  |              |
|    approx_kl            | 0.0063393507 |
|    clip_fraction        | 0.0755       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.94        |
|    explained_variance   | -0.00254     |
|    learning_rate        | 0.0001       |
|    loss                 | 7.06e+03     |
|    n_updates            | 9            |
|    policy_gradient_loss | -0.00589     |
|    value_loss           | 1.16e+04     |
------------------------------------------
Eval num_timesteps=9000, episode_reward=-522.74 +/- 76.41
Episode length: 25.18 +/- 8.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.2     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-517.08 +/- 78.03
Episode length: 23.36 +/- 8.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.4     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-515.54 +/- 93.25
Episode length: 22.36 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.4     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.3     |
|    ep_rew_mean     | -396     |
| time/              |          |
|    fps             | 405      |
|    iterations      | 5        |
|    time_elapsed    | 25       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=10500, episode_reward=-148.51 +/- 132.80
Episode length: 16.74 +/- 4.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -149        |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.006436308 |
|    clip_fraction        | 0.0998      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.84       |
|    explained_variance   | -0.0034     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.18e+03    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.000218   |
|    value_loss           | 1.06e+04    |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=-145.14 +/- 127.85
Episode length: 15.50 +/- 3.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-110.20 +/- 178.21
Episode length: 17.44 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-158.67 +/- 152.14
Episode length: 16.42 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.8     |
|    ep_rew_mean     | -379     |
| time/              |          |
|    fps             | 416      |
|    iterations      | 6        |
|    time_elapsed    | 29       |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=12500, episode_reward=-115.12 +/- 164.64
Episode length: 17.12 +/- 5.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.1        |
|    mean_reward          | -115        |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.005755698 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | -0.00548    |
|    learning_rate        | 0.0001      |
|    loss                 | 6.47e+03    |
|    n_updates            | 11          |
|    policy_gradient_loss | -0.00427    |
|    value_loss           | 1.24e+04    |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=-127.98 +/- 149.12
Episode length: 16.06 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-187.48 +/- 125.76
Episode length: 15.20 +/- 3.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-166.36 +/- 122.24
Episode length: 15.52 +/- 4.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | -349     |
| time/              |          |
|    fps             | 426      |
|    iterations      | 7        |
|    time_elapsed    | 33       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=14500, episode_reward=-177.84 +/- 151.97
Episode length: 15.48 +/- 4.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -178        |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.004401164 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.00599    |
|    learning_rate        | 0.0001      |
|    loss                 | 5.5e+03     |
|    n_updates            | 12          |
|    policy_gradient_loss | -0.00119    |
|    value_loss           | 1.13e+04    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=-127.11 +/- 148.02
Episode length: 16.40 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-177.66 +/- 127.04
Episode length: 15.84 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=-178.31 +/- 140.23
Episode length: 15.70 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | -346     |
| time/              |          |
|    fps             | 434      |
|    iterations      | 8        |
|    time_elapsed    | 37       |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=16500, episode_reward=-135.45 +/- 124.18
Episode length: 15.74 +/- 4.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -135         |
| time/                   |              |
|    total_timesteps      | 16500        |
| train/                  |              |
|    approx_kl            | 0.0073572653 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.58        |
|    explained_variance   | -0.00812     |
|    learning_rate        | 0.0001       |
|    loss                 | 8.03e+03     |
|    n_updates            | 13           |
|    policy_gradient_loss | -0.00904     |
|    value_loss           | 1.12e+04     |
------------------------------------------
Eval num_timesteps=17000, episode_reward=-110.82 +/- 177.57
Episode length: 17.32 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-115.76 +/- 188.22
Episode length: 17.60 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-161.71 +/- 144.24
Episode length: 15.24 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | -336     |
| time/              |          |
|    fps             | 440      |
|    iterations      | 9        |
|    time_elapsed    | 41       |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=18500, episode_reward=-184.34 +/- 148.97
Episode length: 14.72 +/- 4.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.7        |
|    mean_reward          | -184        |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.008639076 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.41       |
|    explained_variance   | -0.00912    |
|    learning_rate        | 0.0001      |
|    loss                 | 4.5e+03     |
|    n_updates            | 14          |
|    policy_gradient_loss | 0.00965     |
|    value_loss           | 1.13e+04    |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=-127.07 +/- 145.87
Episode length: 16.72 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-153.03 +/- 158.27
Episode length: 15.64 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-128.32 +/- 155.91
Episode length: 17.12 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | -337     |
| time/              |          |
|    fps             | 446      |
|    iterations      | 10       |
|    time_elapsed    | 45       |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=20500, episode_reward=-173.07 +/- 160.45
Episode length: 15.42 +/- 4.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -173         |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0073365956 |
|    clip_fraction        | 0.0931       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.33        |
|    explained_variance   | -0.0139      |
|    learning_rate        | 0.0001       |
|    loss                 | 5.35e+03     |
|    n_updates            | 15           |
|    policy_gradient_loss | 0.00654      |
|    value_loss           | 1.11e+04     |
------------------------------------------
Eval num_timesteps=21000, episode_reward=-134.47 +/- 150.05
Episode length: 16.14 +/- 4.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-155.04 +/- 149.16
Episode length: 16.24 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-165.49 +/- 161.65
Episode length: 15.76 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=-146.39 +/- 147.32
Episode length: 15.78 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.7     |
|    ep_rew_mean     | -296     |
| time/              |          |
|    fps             | 445      |
|    iterations      | 11       |
|    time_elapsed    | 50       |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=23000, episode_reward=-133.09 +/- 145.22
Episode length: 16.24 +/- 4.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.2       |
|    mean_reward          | -133       |
| time/                   |            |
|    total_timesteps      | 23000      |
| train/                  |            |
|    approx_kl            | 0.00603072 |
|    clip_fraction        | 0.0547     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | -0.0169    |
|    learning_rate        | 0.0001     |
|    loss                 | 5.57e+03   |
|    n_updates            | 16         |
|    policy_gradient_loss | -0.00597   |
|    value_loss           | 1.34e+04   |
----------------------------------------
Eval num_timesteps=23500, episode_reward=-167.55 +/- 152.37
Episode length: 15.68 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-155.40 +/- 178.09
Episode length: 16.16 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-153.01 +/- 133.23
Episode length: 15.40 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -249     |
| time/              |          |
|    fps             | 450      |
|    iterations      | 12       |
|    time_elapsed    | 54       |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=25000, episode_reward=-61.44 +/- 185.84
Episode length: 18.18 +/- 6.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.2         |
|    mean_reward          | -61.4        |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0099275075 |
|    clip_fraction        | 0.0859       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.959       |
|    explained_variance   | -0.0169      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.24e+03     |
|    n_updates            | 17           |
|    policy_gradient_loss | -0.00738     |
|    value_loss           | 9.23e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=25500, episode_reward=-141.66 +/- 159.29
Episode length: 15.46 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-155.96 +/- 133.54
Episode length: 15.92 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=-155.68 +/- 141.69
Episode length: 15.90 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -225     |
| time/              |          |
|    fps             | 453      |
|    iterations      | 13       |
|    time_elapsed    | 58       |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=27000, episode_reward=-143.79 +/- 172.38
Episode length: 16.50 +/- 5.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -144        |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.006339322 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.735      |
|    explained_variance   | -0.0197     |
|    learning_rate        | 0.0001      |
|    loss                 | 5.05e+03    |
|    n_updates            | 18          |
|    policy_gradient_loss | 0.00807     |
|    value_loss           | 9.05e+03    |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=-141.19 +/- 125.12
Episode length: 16.26 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-129.04 +/- 179.49
Episode length: 17.42 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=-162.69 +/- 152.67
Episode length: 15.12 +/- 4.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -229     |
| time/              |          |
|    fps             | 456      |
|    iterations      | 14       |
|    time_elapsed    | 62       |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=29000, episode_reward=-153.96 +/- 145.74
Episode length: 14.98 +/- 4.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | -154        |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.005155075 |
|    clip_fraction        | 0.0639      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.572      |
|    explained_variance   | -0.02       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.64e+03    |
|    n_updates            | 19          |
|    policy_gradient_loss | -0.000919   |
|    value_loss           | 1.04e+04    |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=-150.49 +/- 162.42
Episode length: 16.38 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-154.74 +/- 176.62
Episode length: 14.76 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-173.16 +/- 125.61
Episode length: 15.00 +/- 4.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 459      |
|    iterations      | 15       |
|    time_elapsed    | 66       |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=31000, episode_reward=-118.12 +/- 178.44
Episode length: 15.82 +/- 6.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -118         |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0039949524 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.476       |
|    explained_variance   | -0.0116      |
|    learning_rate        | 0.0001       |
|    loss                 | 5.67e+03     |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000515    |
|    value_loss           | 1.44e+04     |
------------------------------------------
Eval num_timesteps=31500, episode_reward=-145.34 +/- 133.17
Episode length: 16.86 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=-146.05 +/- 156.96
Episode length: 15.62 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-103.93 +/- 153.98
Episode length: 16.14 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 462      |
|    iterations      | 16       |
|    time_elapsed    | 70       |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
Eval num_timesteps=33000, episode_reward=-180.33 +/- 148.04
Episode length: 15.22 +/- 4.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -180         |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0051709553 |
|    clip_fraction        | 0.037        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.37        |
|    explained_variance   | -0.0169      |
|    learning_rate        | 0.0001       |
|    loss                 | 3.56e+03     |
|    n_updates            | 24           |
|    policy_gradient_loss | 0.00221      |
|    value_loss           | 9.62e+03     |
------------------------------------------
Eval num_timesteps=33500, episode_reward=-179.81 +/- 139.35
Episode length: 15.54 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-139.65 +/- 155.66
Episode length: 16.16 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-165.43 +/- 131.69
Episode length: 16.08 +/- 4.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 463      |
|    iterations      | 17       |
|    time_elapsed    | 75       |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=35000, episode_reward=-108.09 +/- 159.16
Episode length: 16.70 +/- 5.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -108        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.009283918 |
|    clip_fraction        | 0.0183      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.24       |
|    explained_variance   | -0.0116     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.87e+03    |
|    n_updates            | 26          |
|    policy_gradient_loss | -0.000877   |
|    value_loss           | 9.11e+03    |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=-168.15 +/- 151.91
Episode length: 15.48 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-135.17 +/- 146.43
Episode length: 16.30 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-139.06 +/- 162.61
Episode length: 16.46 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 464      |
|    iterations      | 18       |
|    time_elapsed    | 79       |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=37000, episode_reward=-170.80 +/- 147.07
Episode length: 16.14 +/- 4.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -171         |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 0.0034365251 |
|    clip_fraction        | 0.0375       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.354       |
|    explained_variance   | -0.0079      |
|    learning_rate        | 0.0001       |
|    loss                 | 5.92e+03     |
|    n_updates            | 27           |
|    policy_gradient_loss | 0.00119      |
|    value_loss           | 8.71e+03     |
------------------------------------------
Eval num_timesteps=37500, episode_reward=-124.97 +/- 162.84
Episode length: 16.48 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-146.26 +/- 150.70
Episode length: 16.40 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-131.43 +/- 154.50
Episode length: 16.40 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 466      |
|    iterations      | 19       |
|    time_elapsed    | 83       |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=39000, episode_reward=-157.89 +/- 116.78
Episode length: 14.62 +/- 3.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.6        |
|    mean_reward          | -158        |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.004323025 |
|    clip_fraction        | 0.0135      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.214      |
|    explained_variance   | -0.00747    |
|    learning_rate        | 0.0001      |
|    loss                 | 3.72e+03    |
|    n_updates            | 30          |
|    policy_gradient_loss | 0.000491    |
|    value_loss           | 9.44e+03    |
-----------------------------------------
Eval num_timesteps=39500, episode_reward=-167.63 +/- 145.08
Episode length: 15.44 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-149.28 +/- 152.95
Episode length: 16.26 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-134.37 +/- 144.71
Episode length: 15.70 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 467      |
|    iterations      | 20       |
|    time_elapsed    | 87       |
|    total_timesteps | 40960    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=41000, episode_reward=-149.06 +/- 126.46
Episode length: 16.10 +/- 4.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -149        |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.006848626 |
|    clip_fraction        | 0.0152      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.352      |
|    explained_variance   | -0.00566    |
|    learning_rate        | 0.0001      |
|    loss                 | 3.15e+03    |
|    n_updates            | 32          |
|    policy_gradient_loss | 0.000271    |
|    value_loss           | 9.33e+03    |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=-138.31 +/- 128.70
Episode length: 15.78 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-129.89 +/- 149.30
Episode length: 16.64 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-126.52 +/- 147.74
Episode length: 16.52 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-177.96 +/- 135.72
Episode length: 14.90 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 466      |
|    iterations      | 21       |
|    time_elapsed    | 92       |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=43500, episode_reward=-140.95 +/- 151.85
Episode length: 16.82 +/- 4.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | -141        |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.004164151 |
|    clip_fraction        | 0.0281      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.272      |
|    explained_variance   | -0.00673    |
|    learning_rate        | 0.0001      |
|    loss                 | 5.49e+03    |
|    n_updates            | 35          |
|    policy_gradient_loss | 0.0012      |
|    value_loss           | 9.98e+03    |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=-148.10 +/- 188.06
Episode length: 16.44 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-139.57 +/- 144.77
Episode length: 16.36 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-166.89 +/- 130.00
Episode length: 15.52 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -195     |
| time/              |          |
|    fps             | 467      |
|    iterations      | 22       |
|    time_elapsed    | 96       |
|    total_timesteps | 45056    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=45500, episode_reward=-140.46 +/- 137.02
Episode length: 16.52 +/- 4.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -140         |
| time/                   |              |
|    total_timesteps      | 45500        |
| train/                  |              |
|    approx_kl            | 0.0069735297 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.401       |
|    explained_variance   | -0.0093      |
|    learning_rate        | 0.0001       |
|    loss                 | 3.71e+03     |
|    n_updates            | 38           |
|    policy_gradient_loss | 0.00102      |
|    value_loss           | 9.14e+03     |
------------------------------------------
Eval num_timesteps=46000, episode_reward=-142.24 +/- 145.60
Episode length: 16.52 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-155.36 +/- 149.45
Episode length: 15.96 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=-173.92 +/- 137.59
Episode length: 15.10 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 468      |
|    iterations      | 23       |
|    time_elapsed    | 100      |
|    total_timesteps | 47104    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=47500, episode_reward=-124.68 +/- 174.71
Episode length: 16.46 +/- 6.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -125        |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.015811622 |
|    clip_fraction        | 0.0485      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.499      |
|    explained_variance   | -0.00797    |
|    learning_rate        | 0.0001      |
|    loss                 | 5.25e+03    |
|    n_updates            | 42          |
|    policy_gradient_loss | 0.00193     |
|    value_loss           | 9.11e+03    |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=-108.49 +/- 125.99
Episode length: 17.42 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=-174.89 +/- 121.97
Episode length: 14.96 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-185.55 +/- 161.34
Episode length: 15.40 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -189     |
| time/              |          |
|    fps             | 468      |
|    iterations      | 24       |
|    time_elapsed    | 104      |
|    total_timesteps | 49152    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=49500, episode_reward=-144.90 +/- 181.70
Episode length: 16.20 +/- 5.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -145         |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 0.0052059116 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.336       |
|    explained_variance   | -0.0113      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.68e+03     |
|    n_updates            | 43           |
|    policy_gradient_loss | 0.00297      |
|    value_loss           | 7.85e+03     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-113.17 +/- 174.74
Episode length: 17.10 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=-165.92 +/- 127.73
Episode length: 15.22 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=-141.51 +/- 165.78
Episode length: 15.86 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -156     |
| time/              |          |
|    fps             | 470      |
|    iterations      | 25       |
|    time_elapsed    | 108      |
|    total_timesteps | 51200    |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=51500, episode_reward=-144.00 +/- 175.17
Episode length: 15.84 +/- 5.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.8       |
|    mean_reward          | -144       |
| time/                   |            |
|    total_timesteps      | 51500      |
| train/                  |            |
|    approx_kl            | 0.00432684 |
|    clip_fraction        | 0.0259     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.264     |
|    explained_variance   | -0.00897   |
|    learning_rate        | 0.0001     |
|    loss                 | 3.13e+03   |
|    n_updates            | 47         |
|    policy_gradient_loss | 0.000148   |
|    value_loss           | 8.9e+03    |
----------------------------------------
Eval num_timesteps=52000, episode_reward=-123.16 +/- 168.97
Episode length: 16.80 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-183.72 +/- 154.84
Episode length: 15.48 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-152.24 +/- 157.96
Episode length: 16.50 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -224     |
| time/              |          |
|    fps             | 470      |
|    iterations      | 26       |
|    time_elapsed    | 113      |
|    total_timesteps | 53248    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=53500, episode_reward=-125.06 +/- 128.59
Episode length: 16.58 +/- 4.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -125         |
| time/                   |              |
|    total_timesteps      | 53500        |
| train/                  |              |
|    approx_kl            | 0.0073041706 |
|    clip_fraction        | 0.0425       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.418       |
|    explained_variance   | -0.0082      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.71e+03     |
|    n_updates            | 49           |
|    policy_gradient_loss | 0.000498     |
|    value_loss           | 1.04e+04     |
------------------------------------------
Eval num_timesteps=54000, episode_reward=-158.94 +/- 148.19
Episode length: 15.88 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-216.70 +/- 132.63
Episode length: 14.14 +/- 3.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.1     |
|    mean_reward     | -217     |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-147.06 +/- 151.11
Episode length: 16.20 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 471      |
|    iterations      | 27       |
|    time_elapsed    | 117      |
|    total_timesteps | 55296    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=55500, episode_reward=-67.39 +/- 147.78
Episode length: 18.46 +/- 5.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.5         |
|    mean_reward          | -67.4        |
| time/                   |              |
|    total_timesteps      | 55500        |
| train/                  |              |
|    approx_kl            | 0.0022573979 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.329       |
|    explained_variance   | -0.00811     |
|    learning_rate        | 0.0001       |
|    loss                 | 4.18e+03     |
|    n_updates            | 51           |
|    policy_gradient_loss | 0.00157      |
|    value_loss           | 9.8e+03      |
------------------------------------------
Eval num_timesteps=56000, episode_reward=-150.01 +/- 114.88
Episode length: 15.40 +/- 4.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-180.65 +/- 145.52
Episode length: 14.34 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.3     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-149.96 +/- 151.65
Episode length: 15.22 +/- 5.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -195     |
| time/              |          |
|    fps             | 472      |
|    iterations      | 28       |
|    time_elapsed    | 121      |
|    total_timesteps | 57344    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=57500, episode_reward=-130.34 +/- 197.71
Episode length: 16.28 +/- 5.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -130         |
| time/                   |              |
|    total_timesteps      | 57500        |
| train/                  |              |
|    approx_kl            | 0.0026602955 |
|    clip_fraction        | 0.0253       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.495       |
|    explained_variance   | -0.00517     |
|    learning_rate        | 0.0001       |
|    loss                 | 4.59e+03     |
|    n_updates            | 52           |
|    policy_gradient_loss | 0.0019       |
|    value_loss           | 1.04e+04     |
------------------------------------------
Eval num_timesteps=58000, episode_reward=-163.85 +/- 177.54
Episode length: 15.76 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=-162.45 +/- 127.53
Episode length: 15.60 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=-130.97 +/- 127.01
Episode length: 16.20 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 29       |
|    time_elapsed    | 125      |
|    total_timesteps | 59392    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=59500, episode_reward=-134.30 +/- 145.71
Episode length: 16.34 +/- 5.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -134         |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.0024145767 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.626       |
|    explained_variance   | -0.00926     |
|    learning_rate        | 0.0001       |
|    loss                 | 7.24e+03     |
|    n_updates            | 53           |
|    policy_gradient_loss | 0.000146     |
|    value_loss           | 1.03e+04     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-139.14 +/- 184.22
Episode length: 16.46 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-149.60 +/- 127.73
Episode length: 15.74 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-122.37 +/- 140.28
Episode length: 16.48 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 30       |
|    time_elapsed    | 129      |
|    total_timesteps | 61440    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=61500, episode_reward=-153.07 +/- 134.21
Episode length: 15.82 +/- 5.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -153         |
| time/                   |              |
|    total_timesteps      | 61500        |
| train/                  |              |
|    approx_kl            | 0.0032081471 |
|    clip_fraction        | 0.0449       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.488       |
|    explained_variance   | -0.00498     |
|    learning_rate        | 0.0001       |
|    loss                 | 3.76e+03     |
|    n_updates            | 54           |
|    policy_gradient_loss | -0.000239    |
|    value_loss           | 9.5e+03      |
------------------------------------------
Eval num_timesteps=62000, episode_reward=-174.00 +/- 125.84
Episode length: 15.08 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-129.46 +/- 171.43
Episode length: 16.68 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=-137.11 +/- 157.55
Episode length: 16.58 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -196     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 31       |
|    time_elapsed    | 133      |
|    total_timesteps | 63488    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=63500, episode_reward=-129.44 +/- 186.21
Episode length: 16.58 +/- 6.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.6      |
|    mean_reward          | -129      |
| time/                   |           |
|    total_timesteps      | 63500     |
| train/                  |           |
|    approx_kl            | 0.0073198 |
|    clip_fraction        | 0.0393    |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.346    |
|    explained_variance   | -0.00283  |
|    learning_rate        | 0.0001    |
|    loss                 | 5.67e+03  |
|    n_updates            | 57        |
|    policy_gradient_loss | 0.0034    |
|    value_loss           | 9.28e+03  |
---------------------------------------
Eval num_timesteps=64000, episode_reward=-138.49 +/- 151.99
Episode length: 15.86 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-173.23 +/- 155.05
Episode length: 15.60 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-127.18 +/- 115.76
Episode length: 15.58 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=-161.88 +/- 132.60
Episode length: 15.62 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -211     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 32       |
|    time_elapsed    | 138      |
|    total_timesteps | 65536    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=66000, episode_reward=-130.91 +/- 134.92
Episode length: 16.76 +/- 4.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -131         |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0035458198 |
|    clip_fraction        | 0.0448       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.478       |
|    explained_variance   | -0.00421     |
|    learning_rate        | 0.0001       |
|    loss                 | 4.41e+03     |
|    n_updates            | 58           |
|    policy_gradient_loss | 0.00173      |
|    value_loss           | 9.58e+03     |
------------------------------------------
Eval num_timesteps=66500, episode_reward=-177.55 +/- 143.41
Episode length: 15.48 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-139.61 +/- 168.79
Episode length: 15.46 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=-83.98 +/- 167.96
Episode length: 18.46 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.5     |
|    mean_reward     | -84      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -192     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 33       |
|    time_elapsed    | 142      |
|    total_timesteps | 67584    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=68000, episode_reward=-165.99 +/- 131.93
Episode length: 14.94 +/- 3.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.9       |
|    mean_reward          | -166       |
| time/                   |            |
|    total_timesteps      | 68000      |
| train/                  |            |
|    approx_kl            | 0.00458721 |
|    clip_fraction        | 0.0677     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.512     |
|    explained_variance   | -0.00435   |
|    learning_rate        | 0.0001     |
|    loss                 | 4.09e+03   |
|    n_updates            | 59         |
|    policy_gradient_loss | 0.000102   |
|    value_loss           | 8.81e+03   |
----------------------------------------
Eval num_timesteps=68500, episode_reward=-158.62 +/- 181.66
Episode length: 15.54 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-159.87 +/- 161.87
Episode length: 15.20 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=-163.40 +/- 134.89
Episode length: 14.70 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -191     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 34       |
|    time_elapsed    | 146      |
|    total_timesteps | 69632    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=70000, episode_reward=-163.93 +/- 172.62
Episode length: 15.66 +/- 5.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | -164        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.006636888 |
|    clip_fraction        | 0.0829      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.662      |
|    explained_variance   | -0.0104     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.24e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00852     |
|    value_loss           | 9.16e+03    |
-----------------------------------------
Eval num_timesteps=70500, episode_reward=-147.69 +/- 163.43
Episode length: 16.48 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-168.84 +/- 150.17
Episode length: 15.72 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-141.85 +/- 152.21
Episode length: 16.08 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 35       |
|    time_elapsed    | 150      |
|    total_timesteps | 71680    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=72000, episode_reward=-167.75 +/- 140.01
Episode length: 15.48 +/- 4.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -168        |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.006366424 |
|    clip_fraction        | 0.0483      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.464      |
|    explained_variance   | -0.0058     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.18e+03    |
|    n_updates            | 62          |
|    policy_gradient_loss | 5.28e-05    |
|    value_loss           | 9.07e+03    |
-----------------------------------------
Eval num_timesteps=72500, episode_reward=-161.61 +/- 166.33
Episode length: 15.72 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-192.65 +/- 130.54
Episode length: 14.08 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.1     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-129.96 +/- 148.66
Episode length: 16.34 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 36       |
|    time_elapsed    | 154      |
|    total_timesteps | 73728    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=74000, episode_reward=-131.81 +/- 159.88
Episode length: 16.72 +/- 5.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.7       |
|    mean_reward          | -132       |
| time/                   |            |
|    total_timesteps      | 74000      |
| train/                  |            |
|    approx_kl            | 0.00524729 |
|    clip_fraction        | 0.0391     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.341     |
|    explained_variance   | 0.000521   |
|    learning_rate        | 0.0001     |
|    loss                 | 5.31e+03   |
|    n_updates            | 63         |
|    policy_gradient_loss | 0.0015     |
|    value_loss           | 9.92e+03   |
----------------------------------------
Eval num_timesteps=74500, episode_reward=-181.91 +/- 120.44
Episode length: 15.52 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-131.10 +/- 141.04
Episode length: 15.70 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=-127.37 +/- 166.25
Episode length: 16.40 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -216     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 37       |
|    time_elapsed    | 158      |
|    total_timesteps | 75776    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=76000, episode_reward=-173.31 +/- 140.45
Episode length: 15.22 +/- 4.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -173         |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 0.0038506126 |
|    clip_fraction        | 0.0475       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.52        |
|    explained_variance   | 0.00182      |
|    learning_rate        | 0.0001       |
|    loss                 | 5.39e+03     |
|    n_updates            | 64           |
|    policy_gradient_loss | 0.00409      |
|    value_loss           | 9.57e+03     |
------------------------------------------
Eval num_timesteps=76500, episode_reward=-139.64 +/- 148.97
Episode length: 15.62 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-132.47 +/- 135.68
Episode length: 16.80 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=-143.68 +/- 165.54
Episode length: 16.14 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 38       |
|    time_elapsed    | 163      |
|    total_timesteps | 77824    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=78000, episode_reward=-129.30 +/- 131.46
Episode length: 16.80 +/- 4.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | -129        |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.005033243 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.636      |
|    explained_variance   | 0.00369     |
|    learning_rate        | 0.0001      |
|    loss                 | 8.09e+03    |
|    n_updates            | 65          |
|    policy_gradient_loss | -0.00305    |
|    value_loss           | 1.1e+04     |
-----------------------------------------
Eval num_timesteps=78500, episode_reward=-152.58 +/- 157.28
Episode length: 15.68 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-136.34 +/- 124.66
Episode length: 15.92 +/- 4.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-152.46 +/- 179.47
Episode length: 15.86 +/- 5.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -222     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 39       |
|    time_elapsed    | 167      |
|    total_timesteps | 79872    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=80000, episode_reward=-138.40 +/- 153.75
Episode length: 16.14 +/- 5.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -138         |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0050755977 |
|    clip_fraction        | 0.0717       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.683       |
|    explained_variance   | 0.000913     |
|    learning_rate        | 0.0001       |
|    loss                 | 3.79e+03     |
|    n_updates            | 67           |
|    policy_gradient_loss | 0.00256      |
|    value_loss           | 9.47e+03     |
------------------------------------------
Eval num_timesteps=80500, episode_reward=-128.25 +/- 141.55
Episode length: 16.18 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-127.52 +/- 131.87
Episode length: 16.26 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-154.57 +/- 100.98
Episode length: 15.46 +/- 4.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -195     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 40       |
|    time_elapsed    | 171      |
|    total_timesteps | 81920    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=82000, episode_reward=-170.21 +/- 144.41
Episode length: 15.82 +/- 4.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -170         |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0052111065 |
|    clip_fraction        | 0.0688       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.54        |
|    explained_variance   | 0.00961      |
|    learning_rate        | 0.0001       |
|    loss                 | 3.94e+03     |
|    n_updates            | 68           |
|    policy_gradient_loss | 0.00203      |
|    value_loss           | 9.63e+03     |
------------------------------------------
Eval num_timesteps=82500, episode_reward=-106.04 +/- 158.25
Episode length: 17.32 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-137.91 +/- 156.08
Episode length: 15.82 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-176.16 +/- 132.92
Episode length: 14.92 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | -183     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 41       |
|    time_elapsed    | 175      |
|    total_timesteps | 83968    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=84000, episode_reward=-174.15 +/- 144.70
Episode length: 14.78 +/- 3.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -174         |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0035855167 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.691       |
|    explained_variance   | 0.00818      |
|    learning_rate        | 0.0001       |
|    loss                 | 3.06e+03     |
|    n_updates            | 69           |
|    policy_gradient_loss | 0.00114      |
|    value_loss           | 8.03e+03     |
------------------------------------------
Eval num_timesteps=84500, episode_reward=-154.59 +/- 157.74
Episode length: 16.26 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-158.65 +/- 141.57
Episode length: 16.26 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-139.34 +/- 157.22
Episode length: 15.86 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-162.47 +/- 118.29
Episode length: 15.74 +/- 4.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -213     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 42       |
|    time_elapsed    | 179      |
|    total_timesteps | 86016    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=86500, episode_reward=-150.35 +/- 118.21
Episode length: 15.66 +/- 3.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -150         |
| time/                   |              |
|    total_timesteps      | 86500        |
| train/                  |              |
|    approx_kl            | 0.0047989427 |
|    clip_fraction        | 0.0476       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.524       |
|    explained_variance   | 0.012        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.66e+03     |
|    n_updates            | 70           |
|    policy_gradient_loss | 0.00105      |
|    value_loss           | 9.53e+03     |
------------------------------------------
Eval num_timesteps=87000, episode_reward=-155.80 +/- 176.51
Episode length: 16.12 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-159.77 +/- 140.63
Episode length: 14.84 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-161.05 +/- 147.49
Episode length: 16.50 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 43       |
|    time_elapsed    | 184      |
|    total_timesteps | 88064    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=88500, episode_reward=-118.48 +/- 166.09
Episode length: 17.04 +/- 5.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -118         |
| time/                   |              |
|    total_timesteps      | 88500        |
| train/                  |              |
|    approx_kl            | 0.0034685002 |
|    clip_fraction        | 0.0304       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.549       |
|    explained_variance   | 0.0103       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.11e+03     |
|    n_updates            | 71           |
|    policy_gradient_loss | 0.00101      |
|    value_loss           | 8.82e+03     |
------------------------------------------
Eval num_timesteps=89000, episode_reward=-142.24 +/- 125.23
Episode length: 15.94 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-182.09 +/- 109.86
Episode length: 15.32 +/- 3.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-138.23 +/- 174.84
Episode length: 16.12 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -211     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 44       |
|    time_elapsed    | 188      |
|    total_timesteps | 90112    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=90500, episode_reward=-123.96 +/- 154.91
Episode length: 16.52 +/- 5.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -124         |
| time/                   |              |
|    total_timesteps      | 90500        |
| train/                  |              |
|    approx_kl            | 0.0058197184 |
|    clip_fraction        | 0.0732       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.496       |
|    explained_variance   | 0.0132       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.91e+03     |
|    n_updates            | 72           |
|    policy_gradient_loss | 0.00177      |
|    value_loss           | 1.02e+04     |
------------------------------------------
Eval num_timesteps=91000, episode_reward=-174.95 +/- 123.88
Episode length: 14.66 +/- 4.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-172.43 +/- 149.17
Episode length: 14.84 +/- 4.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-152.99 +/- 132.85
Episode length: 15.20 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -183     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 45       |
|    time_elapsed    | 192      |
|    total_timesteps | 92160    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=92500, episode_reward=-176.63 +/- 143.59
Episode length: 15.24 +/- 4.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | -177        |
| time/                   |             |
|    total_timesteps      | 92500       |
| train/                  |             |
|    approx_kl            | 0.006158359 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.328      |
|    explained_variance   | 0.0184      |
|    learning_rate        | 0.0001      |
|    loss                 | 5.86e+03    |
|    n_updates            | 73          |
|    policy_gradient_loss | -0.0052     |
|    value_loss           | 9.53e+03    |
-----------------------------------------
Eval num_timesteps=93000, episode_reward=-141.42 +/- 107.38
Episode length: 16.12 +/- 4.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-144.72 +/- 128.06
Episode length: 15.24 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-160.40 +/- 145.13
Episode length: 15.70 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 46       |
|    time_elapsed    | 196      |
|    total_timesteps | 94208    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=94500, episode_reward=-131.05 +/- 166.01
Episode length: 16.68 +/- 5.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -131        |
| time/                   |             |
|    total_timesteps      | 94500       |
| train/                  |             |
|    approx_kl            | 0.012562774 |
|    clip_fraction        | 0.0211      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.26       |
|    explained_variance   | 0.0209      |
|    learning_rate        | 0.0001      |
|    loss                 | 4.84e+03    |
|    n_updates            | 76          |
|    policy_gradient_loss | 0.00164     |
|    value_loss           | 9.4e+03     |
-----------------------------------------
Eval num_timesteps=95000, episode_reward=-130.17 +/- 138.57
Episode length: 16.52 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-138.80 +/- 135.91
Episode length: 16.22 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=-135.24 +/- 159.06
Episode length: 15.90 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -156     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 47       |
|    time_elapsed    | 200      |
|    total_timesteps | 96256    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=96500, episode_reward=-144.11 +/- 126.00
Episode length: 15.78 +/- 4.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -144         |
| time/                   |              |
|    total_timesteps      | 96500        |
| train/                  |              |
|    approx_kl            | 0.0022436494 |
|    clip_fraction        | 0.0253       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.24        |
|    explained_variance   | 0.0238       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.2e+03      |
|    n_updates            | 77           |
|    policy_gradient_loss | 0.00169      |
|    value_loss           | 9.19e+03     |
------------------------------------------
Eval num_timesteps=97000, episode_reward=-107.45 +/- 193.29
Episode length: 17.14 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-116.34 +/- 151.59
Episode length: 15.74 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=-140.30 +/- 137.92
Episode length: 16.32 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 48       |
|    time_elapsed    | 204      |
|    total_timesteps | 98304    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=98500, episode_reward=-159.59 +/- 136.26
Episode length: 15.06 +/- 4.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.1         |
|    mean_reward          | -160         |
| time/                   |              |
|    total_timesteps      | 98500        |
| train/                  |              |
|    approx_kl            | 0.0071391594 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.469       |
|    explained_variance   | 0.0217       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.59e+03     |
|    n_updates            | 78           |
|    policy_gradient_loss | -0.000376    |
|    value_loss           | 8.2e+03      |
------------------------------------------
Eval num_timesteps=99000, episode_reward=-127.47 +/- 175.05
Episode length: 16.92 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-168.09 +/- 163.02
Episode length: 15.50 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-159.09 +/- 132.61
Episode length: 15.62 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -196     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 49       |
|    time_elapsed    | 208      |
|    total_timesteps | 100352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=100500, episode_reward=-132.90 +/- 139.75
Episode length: 16.64 +/- 5.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -133         |
| time/                   |              |
|    total_timesteps      | 100500       |
| train/                  |              |
|    approx_kl            | 0.0065023988 |
|    clip_fraction        | 0.0748       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.563       |
|    explained_variance   | 0.0199       |
|    learning_rate        | 0.0001       |
|    loss                 | 7.19e+03     |
|    n_updates            | 79           |
|    policy_gradient_loss | 0.00448      |
|    value_loss           | 9.56e+03     |
------------------------------------------
Eval num_timesteps=101000, episode_reward=-174.51 +/- 136.91
Episode length: 16.42 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-112.01 +/- 113.35
Episode length: 17.10 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -112     |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=-154.20 +/- 134.14
Episode length: 16.16 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 50       |
|    time_elapsed    | 212      |
|    total_timesteps | 102400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=102500, episode_reward=-165.86 +/- 166.56
Episode length: 16.00 +/- 4.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -166        |
| time/                   |             |
|    total_timesteps      | 102500      |
| train/                  |             |
|    approx_kl            | 0.006896277 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.337      |
|    explained_variance   | 0.0239      |
|    learning_rate        | 0.0001      |
|    loss                 | 4.38e+03    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00347    |
|    value_loss           | 8.87e+03    |
-----------------------------------------
Eval num_timesteps=103000, episode_reward=-104.18 +/- 169.01
Episode length: 16.76 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-133.16 +/- 170.42
Episode length: 16.18 +/- 5.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-156.14 +/- 158.59
Episode length: 15.66 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 51       |
|    time_elapsed    | 217      |
|    total_timesteps | 104448   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=104500, episode_reward=-140.30 +/- 146.03
Episode length: 16.68 +/- 4.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -140         |
| time/                   |              |
|    total_timesteps      | 104500       |
| train/                  |              |
|    approx_kl            | 0.0058902237 |
|    clip_fraction        | 0.0414       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.275       |
|    explained_variance   | 0.0254       |
|    learning_rate        | 0.0001       |
|    loss                 | 7.16e+03     |
|    n_updates            | 82           |
|    policy_gradient_loss | 0.00518      |
|    value_loss           | 9.4e+03      |
------------------------------------------
Eval num_timesteps=105000, episode_reward=-161.56 +/- 170.24
Episode length: 15.42 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-132.56 +/- 148.74
Episode length: 16.44 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=-161.08 +/- 152.28
Episode length: 16.24 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 52       |
|    time_elapsed    | 221      |
|    total_timesteps | 106496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=106500, episode_reward=-151.80 +/- 141.05
Episode length: 15.56 +/- 4.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -152         |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0058124415 |
|    clip_fraction        | 0.0508       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.355       |
|    explained_variance   | 0.0349       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.76e+03     |
|    n_updates            | 83           |
|    policy_gradient_loss | 0.0031       |
|    value_loss           | 9.1e+03      |
------------------------------------------
Eval num_timesteps=107000, episode_reward=-145.95 +/- 125.59
Episode length: 16.00 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-172.77 +/- 120.21
Episode length: 15.70 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-131.95 +/- 139.70
Episode length: 15.94 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=-141.19 +/- 187.33
Episode length: 16.98 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 53       |
|    time_elapsed    | 225      |
|    total_timesteps | 108544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=109000, episode_reward=-107.72 +/- 167.41
Episode length: 17.30 +/- 5.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.3         |
|    mean_reward          | -108         |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0043675294 |
|    clip_fraction        | 0.0556       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.36        |
|    explained_variance   | 0.0375       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.03e+03     |
|    n_updates            | 84           |
|    policy_gradient_loss | 0.00624      |
|    value_loss           | 7.58e+03     |
------------------------------------------
Eval num_timesteps=109500, episode_reward=-147.73 +/- 148.05
Episode length: 15.98 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-159.97 +/- 165.75
Episode length: 15.50 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=-173.76 +/- 139.17
Episode length: 16.22 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 54       |
|    time_elapsed    | 230      |
|    total_timesteps | 110592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=111000, episode_reward=-153.97 +/- 149.02
Episode length: 15.76 +/- 4.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -154        |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.004014983 |
|    clip_fraction        | 0.0337      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.305      |
|    explained_variance   | 0.0363      |
|    learning_rate        | 0.0001      |
|    loss                 | 4.96e+03    |
|    n_updates            | 85          |
|    policy_gradient_loss | -0.000189   |
|    value_loss           | 9.16e+03    |
-----------------------------------------
Eval num_timesteps=111500, episode_reward=-153.29 +/- 135.04
Episode length: 16.16 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-180.17 +/- 132.88
Episode length: 15.10 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=-162.04 +/- 137.20
Episode length: 15.84 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 55       |
|    time_elapsed    | 234      |
|    total_timesteps | 112640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=113000, episode_reward=-153.36 +/- 131.24
Episode length: 15.54 +/- 4.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -153         |
| time/                   |              |
|    total_timesteps      | 113000       |
| train/                  |              |
|    approx_kl            | 0.0047660647 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.239       |
|    explained_variance   | 0.0383       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.11e+03     |
|    n_updates            | 86           |
|    policy_gradient_loss | 0.00466      |
|    value_loss           | 9.85e+03     |
------------------------------------------
Eval num_timesteps=113500, episode_reward=-103.98 +/- 160.13
Episode length: 17.78 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-185.93 +/- 123.48
Episode length: 14.54 +/- 4.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=-147.86 +/- 161.28
Episode length: 16.52 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 56       |
|    time_elapsed    | 238      |
|    total_timesteps | 114688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=115000, episode_reward=-141.06 +/- 172.23
Episode length: 16.20 +/- 5.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -141        |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.007097403 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.392      |
|    explained_variance   | 0.0433      |
|    learning_rate        | 0.0001      |
|    loss                 | 4.83e+03    |
|    n_updates            | 87          |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 8.58e+03    |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=-134.37 +/- 127.51
Episode length: 16.06 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-133.12 +/- 140.97
Episode length: 16.94 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=-169.66 +/- 160.21
Episode length: 15.84 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 57       |
|    time_elapsed    | 242      |
|    total_timesteps | 116736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=117000, episode_reward=-192.35 +/- 147.77
Episode length: 16.40 +/- 5.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -192        |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.008758007 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.543      |
|    explained_variance   | 0.0402      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.92e+03    |
|    n_updates            | 88          |
|    policy_gradient_loss | 0.00722     |
|    value_loss           | 1.01e+04    |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=-198.43 +/- 122.62
Episode length: 16.10 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-205.77 +/- 113.43
Episode length: 16.18 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=-203.96 +/- 138.77
Episode length: 16.34 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -217     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 58       |
|    time_elapsed    | 246      |
|    total_timesteps | 118784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=119000, episode_reward=-146.78 +/- 180.02
Episode length: 16.58 +/- 4.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -147        |
| time/                   |             |
|    total_timesteps      | 119000      |
| train/                  |             |
|    approx_kl            | 0.008714173 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.501      |
|    explained_variance   | 0.0369      |
|    learning_rate        | 0.0001      |
|    loss                 | 5.27e+03    |
|    n_updates            | 89          |
|    policy_gradient_loss | 0.011       |
|    value_loss           | 9.01e+03    |
-----------------------------------------
Eval num_timesteps=119500, episode_reward=-166.72 +/- 120.89
Episode length: 15.34 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-139.15 +/- 145.28
Episode length: 16.88 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-126.09 +/- 152.95
Episode length: 16.36 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 59       |
|    time_elapsed    | 250      |
|    total_timesteps | 120832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=121000, episode_reward=-151.79 +/- 166.38
Episode length: 16.14 +/- 5.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -152         |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 0.0067210305 |
|    clip_fraction        | 0.081        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.394       |
|    explained_variance   | 0.0478       |
|    learning_rate        | 0.0001       |
|    loss                 | 5.07e+03     |
|    n_updates            | 90           |
|    policy_gradient_loss | 0.00231      |
|    value_loss           | 8.37e+03     |
------------------------------------------
Eval num_timesteps=121500, episode_reward=-160.78 +/- 139.90
Episode length: 15.78 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-142.82 +/- 137.87
Episode length: 16.16 +/- 4.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=-163.45 +/- 179.30
Episode length: 16.52 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -196     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 60       |
|    time_elapsed    | 254      |
|    total_timesteps | 122880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=123000, episode_reward=-176.18 +/- 150.38
Episode length: 15.74 +/- 4.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -176         |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0033762725 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.381       |
|    explained_variance   | 0.0509       |
|    learning_rate        | 0.0001       |
|    loss                 | 5.69e+03     |
|    n_updates            | 91           |
|    policy_gradient_loss | -0.00678     |
|    value_loss           | 9.08e+03     |
------------------------------------------
Eval num_timesteps=123500, episode_reward=-194.15 +/- 129.08
Episode length: 14.84 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-141.10 +/- 147.61
Episode length: 16.20 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=-158.19 +/- 164.13
Episode length: 16.04 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 61       |
|    time_elapsed    | 258      |
|    total_timesteps | 124928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=125000, episode_reward=-163.04 +/- 126.70
Episode length: 14.98 +/- 4.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | -163         |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0044247587 |
|    clip_fraction        | 0.0712       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.469       |
|    explained_variance   | 0.0491       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.96e+03     |
|    n_updates            | 92           |
|    policy_gradient_loss | 0.00398      |
|    value_loss           | 9.42e+03     |
------------------------------------------
Eval num_timesteps=125500, episode_reward=-145.03 +/- 158.50
Episode length: 16.14 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-144.20 +/- 123.26
Episode length: 15.84 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=-169.24 +/- 135.60
Episode length: 14.72 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -198     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 62       |
|    time_elapsed    | 262      |
|    total_timesteps | 126976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=127000, episode_reward=-175.28 +/- 126.58
Episode length: 16.64 +/- 4.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -175         |
| time/                   |              |
|    total_timesteps      | 127000       |
| train/                  |              |
|    approx_kl            | 0.0041654245 |
|    clip_fraction        | 0.0554       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.373       |
|    explained_variance   | 0.0575       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.43e+03     |
|    n_updates            | 93           |
|    policy_gradient_loss | 0.00339      |
|    value_loss           | 8.92e+03     |
------------------------------------------
Eval num_timesteps=127500, episode_reward=-157.93 +/- 145.70
Episode length: 16.36 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-180.87 +/- 157.92
Episode length: 15.88 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-113.96 +/- 168.94
Episode length: 17.52 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -114     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-180.18 +/- 121.60
Episode length: 16.46 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 63       |
|    time_elapsed    | 267      |
|    total_timesteps | 129024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=129500, episode_reward=-164.06 +/- 133.86
Episode length: 15.72 +/- 4.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -164         |
| time/                   |              |
|    total_timesteps      | 129500       |
| train/                  |              |
|    approx_kl            | 0.0058791274 |
|    clip_fraction        | 0.0945       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.428       |
|    explained_variance   | 0.0605       |
|    learning_rate        | 0.0001       |
|    loss                 | 6.21e+03     |
|    n_updates            | 94           |
|    policy_gradient_loss | -0.002       |
|    value_loss           | 9.05e+03     |
------------------------------------------
Eval num_timesteps=130000, episode_reward=-156.53 +/- 153.29
Episode length: 15.76 +/- 5.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-189.68 +/- 178.08
Episode length: 14.82 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-151.41 +/- 143.60
Episode length: 16.40 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 64       |
|    time_elapsed    | 271      |
|    total_timesteps | 131072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=131500, episode_reward=-123.75 +/- 158.33
Episode length: 16.04 +/- 5.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -124         |
| time/                   |              |
|    total_timesteps      | 131500       |
| train/                  |              |
|    approx_kl            | 0.0028913761 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.425       |
|    explained_variance   | 0.0701       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.14e+03     |
|    n_updates            | 95           |
|    policy_gradient_loss | 0.00552      |
|    value_loss           | 8.92e+03     |
------------------------------------------
Eval num_timesteps=132000, episode_reward=-145.21 +/- 131.87
Episode length: 15.50 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-154.59 +/- 111.59
Episode length: 15.60 +/- 4.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-128.29 +/- 128.83
Episode length: 16.64 +/- 4.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -211     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 65       |
|    time_elapsed    | 275      |
|    total_timesteps | 133120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=133500, episode_reward=-187.61 +/- 157.19
Episode length: 15.04 +/- 5.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | -188         |
| time/                   |              |
|    total_timesteps      | 133500       |
| train/                  |              |
|    approx_kl            | 0.0055450015 |
|    clip_fraction        | 0.0807       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.492       |
|    explained_variance   | 0.0623       |
|    learning_rate        | 0.0001       |
|    loss                 | 5.29e+03     |
|    n_updates            | 96           |
|    policy_gradient_loss | 0.0108       |
|    value_loss           | 9.09e+03     |
------------------------------------------
Eval num_timesteps=134000, episode_reward=-174.32 +/- 146.69
Episode length: 15.30 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-164.86 +/- 149.19
Episode length: 15.70 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-151.00 +/- 128.60
Episode length: 15.62 +/- 4.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 66       |
|    time_elapsed    | 279      |
|    total_timesteps | 135168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=135500, episode_reward=-126.97 +/- 156.40
Episode length: 16.34 +/- 5.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -127         |
| time/                   |              |
|    total_timesteps      | 135500       |
| train/                  |              |
|    approx_kl            | 0.0045193434 |
|    clip_fraction        | 0.0635       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.402       |
|    explained_variance   | 0.0613       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.98e+03     |
|    n_updates            | 97           |
|    policy_gradient_loss | 0.00649      |
|    value_loss           | 8.22e+03     |
------------------------------------------
Eval num_timesteps=136000, episode_reward=-133.41 +/- 129.29
Episode length: 15.92 +/- 4.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-142.31 +/- 163.95
Episode length: 16.56 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-138.57 +/- 150.67
Episode length: 15.92 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 67       |
|    time_elapsed    | 283      |
|    total_timesteps | 137216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=137500, episode_reward=-169.25 +/- 138.23
Episode length: 15.52 +/- 4.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -169         |
| time/                   |              |
|    total_timesteps      | 137500       |
| train/                  |              |
|    approx_kl            | 0.0074943206 |
|    clip_fraction        | 0.0851       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.383       |
|    explained_variance   | 0.0653       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.02e+03     |
|    n_updates            | 98           |
|    policy_gradient_loss | 0.00221      |
|    value_loss           | 9.22e+03     |
------------------------------------------
Eval num_timesteps=138000, episode_reward=-132.66 +/- 165.38
Episode length: 17.46 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-137.20 +/- 133.84
Episode length: 16.46 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-196.02 +/- 155.96
Episode length: 14.82 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 68       |
|    time_elapsed    | 287      |
|    total_timesteps | 139264   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=139500, episode_reward=-210.37 +/- 121.58
Episode length: 16.38 +/- 4.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -210        |
| time/                   |             |
|    total_timesteps      | 139500      |
| train/                  |             |
|    approx_kl            | 0.007524777 |
|    clip_fraction        | 0.0574      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.442      |
|    explained_variance   | 0.0584      |
|    learning_rate        | 0.0001      |
|    loss                 | 4.28e+03    |
|    n_updates            | 101         |
|    policy_gradient_loss | -0.00206    |
|    value_loss           | 8.72e+03    |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=-165.37 +/- 150.13
Episode length: 17.94 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-206.52 +/- 110.04
Episode length: 16.04 +/- 4.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-179.41 +/- 147.44
Episode length: 15.84 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 69       |
|    time_elapsed    | 291      |
|    total_timesteps | 141312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=141500, episode_reward=-198.34 +/- 142.16
Episode length: 16.52 +/- 5.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -198         |
| time/                   |              |
|    total_timesteps      | 141500       |
| train/                  |              |
|    approx_kl            | 0.0065834294 |
|    clip_fraction        | 0.0732       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.55        |
|    explained_variance   | 0.0719       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.11e+03     |
|    n_updates            | 102          |
|    policy_gradient_loss | 0.00464      |
|    value_loss           | 8.73e+03     |
------------------------------------------
Eval num_timesteps=142000, episode_reward=-201.00 +/- 115.72
Episode length: 16.52 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-213.40 +/- 144.39
Episode length: 15.74 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-196.67 +/- 126.47
Episode length: 16.88 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 70       |
|    time_elapsed    | 296      |
|    total_timesteps | 143360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=143500, episode_reward=-139.13 +/- 119.81
Episode length: 17.52 +/- 5.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | -139        |
| time/                   |             |
|    total_timesteps      | 143500      |
| train/                  |             |
|    approx_kl            | 0.005259805 |
|    clip_fraction        | 0.0495      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.485      |
|    explained_variance   | 0.0772      |
|    learning_rate        | 0.0001      |
|    loss                 | 4.63e+03    |
|    n_updates            | 103         |
|    policy_gradient_loss | 0.00784     |
|    value_loss           | 8.81e+03    |
-----------------------------------------
Eval num_timesteps=144000, episode_reward=-162.70 +/- 112.15
Episode length: 17.24 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-186.33 +/- 141.62
Episode length: 16.10 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-185.12 +/- 148.71
Episode length: 15.94 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -201     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 71       |
|    time_elapsed    | 300      |
|    total_timesteps | 145408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=145500, episode_reward=-139.52 +/- 164.92
Episode length: 16.28 +/- 5.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -140         |
| time/                   |              |
|    total_timesteps      | 145500       |
| train/                  |              |
|    approx_kl            | 0.0058044167 |
|    clip_fraction        | 0.0566       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.498       |
|    explained_variance   | 0.0743       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.92e+03     |
|    n_updates            | 104          |
|    policy_gradient_loss | -0.000896    |
|    value_loss           | 8.36e+03     |
------------------------------------------
Eval num_timesteps=146000, episode_reward=-186.82 +/- 149.02
Episode length: 16.10 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-179.85 +/- 173.95
Episode length: 16.40 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-197.25 +/- 116.06
Episode length: 14.30 +/- 3.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.3     |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 72       |
|    time_elapsed    | 304      |
|    total_timesteps | 147456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=147500, episode_reward=-162.61 +/- 137.73
Episode length: 16.34 +/- 4.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -163         |
| time/                   |              |
|    total_timesteps      | 147500       |
| train/                  |              |
|    approx_kl            | 0.0076463968 |
|    clip_fraction        | 0.0833       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.45        |
|    explained_variance   | 0.079        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.72e+03     |
|    n_updates            | 105          |
|    policy_gradient_loss | -0.00781     |
|    value_loss           | 1e+04        |
------------------------------------------
Eval num_timesteps=148000, episode_reward=-152.94 +/- 133.81
Episode length: 16.80 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-189.57 +/- 126.02
Episode length: 15.08 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-172.04 +/- 132.51
Episode length: 15.68 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-166.72 +/- 141.70
Episode length: 14.94 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 73       |
|    time_elapsed    | 308      |
|    total_timesteps | 149504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=150000, episode_reward=-152.60 +/- 143.95
Episode length: 15.90 +/- 5.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -153         |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0045100213 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.318       |
|    explained_variance   | 0.0783       |
|    learning_rate        | 0.0001       |
|    loss                 | 5.31e+03     |
|    n_updates            | 106          |
|    policy_gradient_loss | -0.00111     |
|    value_loss           | 8.66e+03     |
------------------------------------------
Eval num_timesteps=150500, episode_reward=-160.97 +/- 116.06
Episode length: 15.60 +/- 3.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-139.88 +/- 184.30
Episode length: 16.38 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=-141.06 +/- 181.30
Episode length: 16.86 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 74       |
|    time_elapsed    | 312      |
|    total_timesteps | 151552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=152000, episode_reward=-141.36 +/- 152.76
Episode length: 16.02 +/- 4.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -141        |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.005652131 |
|    clip_fraction        | 0.0638      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.0893      |
|    learning_rate        | 0.0001      |
|    loss                 | 5.69e+03    |
|    n_updates            | 107         |
|    policy_gradient_loss | 0.00444     |
|    value_loss           | 9.21e+03    |
-----------------------------------------
Eval num_timesteps=152500, episode_reward=-133.04 +/- 122.84
Episode length: 16.94 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-165.54 +/- 162.65
Episode length: 15.80 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=-114.52 +/- 127.53
Episode length: 16.52 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -189     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 75       |
|    time_elapsed    | 317      |
|    total_timesteps | 153600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=154000, episode_reward=-113.32 +/- 179.62
Episode length: 16.68 +/- 5.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -113         |
| time/                   |              |
|    total_timesteps      | 154000       |
| train/                  |              |
|    approx_kl            | 0.0059358114 |
|    clip_fraction        | 0.0848       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.327       |
|    explained_variance   | 0.0968       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.64e+03     |
|    n_updates            | 108          |
|    policy_gradient_loss | -0.00367     |
|    value_loss           | 8.25e+03     |
------------------------------------------
Eval num_timesteps=154500, episode_reward=-159.13 +/- 145.50
Episode length: 16.24 +/- 4.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-110.99 +/- 134.93
Episode length: 16.76 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=-126.11 +/- 172.44
Episode length: 16.52 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 76       |
|    time_elapsed    | 321      |
|    total_timesteps | 155648   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=156000, episode_reward=-139.85 +/- 147.82
Episode length: 16.88 +/- 5.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.9         |
|    mean_reward          | -140         |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0076958174 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.273       |
|    explained_variance   | 0.096        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.13e+03     |
|    n_updates            | 110          |
|    policy_gradient_loss | 0.0024       |
|    value_loss           | 8.49e+03     |
------------------------------------------
Eval num_timesteps=156500, episode_reward=-129.66 +/- 143.90
Episode length: 16.10 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-130.07 +/- 151.53
Episode length: 16.44 +/- 4.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-163.35 +/- 141.71
Episode length: 14.78 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 77       |
|    time_elapsed    | 325      |
|    total_timesteps | 157696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=158000, episode_reward=-155.29 +/- 146.81
Episode length: 16.06 +/- 4.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -155        |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.004337916 |
|    clip_fraction        | 0.0444      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.353      |
|    explained_variance   | 0.103       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.18e+03    |
|    n_updates            | 111         |
|    policy_gradient_loss | 0.00353     |
|    value_loss           | 8.38e+03    |
-----------------------------------------
Eval num_timesteps=158500, episode_reward=-119.59 +/- 148.82
Episode length: 16.26 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=-141.32 +/- 147.61
Episode length: 16.18 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=-124.56 +/- 167.69
Episode length: 16.24 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -195     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 78       |
|    time_elapsed    | 329      |
|    total_timesteps | 159744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=160000, episode_reward=-144.00 +/- 143.11
Episode length: 15.88 +/- 5.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -144         |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0049612853 |
|    clip_fraction        | 0.0686       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.406       |
|    explained_variance   | 0.116        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.5e+03      |
|    n_updates            | 112          |
|    policy_gradient_loss | 0.00823      |
|    value_loss           | 9e+03        |
------------------------------------------
Eval num_timesteps=160500, episode_reward=-145.87 +/- 149.58
Episode length: 16.24 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-139.54 +/- 160.99
Episode length: 16.40 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=-175.83 +/- 136.77
Episode length: 15.08 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 79       |
|    time_elapsed    | 333      |
|    total_timesteps | 161792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=162000, episode_reward=-116.57 +/- 138.22
Episode length: 16.30 +/- 5.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -117         |
| time/                   |              |
|    total_timesteps      | 162000       |
| train/                  |              |
|    approx_kl            | 0.0048056133 |
|    clip_fraction        | 0.0516       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.347       |
|    explained_variance   | 0.107        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.75e+03     |
|    n_updates            | 113          |
|    policy_gradient_loss | 0.0106       |
|    value_loss           | 9.17e+03     |
------------------------------------------
Eval num_timesteps=162500, episode_reward=-155.61 +/- 118.34
Episode length: 15.76 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-161.99 +/- 151.70
Episode length: 15.78 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-165.73 +/- 142.76
Episode length: 15.12 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 80       |
|    time_elapsed    | 337      |
|    total_timesteps | 163840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=164000, episode_reward=-127.16 +/- 116.90
Episode length: 16.02 +/- 4.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -127         |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0045758476 |
|    clip_fraction        | 0.0586       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.384       |
|    explained_variance   | 0.0937       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.8e+03      |
|    n_updates            | 114          |
|    policy_gradient_loss | -0.000221    |
|    value_loss           | 8.74e+03     |
------------------------------------------
Eval num_timesteps=164500, episode_reward=-153.61 +/- 176.82
Episode length: 15.38 +/- 5.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-150.51 +/- 178.68
Episode length: 15.30 +/- 5.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-166.98 +/- 114.57
Episode length: 15.24 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 81       |
|    time_elapsed    | 341      |
|    total_timesteps | 165888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=166000, episode_reward=-153.26 +/- 143.62
Episode length: 16.16 +/- 4.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -153        |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.005485439 |
|    clip_fraction        | 0.0596      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.427      |
|    explained_variance   | 0.108       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.68e+03    |
|    n_updates            | 115         |
|    policy_gradient_loss | 0.00175     |
|    value_loss           | 9.51e+03    |
-----------------------------------------
Eval num_timesteps=166500, episode_reward=-169.10 +/- 149.78
Episode length: 15.66 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-159.13 +/- 173.40
Episode length: 16.12 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=-114.76 +/- 178.02
Episode length: 17.16 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 82       |
|    time_elapsed    | 345      |
|    total_timesteps | 167936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=168000, episode_reward=-167.72 +/- 148.72
Episode length: 15.08 +/- 4.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | -168        |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.005485011 |
|    clip_fraction        | 0.0543      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.334      |
|    explained_variance   | 0.141       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.07e+03    |
|    n_updates            | 116         |
|    policy_gradient_loss | 0.00193     |
|    value_loss           | 8.33e+03    |
-----------------------------------------
Eval num_timesteps=168500, episode_reward=-141.64 +/- 150.95
Episode length: 15.50 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-144.22 +/- 138.86
Episode length: 15.68 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=-123.44 +/- 202.77
Episode length: 17.24 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 83       |
|    time_elapsed    | 349      |
|    total_timesteps | 169984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=170000, episode_reward=-187.48 +/- 132.01
Episode length: 14.70 +/- 3.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.7        |
|    mean_reward          | -187        |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.004671794 |
|    clip_fraction        | 0.0637      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.345      |
|    explained_variance   | 0.122       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.5e+03     |
|    n_updates            | 117         |
|    policy_gradient_loss | 0.00471     |
|    value_loss           | 8.37e+03    |
-----------------------------------------
Eval num_timesteps=170500, episode_reward=-140.81 +/- 159.05
Episode length: 16.80 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=-109.94 +/- 159.85
Episode length: 16.42 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=-179.39 +/- 146.89
Episode length: 15.68 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-100.15 +/- 162.90
Episode length: 17.08 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -100     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 84       |
|    time_elapsed    | 354      |
|    total_timesteps | 172032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=172500, episode_reward=-124.50 +/- 107.75
Episode length: 16.68 +/- 3.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -124         |
| time/                   |              |
|    total_timesteps      | 172500       |
| train/                  |              |
|    approx_kl            | 0.0059340587 |
|    clip_fraction        | 0.0708       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.35        |
|    explained_variance   | 0.127        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.24e+03     |
|    n_updates            | 118          |
|    policy_gradient_loss | 0.00722      |
|    value_loss           | 7.89e+03     |
------------------------------------------
Eval num_timesteps=173000, episode_reward=-134.15 +/- 142.13
Episode length: 16.70 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-141.35 +/- 135.94
Episode length: 16.46 +/- 4.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=-130.53 +/- 154.78
Episode length: 16.22 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 85       |
|    time_elapsed    | 358      |
|    total_timesteps | 174080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=174500, episode_reward=-139.22 +/- 131.95
Episode length: 16.90 +/- 4.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | -139        |
| time/                   |             |
|    total_timesteps      | 174500      |
| train/                  |             |
|    approx_kl            | 0.004433889 |
|    clip_fraction        | 0.057       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.344      |
|    explained_variance   | 0.123       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.77e+03    |
|    n_updates            | 119         |
|    policy_gradient_loss | 0.00198     |
|    value_loss           | 8.37e+03    |
-----------------------------------------
Eval num_timesteps=175000, episode_reward=-174.48 +/- 125.62
Episode length: 14.62 +/- 3.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-192.75 +/- 147.02
Episode length: 14.68 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=-150.14 +/- 173.90
Episode length: 15.96 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -196     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 86       |
|    time_elapsed    | 362      |
|    total_timesteps | 176128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=176500, episode_reward=-165.06 +/- 155.61
Episode length: 15.58 +/- 4.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -165        |
| time/                   |             |
|    total_timesteps      | 176500      |
| train/                  |             |
|    approx_kl            | 0.006764395 |
|    clip_fraction        | 0.0842      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.357      |
|    explained_variance   | 0.149       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.49e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00739    |
|    value_loss           | 8.91e+03    |
-----------------------------------------
Eval num_timesteps=177000, episode_reward=-148.92 +/- 138.55
Episode length: 16.58 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-105.79 +/- 158.21
Episode length: 17.18 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=-131.27 +/- 164.48
Episode length: 16.52 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 87       |
|    time_elapsed    | 366      |
|    total_timesteps | 178176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=178500, episode_reward=-164.51 +/- 125.55
Episode length: 14.84 +/- 4.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -165         |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0059190714 |
|    clip_fraction        | 0.0797       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.399       |
|    explained_variance   | 0.17         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.41e+03     |
|    n_updates            | 121          |
|    policy_gradient_loss | 0.00244      |
|    value_loss           | 7.76e+03     |
------------------------------------------
Eval num_timesteps=179000, episode_reward=-165.46 +/- 139.91
Episode length: 15.08 +/- 4.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-117.90 +/- 142.55
Episode length: 16.16 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-145.85 +/- 116.43
Episode length: 16.28 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -191     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 88       |
|    time_elapsed    | 370      |
|    total_timesteps | 180224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=180500, episode_reward=-162.54 +/- 151.05
Episode length: 15.66 +/- 4.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -163         |
| time/                   |              |
|    total_timesteps      | 180500       |
| train/                  |              |
|    approx_kl            | 0.0045137084 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.385       |
|    explained_variance   | 0.174        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.52e+03     |
|    n_updates            | 122          |
|    policy_gradient_loss | 0.00323      |
|    value_loss           | 7.28e+03     |
------------------------------------------
Eval num_timesteps=181000, episode_reward=-167.26 +/- 162.59
Episode length: 15.54 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-153.13 +/- 162.18
Episode length: 15.64 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=-139.48 +/- 142.67
Episode length: 15.14 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 89       |
|    time_elapsed    | 374      |
|    total_timesteps | 182272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=182500, episode_reward=-196.14 +/- 129.73
Episode length: 15.86 +/- 4.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | -196        |
| time/                   |             |
|    total_timesteps      | 182500      |
| train/                  |             |
|    approx_kl            | 0.006719312 |
|    clip_fraction        | 0.0959      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.385      |
|    explained_variance   | 0.158       |
|    learning_rate        | 0.0001      |
|    loss                 | 6.05e+03    |
|    n_updates            | 123         |
|    policy_gradient_loss | 0.00973     |
|    value_loss           | 7.72e+03    |
-----------------------------------------
Eval num_timesteps=183000, episode_reward=-205.62 +/- 127.22
Episode length: 15.32 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-123.43 +/- 127.56
Episode length: 17.18 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=-179.10 +/- 163.96
Episode length: 15.70 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -213     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 90       |
|    time_elapsed    | 378      |
|    total_timesteps | 184320   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=184500, episode_reward=-144.90 +/- 134.31
Episode length: 15.36 +/- 4.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | -145        |
| time/                   |             |
|    total_timesteps      | 184500      |
| train/                  |             |
|    approx_kl            | 0.007498719 |
|    clip_fraction        | 0.0925      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.402      |
|    explained_variance   | 0.169       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.63e+03    |
|    n_updates            | 125         |
|    policy_gradient_loss | 0.00597     |
|    value_loss           | 8.1e+03     |
-----------------------------------------
Eval num_timesteps=185000, episode_reward=-149.24 +/- 131.97
Episode length: 16.22 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=-193.16 +/- 117.79
Episode length: 14.86 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=-173.61 +/- 131.99
Episode length: 15.50 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -219     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 91       |
|    time_elapsed    | 382      |
|    total_timesteps | 186368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=186500, episode_reward=-167.06 +/- 155.56
Episode length: 16.16 +/- 5.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -167        |
| time/                   |             |
|    total_timesteps      | 186500      |
| train/                  |             |
|    approx_kl            | 0.004070387 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.433      |
|    explained_variance   | 0.2         |
|    learning_rate        | 0.0001      |
|    loss                 | 3.91e+03    |
|    n_updates            | 126         |
|    policy_gradient_loss | 0.00887     |
|    value_loss           | 7.88e+03    |
-----------------------------------------
Eval num_timesteps=187000, episode_reward=-164.87 +/- 135.00
Episode length: 16.36 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-181.12 +/- 165.78
Episode length: 16.94 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=-161.28 +/- 135.95
Episode length: 16.70 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -199     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 92       |
|    time_elapsed    | 386      |
|    total_timesteps | 188416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=188500, episode_reward=-153.08 +/- 146.04
Episode length: 15.94 +/- 4.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -153         |
| time/                   |              |
|    total_timesteps      | 188500       |
| train/                  |              |
|    approx_kl            | 0.0050979247 |
|    clip_fraction        | 0.0827       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.447       |
|    explained_variance   | 0.205        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.2e+03      |
|    n_updates            | 127          |
|    policy_gradient_loss | 0.00495      |
|    value_loss           | 7.2e+03      |
------------------------------------------
Eval num_timesteps=189000, episode_reward=-121.72 +/- 145.37
Episode length: 17.00 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-183.37 +/- 151.66
Episode length: 15.42 +/- 4.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-147.78 +/- 133.66
Episode length: 15.76 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -204     |
| time/              |          |
|    fps             | 487      |
|    iterations      | 93       |
|    time_elapsed    | 391      |
|    total_timesteps | 190464   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=190500, episode_reward=-156.29 +/- 159.32
Episode length: 15.52 +/- 5.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.5       |
|    mean_reward          | -156       |
| time/                   |            |
|    total_timesteps      | 190500     |
| train/                  |            |
|    approx_kl            | 0.00598627 |
|    clip_fraction        | 0.0818     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.429     |
|    explained_variance   | 0.208      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.9e+03    |
|    n_updates            | 130        |
|    policy_gradient_loss | 0.00309    |
|    value_loss           | 7.58e+03   |
----------------------------------------
Eval num_timesteps=191000, episode_reward=-160.75 +/- 136.04
Episode length: 15.48 +/- 3.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-127.06 +/- 139.81
Episode length: 16.28 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-126.33 +/- 135.87
Episode length: 16.34 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-150.04 +/- 159.49
Episode length: 15.66 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -208     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 94       |
|    time_elapsed    | 395      |
|    total_timesteps | 192512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=193000, episode_reward=-93.73 +/- 128.94
Episode length: 17.16 +/- 5.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | -93.7        |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 0.0067480565 |
|    clip_fraction        | 0.0729       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.406       |
|    explained_variance   | 0.189        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.61e+03     |
|    n_updates            | 131          |
|    policy_gradient_loss | 0.00372      |
|    value_loss           | 9.23e+03     |
------------------------------------------
Eval num_timesteps=193500, episode_reward=-146.47 +/- 140.25
Episode length: 16.18 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-144.62 +/- 139.59
Episode length: 16.28 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=-135.31 +/- 151.95
Episode length: 16.40 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -192     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 95       |
|    time_elapsed    | 400      |
|    total_timesteps | 194560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=195000, episode_reward=-172.43 +/- 121.65
Episode length: 16.44 +/- 4.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -172         |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 0.0057704295 |
|    clip_fraction        | 0.0815       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.431       |
|    explained_variance   | 0.191        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.11e+03     |
|    n_updates            | 132          |
|    policy_gradient_loss | -0.00149     |
|    value_loss           | 8.49e+03     |
------------------------------------------
Eval num_timesteps=195500, episode_reward=-148.65 +/- 146.47
Episode length: 17.34 +/- 5.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=-195.59 +/- 145.47
Episode length: 16.26 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=-163.12 +/- 149.20
Episode length: 16.26 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 96       |
|    time_elapsed    | 404      |
|    total_timesteps | 196608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=197000, episode_reward=-138.94 +/- 141.75
Episode length: 17.36 +/- 5.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | -139         |
| time/                   |              |
|    total_timesteps      | 197000       |
| train/                  |              |
|    approx_kl            | 0.0064338166 |
|    clip_fraction        | 0.0816       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.418       |
|    explained_variance   | 0.236        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.36e+03     |
|    n_updates            | 133          |
|    policy_gradient_loss | 0.00569      |
|    value_loss           | 6.46e+03     |
------------------------------------------
Eval num_timesteps=197500, episode_reward=-156.49 +/- 154.39
Episode length: 17.36 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-181.20 +/- 127.03
Episode length: 15.66 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-170.16 +/- 159.43
Episode length: 16.12 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 97       |
|    time_elapsed    | 408      |
|    total_timesteps | 198656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=199000, episode_reward=-127.99 +/- 187.42
Episode length: 17.64 +/- 6.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | -128         |
| time/                   |              |
|    total_timesteps      | 199000       |
| train/                  |              |
|    approx_kl            | 0.0042948504 |
|    clip_fraction        | 0.029        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.477       |
|    explained_variance   | 0.224        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.15e+03     |
|    n_updates            | 134          |
|    policy_gradient_loss | 0.00265      |
|    value_loss           | 7.45e+03     |
------------------------------------------
Eval num_timesteps=199500, episode_reward=-145.53 +/- 152.65
Episode length: 17.28 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-150.73 +/- 140.59
Episode length: 16.66 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=-180.60 +/- 133.73
Episode length: 16.48 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -192     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 98       |
|    time_elapsed    | 412      |
|    total_timesteps | 200704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=201000, episode_reward=-173.85 +/- 133.00
Episode length: 16.12 +/- 4.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -174         |
| time/                   |              |
|    total_timesteps      | 201000       |
| train/                  |              |
|    approx_kl            | 0.0052740416 |
|    clip_fraction        | 0.0745       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.387       |
|    explained_variance   | 0.204        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.25e+03     |
|    n_updates            | 135          |
|    policy_gradient_loss | 0.00106      |
|    value_loss           | 7.96e+03     |
------------------------------------------
Eval num_timesteps=201500, episode_reward=-161.75 +/- 166.16
Episode length: 16.16 +/- 5.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-159.64 +/- 159.14
Episode length: 16.54 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=-159.01 +/- 120.80
Episode length: 17.10 +/- 5.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -195     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 99       |
|    time_elapsed    | 416      |
|    total_timesteps | 202752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=203000, episode_reward=-164.01 +/- 171.61
Episode length: 16.68 +/- 5.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -164         |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 0.0050422964 |
|    clip_fraction        | 0.0651       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.515       |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.42e+03     |
|    n_updates            | 136          |
|    policy_gradient_loss | -0.00018     |
|    value_loss           | 6.31e+03     |
------------------------------------------
Eval num_timesteps=203500, episode_reward=-155.03 +/- 179.56
Episode length: 17.18 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-158.83 +/- 146.82
Episode length: 17.16 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=-156.59 +/- 124.82
Episode length: 16.26 +/- 4.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 100      |
|    time_elapsed    | 421      |
|    total_timesteps | 204800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=205000, episode_reward=-191.33 +/- 119.98
Episode length: 16.44 +/- 4.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -191        |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.007761037 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.425      |
|    explained_variance   | 0.266       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.98e+03    |
|    n_updates            | 137         |
|    policy_gradient_loss | 0.00455     |
|    value_loss           | 6.45e+03    |
-----------------------------------------
Eval num_timesteps=205500, episode_reward=-172.82 +/- 184.85
Episode length: 17.38 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-194.88 +/- 127.24
Episode length: 16.26 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=-189.14 +/- 125.32
Episode length: 15.74 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.5     |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 101      |
|    time_elapsed    | 425      |
|    total_timesteps | 206848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=207000, episode_reward=-152.85 +/- 159.58
Episode length: 16.84 +/- 5.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -153         |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 0.0062485035 |
|    clip_fraction        | 0.051        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.371       |
|    explained_variance   | 0.226        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.33e+03     |
|    n_updates            | 138          |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 7.01e+03     |
------------------------------------------
Eval num_timesteps=207500, episode_reward=-176.68 +/- 141.82
Episode length: 16.60 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=-184.92 +/- 120.48
Episode length: 15.82 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=-160.05 +/- 142.84
Episode length: 17.28 +/- 5.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -183     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 102      |
|    time_elapsed    | 429      |
|    total_timesteps | 208896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=209000, episode_reward=-196.71 +/- 134.88
Episode length: 16.10 +/- 4.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -197         |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 0.0065012434 |
|    clip_fraction        | 0.12         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.419       |
|    explained_variance   | 0.222        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.13e+03     |
|    n_updates            | 139          |
|    policy_gradient_loss | 0.00562      |
|    value_loss           | 7.07e+03     |
------------------------------------------
Eval num_timesteps=209500, episode_reward=-218.07 +/- 135.17
Episode length: 14.78 +/- 4.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-198.01 +/- 138.86
Episode length: 16.30 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=-170.67 +/- 136.24
Episode length: 15.78 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 103      |
|    time_elapsed    | 433      |
|    total_timesteps | 210944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=211000, episode_reward=-149.19 +/- 143.74
Episode length: 16.70 +/- 5.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -149        |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.003319508 |
|    clip_fraction        | 0.0385      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.391      |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.98e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.000528    |
|    value_loss           | 6.42e+03    |
-----------------------------------------
Eval num_timesteps=211500, episode_reward=-163.76 +/- 145.25
Episode length: 16.72 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-182.14 +/- 153.27
Episode length: 16.48 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=-160.58 +/- 167.49
Episode length: 16.94 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -199     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 104      |
|    time_elapsed    | 437      |
|    total_timesteps | 212992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=213000, episode_reward=-190.42 +/- 128.86
Episode length: 15.98 +/- 4.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -190         |
| time/                   |              |
|    total_timesteps      | 213000       |
| train/                  |              |
|    approx_kl            | 0.0064459043 |
|    clip_fraction        | 0.0603       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.41        |
|    explained_variance   | 0.272        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.34e+03     |
|    n_updates            | 141          |
|    policy_gradient_loss | 0.0108       |
|    value_loss           | 7.33e+03     |
------------------------------------------
Eval num_timesteps=213500, episode_reward=-129.77 +/- 137.06
Episode length: 17.08 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=-182.38 +/- 142.53
Episode length: 17.08 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=-161.88 +/- 122.42
Episode length: 17.54 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-147.59 +/- 121.62
Episode length: 17.08 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -204     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 105      |
|    time_elapsed    | 442      |
|    total_timesteps | 215040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=215500, episode_reward=-193.76 +/- 124.25
Episode length: 16.18 +/- 5.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -194         |
| time/                   |              |
|    total_timesteps      | 215500       |
| train/                  |              |
|    approx_kl            | 0.0046066088 |
|    clip_fraction        | 0.0747       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.395       |
|    explained_variance   | 0.234        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.89e+03     |
|    n_updates            | 142          |
|    policy_gradient_loss | -0.00097     |
|    value_loss           | 7.6e+03      |
------------------------------------------
Eval num_timesteps=216000, episode_reward=-165.43 +/- 108.64
Episode length: 16.78 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=-177.39 +/- 127.95
Episode length: 16.82 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=-184.76 +/- 145.66
Episode length: 16.46 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -201     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 106      |
|    time_elapsed    | 447      |
|    total_timesteps | 217088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=217500, episode_reward=-186.64 +/- 118.82
Episode length: 16.06 +/- 4.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -187        |
| time/                   |             |
|    total_timesteps      | 217500      |
| train/                  |             |
|    approx_kl            | 0.007091758 |
|    clip_fraction        | 0.0975      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.383      |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.44e+03    |
|    n_updates            | 143         |
|    policy_gradient_loss | 0.00343     |
|    value_loss           | 6.62e+03    |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=-159.67 +/- 140.68
Episode length: 16.96 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=-139.50 +/- 135.54
Episode length: 16.98 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=-178.19 +/- 148.36
Episode length: 15.60 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -207     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 107      |
|    time_elapsed    | 451      |
|    total_timesteps | 219136   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=219500, episode_reward=-190.51 +/- 132.02
Episode length: 15.50 +/- 4.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -191        |
| time/                   |             |
|    total_timesteps      | 219500      |
| train/                  |             |
|    approx_kl            | 0.006283163 |
|    clip_fraction        | 0.0631      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.378      |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.65e+03    |
|    n_updates            | 145         |
|    policy_gradient_loss | 0.0032      |
|    value_loss           | 7.25e+03    |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=-132.25 +/- 126.78
Episode length: 17.66 +/- 5.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=-128.91 +/- 144.90
Episode length: 17.76 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=-180.14 +/- 109.40
Episode length: 15.98 +/- 5.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 108      |
|    time_elapsed    | 455      |
|    total_timesteps | 221184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=221500, episode_reward=-137.11 +/- 164.12
Episode length: 17.34 +/- 6.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | -137       |
| time/                   |            |
|    total_timesteps      | 221500     |
| train/                  |            |
|    approx_kl            | 0.00611872 |
|    clip_fraction        | 0.0826     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.329     |
|    explained_variance   | 0.302      |
|    learning_rate        | 0.0001     |
|    loss                 | 4.22e+03   |
|    n_updates            | 146        |
|    policy_gradient_loss | 0.00743    |
|    value_loss           | 7.2e+03    |
----------------------------------------
Eval num_timesteps=222000, episode_reward=-182.32 +/- 107.25
Episode length: 15.28 +/- 4.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=-161.34 +/- 144.10
Episode length: 17.06 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=-147.15 +/- 161.51
Episode length: 16.98 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 109      |
|    time_elapsed    | 459      |
|    total_timesteps | 223232   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=223500, episode_reward=-186.15 +/- 126.32
Episode length: 15.26 +/- 4.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | -186        |
| time/                   |             |
|    total_timesteps      | 223500      |
| train/                  |             |
|    approx_kl            | 0.012727942 |
|    clip_fraction        | 0.0674      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.337      |
|    explained_variance   | 0.29        |
|    learning_rate        | 0.0001      |
|    loss                 | 5.07e+03    |
|    n_updates            | 148         |
|    policy_gradient_loss | 0.00587     |
|    value_loss           | 7.04e+03    |
-----------------------------------------
Eval num_timesteps=224000, episode_reward=-173.37 +/- 180.11
Episode length: 16.78 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=-149.14 +/- 128.32
Episode length: 17.16 +/- 5.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-150.05 +/- 131.84
Episode length: 16.52 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 110      |
|    time_elapsed    | 463      |
|    total_timesteps | 225280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=225500, episode_reward=-178.59 +/- 129.95
Episode length: 15.48 +/- 3.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -179        |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 0.005253999 |
|    clip_fraction        | 0.0811      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.351      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.09e+03    |
|    n_updates            | 149         |
|    policy_gradient_loss | 0.00812     |
|    value_loss           | 7.51e+03    |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=-128.81 +/- 151.84
Episode length: 16.32 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=-178.21 +/- 171.06
Episode length: 15.94 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=-152.43 +/- 116.49
Episode length: 15.18 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -209     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 111      |
|    time_elapsed    | 467      |
|    total_timesteps | 227328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=227500, episode_reward=-151.39 +/- 150.40
Episode length: 16.30 +/- 5.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -151        |
| time/                   |             |
|    total_timesteps      | 227500      |
| train/                  |             |
|    approx_kl            | 0.007937715 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.346      |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.93e+03    |
|    n_updates            | 150         |
|    policy_gradient_loss | 0.00689     |
|    value_loss           | 7.74e+03    |
-----------------------------------------
Eval num_timesteps=228000, episode_reward=-157.44 +/- 142.21
Episode length: 15.96 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-182.25 +/- 134.49
Episode length: 15.58 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=-140.88 +/- 124.54
Episode length: 17.50 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 112      |
|    time_elapsed    | 472      |
|    total_timesteps | 229376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=229500, episode_reward=-137.25 +/- 122.76
Episode length: 16.20 +/- 4.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -137        |
| time/                   |             |
|    total_timesteps      | 229500      |
| train/                  |             |
|    approx_kl            | 0.006467442 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.325      |
|    explained_variance   | 0.294       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.03e+03    |
|    n_updates            | 151         |
|    policy_gradient_loss | 0.00437     |
|    value_loss           | 6.44e+03    |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=-159.28 +/- 116.44
Episode length: 15.06 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=-156.89 +/- 176.08
Episode length: 16.12 +/- 5.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=-140.65 +/- 164.89
Episode length: 16.14 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 113      |
|    time_elapsed    | 476      |
|    total_timesteps | 231424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=231500, episode_reward=-136.22 +/- 133.10
Episode length: 16.24 +/- 5.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -136         |
| time/                   |              |
|    total_timesteps      | 231500       |
| train/                  |              |
|    approx_kl            | 0.0071916236 |
|    clip_fraction        | 0.116        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.254       |
|    explained_variance   | 0.285        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.6e+03      |
|    n_updates            | 152          |
|    policy_gradient_loss | -0.00329     |
|    value_loss           | 7.47e+03     |
------------------------------------------
Eval num_timesteps=232000, episode_reward=-143.14 +/- 140.52
Episode length: 15.88 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-153.78 +/- 163.24
Episode length: 15.34 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=-169.54 +/- 153.70
Episode length: 15.32 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -174     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 114      |
|    time_elapsed    | 480      |
|    total_timesteps | 233472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=233500, episode_reward=-121.90 +/- 191.27
Episode length: 16.90 +/- 6.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.9         |
|    mean_reward          | -122         |
| time/                   |              |
|    total_timesteps      | 233500       |
| train/                  |              |
|    approx_kl            | 0.0056899786 |
|    clip_fraction        | 0.0724       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.219       |
|    explained_variance   | 0.292        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.93e+03     |
|    n_updates            | 153          |
|    policy_gradient_loss | 0.00347      |
|    value_loss           | 7.23e+03     |
------------------------------------------
Eval num_timesteps=234000, episode_reward=-159.60 +/- 154.50
Episode length: 15.58 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=-152.34 +/- 125.38
Episode length: 15.40 +/- 4.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=-149.17 +/- 135.08
Episode length: 16.26 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=-152.04 +/- 112.56
Episode length: 16.16 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 115      |
|    time_elapsed    | 485      |
|    total_timesteps | 235520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=236000, episode_reward=-182.72 +/- 127.20
Episode length: 14.76 +/- 3.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -183         |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 0.0033770471 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.296       |
|    explained_variance   | 0.223        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.95e+03     |
|    n_updates            | 154          |
|    policy_gradient_loss | 0.000165     |
|    value_loss           | 7.21e+03     |
------------------------------------------
Eval num_timesteps=236500, episode_reward=-167.04 +/- 134.91
Episode length: 15.24 +/- 4.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=-173.67 +/- 133.18
Episode length: 15.00 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=-156.83 +/- 119.29
Episode length: 14.76 +/- 3.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -147     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 116      |
|    time_elapsed    | 488      |
|    total_timesteps | 237568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=238000, episode_reward=-178.53 +/- 146.55
Episode length: 15.80 +/- 4.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -179        |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.005950915 |
|    clip_fraction        | 0.058       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.278      |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 3.71e+03    |
|    n_updates            | 155         |
|    policy_gradient_loss | 0.0102      |
|    value_loss           | 7.98e+03    |
-----------------------------------------
Eval num_timesteps=238500, episode_reward=-156.22 +/- 147.72
Episode length: 16.90 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-215.02 +/- 141.67
Episode length: 14.44 +/- 4.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=-190.94 +/- 131.95
Episode length: 16.00 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 117      |
|    time_elapsed    | 493      |
|    total_timesteps | 239616   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=240000, episode_reward=-163.97 +/- 132.22
Episode length: 15.92 +/- 4.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | -164        |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.006351621 |
|    clip_fraction        | 0.081       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.301      |
|    explained_variance   | 0.273       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.99e+03    |
|    n_updates            | 157         |
|    policy_gradient_loss | 0.00533     |
|    value_loss           | 6.54e+03    |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=-192.55 +/- 168.95
Episode length: 14.90 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=-131.71 +/- 157.26
Episode length: 16.94 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=-166.25 +/- 124.41
Episode length: 15.94 +/- 4.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -183     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 118      |
|    time_elapsed    | 497      |
|    total_timesteps | 241664   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=242000, episode_reward=-156.35 +/- 125.03
Episode length: 16.74 +/- 4.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -156        |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.004261688 |
|    clip_fraction        | 0.0604      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.292      |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.5e+03     |
|    n_updates            | 159         |
|    policy_gradient_loss | 0.00497     |
|    value_loss           | 7.35e+03    |
-----------------------------------------
Eval num_timesteps=242500, episode_reward=-142.73 +/- 126.54
Episode length: 17.76 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=-141.85 +/- 129.15
Episode length: 17.06 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=-165.52 +/- 154.77
Episode length: 17.18 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 119      |
|    time_elapsed    | 501      |
|    total_timesteps | 243712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=244000, episode_reward=-144.62 +/- 148.65
Episode length: 15.92 +/- 4.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -145         |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0041103116 |
|    clip_fraction        | 0.0599       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.307       |
|    explained_variance   | 0.266        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.42e+03     |
|    n_updates            | 160          |
|    policy_gradient_loss | 0.00204      |
|    value_loss           | 8.34e+03     |
------------------------------------------
Eval num_timesteps=244500, episode_reward=-128.89 +/- 175.94
Episode length: 17.58 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=-156.66 +/- 138.56
Episode length: 15.54 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=-154.53 +/- 136.28
Episode length: 15.70 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 120      |
|    time_elapsed    | 505      |
|    total_timesteps | 245760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=246000, episode_reward=-147.82 +/- 158.59
Episode length: 16.28 +/- 5.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -148         |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0053417785 |
|    clip_fraction        | 0.0923       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.244       |
|    explained_variance   | 0.269        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.85e+03     |
|    n_updates            | 161          |
|    policy_gradient_loss | 0.00325      |
|    value_loss           | 7.41e+03     |
------------------------------------------
Eval num_timesteps=246500, episode_reward=-168.55 +/- 113.10
Episode length: 14.80 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=-144.81 +/- 165.11
Episode length: 16.24 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=-142.92 +/- 144.48
Episode length: 16.38 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 121      |
|    time_elapsed    | 509      |
|    total_timesteps | 247808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=248000, episode_reward=-154.41 +/- 134.77
Episode length: 15.28 +/- 4.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -154         |
| time/                   |              |
|    total_timesteps      | 248000       |
| train/                  |              |
|    approx_kl            | 0.0046593426 |
|    clip_fraction        | 0.0641       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.248       |
|    explained_variance   | 0.29         |
|    learning_rate        | 0.0001       |
|    loss                 | 4.06e+03     |
|    n_updates            | 162          |
|    policy_gradient_loss | 0.00216      |
|    value_loss           | 7.91e+03     |
------------------------------------------
Eval num_timesteps=248500, episode_reward=-139.02 +/- 148.25
Episode length: 16.42 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=-141.83 +/- 139.54
Episode length: 15.82 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=-167.25 +/- 164.39
Episode length: 16.02 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 122      |
|    time_elapsed    | 514      |
|    total_timesteps | 249856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=250000, episode_reward=-158.29 +/- 117.19
Episode length: 16.66 +/- 4.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -158        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.003288524 |
|    clip_fraction        | 0.0385      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.219      |
|    explained_variance   | 0.307       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.26e+03    |
|    n_updates            | 163         |
|    policy_gradient_loss | 0.00496     |
|    value_loss           | 7.2e+03     |
-----------------------------------------
Eval num_timesteps=250500, episode_reward=-148.87 +/- 141.86
Episode length: 16.92 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=-176.26 +/- 142.64
Episode length: 16.50 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=-163.58 +/- 138.91
Episode length: 17.08 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 123      |
|    time_elapsed    | 518      |
|    total_timesteps | 251904   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=252000, episode_reward=-148.93 +/- 138.59
Episode length: 16.34 +/- 5.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -149         |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 0.0037322782 |
|    clip_fraction        | 0.0523       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.231       |
|    explained_variance   | 0.316        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.61e+03     |
|    n_updates            | 164          |
|    policy_gradient_loss | 0.00816      |
|    value_loss           | 7.18e+03     |
------------------------------------------
Eval num_timesteps=252500, episode_reward=-164.89 +/- 120.00
Episode length: 16.86 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=-165.01 +/- 142.51
Episode length: 16.68 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=-137.95 +/- 138.45
Episode length: 17.70 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 124      |
|    time_elapsed    | 522      |
|    total_timesteps | 253952   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=254000, episode_reward=-156.55 +/- 158.01
Episode length: 16.38 +/- 5.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -157        |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.006799492 |
|    clip_fraction        | 0.0465      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.227      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.51e+03    |
|    n_updates            | 166         |
|    policy_gradient_loss | 0.00192     |
|    value_loss           | 6.13e+03    |
-----------------------------------------
Eval num_timesteps=254500, episode_reward=-174.44 +/- 140.31
Episode length: 16.38 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=-167.59 +/- 131.98
Episode length: 16.78 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=-175.19 +/- 148.19
Episode length: 16.38 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-151.88 +/- 134.29
Episode length: 16.96 +/- 5.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 125      |
|    time_elapsed    | 527      |
|    total_timesteps | 256000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=256500, episode_reward=-186.60 +/- 120.68
Episode length: 15.84 +/- 4.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -187        |
| time/                   |             |
|    total_timesteps      | 256500      |
| train/                  |             |
|    approx_kl            | 0.005605958 |
|    clip_fraction        | 0.0604      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.253      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.61e+03    |
|    n_updates            | 167         |
|    policy_gradient_loss | 0.00209     |
|    value_loss           | 6.55e+03    |
-----------------------------------------
Eval num_timesteps=257000, episode_reward=-150.15 +/- 138.18
Episode length: 16.72 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=-156.34 +/- 135.81
Episode length: 17.30 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=-156.27 +/- 140.09
Episode length: 16.24 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -192     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 126      |
|    time_elapsed    | 531      |
|    total_timesteps | 258048   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=258500, episode_reward=-152.19 +/- 144.62
Episode length: 17.42 +/- 5.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | -152         |
| time/                   |              |
|    total_timesteps      | 258500       |
| train/                  |              |
|    approx_kl            | 0.0057639545 |
|    clip_fraction        | 0.084        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.319       |
|    explained_variance   | 0.345        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.49e+03     |
|    n_updates            | 168          |
|    policy_gradient_loss | 0.00299      |
|    value_loss           | 6.71e+03     |
------------------------------------------
Eval num_timesteps=259000, episode_reward=-165.96 +/- 135.69
Episode length: 15.84 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=-170.26 +/- 120.46
Episode length: 15.50 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-144.49 +/- 162.35
Episode length: 17.96 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 127      |
|    time_elapsed    | 535      |
|    total_timesteps | 260096   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=260500, episode_reward=-161.81 +/- 129.01
Episode length: 16.72 +/- 4.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -162         |
| time/                   |              |
|    total_timesteps      | 260500       |
| train/                  |              |
|    approx_kl            | 0.0057705236 |
|    clip_fraction        | 0.0434       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.294       |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.89e+03     |
|    n_updates            | 170          |
|    policy_gradient_loss | 0.00412      |
|    value_loss           | 6.35e+03     |
------------------------------------------
Eval num_timesteps=261000, episode_reward=-173.83 +/- 150.49
Episode length: 16.06 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=-166.22 +/- 165.23
Episode length: 17.20 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=-193.36 +/- 129.18
Episode length: 15.52 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -212     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 128      |
|    time_elapsed    | 540      |
|    total_timesteps | 262144   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=262500, episode_reward=-180.29 +/- 154.76
Episode length: 16.90 +/- 5.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | -180        |
| time/                   |             |
|    total_timesteps      | 262500      |
| train/                  |             |
|    approx_kl            | 0.006346326 |
|    clip_fraction        | 0.096       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.34       |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.71e+03    |
|    n_updates            | 171         |
|    policy_gradient_loss | 0.00253     |
|    value_loss           | 7.48e+03    |
-----------------------------------------
Eval num_timesteps=263000, episode_reward=-224.75 +/- 124.69
Episode length: 15.50 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=-172.88 +/- 132.86
Episode length: 16.90 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=-191.22 +/- 143.35
Episode length: 16.52 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 129      |
|    time_elapsed    | 544      |
|    total_timesteps | 264192   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=264500, episode_reward=-193.86 +/- 145.21
Episode length: 16.38 +/- 5.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -194        |
| time/                   |             |
|    total_timesteps      | 264500      |
| train/                  |             |
|    approx_kl            | 0.015197231 |
|    clip_fraction        | 0.0668      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.294      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.0001      |
|    loss                 | 3.79e+03    |
|    n_updates            | 173         |
|    policy_gradient_loss | 0.00304     |
|    value_loss           | 6.49e+03    |
-----------------------------------------
Eval num_timesteps=265000, episode_reward=-168.57 +/- 132.80
Episode length: 15.60 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=-194.83 +/- 131.48
Episode length: 16.46 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=-201.22 +/- 142.07
Episode length: 14.90 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 130      |
|    time_elapsed    | 548      |
|    total_timesteps | 266240   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=266500, episode_reward=-193.89 +/- 129.25
Episode length: 16.10 +/- 5.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -194        |
| time/                   |             |
|    total_timesteps      | 266500      |
| train/                  |             |
|    approx_kl            | 0.006327809 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.314      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.24e+03    |
|    n_updates            | 174         |
|    policy_gradient_loss | 0.00483     |
|    value_loss           | 7.18e+03    |
-----------------------------------------
Eval num_timesteps=267000, episode_reward=-138.59 +/- 149.55
Episode length: 17.72 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=-211.98 +/- 111.43
Episode length: 15.42 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=-171.57 +/- 144.72
Episode length: 17.22 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -156     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 131      |
|    time_elapsed    | 552      |
|    total_timesteps | 268288   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=268500, episode_reward=-151.45 +/- 118.47
Episode length: 16.34 +/- 4.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -151         |
| time/                   |              |
|    total_timesteps      | 268500       |
| train/                  |              |
|    approx_kl            | 0.0059939492 |
|    clip_fraction        | 0.0675       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.332       |
|    explained_variance   | 0.305        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.52e+03     |
|    n_updates            | 177          |
|    policy_gradient_loss | 0.0055       |
|    value_loss           | 7.08e+03     |
------------------------------------------
Eval num_timesteps=269000, episode_reward=-175.14 +/- 165.78
Episode length: 15.62 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=-146.70 +/- 155.69
Episode length: 15.92 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-152.17 +/- 160.35
Episode length: 15.98 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 132      |
|    time_elapsed    | 556      |
|    total_timesteps | 270336   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=270500, episode_reward=-135.37 +/- 141.99
Episode length: 16.74 +/- 4.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -135        |
| time/                   |             |
|    total_timesteps      | 270500      |
| train/                  |             |
|    approx_kl            | 0.006119569 |
|    clip_fraction        | 0.0691      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.309      |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.36e+03    |
|    n_updates            | 178         |
|    policy_gradient_loss | 0.00618     |
|    value_loss           | 6.65e+03    |
-----------------------------------------
Eval num_timesteps=271000, episode_reward=-165.65 +/- 126.36
Episode length: 16.12 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=-175.02 +/- 128.90
Episode length: 15.92 +/- 3.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=-153.53 +/- 141.97
Episode length: 15.60 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 133      |
|    time_elapsed    | 560      |
|    total_timesteps | 272384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=272500, episode_reward=-147.68 +/- 144.83
Episode length: 15.58 +/- 4.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -148         |
| time/                   |              |
|    total_timesteps      | 272500       |
| train/                  |              |
|    approx_kl            | 0.0048025707 |
|    clip_fraction        | 0.0709       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.29        |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.93e+03     |
|    n_updates            | 179          |
|    policy_gradient_loss | 0.000866     |
|    value_loss           | 7.22e+03     |
------------------------------------------
Eval num_timesteps=273000, episode_reward=-129.98 +/- 195.13
Episode length: 16.64 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=-155.43 +/- 156.61
Episode length: 15.94 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=-149.85 +/- 143.81
Episode length: 15.48 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 134      |
|    time_elapsed    | 564      |
|    total_timesteps | 274432   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=274500, episode_reward=-190.00 +/- 113.84
Episode length: 14.64 +/- 3.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.6        |
|    mean_reward          | -190        |
| time/                   |             |
|    total_timesteps      | 274500      |
| train/                  |             |
|    approx_kl            | 0.014064841 |
|    clip_fraction        | 0.054       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.271      |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.15e+03    |
|    n_updates            | 181         |
|    policy_gradient_loss | 0.00234     |
|    value_loss           | 6.45e+03    |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=-135.33 +/- 186.68
Episode length: 16.64 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=-158.84 +/- 136.86
Episode length: 14.74 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=-157.39 +/- 177.93
Episode length: 15.66 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 135      |
|    time_elapsed    | 568      |
|    total_timesteps | 276480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=276500, episode_reward=-150.32 +/- 142.72
Episode length: 16.50 +/- 4.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -150        |
| time/                   |             |
|    total_timesteps      | 276500      |
| train/                  |             |
|    approx_kl            | 0.006029427 |
|    clip_fraction        | 0.0738      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.193      |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.15e+03    |
|    n_updates            | 182         |
|    policy_gradient_loss | 0.00233     |
|    value_loss           | 7.79e+03    |
-----------------------------------------
Eval num_timesteps=277000, episode_reward=-144.07 +/- 161.45
Episode length: 15.96 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=-168.82 +/- 122.91
Episode length: 15.78 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=-158.62 +/- 156.05
Episode length: 15.44 +/- 4.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=-174.43 +/- 115.14
Episode length: 15.78 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 136      |
|    time_elapsed    | 573      |
|    total_timesteps | 278528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=279000, episode_reward=-164.70 +/- 116.69
Episode length: 15.78 +/- 4.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -165        |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.006939259 |
|    clip_fraction        | 0.0755      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.328      |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.98e+03    |
|    n_updates            | 183         |
|    policy_gradient_loss | 0.00727     |
|    value_loss           | 6.67e+03    |
-----------------------------------------
Eval num_timesteps=279500, episode_reward=-178.16 +/- 146.28
Episode length: 16.16 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-159.00 +/- 131.78
Episode length: 15.64 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=-171.57 +/- 131.62
Episode length: 15.98 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 137      |
|    time_elapsed    | 577      |
|    total_timesteps | 280576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=281000, episode_reward=-217.55 +/- 161.83
Episode length: 14.84 +/- 4.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | -218        |
| time/                   |             |
|    total_timesteps      | 281000      |
| train/                  |             |
|    approx_kl            | 0.004378805 |
|    clip_fraction        | 0.0553      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.317      |
|    explained_variance   | 0.327       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.69e+03    |
|    n_updates            | 184         |
|    policy_gradient_loss | 0.00822     |
|    value_loss           | 6.4e+03     |
-----------------------------------------
Eval num_timesteps=281500, episode_reward=-172.40 +/- 127.75
Episode length: 15.78 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=-180.86 +/- 126.78
Episode length: 15.72 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=-149.90 +/- 179.56
Episode length: 16.82 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 138      |
|    time_elapsed    | 581      |
|    total_timesteps | 282624   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=283000, episode_reward=-197.41 +/- 131.67
Episode length: 16.06 +/- 4.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -197         |
| time/                   |              |
|    total_timesteps      | 283000       |
| train/                  |              |
|    approx_kl            | 0.0146244895 |
|    clip_fraction        | 0.0473       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.339       |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.82e+03     |
|    n_updates            | 186          |
|    policy_gradient_loss | 0.00359      |
|    value_loss           | 7.21e+03     |
------------------------------------------
Eval num_timesteps=283500, episode_reward=-205.36 +/- 172.99
Episode length: 16.02 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=-176.30 +/- 118.28
Episode length: 15.68 +/- 4.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=-228.43 +/- 125.84
Episode length: 15.24 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -192     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 139      |
|    time_elapsed    | 585      |
|    total_timesteps | 284672   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=285000, episode_reward=-194.57 +/- 140.66
Episode length: 15.68 +/- 5.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | -195        |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.005690489 |
|    clip_fraction        | 0.0641      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.289      |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.78e+03    |
|    n_updates            | 188         |
|    policy_gradient_loss | 0.00468     |
|    value_loss           | 7.29e+03    |
-----------------------------------------
Eval num_timesteps=285500, episode_reward=-187.10 +/- 135.19
Episode length: 15.64 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=-186.04 +/- 128.83
Episode length: 17.14 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=-170.60 +/- 121.01
Episode length: 16.72 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -207     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 140      |
|    time_elapsed    | 590      |
|    total_timesteps | 286720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=287000, episode_reward=-215.85 +/- 114.06
Episode length: 15.64 +/- 4.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -216        |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 0.004397887 |
|    clip_fraction        | 0.0357      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.279      |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.97e+03    |
|    n_updates            | 189         |
|    policy_gradient_loss | 0.00271     |
|    value_loss           | 6.62e+03    |
-----------------------------------------
Eval num_timesteps=287500, episode_reward=-172.78 +/- 134.77
Episode length: 17.06 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=-202.84 +/- 118.86
Episode length: 16.58 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=-222.24 +/- 111.15
Episode length: 14.94 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -215     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 141      |
|    time_elapsed    | 594      |
|    total_timesteps | 288768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=289000, episode_reward=-197.02 +/- 112.44
Episode length: 16.14 +/- 4.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -197         |
| time/                   |              |
|    total_timesteps      | 289000       |
| train/                  |              |
|    approx_kl            | 0.0043837163 |
|    clip_fraction        | 0.0885       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.226       |
|    explained_variance   | 0.354        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.02e+03     |
|    n_updates            | 190          |
|    policy_gradient_loss | 0.00242      |
|    value_loss           | 6.68e+03     |
------------------------------------------
Eval num_timesteps=289500, episode_reward=-188.22 +/- 124.33
Episode length: 15.62 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-215.88 +/- 120.36
Episode length: 16.22 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=-158.48 +/- 136.72
Episode length: 16.64 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -198     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 142      |
|    time_elapsed    | 598      |
|    total_timesteps | 290816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=291000, episode_reward=-175.62 +/- 109.73
Episode length: 17.04 +/- 4.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -176         |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 0.0042629964 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.257       |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.28e+03     |
|    n_updates            | 191          |
|    policy_gradient_loss | 0.0028       |
|    value_loss           | 6.76e+03     |
------------------------------------------
Eval num_timesteps=291500, episode_reward=-180.91 +/- 126.47
Episode length: 15.88 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=-191.43 +/- 137.56
Episode length: 17.06 +/- 5.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=-183.70 +/- 150.20
Episode length: 16.90 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -211     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 143      |
|    time_elapsed    | 602      |
|    total_timesteps | 292864   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=293000, episode_reward=-200.73 +/- 124.15
Episode length: 15.82 +/- 4.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -201        |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.004310736 |
|    clip_fraction        | 0.0182      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.286      |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.93e+03    |
|    n_updates            | 192         |
|    policy_gradient_loss | 0.00102     |
|    value_loss           | 6.93e+03    |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=-176.15 +/- 121.78
Episode length: 17.10 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=-194.25 +/- 133.48
Episode length: 16.86 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=-201.35 +/- 133.57
Episode length: 16.20 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 144      |
|    time_elapsed    | 606      |
|    total_timesteps | 294912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=295000, episode_reward=-193.59 +/- 128.45
Episode length: 16.00 +/- 5.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -194         |
| time/                   |              |
|    total_timesteps      | 295000       |
| train/                  |              |
|    approx_kl            | 0.0053112176 |
|    clip_fraction        | 0.0616       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.29        |
|    explained_variance   | 0.346        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.38e+03     |
|    n_updates            | 193          |
|    policy_gradient_loss | 0.00463      |
|    value_loss           | 6.6e+03      |
------------------------------------------
Eval num_timesteps=295500, episode_reward=-186.78 +/- 147.45
Episode length: 16.58 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=-164.76 +/- 117.88
Episode length: 17.00 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=-189.25 +/- 148.68
Episode length: 17.16 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 145      |
|    time_elapsed    | 610      |
|    total_timesteps | 296960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=297000, episode_reward=-188.44 +/- 113.60
Episode length: 16.08 +/- 4.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -188        |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.004459074 |
|    clip_fraction        | 0.0564      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.272      |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.64e+03    |
|    n_updates            | 194         |
|    policy_gradient_loss | 0.00385     |
|    value_loss           | 6.86e+03    |
-----------------------------------------
Eval num_timesteps=297500, episode_reward=-238.11 +/- 116.05
Episode length: 14.96 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=-214.20 +/- 129.55
Episode length: 15.72 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=-191.49 +/- 156.45
Episode length: 17.08 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=-198.97 +/- 133.35
Episode length: 16.76 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 146      |
|    time_elapsed    | 615      |
|    total_timesteps | 299008   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=299500, episode_reward=-227.78 +/- 122.67
Episode length: 15.86 +/- 4.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -228         |
| time/                   |              |
|    total_timesteps      | 299500       |
| train/                  |              |
|    approx_kl            | 0.0063812956 |
|    clip_fraction        | 0.0592       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.276       |
|    explained_variance   | 0.347        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.46e+03     |
|    n_updates            | 196          |
|    policy_gradient_loss | 0.00384      |
|    value_loss           | 6.33e+03     |
------------------------------------------
Eval num_timesteps=300000, episode_reward=-133.31 +/- 138.34
Episode length: 18.48 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.5     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=-179.50 +/- 123.07
Episode length: 16.16 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=-205.94 +/- 96.80
Episode length: 15.28 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 147      |
|    time_elapsed    | 619      |
|    total_timesteps | 301056   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=301500, episode_reward=-194.40 +/- 117.19
Episode length: 16.40 +/- 4.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -194        |
| time/                   |             |
|    total_timesteps      | 301500      |
| train/                  |             |
|    approx_kl            | 0.004832516 |
|    clip_fraction        | 0.0642      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.293      |
|    explained_variance   | 0.304       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.59e+03    |
|    n_updates            | 197         |
|    policy_gradient_loss | 0.00889     |
|    value_loss           | 8.19e+03    |
-----------------------------------------
Eval num_timesteps=302000, episode_reward=-143.97 +/- 148.82
Episode length: 17.52 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=-180.86 +/- 126.57
Episode length: 16.50 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=-182.98 +/- 111.12
Episode length: 15.90 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 148      |
|    time_elapsed    | 624      |
|    total_timesteps | 303104   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=303500, episode_reward=-188.76 +/- 165.24
Episode length: 16.56 +/- 5.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -189         |
| time/                   |              |
|    total_timesteps      | 303500       |
| train/                  |              |
|    approx_kl            | 0.0057994467 |
|    clip_fraction        | 0.064        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.309       |
|    explained_variance   | 0.307        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.82e+03     |
|    n_updates            | 199          |
|    policy_gradient_loss | 0.00337      |
|    value_loss           | 6.58e+03     |
------------------------------------------
Eval num_timesteps=304000, episode_reward=-199.94 +/- 118.59
Episode length: 15.56 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=-187.92 +/- 135.19
Episode length: 16.12 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=-179.62 +/- 129.07
Episode length: 16.26 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 149      |
|    time_elapsed    | 628      |
|    total_timesteps | 305152   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=305500, episode_reward=-151.63 +/- 177.14
Episode length: 16.34 +/- 5.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.3       |
|    mean_reward          | -152       |
| time/                   |            |
|    total_timesteps      | 305500     |
| train/                  |            |
|    approx_kl            | 0.00595324 |
|    clip_fraction        | 0.0762     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.272     |
|    explained_variance   | 0.301      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.22e+03   |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.00881   |
|    value_loss           | 7.45e+03   |
----------------------------------------
Eval num_timesteps=306000, episode_reward=-131.68 +/- 167.10
Episode length: 15.62 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=-142.06 +/- 141.88
Episode length: 14.98 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=-153.02 +/- 126.22
Episode length: 15.56 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 150      |
|    time_elapsed    | 632      |
|    total_timesteps | 307200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=307500, episode_reward=-154.79 +/- 129.82
Episode length: 16.16 +/- 4.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -155        |
| time/                   |             |
|    total_timesteps      | 307500      |
| train/                  |             |
|    approx_kl            | 0.004171136 |
|    clip_fraction        | 0.0654      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.302      |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.2e+03     |
|    n_updates            | 201         |
|    policy_gradient_loss | 0.00441     |
|    value_loss           | 7.15e+03    |
-----------------------------------------
Eval num_timesteps=308000, episode_reward=-140.07 +/- 179.84
Episode length: 16.48 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=-146.62 +/- 117.58
Episode length: 16.00 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=-153.80 +/- 138.54
Episode length: 15.58 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 151      |
|    time_elapsed    | 636      |
|    total_timesteps | 309248   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=309500, episode_reward=-166.23 +/- 129.22
Episode length: 15.66 +/- 3.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -166         |
| time/                   |              |
|    total_timesteps      | 309500       |
| train/                  |              |
|    approx_kl            | 0.0053136447 |
|    clip_fraction        | 0.0619       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.28        |
|    explained_variance   | 0.314        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.44e+03     |
|    n_updates            | 203          |
|    policy_gradient_loss | 0.0013       |
|    value_loss           | 7.29e+03     |
------------------------------------------
Eval num_timesteps=310000, episode_reward=-143.50 +/- 142.14
Episode length: 16.14 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=-154.29 +/- 145.42
Episode length: 16.40 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=-169.86 +/- 146.37
Episode length: 15.76 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 152      |
|    time_elapsed    | 640      |
|    total_timesteps | 311296   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=311500, episode_reward=-148.83 +/- 156.02
Episode length: 16.02 +/- 4.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -149        |
| time/                   |             |
|    total_timesteps      | 311500      |
| train/                  |             |
|    approx_kl            | 0.006170656 |
|    clip_fraction        | 0.0946      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.302      |
|    explained_variance   | 0.317       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.88e+03    |
|    n_updates            | 204         |
|    policy_gradient_loss | -0.000861   |
|    value_loss           | 6.34e+03    |
-----------------------------------------
Eval num_timesteps=312000, episode_reward=-146.25 +/- 175.83
Episode length: 16.64 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=-165.13 +/- 131.38
Episode length: 15.70 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=-161.77 +/- 168.86
Episode length: 15.64 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 153      |
|    time_elapsed    | 644      |
|    total_timesteps | 313344   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=313500, episode_reward=-114.00 +/- 207.33
Episode length: 16.88 +/- 6.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.9         |
|    mean_reward          | -114         |
| time/                   |              |
|    total_timesteps      | 313500       |
| train/                  |              |
|    approx_kl            | 0.0033951001 |
|    clip_fraction        | 0.0521       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.304       |
|    explained_variance   | 0.336        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.81e+03     |
|    n_updates            | 205          |
|    policy_gradient_loss | 0.00411      |
|    value_loss           | 7.52e+03     |
------------------------------------------
Eval num_timesteps=314000, episode_reward=-163.70 +/- 115.71
Episode length: 15.58 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=-162.01 +/- 178.13
Episode length: 15.54 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=-130.46 +/- 160.12
Episode length: 15.62 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 154      |
|    time_elapsed    | 648      |
|    total_timesteps | 315392   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=315500, episode_reward=-141.89 +/- 117.68
Episode length: 15.46 +/- 3.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -142        |
| time/                   |             |
|    total_timesteps      | 315500      |
| train/                  |             |
|    approx_kl            | 0.009034413 |
|    clip_fraction        | 0.0678      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.257      |
|    explained_variance   | 0.3         |
|    learning_rate        | 0.0001      |
|    loss                 | 3.94e+03    |
|    n_updates            | 207         |
|    policy_gradient_loss | 0.00577     |
|    value_loss           | 7.02e+03    |
-----------------------------------------
Eval num_timesteps=316000, episode_reward=-123.22 +/- 150.99
Episode length: 16.58 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=-197.22 +/- 150.59
Episode length: 15.74 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=-165.08 +/- 157.95
Episode length: 15.78 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -181     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 155      |
|    time_elapsed    | 652      |
|    total_timesteps | 317440   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=317500, episode_reward=-188.72 +/- 136.33
Episode length: 14.94 +/- 4.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.9         |
|    mean_reward          | -189         |
| time/                   |              |
|    total_timesteps      | 317500       |
| train/                  |              |
|    approx_kl            | 0.0073749796 |
|    clip_fraction        | 0.049        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.217       |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.21e+03     |
|    n_updates            | 209          |
|    policy_gradient_loss | 0.00451      |
|    value_loss           | 7.49e+03     |
------------------------------------------
Eval num_timesteps=318000, episode_reward=-175.83 +/- 126.43
Episode length: 14.36 +/- 4.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=-148.65 +/- 158.60
Episode length: 16.18 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=-158.22 +/- 135.08
Episode length: 15.38 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 156      |
|    time_elapsed    | 656      |
|    total_timesteps | 319488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=319500, episode_reward=-152.37 +/- 144.03
Episode length: 16.02 +/- 4.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -152         |
| time/                   |              |
|    total_timesteps      | 319500       |
| train/                  |              |
|    approx_kl            | 0.0046549058 |
|    clip_fraction        | 0.0556       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.265       |
|    explained_variance   | 0.32         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.02e+03     |
|    n_updates            | 210          |
|    policy_gradient_loss | 0.00635      |
|    value_loss           | 6.82e+03     |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-144.61 +/- 173.18
Episode length: 16.66 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=-157.63 +/- 132.95
Episode length: 15.38 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=-195.49 +/- 165.83
Episode length: 15.42 +/- 5.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=-145.18 +/- 121.27
Episode length: 15.80 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 157      |
|    time_elapsed    | 661      |
|    total_timesteps | 321536   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=322000, episode_reward=-159.35 +/- 139.67
Episode length: 15.72 +/- 3.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -159         |
| time/                   |              |
|    total_timesteps      | 322000       |
| train/                  |              |
|    approx_kl            | 0.0064317742 |
|    clip_fraction        | 0.0304       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | 0.297        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.2e+03      |
|    n_updates            | 212          |
|    policy_gradient_loss | 0.00231      |
|    value_loss           | 7.68e+03     |
------------------------------------------
Eval num_timesteps=322500, episode_reward=-187.91 +/- 127.25
Episode length: 15.20 +/- 3.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=-166.51 +/- 168.99
Episode length: 15.88 +/- 4.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=-131.72 +/- 160.79
Episode length: 16.12 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 158      |
|    time_elapsed    | 665      |
|    total_timesteps | 323584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=324000, episode_reward=-157.75 +/- 149.92
Episode length: 15.44 +/- 4.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | -158        |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.008926636 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.3         |
|    learning_rate        | 0.0001      |
|    loss                 | 5.4e+03     |
|    n_updates            | 213         |
|    policy_gradient_loss | -0.000342   |
|    value_loss           | 8.56e+03    |
-----------------------------------------
Eval num_timesteps=324500, episode_reward=-139.78 +/- 177.81
Episode length: 16.06 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-171.99 +/- 152.14
Episode length: 14.98 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=-166.53 +/- 164.75
Episode length: 15.34 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -144     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 159      |
|    time_elapsed    | 669      |
|    total_timesteps | 325632   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=326000, episode_reward=-112.99 +/- 183.10
Episode length: 18.00 +/- 5.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | -113         |
| time/                   |              |
|    total_timesteps      | 326000       |
| train/                  |              |
|    approx_kl            | 0.0108481925 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0549      |
|    explained_variance   | 0.255        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.15e+03     |
|    n_updates            | 215          |
|    policy_gradient_loss | 0.000937     |
|    value_loss           | 7.3e+03      |
------------------------------------------
Eval num_timesteps=326500, episode_reward=-160.13 +/- 130.60
Episode length: 15.22 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=-164.61 +/- 147.34
Episode length: 14.90 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=-142.48 +/- 139.43
Episode length: 16.26 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 160      |
|    time_elapsed    | 673      |
|    total_timesteps | 327680   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=328000, episode_reward=-129.09 +/- 166.50
Episode length: 16.06 +/- 5.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -129        |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.004836639 |
|    clip_fraction        | 0.0222      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.395       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.86e+03    |
|    n_updates            | 216         |
|    policy_gradient_loss | 0.00555     |
|    value_loss           | 5.96e+03    |
-----------------------------------------
Eval num_timesteps=328500, episode_reward=-126.33 +/- 169.29
Episode length: 16.52 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=-149.56 +/- 143.67
Episode length: 15.86 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=-142.27 +/- 130.97
Episode length: 16.02 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 161      |
|    time_elapsed    | 678      |
|    total_timesteps | 329728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=330000, episode_reward=-116.56 +/- 161.80
Episode length: 17.30 +/- 5.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.3         |
|    mean_reward          | -117         |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0038909905 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.082       |
|    explained_variance   | 0.281        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.02e+03     |
|    n_updates            | 217          |
|    policy_gradient_loss | 0.00136      |
|    value_loss           | 7.56e+03     |
------------------------------------------
Eval num_timesteps=330500, episode_reward=-128.32 +/- 145.73
Episode length: 15.98 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=-132.07 +/- 180.47
Episode length: 16.74 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=-147.28 +/- 154.38
Episode length: 16.08 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 162      |
|    time_elapsed    | 682      |
|    total_timesteps | 331776   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=332000, episode_reward=-148.08 +/- 193.73
Episode length: 16.08 +/- 5.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -148        |
| time/                   |             |
|    total_timesteps      | 332000      |
| train/                  |             |
|    approx_kl            | 0.002199808 |
|    clip_fraction        | 0.0162      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.143      |
|    explained_variance   | 0.269       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.11e+03    |
|    n_updates            | 218         |
|    policy_gradient_loss | 0.00483     |
|    value_loss           | 6.58e+03    |
-----------------------------------------
Eval num_timesteps=332500, episode_reward=-159.53 +/- 158.17
Episode length: 15.18 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=-137.59 +/- 145.50
Episode length: 15.48 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=-134.88 +/- 180.77
Episode length: 16.40 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 163      |
|    time_elapsed    | 686      |
|    total_timesteps | 333824   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=334000, episode_reward=-160.88 +/- 136.85
Episode length: 16.82 +/- 4.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.8       |
|    mean_reward          | -161       |
| time/                   |            |
|    total_timesteps      | 334000     |
| train/                  |            |
|    approx_kl            | 0.01782998 |
|    clip_fraction        | 0.023      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.169     |
|    explained_variance   | 0.294      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.7e+03    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.00126   |
|    value_loss           | 6.87e+03   |
----------------------------------------
Eval num_timesteps=334500, episode_reward=-113.76 +/- 124.75
Episode length: 16.56 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -114     |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=-138.28 +/- 119.36
Episode length: 16.82 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=-134.74 +/- 128.83
Episode length: 16.38 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 164      |
|    time_elapsed    | 690      |
|    total_timesteps | 335872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=336000, episode_reward=-148.24 +/- 141.33
Episode length: 16.38 +/- 5.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -148        |
| time/                   |             |
|    total_timesteps      | 336000      |
| train/                  |             |
|    approx_kl            | 0.002802187 |
|    clip_fraction        | 0.0148      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.238       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.96e+03    |
|    n_updates            | 221         |
|    policy_gradient_loss | 0.00112     |
|    value_loss           | 7.53e+03    |
-----------------------------------------
Eval num_timesteps=336500, episode_reward=-156.12 +/- 153.53
Episode length: 15.98 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=-140.51 +/- 143.00
Episode length: 15.64 +/- 4.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=-163.26 +/- 153.18
Episode length: 15.60 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -156     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 165      |
|    time_elapsed    | 694      |
|    total_timesteps | 337920   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=338000, episode_reward=-138.91 +/- 120.62
Episode length: 16.04 +/- 4.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -139        |
| time/                   |             |
|    total_timesteps      | 338000      |
| train/                  |             |
|    approx_kl            | 0.002428452 |
|    clip_fraction        | 0.00563     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0337     |
|    explained_variance   | 0.209       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.13e+03    |
|    n_updates            | 225         |
|    policy_gradient_loss | 0.0015      |
|    value_loss           | 7.47e+03    |
-----------------------------------------
Eval num_timesteps=338500, episode_reward=-174.13 +/- 134.44
Episode length: 14.84 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=-141.29 +/- 196.50
Episode length: 16.94 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=-182.60 +/- 150.63
Episode length: 15.24 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -146     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 166      |
|    time_elapsed    | 698      |
|    total_timesteps | 339968   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=340000, episode_reward=-146.81 +/- 188.11
Episode length: 16.22 +/- 5.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -147         |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0012838093 |
|    clip_fraction        | 0.000638     |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00544     |
|    explained_variance   | 0.238        |
|    learning_rate        | 0.0001       |
|    loss                 | 4e+03        |
|    n_updates            | 227          |
|    policy_gradient_loss | -0.000365    |
|    value_loss           | 6.97e+03     |
------------------------------------------
Eval num_timesteps=340500, episode_reward=-151.87 +/- 155.14
Episode length: 16.66 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=-149.94 +/- 152.21
Episode length: 16.36 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=-180.74 +/- 159.92
Episode length: 15.66 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=-139.38 +/- 139.24
Episode length: 15.90 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 167      |
|    time_elapsed    | 703      |
|    total_timesteps | 342016   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=342500, episode_reward=-203.02 +/- 130.07
Episode length: 14.36 +/- 3.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | -203         |
| time/                   |              |
|    total_timesteps      | 342500       |
| train/                  |              |
|    approx_kl            | 0.0073555144 |
|    clip_fraction        | 0.000446     |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00419     |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.97e+03     |
|    n_updates            | 229          |
|    policy_gradient_loss | -0.000114    |
|    value_loss           | 6.58e+03     |
------------------------------------------
Eval num_timesteps=343000, episode_reward=-183.12 +/- 159.90
Episode length: 15.06 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=-156.33 +/- 144.60
Episode length: 15.56 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=-142.79 +/- 160.48
Episode length: 16.86 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 168      |
|    time_elapsed    | 707      |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=-175.56 +/- 145.78
Episode length: 15.68 +/- 4.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.7          |
|    mean_reward          | -176          |
| time/                   |               |
|    total_timesteps      | 344500        |
| train/                  |               |
|    approx_kl            | 1.3370882e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00149      |
|    explained_variance   | 0.225         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.19e+03      |
|    n_updates            | 239           |
|    policy_gradient_loss | -0.00013      |
|    value_loss           | 7.21e+03      |
-------------------------------------------
Eval num_timesteps=345000, episode_reward=-138.93 +/- 158.93
Episode length: 16.76 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=-131.91 +/- 158.91
Episode length: 16.70 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=-179.40 +/- 128.85
Episode length: 14.56 +/- 3.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 169      |
|    time_elapsed    | 712      |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=-178.87 +/- 142.98
Episode length: 15.40 +/- 4.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 15.4      |
|    mean_reward          | -179      |
| time/                   |           |
|    total_timesteps      | 346500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.000212 |
|    explained_variance   | 0.236     |
|    learning_rate        | 0.0001    |
|    loss                 | 3.77e+03  |
|    n_updates            | 249       |
|    policy_gradient_loss | -1.49e-06 |
|    value_loss           | 7.93e+03  |
---------------------------------------
Eval num_timesteps=347000, episode_reward=-142.26 +/- 134.41
Episode length: 16.38 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=-127.72 +/- 177.86
Episode length: 16.56 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=-112.99 +/- 177.61
Episode length: 16.62 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 170      |
|    time_elapsed    | 717      |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=-135.10 +/- 144.71
Episode length: 16.20 +/- 4.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -135          |
| time/                   |               |
|    total_timesteps      | 348500        |
| train/                  |               |
|    approx_kl            | 5.3842086e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000281     |
|    explained_variance   | 0.282         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.28e+03      |
|    n_updates            | 259           |
|    policy_gradient_loss | 1.15e-06      |
|    value_loss           | 6.91e+03      |
-------------------------------------------
Eval num_timesteps=349000, episode_reward=-186.91 +/- 124.46
Episode length: 14.70 +/- 4.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=-167.12 +/- 162.08
Episode length: 16.22 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-188.08 +/- 147.11
Episode length: 14.96 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -146     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 171      |
|    time_elapsed    | 721      |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=-114.30 +/- 120.75
Episode length: 17.06 +/- 4.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.1          |
|    mean_reward          | -114          |
| time/                   |               |
|    total_timesteps      | 350500        |
| train/                  |               |
|    approx_kl            | 1.9033905e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000769     |
|    explained_variance   | 0.27          |
|    learning_rate        | 0.0001        |
|    loss                 | 5.02e+03      |
|    n_updates            | 269           |
|    policy_gradient_loss | -2.64e-06     |
|    value_loss           | 7.99e+03      |
-------------------------------------------
Eval num_timesteps=351000, episode_reward=-171.90 +/- 135.87
Episode length: 15.18 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=-141.63 +/- 138.47
Episode length: 16.40 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=-147.90 +/- 162.84
Episode length: 15.62 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -144     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 172      |
|    time_elapsed    | 726      |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=-159.89 +/- 135.43
Episode length: 16.40 +/- 4.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.4      |
|    mean_reward          | -160      |
| time/                   |           |
|    total_timesteps      | 352500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.000706 |
|    explained_variance   | 0.262     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.57e+03  |
|    n_updates            | 279       |
|    policy_gradient_loss | 1.65e-06  |
|    value_loss           | 7.41e+03  |
---------------------------------------
Eval num_timesteps=353000, episode_reward=-109.37 +/- 127.18
Episode length: 17.02 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=-128.59 +/- 160.80
Episode length: 16.80 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=-134.20 +/- 174.56
Episode length: 16.04 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 173      |
|    time_elapsed    | 731      |
|    total_timesteps | 354304   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.06
Eval num_timesteps=354500, episode_reward=-178.85 +/- 162.13
Episode length: 15.64 +/- 5.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -179         |
| time/                   |              |
|    total_timesteps      | 354500       |
| train/                  |              |
|    approx_kl            | 0.0019966979 |
|    clip_fraction        | 0.000504     |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00175     |
|    explained_variance   | 0.311        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.58e+03     |
|    n_updates            | 282          |
|    policy_gradient_loss | 1.4e-05      |
|    value_loss           | 6.68e+03     |
------------------------------------------
Eval num_timesteps=355000, episode_reward=-147.61 +/- 149.29
Episode length: 16.02 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=-173.58 +/- 145.98
Episode length: 14.96 +/- 3.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=-140.24 +/- 154.59
Episode length: 16.52 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 174      |
|    time_elapsed    | 735      |
|    total_timesteps | 356352   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=356500, episode_reward=-146.45 +/- 123.74
Episode length: 16.00 +/- 4.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -146         |
| time/                   |              |
|    total_timesteps      | 356500       |
| train/                  |              |
|    approx_kl            | 0.0036615722 |
|    clip_fraction        | 0.000651     |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00716     |
|    explained_variance   | 0.317        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.95e+03     |
|    n_updates            | 284          |
|    policy_gradient_loss | -4.84e-05    |
|    value_loss           | 6.8e+03      |
------------------------------------------
Eval num_timesteps=357000, episode_reward=-165.42 +/- 142.87
Episode length: 15.56 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=-199.56 +/- 137.39
Episode length: 14.80 +/- 3.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=-123.30 +/- 155.28
Episode length: 16.62 +/- 5.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 175      |
|    time_elapsed    | 739      |
|    total_timesteps | 358400   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=358500, episode_reward=-153.66 +/- 114.68
Episode length: 15.56 +/- 3.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -154        |
| time/                   |             |
|    total_timesteps      | 358500      |
| train/                  |             |
|    approx_kl            | 0.009374181 |
|    clip_fraction        | 0.00217     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0172     |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.28e+03    |
|    n_updates            | 286         |
|    policy_gradient_loss | 0.000373    |
|    value_loss           | 6.54e+03    |
-----------------------------------------
Eval num_timesteps=359000, episode_reward=-156.08 +/- 152.89
Episode length: 15.66 +/- 4.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=-162.95 +/- 164.06
Episode length: 16.10 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-150.12 +/- 178.29
Episode length: 15.52 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 176      |
|    time_elapsed    | 743      |
|    total_timesteps | 360448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=360500, episode_reward=-183.54 +/- 134.46
Episode length: 15.32 +/- 3.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | -184        |
| time/                   |             |
|    total_timesteps      | 360500      |
| train/                  |             |
|    approx_kl            | 0.002620508 |
|    clip_fraction        | 0.00313     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0439     |
|    explained_variance   | 0.293       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.69e+03    |
|    n_updates            | 287         |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 6.67e+03    |
-----------------------------------------
Eval num_timesteps=361000, episode_reward=-102.71 +/- 198.59
Episode length: 17.70 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -103     |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=-162.71 +/- 163.42
Episode length: 16.12 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=-163.40 +/- 132.14
Episode length: 14.54 +/- 3.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 177      |
|    time_elapsed    | 747      |
|    total_timesteps | 362496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=362500, episode_reward=-163.86 +/- 151.07
Episode length: 15.88 +/- 4.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | -164        |
| time/                   |             |
|    total_timesteps      | 362500      |
| train/                  |             |
|    approx_kl            | 0.006841001 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.15e+03    |
|    n_updates            | 288         |
|    policy_gradient_loss | 0.00346     |
|    value_loss           | 6.73e+03    |
-----------------------------------------
Eval num_timesteps=363000, episode_reward=-148.26 +/- 161.72
Episode length: 16.24 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=-165.43 +/- 181.86
Episode length: 16.44 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=-144.20 +/- 150.67
Episode length: 15.88 +/- 4.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=-146.50 +/- 142.52
Episode length: 15.50 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 178      |
|    time_elapsed    | 752      |
|    total_timesteps | 364544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=365000, episode_reward=-131.46 +/- 192.33
Episode length: 16.60 +/- 5.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -131        |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.007727866 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.245      |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.09e+03    |
|    n_updates            | 289         |
|    policy_gradient_loss | 0.0126      |
|    value_loss           | 6.61e+03    |
-----------------------------------------
Eval num_timesteps=365500, episode_reward=-156.57 +/- 167.17
Episode length: 16.38 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=-165.86 +/- 129.36
Episode length: 15.52 +/- 4.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=-133.14 +/- 126.51
Episode length: 15.74 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 179      |
|    time_elapsed    | 756      |
|    total_timesteps | 366592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=367000, episode_reward=-182.83 +/- 126.71
Episode length: 15.58 +/- 4.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -183         |
| time/                   |              |
|    total_timesteps      | 367000       |
| train/                  |              |
|    approx_kl            | 0.0056892256 |
|    clip_fraction        | 0.0881       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.256       |
|    explained_variance   | 0.301        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.36e+03     |
|    n_updates            | 290          |
|    policy_gradient_loss | 0.00691      |
|    value_loss           | 7.18e+03     |
------------------------------------------
Eval num_timesteps=367500, episode_reward=-155.68 +/- 127.60
Episode length: 16.16 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=-174.37 +/- 142.54
Episode length: 17.08 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=-123.29 +/- 142.50
Episode length: 17.70 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 180      |
|    time_elapsed    | 760      |
|    total_timesteps | 368640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=369000, episode_reward=-193.23 +/- 125.28
Episode length: 16.00 +/- 4.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -193         |
| time/                   |              |
|    total_timesteps      | 369000       |
| train/                  |              |
|    approx_kl            | 0.0046243616 |
|    clip_fraction        | 0.0595       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.241       |
|    explained_variance   | 0.387        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.61e+03     |
|    n_updates            | 291          |
|    policy_gradient_loss | 0.002        |
|    value_loss           | 5.45e+03     |
------------------------------------------
Eval num_timesteps=369500, episode_reward=-182.30 +/- 144.26
Episode length: 17.04 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-195.97 +/- 127.61
Episode length: 15.44 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=-181.51 +/- 154.92
Episode length: 16.02 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 181      |
|    time_elapsed    | 764      |
|    total_timesteps | 370688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=371000, episode_reward=-175.90 +/- 152.73
Episode length: 16.54 +/- 5.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -176        |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.010563342 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.356      |
|    explained_variance   | 0.365       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.03e+03    |
|    n_updates            | 292         |
|    policy_gradient_loss | 0.0125      |
|    value_loss           | 8.46e+03    |
-----------------------------------------
Eval num_timesteps=371500, episode_reward=-213.43 +/- 114.59
Episode length: 14.24 +/- 3.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=-126.36 +/- 190.90
Episode length: 18.22 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=-167.39 +/- 142.18
Episode length: 16.42 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -183     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 182      |
|    time_elapsed    | 769      |
|    total_timesteps | 372736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=373000, episode_reward=-160.52 +/- 141.07
Episode length: 16.52 +/- 5.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -161         |
| time/                   |              |
|    total_timesteps      | 373000       |
| train/                  |              |
|    approx_kl            | 0.0057479045 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.347       |
|    explained_variance   | 0.349        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.48e+03     |
|    n_updates            | 293          |
|    policy_gradient_loss | 0.0033       |
|    value_loss           | 6.62e+03     |
------------------------------------------
Eval num_timesteps=373500, episode_reward=-169.04 +/- 157.16
Episode length: 16.46 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=-170.89 +/- 122.50
Episode length: 15.56 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=-204.03 +/- 108.74
Episode length: 15.14 +/- 4.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 183      |
|    time_elapsed    | 773      |
|    total_timesteps | 374784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=375000, episode_reward=-124.82 +/- 158.25
Episode length: 17.88 +/- 4.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.9        |
|    mean_reward          | -125        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.007261174 |
|    clip_fraction        | 0.0962      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.331      |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.23e+03    |
|    n_updates            | 294         |
|    policy_gradient_loss | 0.00564     |
|    value_loss           | 5.93e+03    |
-----------------------------------------
Eval num_timesteps=375500, episode_reward=-197.90 +/- 143.41
Episode length: 15.08 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=-195.44 +/- 146.54
Episode length: 16.64 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=-189.72 +/- 144.10
Episode length: 15.68 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 184      |
|    time_elapsed    | 777      |
|    total_timesteps | 376832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=377000, episode_reward=-161.86 +/- 173.57
Episode length: 16.72 +/- 5.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -162        |
| time/                   |             |
|    total_timesteps      | 377000      |
| train/                  |             |
|    approx_kl            | 0.006811737 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.341      |
|    explained_variance   | 0.443       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.24e+03    |
|    n_updates            | 295         |
|    policy_gradient_loss | -0.00268    |
|    value_loss           | 5.81e+03    |
-----------------------------------------
Eval num_timesteps=377500, episode_reward=-154.53 +/- 121.38
Episode length: 16.68 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=-178.97 +/- 151.50
Episode length: 16.24 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=-161.41 +/- 163.14
Episode length: 16.80 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 185      |
|    time_elapsed    | 781      |
|    total_timesteps | 378880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=379000, episode_reward=-142.40 +/- 135.09
Episode length: 15.80 +/- 5.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -142         |
| time/                   |              |
|    total_timesteps      | 379000       |
| train/                  |              |
|    approx_kl            | 0.0050066877 |
|    clip_fraction        | 0.0694       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.309       |
|    explained_variance   | 0.359        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.88e+03     |
|    n_updates            | 296          |
|    policy_gradient_loss | 0.00821      |
|    value_loss           | 6.47e+03     |
------------------------------------------
Eval num_timesteps=379500, episode_reward=-140.69 +/- 174.33
Episode length: 16.70 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-183.84 +/- 166.12
Episode length: 16.00 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=-108.40 +/- 126.44
Episode length: 17.82 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 186      |
|    time_elapsed    | 785      |
|    total_timesteps | 380928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=381000, episode_reward=-147.17 +/- 117.44
Episode length: 16.22 +/- 4.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -147         |
| time/                   |              |
|    total_timesteps      | 381000       |
| train/                  |              |
|    approx_kl            | 0.0040593054 |
|    clip_fraction        | 0.0594       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.317       |
|    explained_variance   | 0.375        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.17e+03     |
|    n_updates            | 297          |
|    policy_gradient_loss | -0.00256     |
|    value_loss           | 6.46e+03     |
------------------------------------------
Eval num_timesteps=381500, episode_reward=-174.16 +/- 111.93
Episode length: 16.14 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=-137.12 +/- 161.77
Episode length: 17.78 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=-155.35 +/- 163.81
Episode length: 16.94 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 187      |
|    time_elapsed    | 789      |
|    total_timesteps | 382976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=383000, episode_reward=-195.73 +/- 132.94
Episode length: 15.70 +/- 4.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | -196        |
| time/                   |             |
|    total_timesteps      | 383000      |
| train/                  |             |
|    approx_kl            | 0.006096819 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.284      |
|    explained_variance   | 0.367       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.4e+03     |
|    n_updates            | 298         |
|    policy_gradient_loss | 0.00821     |
|    value_loss           | 7.57e+03    |
-----------------------------------------
Eval num_timesteps=383500, episode_reward=-153.88 +/- 149.15
Episode length: 16.68 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=-149.01 +/- 147.32
Episode length: 17.26 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=-162.42 +/- 123.18
Episode length: 16.14 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=-170.98 +/- 122.52
Episode length: 16.02 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 188      |
|    time_elapsed    | 794      |
|    total_timesteps | 385024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=385500, episode_reward=-145.04 +/- 171.22
Episode length: 16.42 +/- 5.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -145        |
| time/                   |             |
|    total_timesteps      | 385500      |
| train/                  |             |
|    approx_kl            | 0.007761522 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.22       |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.52e+03    |
|    n_updates            | 299         |
|    policy_gradient_loss | 0.00284     |
|    value_loss           | 6.19e+03    |
-----------------------------------------
Eval num_timesteps=386000, episode_reward=-156.20 +/- 157.37
Episode length: 15.56 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=-150.95 +/- 167.79
Episode length: 15.74 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=-124.46 +/- 196.59
Episode length: 16.66 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 189      |
|    time_elapsed    | 798      |
|    total_timesteps | 387072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=387500, episode_reward=-105.71 +/- 191.55
Episode length: 17.04 +/- 5.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | -106        |
| time/                   |             |
|    total_timesteps      | 387500      |
| train/                  |             |
|    approx_kl            | 0.002963753 |
|    clip_fraction        | 0.0387      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.271      |
|    explained_variance   | 0.292       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.37e+03    |
|    n_updates            | 300         |
|    policy_gradient_loss | 0.00647     |
|    value_loss           | 7.04e+03    |
-----------------------------------------
Eval num_timesteps=388000, episode_reward=-123.16 +/- 164.50
Episode length: 16.34 +/- 5.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=-111.75 +/- 143.12
Episode length: 16.66 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -112     |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=-166.82 +/- 158.18
Episode length: 15.86 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 190      |
|    time_elapsed    | 802      |
|    total_timesteps | 389120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=389500, episode_reward=-167.01 +/- 143.12
Episode length: 15.62 +/- 4.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -167        |
| time/                   |             |
|    total_timesteps      | 389500      |
| train/                  |             |
|    approx_kl            | 0.005693122 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.271      |
|    explained_variance   | 0.276       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.19e+03    |
|    n_updates            | 301         |
|    policy_gradient_loss | 0.00134     |
|    value_loss           | 6.1e+03     |
-----------------------------------------
Eval num_timesteps=390000, episode_reward=-121.05 +/- 131.78
Episode length: 15.96 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=-161.03 +/- 147.37
Episode length: 15.56 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=-185.57 +/- 149.82
Episode length: 15.18 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 191      |
|    time_elapsed    | 806      |
|    total_timesteps | 391168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=391500, episode_reward=-134.75 +/- 154.21
Episode length: 16.36 +/- 5.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -135        |
| time/                   |             |
|    total_timesteps      | 391500      |
| train/                  |             |
|    approx_kl            | 0.004764293 |
|    clip_fraction        | 0.0612      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.266      |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.49e+03    |
|    n_updates            | 302         |
|    policy_gradient_loss | 0.00319     |
|    value_loss           | 7.16e+03    |
-----------------------------------------
Eval num_timesteps=392000, episode_reward=-142.39 +/- 134.82
Episode length: 15.90 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=-153.28 +/- 126.37
Episode length: 15.72 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=-115.46 +/- 172.63
Episode length: 17.30 +/- 5.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 192      |
|    time_elapsed    | 811      |
|    total_timesteps | 393216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=393500, episode_reward=-144.95 +/- 182.22
Episode length: 16.28 +/- 5.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -145         |
| time/                   |              |
|    total_timesteps      | 393500       |
| train/                  |              |
|    approx_kl            | 0.0058713006 |
|    clip_fraction        | 0.105        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.345       |
|    explained_variance   | 0.294        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.58e+03     |
|    n_updates            | 303          |
|    policy_gradient_loss | 0.00441      |
|    value_loss           | 6.98e+03     |
------------------------------------------
Eval num_timesteps=394000, episode_reward=-163.73 +/- 129.48
Episode length: 15.16 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=-146.41 +/- 158.84
Episode length: 14.94 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=-154.38 +/- 167.54
Episode length: 15.32 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 193      |
|    time_elapsed    | 815      |
|    total_timesteps | 395264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=395500, episode_reward=-164.60 +/- 136.53
Episode length: 15.82 +/- 4.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -165        |
| time/                   |             |
|    total_timesteps      | 395500      |
| train/                  |             |
|    approx_kl            | 0.004878209 |
|    clip_fraction        | 0.0442      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.267      |
|    explained_variance   | 0.304       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.2e+03     |
|    n_updates            | 304         |
|    policy_gradient_loss | 0.00446     |
|    value_loss           | 7e+03       |
-----------------------------------------
Eval num_timesteps=396000, episode_reward=-164.38 +/- 131.80
Episode length: 15.16 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=-167.17 +/- 167.90
Episode length: 15.70 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=-141.26 +/- 142.17
Episode length: 15.62 +/- 4.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 194      |
|    time_elapsed    | 819      |
|    total_timesteps | 397312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=397500, episode_reward=-166.30 +/- 156.64
Episode length: 15.94 +/- 5.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -166         |
| time/                   |              |
|    total_timesteps      | 397500       |
| train/                  |              |
|    approx_kl            | 0.0039611678 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.281       |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.21e+03     |
|    n_updates            | 305          |
|    policy_gradient_loss | 0.00156      |
|    value_loss           | 8.3e+03      |
------------------------------------------
Eval num_timesteps=398000, episode_reward=-181.13 +/- 134.19
Episode length: 15.18 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=-179.76 +/- 127.46
Episode length: 15.52 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=-227.56 +/- 148.72
Episode length: 14.24 +/- 3.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 195      |
|    time_elapsed    | 823      |
|    total_timesteps | 399360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=399500, episode_reward=-136.09 +/- 160.46
Episode length: 16.60 +/- 5.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -136        |
| time/                   |             |
|    total_timesteps      | 399500      |
| train/                  |             |
|    approx_kl            | 0.003030642 |
|    clip_fraction        | 0.0275      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.221      |
|    explained_variance   | 0.263       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.56e+03    |
|    n_updates            | 306         |
|    policy_gradient_loss | 0.00198     |
|    value_loss           | 7.14e+03    |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=-171.65 +/- 125.27
Episode length: 15.16 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=-144.96 +/- 144.11
Episode length: 16.20 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=-200.65 +/- 122.00
Episode length: 14.08 +/- 3.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.1     |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 196      |
|    time_elapsed    | 827      |
|    total_timesteps | 401408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=401500, episode_reward=-135.22 +/- 143.97
Episode length: 16.92 +/- 4.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | -135        |
| time/                   |             |
|    total_timesteps      | 401500      |
| train/                  |             |
|    approx_kl            | 0.004304455 |
|    clip_fraction        | 0.0257      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.191      |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.63e+03    |
|    n_updates            | 307         |
|    policy_gradient_loss | 0.00206     |
|    value_loss           | 7.58e+03    |
-----------------------------------------
Eval num_timesteps=402000, episode_reward=-119.91 +/- 165.13
Episode length: 16.68 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=-150.30 +/- 162.78
Episode length: 15.82 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=-162.86 +/- 143.19
Episode length: 15.38 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 197      |
|    time_elapsed    | 831      |
|    total_timesteps | 403456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=403500, episode_reward=-150.70 +/- 135.84
Episode length: 15.68 +/- 4.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -151         |
| time/                   |              |
|    total_timesteps      | 403500       |
| train/                  |              |
|    approx_kl            | 0.0043285987 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.23        |
|    explained_variance   | 0.365        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.56e+03     |
|    n_updates            | 308          |
|    policy_gradient_loss | 0.0026       |
|    value_loss           | 6.55e+03     |
------------------------------------------
Eval num_timesteps=404000, episode_reward=-169.89 +/- 146.65
Episode length: 14.74 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=-139.53 +/- 145.36
Episode length: 15.74 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=-121.48 +/- 156.69
Episode length: 15.52 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=-153.27 +/- 167.99
Episode length: 16.68 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 198      |
|    time_elapsed    | 836      |
|    total_timesteps | 405504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=406000, episode_reward=-126.66 +/- 187.67
Episode length: 17.08 +/- 6.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.1         |
|    mean_reward          | -127         |
| time/                   |              |
|    total_timesteps      | 406000       |
| train/                  |              |
|    approx_kl            | 0.0045992504 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.148       |
|    explained_variance   | 0.356        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.06e+03     |
|    n_updates            | 309          |
|    policy_gradient_loss | 0.00135      |
|    value_loss           | 6.9e+03      |
------------------------------------------
Eval num_timesteps=406500, episode_reward=-138.20 +/- 171.90
Episode length: 16.84 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=-164.12 +/- 160.05
Episode length: 15.30 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=-190.51 +/- 140.08
Episode length: 15.00 +/- 4.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 199      |
|    time_elapsed    | 840      |
|    total_timesteps | 407552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=408000, episode_reward=-196.56 +/- 105.54
Episode length: 14.64 +/- 3.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | -197         |
| time/                   |              |
|    total_timesteps      | 408000       |
| train/                  |              |
|    approx_kl            | 0.0028649347 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.101       |
|    explained_variance   | 0.374        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.14e+03     |
|    n_updates            | 310          |
|    policy_gradient_loss | 0.00155      |
|    value_loss           | 6.27e+03     |
------------------------------------------
Eval num_timesteps=408500, episode_reward=-158.84 +/- 156.51
Episode length: 15.46 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=-185.34 +/- 135.87
Episode length: 15.24 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=-133.97 +/- 132.75
Episode length: 16.40 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | -189     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 200      |
|    time_elapsed    | 844      |
|    total_timesteps | 409600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=410000, episode_reward=-157.20 +/- 125.72
Episode length: 15.88 +/- 4.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -157         |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0044056117 |
|    clip_fraction        | 0.00608      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0345      |
|    explained_variance   | 0.388        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.2e+03      |
|    n_updates            | 311          |
|    policy_gradient_loss | 0.001        |
|    value_loss           | 6.02e+03     |
------------------------------------------
Eval num_timesteps=410500, episode_reward=-148.96 +/- 126.70
Episode length: 15.70 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=-113.21 +/- 183.22
Episode length: 17.02 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=-196.69 +/- 140.66
Episode length: 14.54 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 201      |
|    time_elapsed    | 848      |
|    total_timesteps | 411648   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=412000, episode_reward=-187.06 +/- 116.83
Episode length: 14.78 +/- 4.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -187         |
| time/                   |              |
|    total_timesteps      | 412000       |
| train/                  |              |
|    approx_kl            | 0.0010983348 |
|    clip_fraction        | 0.00128      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00746     |
|    explained_variance   | 0.372        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.41e+03     |
|    n_updates            | 313          |
|    policy_gradient_loss | 0.000151     |
|    value_loss           | 6.76e+03     |
------------------------------------------
Eval num_timesteps=412500, episode_reward=-163.75 +/- 162.18
Episode length: 15.96 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=-173.72 +/- 133.07
Episode length: 15.64 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=-195.59 +/- 127.04
Episode length: 14.74 +/- 4.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 202      |
|    time_elapsed    | 852      |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=414000, episode_reward=-208.96 +/- 131.52
Episode length: 14.26 +/- 3.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.3          |
|    mean_reward          | -209          |
| time/                   |               |
|    total_timesteps      | 414000        |
| train/                  |               |
|    approx_kl            | 2.4447218e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00172      |
|    explained_variance   | 0.356         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.28e+03      |
|    n_updates            | 323           |
|    policy_gradient_loss | -6.41e-07     |
|    value_loss           | 6.78e+03      |
-------------------------------------------
Eval num_timesteps=414500, episode_reward=-173.89 +/- 159.75
Episode length: 15.34 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=-159.41 +/- 157.38
Episode length: 15.24 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=-190.85 +/- 150.28
Episode length: 15.20 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 203      |
|    time_elapsed    | 857      |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=-145.04 +/- 157.81
Episode length: 15.92 +/- 4.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -145          |
| time/                   |               |
|    total_timesteps      | 416000        |
| train/                  |               |
|    approx_kl            | 6.4610504e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00108      |
|    explained_variance   | 0.292         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.94e+03      |
|    n_updates            | 333           |
|    policy_gradient_loss | 2.31e-06      |
|    value_loss           | 6.89e+03      |
-------------------------------------------
Eval num_timesteps=416500, episode_reward=-160.92 +/- 161.79
Episode length: 15.94 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=-163.16 +/- 131.77
Episode length: 15.86 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=-164.30 +/- 145.75
Episode length: 16.10 +/- 4.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 204      |
|    time_elapsed    | 861      |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=-162.02 +/- 126.95
Episode length: 15.10 +/- 3.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.1          |
|    mean_reward          | -162          |
| time/                   |               |
|    total_timesteps      | 418000        |
| train/                  |               |
|    approx_kl            | 3.5593985e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00251      |
|    explained_variance   | 0.247         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.69e+03      |
|    n_updates            | 343           |
|    policy_gradient_loss | 8.07e-06      |
|    value_loss           | 8.87e+03      |
-------------------------------------------
Eval num_timesteps=418500, episode_reward=-140.64 +/- 166.41
Episode length: 16.66 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=-149.36 +/- 133.92
Episode length: 15.70 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=-128.77 +/- 154.58
Episode length: 16.00 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 205      |
|    time_elapsed    | 866      |
|    total_timesteps | 419840   |
---------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
Eval num_timesteps=420000, episode_reward=-166.76 +/- 123.42
Episode length: 15.42 +/- 3.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -167         |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0072144666 |
|    clip_fraction        | 0.00656      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0723      |
|    explained_variance   | 0.309        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.52e+03     |
|    n_updates            | 353          |
|    policy_gradient_loss | -0.00053     |
|    value_loss           | 6.61e+03     |
------------------------------------------
Eval num_timesteps=420500, episode_reward=-147.06 +/- 178.82
Episode length: 15.98 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=-147.13 +/- 152.85
Episode length: 16.28 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=-161.72 +/- 152.64
Episode length: 15.72 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -222     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 206      |
|    time_elapsed    | 871      |
|    total_timesteps | 421888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=422000, episode_reward=-179.78 +/- 126.20
Episode length: 14.90 +/- 3.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | -180        |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.010456976 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.667      |
|    explained_variance   | 0.255       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.73e+03    |
|    n_updates            | 354         |
|    policy_gradient_loss | -0.00182    |
|    value_loss           | 1.01e+04    |
-----------------------------------------
Eval num_timesteps=422500, episode_reward=-149.77 +/- 138.97
Episode length: 15.28 +/- 4.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=-144.69 +/- 140.97
Episode length: 15.78 +/- 4.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=-153.58 +/- 124.59
Episode length: 15.04 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -233     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 207      |
|    time_elapsed    | 875      |
|    total_timesteps | 423936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=424000, episode_reward=-137.11 +/- 167.60
Episode length: 16.48 +/- 5.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -137        |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.008987343 |
|    clip_fraction        | 0.0755      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.0001      |
|    loss                 | 4.26e+03    |
|    n_updates            | 355         |
|    policy_gradient_loss | -0.00149    |
|    value_loss           | 9.92e+03    |
-----------------------------------------
Eval num_timesteps=424500, episode_reward=-199.55 +/- 114.68
Episode length: 15.04 +/- 3.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=-149.41 +/- 169.53
Episode length: 16.08 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=-129.04 +/- 149.67
Episode length: 16.56 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 208      |
|    time_elapsed    | 879      |
|    total_timesteps | 425984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=426000, episode_reward=-142.35 +/- 160.71
Episode length: 15.48 +/- 5.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -142         |
| time/                   |              |
|    total_timesteps      | 426000       |
| train/                  |              |
|    approx_kl            | 0.0052324054 |
|    clip_fraction        | 0.0599       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.418       |
|    explained_variance   | 0.268        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.8e+03      |
|    n_updates            | 356          |
|    policy_gradient_loss | -0.00192     |
|    value_loss           | 8.13e+03     |
------------------------------------------
Eval num_timesteps=426500, episode_reward=-134.84 +/- 154.58
Episode length: 15.66 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=-156.73 +/- 136.22
Episode length: 15.86 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=-137.00 +/- 151.28
Episode length: 16.00 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=-125.70 +/- 152.41
Episode length: 16.72 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 209      |
|    time_elapsed    | 883      |
|    total_timesteps | 428032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=428500, episode_reward=-170.04 +/- 133.65
Episode length: 15.78 +/- 4.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -170        |
| time/                   |             |
|    total_timesteps      | 428500      |
| train/                  |             |
|    approx_kl            | 0.009037523 |
|    clip_fraction        | 0.0558      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | 0.301       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.58e+03    |
|    n_updates            | 357         |
|    policy_gradient_loss | -0.00936    |
|    value_loss           | 7.08e+03    |
-----------------------------------------
Eval num_timesteps=429000, episode_reward=-147.90 +/- 135.90
Episode length: 16.26 +/- 4.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=-138.84 +/- 151.64
Episode length: 16.48 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-151.71 +/- 147.07
Episode length: 15.24 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 210      |
|    time_elapsed    | 887      |
|    total_timesteps | 430080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=430500, episode_reward=-138.76 +/- 140.38
Episode length: 16.06 +/- 4.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -139         |
| time/                   |              |
|    total_timesteps      | 430500       |
| train/                  |              |
|    approx_kl            | 0.0051181647 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.154       |
|    explained_variance   | 0.261        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.53e+03     |
|    n_updates            | 358          |
|    policy_gradient_loss | 0.000819     |
|    value_loss           | 7.05e+03     |
------------------------------------------
Eval num_timesteps=431000, episode_reward=-148.93 +/- 124.38
Episode length: 15.42 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=-128.58 +/- 200.82
Episode length: 16.18 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=-128.32 +/- 150.40
Episode length: 16.86 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 211      |
|    time_elapsed    | 891      |
|    total_timesteps | 432128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=432500, episode_reward=-128.76 +/- 170.10
Episode length: 15.26 +/- 6.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | -129        |
| time/                   |             |
|    total_timesteps      | 432500      |
| train/                  |             |
|    approx_kl            | 0.004042661 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.286       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.9e+03     |
|    n_updates            | 359         |
|    policy_gradient_loss | -0.00239    |
|    value_loss           | 7.09e+03    |
-----------------------------------------
Eval num_timesteps=433000, episode_reward=-119.58 +/- 151.24
Episode length: 17.52 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=-144.57 +/- 164.59
Episode length: 15.64 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=-117.14 +/- 147.94
Episode length: 15.74 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 212      |
|    time_elapsed    | 895      |
|    total_timesteps | 434176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=434500, episode_reward=-115.41 +/- 164.40
Episode length: 17.78 +/- 5.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.8         |
|    mean_reward          | -115         |
| time/                   |              |
|    total_timesteps      | 434500       |
| train/                  |              |
|    approx_kl            | 0.0032828688 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.157       |
|    explained_variance   | 0.308        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.38e+03     |
|    n_updates            | 360          |
|    policy_gradient_loss | 0.00506      |
|    value_loss           | 6.37e+03     |
------------------------------------------
Eval num_timesteps=435000, episode_reward=-131.54 +/- 167.68
Episode length: 15.66 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=-166.25 +/- 114.93
Episode length: 15.38 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=-161.97 +/- 125.97
Episode length: 15.78 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 213      |
|    time_elapsed    | 900      |
|    total_timesteps | 436224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=436500, episode_reward=-134.56 +/- 154.88
Episode length: 16.30 +/- 4.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -135         |
| time/                   |              |
|    total_timesteps      | 436500       |
| train/                  |              |
|    approx_kl            | 0.0031946872 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.244       |
|    explained_variance   | 0.262        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.6e+03      |
|    n_updates            | 361          |
|    policy_gradient_loss | 0.000176     |
|    value_loss           | 8.46e+03     |
------------------------------------------
Eval num_timesteps=437000, episode_reward=-159.69 +/- 140.44
Episode length: 16.02 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=-164.64 +/- 153.31
Episode length: 15.14 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=-172.68 +/- 149.81
Episode length: 15.30 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 214      |
|    time_elapsed    | 904      |
|    total_timesteps | 438272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=438500, episode_reward=-111.80 +/- 142.21
Episode length: 16.44 +/- 5.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -112         |
| time/                   |              |
|    total_timesteps      | 438500       |
| train/                  |              |
|    approx_kl            | 0.0032856846 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.171       |
|    explained_variance   | 0.312        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.57e+03     |
|    n_updates            | 362          |
|    policy_gradient_loss | 0.0022       |
|    value_loss           | 7.33e+03     |
------------------------------------------
Eval num_timesteps=439000, episode_reward=-115.86 +/- 127.82
Episode length: 17.30 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=-171.01 +/- 162.68
Episode length: 16.10 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-142.32 +/- 166.63
Episode length: 16.34 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -174     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 215      |
|    time_elapsed    | 908      |
|    total_timesteps | 440320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=440500, episode_reward=-162.10 +/- 144.03
Episode length: 15.30 +/- 4.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | -162        |
| time/                   |             |
|    total_timesteps      | 440500      |
| train/                  |             |
|    approx_kl            | 0.002470217 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.253      |
|    explained_variance   | 0.316       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.54e+03    |
|    n_updates            | 363         |
|    policy_gradient_loss | 0.00231     |
|    value_loss           | 6.56e+03    |
-----------------------------------------
Eval num_timesteps=441000, episode_reward=-152.01 +/- 142.10
Episode length: 15.54 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=-138.17 +/- 175.28
Episode length: 15.82 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=-116.67 +/- 222.21
Episode length: 16.76 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 216      |
|    time_elapsed    | 912      |
|    total_timesteps | 442368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=442500, episode_reward=-139.16 +/- 163.86
Episode length: 16.88 +/- 5.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.9         |
|    mean_reward          | -139         |
| time/                   |              |
|    total_timesteps      | 442500       |
| train/                  |              |
|    approx_kl            | 0.0049183783 |
|    clip_fraction        | 0.0256       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.168       |
|    explained_variance   | 0.25         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.73e+03     |
|    n_updates            | 364          |
|    policy_gradient_loss | 0.00296      |
|    value_loss           | 7.7e+03      |
------------------------------------------
Eval num_timesteps=443000, episode_reward=-126.98 +/- 132.50
Episode length: 16.08 +/- 5.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=-186.07 +/- 134.34
Episode length: 15.18 +/- 3.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=-146.45 +/- 130.78
Episode length: 15.14 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 217      |
|    time_elapsed    | 916      |
|    total_timesteps | 444416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=444500, episode_reward=-121.29 +/- 141.15
Episode length: 17.16 +/- 4.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | -121         |
| time/                   |              |
|    total_timesteps      | 444500       |
| train/                  |              |
|    approx_kl            | 0.0038424863 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0695      |
|    explained_variance   | 0.239        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.05e+03     |
|    n_updates            | 365          |
|    policy_gradient_loss | -0.00157     |
|    value_loss           | 7.73e+03     |
------------------------------------------
Eval num_timesteps=445000, episode_reward=-171.64 +/- 155.41
Episode length: 15.00 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=-156.67 +/- 142.97
Episode length: 15.34 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=-129.66 +/- 153.28
Episode length: 16.18 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 218      |
|    time_elapsed    | 920      |
|    total_timesteps | 446464   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=446500, episode_reward=-178.15 +/- 117.39
Episode length: 14.98 +/- 3.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | -178        |
| time/                   |             |
|    total_timesteps      | 446500      |
| train/                  |             |
|    approx_kl            | 0.005028856 |
|    clip_fraction        | 0.00223     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0415     |
|    explained_variance   | 0.301       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.24e+03    |
|    n_updates            | 367         |
|    policy_gradient_loss | -8.75e-05   |
|    value_loss           | 7.14e+03    |
-----------------------------------------
Eval num_timesteps=447000, episode_reward=-172.71 +/- 141.73
Episode length: 16.36 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=-179.89 +/- 159.90
Episode length: 15.80 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=-173.00 +/- 157.60
Episode length: 15.54 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=-165.88 +/- 171.68
Episode length: 15.74 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 219      |
|    time_elapsed    | 925      |
|    total_timesteps | 448512   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=449000, episode_reward=-147.32 +/- 161.25
Episode length: 16.34 +/- 5.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -147         |
| time/                   |              |
|    total_timesteps      | 449000       |
| train/                  |              |
|    approx_kl            | 0.0018128271 |
|    clip_fraction        | 0.00223      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00808     |
|    explained_variance   | 0.318        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.94e+03     |
|    n_updates            | 369          |
|    policy_gradient_loss | 0.000871     |
|    value_loss           | 6.17e+03     |
------------------------------------------
Eval num_timesteps=449500, episode_reward=-143.38 +/- 167.03
Episode length: 15.66 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-164.69 +/- 160.23
Episode length: 15.02 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=-139.36 +/- 175.29
Episode length: 16.76 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -144     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 220      |
|    time_elapsed    | 929      |
|    total_timesteps | 450560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=451000, episode_reward=-112.70 +/- 192.03
Episode length: 17.30 +/- 5.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | -113        |
| time/                   |             |
|    total_timesteps      | 451000      |
| train/                  |             |
|    approx_kl            | 0.002376827 |
|    clip_fraction        | 0.0024      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0143     |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.79e+03    |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.000267   |
|    value_loss           | 6.26e+03    |
-----------------------------------------
Eval num_timesteps=451500, episode_reward=-175.01 +/- 139.83
Episode length: 14.76 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=-183.90 +/- 157.49
Episode length: 15.60 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=-145.17 +/- 160.15
Episode length: 15.56 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -144     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 221      |
|    time_elapsed    | 933      |
|    total_timesteps | 452608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=453000, episode_reward=-112.82 +/- 216.90
Episode length: 17.96 +/- 6.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | -113         |
| time/                   |              |
|    total_timesteps      | 453000       |
| train/                  |              |
|    approx_kl            | 0.0011784685 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00191     |
|    explained_variance   | 0.307        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.6e+03      |
|    n_updates            | 371          |
|    policy_gradient_loss | 0.000101     |
|    value_loss           | 7.22e+03     |
------------------------------------------
Eval num_timesteps=453500, episode_reward=-163.01 +/- 166.53
Episode length: 15.88 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=-156.48 +/- 169.76
Episode length: 16.34 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=-143.67 +/- 142.92
Episode length: 15.46 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 222      |
|    time_elapsed    | 937      |
|    total_timesteps | 454656   |
---------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
Eval num_timesteps=455000, episode_reward=-155.14 +/- 158.33
Episode length: 14.98 +/- 4.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | -155         |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0030113996 |
|    clip_fraction        | 0.00133      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0195      |
|    explained_variance   | 0.335        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.03e+03     |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.000156    |
|    value_loss           | 7.15e+03     |
------------------------------------------
Eval num_timesteps=455500, episode_reward=-133.85 +/- 166.63
Episode length: 16.96 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=-173.22 +/- 146.10
Episode length: 15.30 +/- 4.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=-138.43 +/- 168.89
Episode length: 16.30 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -210     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 223      |
|    time_elapsed    | 942      |
|    total_timesteps | 456704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=457000, episode_reward=-121.20 +/- 183.19
Episode length: 17.08 +/- 6.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.1        |
|    mean_reward          | -121        |
| time/                   |             |
|    total_timesteps      | 457000      |
| train/                  |             |
|    approx_kl            | 0.006661031 |
|    clip_fraction        | 0.0495      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.507      |
|    explained_variance   | 0.318       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.14e+03    |
|    n_updates            | 381         |
|    policy_gradient_loss | 0.0106      |
|    value_loss           | 7.39e+03    |
-----------------------------------------
Eval num_timesteps=457500, episode_reward=-152.96 +/- 150.94
Episode length: 16.86 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=-167.08 +/- 159.36
Episode length: 15.28 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=-157.30 +/- 127.37
Episode length: 15.38 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 224      |
|    time_elapsed    | 946      |
|    total_timesteps | 458752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=459000, episode_reward=-130.08 +/- 152.40
Episode length: 16.28 +/- 4.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -130        |
| time/                   |             |
|    total_timesteps      | 459000      |
| train/                  |             |
|    approx_kl            | 0.011048904 |
|    clip_fraction        | 0.0906      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.441      |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.68e+03    |
|    n_updates            | 382         |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 9.11e+03    |
-----------------------------------------
Eval num_timesteps=459500, episode_reward=-129.43 +/- 127.99
Episode length: 16.12 +/- 4.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-122.04 +/- 166.17
Episode length: 17.16 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=-153.53 +/- 118.51
Episode length: 15.82 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 225      |
|    time_elapsed    | 950      |
|    total_timesteps | 460800   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=461000, episode_reward=-155.36 +/- 144.52
Episode length: 15.60 +/- 4.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -155         |
| time/                   |              |
|    total_timesteps      | 461000       |
| train/                  |              |
|    approx_kl            | 0.0071603563 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.278       |
|    explained_variance   | 0.295        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.14e+03     |
|    n_updates            | 383          |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 6.2e+03      |
------------------------------------------
Eval num_timesteps=461500, episode_reward=-150.49 +/- 155.50
Episode length: 15.52 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=-117.78 +/- 138.74
Episode length: 17.00 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=-151.04 +/- 140.35
Episode length: 16.72 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 226      |
|    time_elapsed    | 954      |
|    total_timesteps | 462848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=463000, episode_reward=-136.69 +/- 151.87
Episode length: 16.28 +/- 4.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -137         |
| time/                   |              |
|    total_timesteps      | 463000       |
| train/                  |              |
|    approx_kl            | 0.0035050039 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.244       |
|    explained_variance   | 0.34         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.47e+03     |
|    n_updates            | 384          |
|    policy_gradient_loss | 0.00352      |
|    value_loss           | 6e+03        |
------------------------------------------
Eval num_timesteps=463500, episode_reward=-173.46 +/- 168.03
Episode length: 15.84 +/- 4.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=-164.46 +/- 134.82
Episode length: 15.74 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=-128.59 +/- 154.37
Episode length: 16.34 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 227      |
|    time_elapsed    | 958      |
|    total_timesteps | 464896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=465000, episode_reward=-160.49 +/- 127.53
Episode length: 15.54 +/- 4.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -160        |
| time/                   |             |
|    total_timesteps      | 465000      |
| train/                  |             |
|    approx_kl            | 0.007771244 |
|    clip_fraction        | 0.0664      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.432      |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.77e+03    |
|    n_updates            | 385         |
|    policy_gradient_loss | 0.0189      |
|    value_loss           | 7.3e+03     |
-----------------------------------------
Eval num_timesteps=465500, episode_reward=-132.22 +/- 170.36
Episode length: 16.30 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=-143.06 +/- 152.84
Episode length: 16.16 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=-150.47 +/- 153.44
Episode length: 15.80 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 228      |
|    time_elapsed    | 962      |
|    total_timesteps | 466944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=467000, episode_reward=-129.00 +/- 189.91
Episode length: 16.96 +/- 5.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -129         |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0055342917 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.284       |
|    explained_variance   | 0.324        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.5e+03      |
|    n_updates            | 386          |
|    policy_gradient_loss | 0.00398      |
|    value_loss           | 6.38e+03     |
------------------------------------------
Eval num_timesteps=467500, episode_reward=-155.01 +/- 130.49
Episode length: 15.90 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=-174.21 +/- 130.88
Episode length: 15.62 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=-175.59 +/- 162.75
Episode length: 15.16 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 229      |
|    time_elapsed    | 967      |
|    total_timesteps | 468992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=469000, episode_reward=-136.09 +/- 153.69
Episode length: 16.12 +/- 5.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -136         |
| time/                   |              |
|    total_timesteps      | 469000       |
| train/                  |              |
|    approx_kl            | 0.0040719965 |
|    clip_fraction        | 0.0385       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.302       |
|    explained_variance   | 0.306        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.8e+03      |
|    n_updates            | 387          |
|    policy_gradient_loss | 0.00228      |
|    value_loss           | 6.88e+03     |
------------------------------------------
Eval num_timesteps=469500, episode_reward=-132.07 +/- 159.28
Episode length: 16.12 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-137.75 +/- 154.15
Episode length: 16.74 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=-114.34 +/- 130.53
Episode length: 16.54 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -114     |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=-162.83 +/- 145.98
Episode length: 15.02 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 230      |
|    time_elapsed    | 971      |
|    total_timesteps | 471040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=471500, episode_reward=-148.86 +/- 155.96
Episode length: 16.16 +/- 4.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -149        |
| time/                   |             |
|    total_timesteps      | 471500      |
| train/                  |             |
|    approx_kl            | 0.008090523 |
|    clip_fraction        | 0.0603      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.23       |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.87e+03    |
|    n_updates            | 388         |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 6.58e+03    |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=-187.40 +/- 144.01
Episode length: 15.38 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=-170.07 +/- 145.19
Episode length: 15.58 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=-145.20 +/- 183.76
Episode length: 16.48 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 231      |
|    time_elapsed    | 975      |
|    total_timesteps | 473088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=473500, episode_reward=-153.96 +/- 155.65
Episode length: 16.58 +/- 4.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -154        |
| time/                   |             |
|    total_timesteps      | 473500      |
| train/                  |             |
|    approx_kl            | 0.008497579 |
|    clip_fraction        | 0.0977      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.449      |
|    explained_variance   | 0.385       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.72e+03    |
|    n_updates            | 389         |
|    policy_gradient_loss | 0.0187      |
|    value_loss           | 5.65e+03    |
-----------------------------------------
Eval num_timesteps=474000, episode_reward=-141.76 +/- 170.86
Episode length: 16.18 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=-149.33 +/- 124.31
Episode length: 15.66 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=-206.47 +/- 128.58
Episode length: 14.22 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 232      |
|    time_elapsed    | 979      |
|    total_timesteps | 475136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=475500, episode_reward=-141.77 +/- 127.60
Episode length: 16.32 +/- 4.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -142        |
| time/                   |             |
|    total_timesteps      | 475500      |
| train/                  |             |
|    approx_kl            | 0.007890862 |
|    clip_fraction        | 0.0871      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.505      |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.43e+03    |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.0197      |
|    value_loss           | 6.77e+03    |
-----------------------------------------
Eval num_timesteps=476000, episode_reward=-179.36 +/- 151.24
Episode length: 14.78 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=-93.00 +/- 133.41
Episode length: 17.34 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -93      |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=-124.69 +/- 167.68
Episode length: 16.34 +/- 4.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 233      |
|    time_elapsed    | 983      |
|    total_timesteps | 477184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=477500, episode_reward=-181.78 +/- 132.44
Episode length: 14.78 +/- 4.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | -182        |
| time/                   |             |
|    total_timesteps      | 477500      |
| train/                  |             |
|    approx_kl            | 0.007816514 |
|    clip_fraction        | 0.0932      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.431      |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.83e+03    |
|    n_updates            | 391         |
|    policy_gradient_loss | 0.00182     |
|    value_loss           | 6.7e+03     |
-----------------------------------------
Eval num_timesteps=478000, episode_reward=-112.14 +/- 106.14
Episode length: 17.20 +/- 4.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -112     |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=-153.79 +/- 148.75
Episode length: 16.12 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=-142.24 +/- 183.45
Episode length: 16.22 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -191     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 234      |
|    time_elapsed    | 988      |
|    total_timesteps | 479232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=479500, episode_reward=-207.44 +/- 126.96
Episode length: 15.70 +/- 4.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | -207        |
| time/                   |             |
|    total_timesteps      | 479500      |
| train/                  |             |
|    approx_kl            | 0.005462169 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.446      |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.4e+03     |
|    n_updates            | 392         |
|    policy_gradient_loss | 0.00795     |
|    value_loss           | 7.12e+03    |
-----------------------------------------
Eval num_timesteps=480000, episode_reward=-189.41 +/- 121.87
Episode length: 15.40 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=-210.77 +/- 125.43
Episode length: 16.42 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=-173.82 +/- 139.49
Episode length: 17.06 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 235      |
|    time_elapsed    | 992      |
|    total_timesteps | 481280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=481500, episode_reward=-181.90 +/- 131.91
Episode length: 16.62 +/- 5.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -182        |
| time/                   |             |
|    total_timesteps      | 481500      |
| train/                  |             |
|    approx_kl            | 0.006296082 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.508      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.0001      |
|    loss                 | 3.35e+03    |
|    n_updates            | 393         |
|    policy_gradient_loss | 0.00467     |
|    value_loss           | 6.99e+03    |
-----------------------------------------
Eval num_timesteps=482000, episode_reward=-194.59 +/- 132.80
Episode length: 17.18 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=-159.26 +/- 130.32
Episode length: 17.64 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=-201.26 +/- 113.81
Episode length: 14.54 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -220     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 236      |
|    time_elapsed    | 996      |
|    total_timesteps | 483328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=483500, episode_reward=-194.82 +/- 113.28
Episode length: 15.58 +/- 4.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -195        |
| time/                   |             |
|    total_timesteps      | 483500      |
| train/                  |             |
|    approx_kl            | 0.008749745 |
|    clip_fraction        | 0.0948      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.464      |
|    explained_variance   | 0.314       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.01e+03    |
|    n_updates            | 394         |
|    policy_gradient_loss | -0.000175   |
|    value_loss           | 6.87e+03    |
-----------------------------------------
Eval num_timesteps=484000, episode_reward=-172.51 +/- 143.84
Episode length: 16.80 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=-203.21 +/- 131.81
Episode length: 15.48 +/- 4.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=-211.82 +/- 108.27
Episode length: 16.26 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 237      |
|    time_elapsed    | 1000     |
|    total_timesteps | 485376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=485500, episode_reward=-160.78 +/- 156.98
Episode length: 16.56 +/- 5.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -161         |
| time/                   |              |
|    total_timesteps      | 485500       |
| train/                  |              |
|    approx_kl            | 0.0044649285 |
|    clip_fraction        | 0.0599       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.468       |
|    explained_variance   | 0.336        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.33e+03     |
|    n_updates            | 395          |
|    policy_gradient_loss | -0.00942     |
|    value_loss           | 7.11e+03     |
------------------------------------------
Eval num_timesteps=486000, episode_reward=-135.74 +/- 127.92
Episode length: 16.30 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=-165.04 +/- 139.28
Episode length: 15.98 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=-189.36 +/- 143.74
Episode length: 15.04 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 238      |
|    time_elapsed    | 1004     |
|    total_timesteps | 487424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=487500, episode_reward=-172.42 +/- 172.15
Episode length: 15.12 +/- 5.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.1         |
|    mean_reward          | -172         |
| time/                   |              |
|    total_timesteps      | 487500       |
| train/                  |              |
|    approx_kl            | 0.0066861827 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.421       |
|    explained_variance   | 0.326        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.84e+03     |
|    n_updates            | 396          |
|    policy_gradient_loss | 0.00142      |
|    value_loss           | 7.22e+03     |
------------------------------------------
Eval num_timesteps=488000, episode_reward=-120.06 +/- 167.09
Episode length: 17.38 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=-165.24 +/- 166.08
Episode length: 15.10 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=-173.08 +/- 145.15
Episode length: 15.56 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 239      |
|    time_elapsed    | 1008     |
|    total_timesteps | 489472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=489500, episode_reward=-179.01 +/- 159.45
Episode length: 15.56 +/- 4.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -179        |
| time/                   |             |
|    total_timesteps      | 489500      |
| train/                  |             |
|    approx_kl            | 0.009533646 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.306      |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.0001      |
|    loss                 | 2.68e+03    |
|    n_updates            | 397         |
|    policy_gradient_loss | 0.0137      |
|    value_loss           | 6.75e+03    |
-----------------------------------------
Eval num_timesteps=490000, episode_reward=-139.62 +/- 132.42
Episode length: 15.10 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=-143.94 +/- 118.63
Episode length: 16.04 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=-142.31 +/- 172.15
Episode length: 16.60 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=-163.52 +/- 157.78
Episode length: 16.80 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 240      |
|    time_elapsed    | 1013     |
|    total_timesteps | 491520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=492000, episode_reward=-178.86 +/- 148.02
Episode length: 15.82 +/- 4.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -179        |
| time/                   |             |
|    total_timesteps      | 492000      |
| train/                  |             |
|    approx_kl            | 0.003930385 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.226      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.0001      |
|    loss                 | 3.65e+03    |
|    n_updates            | 398         |
|    policy_gradient_loss | 0.000336    |
|    value_loss           | 6.58e+03    |
-----------------------------------------
Eval num_timesteps=492500, episode_reward=-157.52 +/- 112.42
Episode length: 15.24 +/- 3.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=-167.70 +/- 154.56
Episode length: 16.00 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=-167.96 +/- 129.11
Episode length: 15.26 +/- 4.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 241      |
|    time_elapsed    | 1017     |
|    total_timesteps | 493568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=494000, episode_reward=-166.88 +/- 138.75
Episode length: 14.96 +/- 4.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | -167         |
| time/                   |              |
|    total_timesteps      | 494000       |
| train/                  |              |
|    approx_kl            | 0.0044749235 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.177       |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.26e+03     |
|    n_updates            | 399          |
|    policy_gradient_loss | 0.00239      |
|    value_loss           | 6.52e+03     |
------------------------------------------
Eval num_timesteps=494500, episode_reward=-155.76 +/- 128.24
Episode length: 15.62 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=-128.52 +/- 184.92
Episode length: 16.46 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=-144.86 +/- 200.39
Episode length: 15.94 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 242      |
|    time_elapsed    | 1021     |
|    total_timesteps | 495616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=496000, episode_reward=-157.13 +/- 152.03
Episode length: 15.18 +/- 5.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -157         |
| time/                   |              |
|    total_timesteps      | 496000       |
| train/                  |              |
|    approx_kl            | 0.0020397806 |
|    clip_fraction        | 0.00781      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.1         |
|    explained_variance   | 0.323        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.11e+03     |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 6.19e+03     |
------------------------------------------
Eval num_timesteps=496500, episode_reward=-124.26 +/- 142.43
Episode length: 15.74 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=-153.81 +/- 124.39
Episode length: 15.36 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=-152.76 +/- 150.37
Episode length: 15.36 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 243      |
|    time_elapsed    | 1025     |
|    total_timesteps | 497664   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=498000, episode_reward=-148.95 +/- 167.91
Episode length: 16.44 +/- 5.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -149        |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.003100128 |
|    clip_fraction        | 0.0202      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.319       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.4e+03     |
|    n_updates            | 402         |
|    policy_gradient_loss | 0.00643     |
|    value_loss           | 6.33e+03    |
-----------------------------------------
Eval num_timesteps=498500, episode_reward=-144.49 +/- 152.03
Episode length: 15.36 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=-137.26 +/- 133.57
Episode length: 16.16 +/- 4.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=-143.19 +/- 121.87
Episode length: 15.74 +/- 4.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 244      |
|    time_elapsed    | 1029     |
|    total_timesteps | 499712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=500000, episode_reward=-121.05 +/- 163.59
Episode length: 16.68 +/- 5.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -121         |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0021344253 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.176       |
|    explained_variance   | 0.287        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.26e+03     |
|    n_updates            | 403          |
|    policy_gradient_loss | 0.00257      |
|    value_loss           | 7.58e+03     |
------------------------------------------
Eval num_timesteps=500500, episode_reward=-188.41 +/- 135.49
Episode length: 15.56 +/- 4.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=-165.23 +/- 165.79
Episode length: 16.28 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=-150.59 +/- 159.40
Episode length: 15.96 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 245      |
|    time_elapsed    | 1033     |
|    total_timesteps | 501760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=502000, episode_reward=-181.47 +/- 132.56
Episode length: 14.74 +/- 3.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.7        |
|    mean_reward          | -181        |
| time/                   |             |
|    total_timesteps      | 502000      |
| train/                  |             |
|    approx_kl            | 0.005278893 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0932     |
|    explained_variance   | 0.321       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.62e+03    |
|    n_updates            | 404         |
|    policy_gradient_loss | 0.00169     |
|    value_loss           | 5.5e+03     |
-----------------------------------------
Eval num_timesteps=502500, episode_reward=-133.87 +/- 136.92
Episode length: 16.50 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 502500   |
---------------------------------
Eval num_timesteps=503000, episode_reward=-154.95 +/- 137.60
Episode length: 15.98 +/- 4.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 503000   |
---------------------------------
Eval num_timesteps=503500, episode_reward=-178.83 +/- 134.55
Episode length: 14.98 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 503500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 246      |
|    time_elapsed    | 1037     |
|    total_timesteps | 503808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.15
Eval num_timesteps=504000, episode_reward=-157.85 +/- 157.30
Episode length: 15.78 +/- 4.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -158         |
| time/                   |              |
|    total_timesteps      | 504000       |
| train/                  |              |
|    approx_kl            | 0.0108175455 |
|    clip_fraction        | 0.0322       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0809      |
|    explained_variance   | 0.311        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.58e+03     |
|    n_updates            | 405          |
|    policy_gradient_loss | 0.011        |
|    value_loss           | 7.26e+03     |
------------------------------------------
Eval num_timesteps=504500, episode_reward=-163.56 +/- 145.94
Episode length: 15.08 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 504500   |
---------------------------------
Eval num_timesteps=505000, episode_reward=-125.43 +/- 124.14
Episode length: 15.66 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
Eval num_timesteps=505500, episode_reward=-177.04 +/- 135.47
Episode length: 16.40 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 505500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 247      |
|    time_elapsed    | 1041     |
|    total_timesteps | 505856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=506000, episode_reward=-179.52 +/- 143.18
Episode length: 16.08 +/- 5.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -180         |
| time/                   |              |
|    total_timesteps      | 506000       |
| train/                  |              |
|    approx_kl            | 0.0047071245 |
|    clip_fraction        | 0.0595       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.183       |
|    explained_variance   | 0.353        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.52e+03     |
|    n_updates            | 406          |
|    policy_gradient_loss | 0.0055       |
|    value_loss           | 6.74e+03     |
------------------------------------------
Eval num_timesteps=506500, episode_reward=-127.96 +/- 154.97
Episode length: 17.74 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 506500   |
---------------------------------
Eval num_timesteps=507000, episode_reward=-166.83 +/- 134.76
Episode length: 15.82 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 507000   |
---------------------------------
Eval num_timesteps=507500, episode_reward=-144.49 +/- 127.41
Episode length: 17.28 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 507500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 248      |
|    time_elapsed    | 1045     |
|    total_timesteps | 507904   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=508000, episode_reward=-126.33 +/- 143.72
Episode length: 17.56 +/- 5.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.6        |
|    mean_reward          | -126        |
| time/                   |             |
|    total_timesteps      | 508000      |
| train/                  |             |
|    approx_kl            | 0.004883012 |
|    clip_fraction        | 0.0724      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.256      |
|    explained_variance   | 0.407       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.89e+03    |
|    n_updates            | 407         |
|    policy_gradient_loss | 0.00872     |
|    value_loss           | 5.66e+03    |
-----------------------------------------
Eval num_timesteps=508500, episode_reward=-177.54 +/- 134.44
Episode length: 15.76 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 508500   |
---------------------------------
Eval num_timesteps=509000, episode_reward=-143.80 +/- 155.34
Episode length: 17.84 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 509000   |
---------------------------------
Eval num_timesteps=509500, episode_reward=-170.36 +/- 154.40
Episode length: 16.92 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 509500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 249      |
|    time_elapsed    | 1050     |
|    total_timesteps | 509952   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=510000, episode_reward=-150.44 +/- 148.23
Episode length: 17.68 +/- 5.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.7         |
|    mean_reward          | -150         |
| time/                   |              |
|    total_timesteps      | 510000       |
| train/                  |              |
|    approx_kl            | 0.0038402618 |
|    clip_fraction        | 0.0145       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.237       |
|    explained_variance   | 0.338        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.71e+03     |
|    n_updates            | 408          |
|    policy_gradient_loss | 0.00452      |
|    value_loss           | 5.82e+03     |
------------------------------------------
Eval num_timesteps=510500, episode_reward=-192.77 +/- 126.14
Episode length: 15.06 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 510500   |
---------------------------------
Eval num_timesteps=511000, episode_reward=-146.97 +/- 139.75
Episode length: 17.06 +/- 5.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 511000   |
---------------------------------
Eval num_timesteps=511500, episode_reward=-165.34 +/- 133.25
Episode length: 16.84 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 511500   |
---------------------------------
Eval num_timesteps=512000, episode_reward=-155.29 +/- 121.42
Episode length: 16.50 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -188     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 250      |
|    time_elapsed    | 1054     |
|    total_timesteps | 512000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=512500, episode_reward=-127.99 +/- 149.19
Episode length: 17.28 +/- 5.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.3         |
|    mean_reward          | -128         |
| time/                   |              |
|    total_timesteps      | 512500       |
| train/                  |              |
|    approx_kl            | 0.0064145327 |
|    clip_fraction        | 0.0948       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.302       |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.94e+03     |
|    n_updates            | 409          |
|    policy_gradient_loss | 0.00303      |
|    value_loss           | 6.46e+03     |
------------------------------------------
Eval num_timesteps=513000, episode_reward=-121.05 +/- 133.92
Episode length: 18.04 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 513000   |
---------------------------------
Eval num_timesteps=513500, episode_reward=-150.25 +/- 134.62
Episode length: 16.78 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 513500   |
---------------------------------
Eval num_timesteps=514000, episode_reward=-149.57 +/- 124.11
Episode length: 16.26 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 251      |
|    time_elapsed    | 1059     |
|    total_timesteps | 514048   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=514500, episode_reward=-180.02 +/- 144.53
Episode length: 14.98 +/- 4.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15         |
|    mean_reward          | -180       |
| time/                   |            |
|    total_timesteps      | 514500     |
| train/                  |            |
|    approx_kl            | 0.00352948 |
|    clip_fraction        | 0.0526     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.266     |
|    explained_variance   | 0.396      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.17e+03   |
|    n_updates            | 410        |
|    policy_gradient_loss | 0.00188    |
|    value_loss           | 6.24e+03   |
----------------------------------------
Eval num_timesteps=515000, episode_reward=-163.58 +/- 126.63
Episode length: 16.68 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
Eval num_timesteps=515500, episode_reward=-176.96 +/- 124.10
Episode length: 15.54 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 515500   |
---------------------------------
Eval num_timesteps=516000, episode_reward=-201.50 +/- 105.73
Episode length: 14.88 +/- 3.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 252      |
|    time_elapsed    | 1063     |
|    total_timesteps | 516096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=516500, episode_reward=-162.63 +/- 124.36
Episode length: 16.74 +/- 4.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -163        |
| time/                   |             |
|    total_timesteps      | 516500      |
| train/                  |             |
|    approx_kl            | 0.004936124 |
|    clip_fraction        | 0.0491      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.233      |
|    explained_variance   | 0.336       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.15e+03    |
|    n_updates            | 411         |
|    policy_gradient_loss | 0.00622     |
|    value_loss           | 7.1e+03     |
-----------------------------------------
Eval num_timesteps=517000, episode_reward=-187.59 +/- 132.43
Episode length: 14.88 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 517000   |
---------------------------------
Eval num_timesteps=517500, episode_reward=-170.45 +/- 145.97
Episode length: 16.48 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 517500   |
---------------------------------
Eval num_timesteps=518000, episode_reward=-199.06 +/- 120.95
Episode length: 15.18 +/- 4.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 253      |
|    time_elapsed    | 1067     |
|    total_timesteps | 518144   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=518500, episode_reward=-134.45 +/- 133.25
Episode length: 16.50 +/- 5.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -134        |
| time/                   |             |
|    total_timesteps      | 518500      |
| train/                  |             |
|    approx_kl            | 0.004515653 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.26       |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.0001      |
|    loss                 | 4.08e+03    |
|    n_updates            | 412         |
|    policy_gradient_loss | 0.00702     |
|    value_loss           | 7.27e+03    |
-----------------------------------------
Eval num_timesteps=519000, episode_reward=-176.47 +/- 134.64
Episode length: 15.46 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 519000   |
---------------------------------
Eval num_timesteps=519500, episode_reward=-145.51 +/- 152.19
Episode length: 16.30 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 519500   |
---------------------------------
Eval num_timesteps=520000, episode_reward=-164.98 +/- 164.07
Episode length: 15.50 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -174     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 254      |
|    time_elapsed    | 1071     |
|    total_timesteps | 520192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=520500, episode_reward=-179.34 +/- 130.23
Episode length: 16.50 +/- 4.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -179        |
| time/                   |             |
|    total_timesteps      | 520500      |
| train/                  |             |
|    approx_kl            | 0.005519488 |
|    clip_fraction        | 0.0547      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.257      |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.95e+03    |
|    n_updates            | 413         |
|    policy_gradient_loss | 0.00337     |
|    value_loss           | 5.85e+03    |
-----------------------------------------
Eval num_timesteps=521000, episode_reward=-173.82 +/- 140.94
Episode length: 16.74 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 521000   |
---------------------------------
Eval num_timesteps=521500, episode_reward=-189.85 +/- 119.26
Episode length: 15.36 +/- 4.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 521500   |
---------------------------------
Eval num_timesteps=522000, episode_reward=-180.76 +/- 179.42
Episode length: 16.00 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 255      |
|    time_elapsed    | 1075     |
|    total_timesteps | 522240   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=522500, episode_reward=-174.21 +/- 112.98
Episode length: 16.02 +/- 4.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -174        |
| time/                   |             |
|    total_timesteps      | 522500      |
| train/                  |             |
|    approx_kl            | 0.005617235 |
|    clip_fraction        | 0.0662      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.298      |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.69e+03    |
|    n_updates            | 415         |
|    policy_gradient_loss | 0.00609     |
|    value_loss           | 5.03e+03    |
-----------------------------------------
Eval num_timesteps=523000, episode_reward=-198.15 +/- 141.44
Episode length: 15.86 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 523000   |
---------------------------------
Eval num_timesteps=523500, episode_reward=-155.85 +/- 173.82
Episode length: 17.42 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 523500   |
---------------------------------
Eval num_timesteps=524000, episode_reward=-147.50 +/- 140.30
Episode length: 15.94 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 256      |
|    time_elapsed    | 1079     |
|    total_timesteps | 524288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=524500, episode_reward=-189.89 +/- 138.18
Episode length: 15.30 +/- 4.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | -190        |
| time/                   |             |
|    total_timesteps      | 524500      |
| train/                  |             |
|    approx_kl            | 0.008250444 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.298      |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.58e+03    |
|    n_updates            | 416         |
|    policy_gradient_loss | 0.00816     |
|    value_loss           | 4.96e+03    |
-----------------------------------------
Eval num_timesteps=525000, episode_reward=-183.44 +/- 120.07
Episode length: 15.88 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 525000   |
---------------------------------
Eval num_timesteps=525500, episode_reward=-221.33 +/- 141.37
Episode length: 15.64 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -221     |
| time/              |          |
|    total_timesteps | 525500   |
---------------------------------
Eval num_timesteps=526000, episode_reward=-150.42 +/- 162.55
Episode length: 17.26 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -202     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 257      |
|    time_elapsed    | 1083     |
|    total_timesteps | 526336   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=526500, episode_reward=-143.42 +/- 160.51
Episode length: 17.10 +/- 5.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.1        |
|    mean_reward          | -143        |
| time/                   |             |
|    total_timesteps      | 526500      |
| train/                  |             |
|    approx_kl            | 0.018380025 |
|    clip_fraction        | 0.046       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.26       |
|    explained_variance   | 0.441       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.98e+03    |
|    n_updates            | 418         |
|    policy_gradient_loss | 0.00173     |
|    value_loss           | 5.88e+03    |
-----------------------------------------
Eval num_timesteps=527000, episode_reward=-176.86 +/- 141.69
Episode length: 15.94 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 527000   |
---------------------------------
Eval num_timesteps=527500, episode_reward=-172.10 +/- 163.95
Episode length: 16.10 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 527500   |
---------------------------------
Eval num_timesteps=528000, episode_reward=-150.92 +/- 139.81
Episode length: 16.48 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 258      |
|    time_elapsed    | 1087     |
|    total_timesteps | 528384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=528500, episode_reward=-145.64 +/- 113.46
Episode length: 15.60 +/- 4.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -146         |
| time/                   |              |
|    total_timesteps      | 528500       |
| train/                  |              |
|    approx_kl            | 0.0054291273 |
|    clip_fraction        | 0.102        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.241       |
|    explained_variance   | 0.298        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.65e+03     |
|    n_updates            | 419          |
|    policy_gradient_loss | 0.00346      |
|    value_loss           | 5.93e+03     |
------------------------------------------
Eval num_timesteps=529000, episode_reward=-165.00 +/- 113.13
Episode length: 15.42 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 529000   |
---------------------------------
Eval num_timesteps=529500, episode_reward=-99.17 +/- 171.94
Episode length: 17.28 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -99.2    |
| time/              |          |
|    total_timesteps | 529500   |
---------------------------------
Eval num_timesteps=530000, episode_reward=-193.45 +/- 160.35
Episode length: 15.62 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -179     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 259      |
|    time_elapsed    | 1092     |
|    total_timesteps | 530432   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=530500, episode_reward=-173.44 +/- 127.14
Episode length: 15.22 +/- 4.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | -173        |
| time/                   |             |
|    total_timesteps      | 530500      |
| train/                  |             |
|    approx_kl            | 0.004632088 |
|    clip_fraction        | 0.0558      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.278      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.0001      |
|    loss                 | 2.75e+03    |
|    n_updates            | 420         |
|    policy_gradient_loss | 0.00763     |
|    value_loss           | 7.21e+03    |
-----------------------------------------
Eval num_timesteps=531000, episode_reward=-197.82 +/- 123.67
Episode length: 15.04 +/- 4.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 531000   |
---------------------------------
Eval num_timesteps=531500, episode_reward=-138.15 +/- 126.29
Episode length: 16.32 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 531500   |
---------------------------------
Eval num_timesteps=532000, episode_reward=-195.28 +/- 151.92
Episode length: 15.50 +/- 4.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 260      |
|    time_elapsed    | 1096     |
|    total_timesteps | 532480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=532500, episode_reward=-117.92 +/- 189.83
Episode length: 16.64 +/- 6.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -118         |
| time/                   |              |
|    total_timesteps      | 532500       |
| train/                  |              |
|    approx_kl            | 0.0054183435 |
|    clip_fraction        | 0.063        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.303       |
|    explained_variance   | 0.429        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.54e+03     |
|    n_updates            | 421          |
|    policy_gradient_loss | 0.005        |
|    value_loss           | 5.95e+03     |
------------------------------------------
Eval num_timesteps=533000, episode_reward=-155.07 +/- 157.36
Episode length: 16.08 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 533000   |
---------------------------------
Eval num_timesteps=533500, episode_reward=-93.52 +/- 144.99
Episode length: 18.38 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | -93.5    |
| time/              |          |
|    total_timesteps | 533500   |
---------------------------------
Eval num_timesteps=534000, episode_reward=-174.25 +/- 160.31
Episode length: 15.42 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
Eval num_timesteps=534500, episode_reward=-175.89 +/- 134.58
Episode length: 16.32 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 534500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -145     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 261      |
|    time_elapsed    | 1100     |
|    total_timesteps | 534528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=535000, episode_reward=-121.49 +/- 144.86
Episode length: 15.94 +/- 4.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -121         |
| time/                   |              |
|    total_timesteps      | 535000       |
| train/                  |              |
|    approx_kl            | 0.0045307763 |
|    clip_fraction        | 0.0666       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.259       |
|    explained_variance   | 0.371        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.88e+03     |
|    n_updates            | 422          |
|    policy_gradient_loss | 0.0066       |
|    value_loss           | 5.7e+03      |
------------------------------------------
Eval num_timesteps=535500, episode_reward=-107.20 +/- 171.25
Episode length: 16.84 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 535500   |
---------------------------------
Eval num_timesteps=536000, episode_reward=-129.14 +/- 155.42
Episode length: 15.96 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
Eval num_timesteps=536500, episode_reward=-148.59 +/- 131.19
Episode length: 15.64 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 536500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 262      |
|    time_elapsed    | 1104     |
|    total_timesteps | 536576   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=537000, episode_reward=-112.12 +/- 158.56
Episode length: 16.74 +/- 5.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -112         |
| time/                   |              |
|    total_timesteps      | 537000       |
| train/                  |              |
|    approx_kl            | 0.0045695594 |
|    clip_fraction        | 0.0748       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.246       |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.96e+03     |
|    n_updates            | 423          |
|    policy_gradient_loss | 0.00183      |
|    value_loss           | 5.63e+03     |
------------------------------------------
Eval num_timesteps=537500, episode_reward=-147.21 +/- 110.10
Episode length: 16.00 +/- 4.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 537500   |
---------------------------------
Eval num_timesteps=538000, episode_reward=-141.73 +/- 156.72
Episode length: 16.76 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
Eval num_timesteps=538500, episode_reward=-154.40 +/- 141.35
Episode length: 15.48 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 538500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 263      |
|    time_elapsed    | 1109     |
|    total_timesteps | 538624   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=539000, episode_reward=-150.46 +/- 145.46
Episode length: 15.38 +/- 5.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -150         |
| time/                   |              |
|    total_timesteps      | 539000       |
| train/                  |              |
|    approx_kl            | 0.0061698356 |
|    clip_fraction        | 0.048        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.189       |
|    explained_variance   | 0.329        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.2e+03      |
|    n_updates            | 425          |
|    policy_gradient_loss | 0.00388      |
|    value_loss           | 6.7e+03      |
------------------------------------------
Eval num_timesteps=539500, episode_reward=-144.15 +/- 160.08
Episode length: 16.30 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 539500   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-150.42 +/- 146.40
Episode length: 15.88 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=540500, episode_reward=-163.09 +/- 143.42
Episode length: 15.84 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 540500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 264      |
|    time_elapsed    | 1113     |
|    total_timesteps | 540672   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=541000, episode_reward=-140.21 +/- 117.67
Episode length: 16.72 +/- 4.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -140         |
| time/                   |              |
|    total_timesteps      | 541000       |
| train/                  |              |
|    approx_kl            | 0.0044211578 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.169       |
|    explained_variance   | 0.378        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.6e+03      |
|    n_updates            | 427          |
|    policy_gradient_loss | 0.00239      |
|    value_loss           | 6.42e+03     |
------------------------------------------
Eval num_timesteps=541500, episode_reward=-152.47 +/- 129.11
Episode length: 15.74 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 541500   |
---------------------------------
Eval num_timesteps=542000, episode_reward=-160.78 +/- 117.93
Episode length: 15.54 +/- 4.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
Eval num_timesteps=542500, episode_reward=-117.77 +/- 183.62
Episode length: 16.64 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 542500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 265      |
|    time_elapsed    | 1117     |
|    total_timesteps | 542720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=543000, episode_reward=-154.54 +/- 134.19
Episode length: 15.70 +/- 4.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -155         |
| time/                   |              |
|    total_timesteps      | 543000       |
| train/                  |              |
|    approx_kl            | 0.0049123624 |
|    clip_fraction        | 0.0443       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.23        |
|    explained_variance   | 0.366        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.73e+03     |
|    n_updates            | 428          |
|    policy_gradient_loss | 0.0106       |
|    value_loss           | 6.83e+03     |
------------------------------------------
Eval num_timesteps=543500, episode_reward=-120.92 +/- 134.27
Episode length: 16.88 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 543500   |
---------------------------------
Eval num_timesteps=544000, episode_reward=-125.62 +/- 142.61
Episode length: 16.80 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
Eval num_timesteps=544500, episode_reward=-170.25 +/- 158.41
Episode length: 15.02 +/- 4.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 544500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 266      |
|    time_elapsed    | 1121     |
|    total_timesteps | 544768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=545000, episode_reward=-127.98 +/- 147.91
Episode length: 16.46 +/- 5.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -128         |
| time/                   |              |
|    total_timesteps      | 545000       |
| train/                  |              |
|    approx_kl            | 0.0030805313 |
|    clip_fraction        | 0.0254       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.14        |
|    explained_variance   | 0.28         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.32e+03     |
|    n_updates            | 429          |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 6.72e+03     |
------------------------------------------
Eval num_timesteps=545500, episode_reward=-135.26 +/- 155.66
Episode length: 16.32 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 545500   |
---------------------------------
Eval num_timesteps=546000, episode_reward=-156.39 +/- 134.25
Episode length: 15.54 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
Eval num_timesteps=546500, episode_reward=-140.80 +/- 131.05
Episode length: 15.96 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 546500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 267      |
|    time_elapsed    | 1125     |
|    total_timesteps | 546816   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=547000, episode_reward=-178.71 +/- 154.81
Episode length: 15.74 +/- 4.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | -179        |
| time/                   |             |
|    total_timesteps      | 547000      |
| train/                  |             |
|    approx_kl            | 0.002042897 |
|    clip_fraction        | 0.0136      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.1        |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.3e+03     |
|    n_updates            | 431         |
|    policy_gradient_loss | 0.00223     |
|    value_loss           | 6.61e+03    |
-----------------------------------------
Eval num_timesteps=547500, episode_reward=-117.56 +/- 184.47
Episode length: 16.72 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 547500   |
---------------------------------
Eval num_timesteps=548000, episode_reward=-135.57 +/- 138.07
Episode length: 15.52 +/- 3.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
Eval num_timesteps=548500, episode_reward=-165.11 +/- 152.65
Episode length: 15.24 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 548500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 268      |
|    time_elapsed    | 1129     |
|    total_timesteps | 548864   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=549000, episode_reward=-162.97 +/- 137.39
Episode length: 15.38 +/- 4.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -163         |
| time/                   |              |
|    total_timesteps      | 549000       |
| train/                  |              |
|    approx_kl            | 0.0025552209 |
|    clip_fraction        | 0.00943      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0996      |
|    explained_variance   | 0.309        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.31e+03     |
|    n_updates            | 433          |
|    policy_gradient_loss | 0.00275      |
|    value_loss           | 6.89e+03     |
------------------------------------------
Eval num_timesteps=549500, episode_reward=-152.29 +/- 140.57
Episode length: 15.88 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 549500   |
---------------------------------
Eval num_timesteps=550000, episode_reward=-141.49 +/- 108.13
Episode length: 15.52 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
Eval num_timesteps=550500, episode_reward=-170.07 +/- 160.40
Episode length: 14.90 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 550500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 269      |
|    time_elapsed    | 1133     |
|    total_timesteps | 550912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=551000, episode_reward=-116.75 +/- 137.57
Episode length: 16.84 +/- 5.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -117         |
| time/                   |              |
|    total_timesteps      | 551000       |
| train/                  |              |
|    approx_kl            | 0.0031073606 |
|    clip_fraction        | 0.00841      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0262      |
|    explained_variance   | 0.315        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.7e+03      |
|    n_updates            | 434          |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 6.5e+03      |
------------------------------------------
Eval num_timesteps=551500, episode_reward=-132.91 +/- 164.27
Episode length: 16.72 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 551500   |
---------------------------------
Eval num_timesteps=552000, episode_reward=-159.40 +/- 173.77
Episode length: 15.50 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
Eval num_timesteps=552500, episode_reward=-122.59 +/- 153.84
Episode length: 16.30 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 552500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 270      |
|    time_elapsed    | 1137     |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=553000, episode_reward=-143.29 +/- 159.58
Episode length: 16.68 +/- 4.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -143        |
| time/                   |             |
|    total_timesteps      | 553000      |
| train/                  |             |
|    approx_kl            | 0.000872869 |
|    clip_fraction        | 0.00239     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00696    |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.05e+03    |
|    n_updates            | 444         |
|    policy_gradient_loss | -2.25e-05   |
|    value_loss           | 6.29e+03    |
-----------------------------------------
Eval num_timesteps=553500, episode_reward=-140.63 +/- 146.99
Episode length: 16.74 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 553500   |
---------------------------------
Eval num_timesteps=554000, episode_reward=-145.92 +/- 179.60
Episode length: 16.34 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 554000   |
---------------------------------
Eval num_timesteps=554500, episode_reward=-128.18 +/- 162.69
Episode length: 16.70 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 554500   |
---------------------------------
Eval num_timesteps=555000, episode_reward=-175.84 +/- 144.65
Episode length: 15.86 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 271      |
|    time_elapsed    | 1143     |
|    total_timesteps | 555008   |
---------------------------------
Eval num_timesteps=555500, episode_reward=-140.31 +/- 196.53
Episode length: 16.06 +/- 5.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.1          |
|    mean_reward          | -140          |
| time/                   |               |
|    total_timesteps      | 555500        |
| train/                  |               |
|    approx_kl            | 0.00037955045 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00222      |
|    explained_variance   | 0.308         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.41e+03      |
|    n_updates            | 454           |
|    policy_gradient_loss | -7.05e-05     |
|    value_loss           | 7.23e+03      |
-------------------------------------------
Eval num_timesteps=556000, episode_reward=-132.91 +/- 158.38
Episode length: 17.14 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 556000   |
---------------------------------
Eval num_timesteps=556500, episode_reward=-155.76 +/- 140.26
Episode length: 16.36 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 556500   |
---------------------------------
Eval num_timesteps=557000, episode_reward=-122.25 +/- 165.84
Episode length: 16.64 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 272      |
|    time_elapsed    | 1148     |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=557500, episode_reward=-133.81 +/- 151.73
Episode length: 16.50 +/- 5.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.5          |
|    mean_reward          | -134          |
| time/                   |               |
|    total_timesteps      | 557500        |
| train/                  |               |
|    approx_kl            | 1.5526894e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00189      |
|    explained_variance   | 0.312         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.95e+03      |
|    n_updates            | 464           |
|    policy_gradient_loss | -1.67e-06     |
|    value_loss           | 7.16e+03      |
-------------------------------------------
Eval num_timesteps=558000, episode_reward=-119.52 +/- 133.69
Episode length: 16.02 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
Eval num_timesteps=558500, episode_reward=-128.10 +/- 179.12
Episode length: 16.04 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 558500   |
---------------------------------
Eval num_timesteps=559000, episode_reward=-173.01 +/- 113.28
Episode length: 14.30 +/- 4.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.3     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 273      |
|    time_elapsed    | 1152     |
|    total_timesteps | 559104   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=559500, episode_reward=-166.36 +/- 183.69
Episode length: 16.02 +/- 5.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -166          |
| time/                   |               |
|    total_timesteps      | 559500        |
| train/                  |               |
|    approx_kl            | 0.00067260565 |
|    clip_fraction        | 0.000504      |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.0088       |
|    explained_variance   | 0.332         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.27e+03      |
|    n_updates            | 467           |
|    policy_gradient_loss | -4.17e-05     |
|    value_loss           | 6.28e+03      |
-------------------------------------------
Eval num_timesteps=560000, episode_reward=-182.41 +/- 143.82
Episode length: 15.26 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=560500, episode_reward=-145.94 +/- 155.06
Episode length: 15.86 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 560500   |
---------------------------------
Eval num_timesteps=561000, episode_reward=-163.36 +/- 143.92
Episode length: 15.74 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 274      |
|    time_elapsed    | 1156     |
|    total_timesteps | 561152   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=561500, episode_reward=-131.67 +/- 147.04
Episode length: 16.40 +/- 4.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -132         |
| time/                   |              |
|    total_timesteps      | 561500       |
| train/                  |              |
|    approx_kl            | 0.0014362525 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0041      |
|    explained_variance   | 0.357        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.83e+03     |
|    n_updates            | 468          |
|    policy_gradient_loss | -3.3e-05     |
|    value_loss           | 6.61e+03     |
------------------------------------------
Eval num_timesteps=562000, episode_reward=-147.72 +/- 123.10
Episode length: 15.58 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 562000   |
---------------------------------
Eval num_timesteps=562500, episode_reward=-154.15 +/- 118.87
Episode length: 15.54 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 562500   |
---------------------------------
Eval num_timesteps=563000, episode_reward=-123.85 +/- 189.09
Episode length: 16.94 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 275      |
|    time_elapsed    | 1161     |
|    total_timesteps | 563200   |
---------------------------------
Eval num_timesteps=563500, episode_reward=-160.01 +/- 131.15
Episode length: 15.54 +/- 4.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.5          |
|    mean_reward          | -160          |
| time/                   |               |
|    total_timesteps      | 563500        |
| train/                  |               |
|    approx_kl            | 2.0751031e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00295      |
|    explained_variance   | 0.312         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.99e+03      |
|    n_updates            | 478           |
|    policy_gradient_loss | 2.04e-06      |
|    value_loss           | 6.71e+03      |
-------------------------------------------
Eval num_timesteps=564000, episode_reward=-172.20 +/- 153.75
Episode length: 16.00 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
Eval num_timesteps=564500, episode_reward=-154.46 +/- 148.50
Episode length: 16.08 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 564500   |
---------------------------------
Eval num_timesteps=565000, episode_reward=-137.56 +/- 179.31
Episode length: 15.66 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 276      |
|    time_elapsed    | 1165     |
|    total_timesteps | 565248   |
---------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Eval num_timesteps=565500, episode_reward=-163.48 +/- 177.40
Episode length: 15.78 +/- 5.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -163         |
| time/                   |              |
|    total_timesteps      | 565500       |
| train/                  |              |
|    approx_kl            | 0.0049019833 |
|    clip_fraction        | 0.0047       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0294      |
|    explained_variance   | 0.339        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.24e+03     |
|    n_updates            | 486          |
|    policy_gradient_loss | -0.00049     |
|    value_loss           | 6.1e+03      |
------------------------------------------
Eval num_timesteps=566000, episode_reward=-159.38 +/- 139.59
Episode length: 15.44 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 566000   |
---------------------------------
Eval num_timesteps=566500, episode_reward=-137.28 +/- 148.05
Episode length: 16.22 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 566500   |
---------------------------------
Eval num_timesteps=567000, episode_reward=-141.25 +/- 145.27
Episode length: 15.68 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 277      |
|    time_elapsed    | 1170     |
|    total_timesteps | 567296   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=567500, episode_reward=-199.64 +/- 157.43
Episode length: 15.14 +/- 4.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | -200        |
| time/                   |             |
|    total_timesteps      | 567500      |
| train/                  |             |
|    approx_kl            | 0.006974171 |
|    clip_fraction        | 0.05        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.307       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.41e+03    |
|    n_updates            | 487         |
|    policy_gradient_loss | 0.00424     |
|    value_loss           | 7.71e+03    |
-----------------------------------------
Eval num_timesteps=568000, episode_reward=-133.24 +/- 151.30
Episode length: 16.50 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 568000   |
---------------------------------
Eval num_timesteps=568500, episode_reward=-106.59 +/- 178.63
Episode length: 16.88 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 568500   |
---------------------------------
Eval num_timesteps=569000, episode_reward=-130.13 +/- 153.56
Episode length: 16.62 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -208     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 278      |
|    time_elapsed    | 1174     |
|    total_timesteps | 569344   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=569500, episode_reward=-146.06 +/- 160.11
Episode length: 16.54 +/- 5.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -146         |
| time/                   |              |
|    total_timesteps      | 569500       |
| train/                  |              |
|    approx_kl            | 0.0065296483 |
|    clip_fraction        | 0.0597       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.41        |
|    explained_variance   | 0.277        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.18e+03     |
|    n_updates            | 488          |
|    policy_gradient_loss | -0.00333     |
|    value_loss           | 7.06e+03     |
------------------------------------------
Eval num_timesteps=570000, episode_reward=-130.63 +/- 128.66
Episode length: 16.52 +/- 4.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
Eval num_timesteps=570500, episode_reward=-154.64 +/- 136.99
Episode length: 16.64 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 570500   |
---------------------------------
Eval num_timesteps=571000, episode_reward=-180.43 +/- 162.24
Episode length: 15.10 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 279      |
|    time_elapsed    | 1178     |
|    total_timesteps | 571392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=571500, episode_reward=-191.04 +/- 179.84
Episode length: 15.74 +/- 5.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -191         |
| time/                   |              |
|    total_timesteps      | 571500       |
| train/                  |              |
|    approx_kl            | 0.0049929195 |
|    clip_fraction        | 0.0481       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.261       |
|    explained_variance   | 0.251        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.69e+03     |
|    n_updates            | 489          |
|    policy_gradient_loss | 0.00448      |
|    value_loss           | 7.21e+03     |
------------------------------------------
Eval num_timesteps=572000, episode_reward=-158.16 +/- 179.07
Episode length: 15.94 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 572000   |
---------------------------------
Eval num_timesteps=572500, episode_reward=-147.71 +/- 125.27
Episode length: 15.54 +/- 4.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 572500   |
---------------------------------
Eval num_timesteps=573000, episode_reward=-156.99 +/- 146.24
Episode length: 15.44 +/- 5.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | -202     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 280      |
|    time_elapsed    | 1182     |
|    total_timesteps | 573440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=573500, episode_reward=-146.48 +/- 137.90
Episode length: 16.16 +/- 4.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -146        |
| time/                   |             |
|    total_timesteps      | 573500      |
| train/                  |             |
|    approx_kl            | 0.012924762 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.345       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.48e+03    |
|    n_updates            | 490         |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 6.64e+03    |
-----------------------------------------
Eval num_timesteps=574000, episode_reward=-138.99 +/- 145.80
Episode length: 16.00 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 574000   |
---------------------------------
Eval num_timesteps=574500, episode_reward=-157.46 +/- 126.02
Episode length: 16.18 +/- 5.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 574500   |
---------------------------------
Eval num_timesteps=575000, episode_reward=-146.44 +/- 163.82
Episode length: 15.98 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -143     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 281      |
|    time_elapsed    | 1186     |
|    total_timesteps | 575488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=575500, episode_reward=-154.98 +/- 151.27
Episode length: 15.62 +/- 4.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -155        |
| time/                   |             |
|    total_timesteps      | 575500      |
| train/                  |             |
|    approx_kl            | 0.004783379 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.157      |
|    explained_variance   | 0.305       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.05e+03    |
|    n_updates            | 491         |
|    policy_gradient_loss | 0.00933     |
|    value_loss           | 6.4e+03     |
-----------------------------------------
Eval num_timesteps=576000, episode_reward=-197.07 +/- 178.13
Episode length: 15.72 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=576500, episode_reward=-160.36 +/- 175.88
Episode length: 16.08 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 576500   |
---------------------------------
Eval num_timesteps=577000, episode_reward=-114.01 +/- 141.72
Episode length: 17.34 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -114     |
| time/              |          |
|    total_timesteps | 577000   |
---------------------------------
Eval num_timesteps=577500, episode_reward=-161.95 +/- 139.54
Episode length: 15.34 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 577500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -146     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 282      |
|    time_elapsed    | 1191     |
|    total_timesteps | 577536   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=578000, episode_reward=-170.08 +/- 167.36
Episode length: 15.14 +/- 4.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | -170        |
| time/                   |             |
|    total_timesteps      | 578000      |
| train/                  |             |
|    approx_kl            | 0.004835495 |
|    clip_fraction        | 0.0321      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.173      |
|    explained_variance   | 0.309       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.29e+03    |
|    n_updates            | 492         |
|    policy_gradient_loss | -0.00188    |
|    value_loss           | 6.22e+03    |
-----------------------------------------
Eval num_timesteps=578500, episode_reward=-149.65 +/- 139.30
Episode length: 16.12 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 578500   |
---------------------------------
Eval num_timesteps=579000, episode_reward=-166.93 +/- 137.28
Episode length: 15.08 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 579000   |
---------------------------------
Eval num_timesteps=579500, episode_reward=-160.20 +/- 136.61
Episode length: 15.68 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 579500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -156     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 283      |
|    time_elapsed    | 1195     |
|    total_timesteps | 579584   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=580000, episode_reward=-103.78 +/- 158.93
Episode length: 16.86 +/- 5.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | -104        |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.002434508 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.167      |
|    explained_variance   | 0.386       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.24e+03    |
|    n_updates            | 493         |
|    policy_gradient_loss | 0.000682    |
|    value_loss           | 5.26e+03    |
-----------------------------------------
Eval num_timesteps=580500, episode_reward=-150.69 +/- 159.09
Episode length: 16.42 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 580500   |
---------------------------------
Eval num_timesteps=581000, episode_reward=-174.79 +/- 140.14
Episode length: 15.02 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 581000   |
---------------------------------
Eval num_timesteps=581500, episode_reward=-127.63 +/- 168.94
Episode length: 16.44 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 581500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 284      |
|    time_elapsed    | 1199     |
|    total_timesteps | 581632   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=582000, episode_reward=-168.90 +/- 164.93
Episode length: 15.38 +/- 5.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -169         |
| time/                   |              |
|    total_timesteps      | 582000       |
| train/                  |              |
|    approx_kl            | 0.0030979717 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.151       |
|    explained_variance   | 0.321        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.59e+03     |
|    n_updates            | 494          |
|    policy_gradient_loss | 0.00268      |
|    value_loss           | 6.56e+03     |
------------------------------------------
Eval num_timesteps=582500, episode_reward=-106.51 +/- 201.38
Episode length: 17.06 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 582500   |
---------------------------------
Eval num_timesteps=583000, episode_reward=-166.67 +/- 172.57
Episode length: 14.80 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 583000   |
---------------------------------
Eval num_timesteps=583500, episode_reward=-157.84 +/- 141.67
Episode length: 16.56 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 583500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 285      |
|    time_elapsed    | 1203     |
|    total_timesteps | 583680   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=584000, episode_reward=-154.58 +/- 129.46
Episode length: 16.48 +/- 4.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -155        |
| time/                   |             |
|    total_timesteps      | 584000      |
| train/                  |             |
|    approx_kl            | 0.028863758 |
|    clip_fraction        | 0.0152      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.286       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.25e+03    |
|    n_updates            | 496         |
|    policy_gradient_loss | 0.00176     |
|    value_loss           | 7.28e+03    |
-----------------------------------------
Eval num_timesteps=584500, episode_reward=-163.65 +/- 142.63
Episode length: 15.90 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 584500   |
---------------------------------
Eval num_timesteps=585000, episode_reward=-100.85 +/- 142.10
Episode length: 16.72 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -101     |
| time/              |          |
|    total_timesteps | 585000   |
---------------------------------
Eval num_timesteps=585500, episode_reward=-181.09 +/- 146.46
Episode length: 14.96 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 585500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 286      |
|    time_elapsed    | 1207     |
|    total_timesteps | 585728   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=586000, episode_reward=-129.30 +/- 156.33
Episode length: 16.92 +/- 4.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | -129        |
| time/                   |             |
|    total_timesteps      | 586000      |
| train/                  |             |
|    approx_kl            | 0.007803659 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.552      |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.52e+03    |
|    n_updates            | 497         |
|    policy_gradient_loss | 0.00754     |
|    value_loss           | 7.77e+03    |
-----------------------------------------
Eval num_timesteps=586500, episode_reward=-150.78 +/- 165.06
Episode length: 15.70 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 586500   |
---------------------------------
Eval num_timesteps=587000, episode_reward=-138.91 +/- 153.60
Episode length: 16.24 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 587000   |
---------------------------------
Eval num_timesteps=587500, episode_reward=-125.62 +/- 112.30
Episode length: 16.24 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 587500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 287      |
|    time_elapsed    | 1211     |
|    total_timesteps | 587776   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=588000, episode_reward=-202.59 +/- 148.33
Episode length: 14.32 +/- 4.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.3         |
|    mean_reward          | -203         |
| time/                   |              |
|    total_timesteps      | 588000       |
| train/                  |              |
|    approx_kl            | 0.0063696518 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.6         |
|    explained_variance   | 0.301        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.59e+03     |
|    n_updates            | 498          |
|    policy_gradient_loss | -0.00209     |
|    value_loss           | 6.23e+03     |
------------------------------------------
Eval num_timesteps=588500, episode_reward=-124.08 +/- 140.96
Episode length: 16.44 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 588500   |
---------------------------------
Eval num_timesteps=589000, episode_reward=-156.52 +/- 135.27
Episode length: 16.10 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 589000   |
---------------------------------
Eval num_timesteps=589500, episode_reward=-155.42 +/- 143.99
Episode length: 15.16 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 589500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -204     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 288      |
|    time_elapsed    | 1216     |
|    total_timesteps | 589824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=590000, episode_reward=-148.11 +/- 142.88
Episode length: 15.58 +/- 4.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -148        |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.006931571 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.539      |
|    explained_variance   | 0.31        |
|    learning_rate        | 0.0001      |
|    loss                 | 2.97e+03    |
|    n_updates            | 499         |
|    policy_gradient_loss | -0.000242   |
|    value_loss           | 6.82e+03    |
-----------------------------------------
Eval num_timesteps=590500, episode_reward=-175.34 +/- 143.63
Episode length: 15.44 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 590500   |
---------------------------------
Eval num_timesteps=591000, episode_reward=-147.05 +/- 171.66
Episode length: 15.98 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 591000   |
---------------------------------
Eval num_timesteps=591500, episode_reward=-128.81 +/- 179.70
Episode length: 16.70 +/- 4.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 591500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -198     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 289      |
|    time_elapsed    | 1220     |
|    total_timesteps | 591872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=592000, episode_reward=-115.54 +/- 174.73
Episode length: 17.42 +/- 5.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.4        |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 592000      |
| train/                  |             |
|    approx_kl            | 0.004741111 |
|    clip_fraction        | 0.043       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.426      |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.18e+03    |
|    n_updates            | 500         |
|    policy_gradient_loss | 0.00106     |
|    value_loss           | 6.59e+03    |
-----------------------------------------
Eval num_timesteps=592500, episode_reward=-171.20 +/- 139.38
Episode length: 15.44 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 592500   |
---------------------------------
Eval num_timesteps=593000, episode_reward=-137.86 +/- 152.94
Episode length: 17.08 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 593000   |
---------------------------------
Eval num_timesteps=593500, episode_reward=-124.31 +/- 176.48
Episode length: 17.26 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 593500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -196     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 290      |
|    time_elapsed    | 1224     |
|    total_timesteps | 593920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=594000, episode_reward=-134.97 +/- 162.10
Episode length: 16.90 +/- 5.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | -135        |
| time/                   |             |
|    total_timesteps      | 594000      |
| train/                  |             |
|    approx_kl            | 0.007204911 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.364      |
|    explained_variance   | 0.346       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.44e+03    |
|    n_updates            | 501         |
|    policy_gradient_loss | -0.00147    |
|    value_loss           | 8.03e+03    |
-----------------------------------------
Eval num_timesteps=594500, episode_reward=-144.79 +/- 161.42
Episode length: 15.80 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 594500   |
---------------------------------
Eval num_timesteps=595000, episode_reward=-180.73 +/- 140.96
Episode length: 15.62 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
Eval num_timesteps=595500, episode_reward=-134.28 +/- 177.04
Episode length: 16.38 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 595500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 291      |
|    time_elapsed    | 1228     |
|    total_timesteps | 595968   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=596000, episode_reward=-120.48 +/- 197.97
Episode length: 17.14 +/- 6.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.1        |
|    mean_reward          | -120        |
| time/                   |             |
|    total_timesteps      | 596000      |
| train/                  |             |
|    approx_kl            | 0.008701712 |
|    clip_fraction        | 0.0563      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.271      |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.85e+03    |
|    n_updates            | 502         |
|    policy_gradient_loss | -0.00396    |
|    value_loss           | 5.49e+03    |
-----------------------------------------
Eval num_timesteps=596500, episode_reward=-142.72 +/- 165.04
Episode length: 16.12 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 596500   |
---------------------------------
Eval num_timesteps=597000, episode_reward=-148.31 +/- 154.18
Episode length: 16.54 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 597000   |
---------------------------------
Eval num_timesteps=597500, episode_reward=-162.03 +/- 142.74
Episode length: 15.46 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 597500   |
---------------------------------
Eval num_timesteps=598000, episode_reward=-157.09 +/- 157.16
Episode length: 15.70 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 292      |
|    time_elapsed    | 1233     |
|    total_timesteps | 598016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=598500, episode_reward=-138.96 +/- 153.93
Episode length: 16.56 +/- 4.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -139         |
| time/                   |              |
|    total_timesteps      | 598500       |
| train/                  |              |
|    approx_kl            | 0.0047802194 |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.286       |
|    explained_variance   | 0.416        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.27e+03     |
|    n_updates            | 503          |
|    policy_gradient_loss | 0.00321      |
|    value_loss           | 5.51e+03     |
------------------------------------------
Eval num_timesteps=599000, episode_reward=-166.14 +/- 159.11
Episode length: 15.88 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 599000   |
---------------------------------
Eval num_timesteps=599500, episode_reward=-142.21 +/- 143.51
Episode length: 15.66 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 599500   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-121.96 +/- 149.69
Episode length: 16.90 +/- 5.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 293      |
|    time_elapsed    | 1237     |
|    total_timesteps | 600064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=600500, episode_reward=-142.94 +/- 182.50
Episode length: 16.02 +/- 5.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -143        |
| time/                   |             |
|    total_timesteps      | 600500      |
| train/                  |             |
|    approx_kl            | 0.005785176 |
|    clip_fraction        | 0.0882      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.323      |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.86e+03    |
|    n_updates            | 504         |
|    policy_gradient_loss | 0.0066      |
|    value_loss           | 6.03e+03    |
-----------------------------------------
Eval num_timesteps=601000, episode_reward=-164.87 +/- 150.27
Episode length: 15.66 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 601000   |
---------------------------------
Eval num_timesteps=601500, episode_reward=-165.73 +/- 128.83
Episode length: 15.10 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 601500   |
---------------------------------
Eval num_timesteps=602000, episode_reward=-190.81 +/- 145.92
Episode length: 14.76 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 602000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -191     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 294      |
|    time_elapsed    | 1241     |
|    total_timesteps | 602112   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=602500, episode_reward=-154.02 +/- 132.37
Episode length: 17.60 +/- 5.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | -154         |
| time/                   |              |
|    total_timesteps      | 602500       |
| train/                  |              |
|    approx_kl            | 0.0061255777 |
|    clip_fraction        | 0.084        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.356       |
|    explained_variance   | 0.43         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.36e+03     |
|    n_updates            | 505          |
|    policy_gradient_loss | -0.00012     |
|    value_loss           | 5.64e+03     |
------------------------------------------
Eval num_timesteps=603000, episode_reward=-160.07 +/- 137.03
Episode length: 16.78 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 603000   |
---------------------------------
Eval num_timesteps=603500, episode_reward=-179.33 +/- 148.71
Episode length: 15.90 +/- 5.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 603500   |
---------------------------------
Eval num_timesteps=604000, episode_reward=-156.55 +/- 127.22
Episode length: 15.54 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 604000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 295      |
|    time_elapsed    | 1245     |
|    total_timesteps | 604160   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=604500, episode_reward=-182.62 +/- 146.12
Episode length: 16.58 +/- 4.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -183        |
| time/                   |             |
|    total_timesteps      | 604500      |
| train/                  |             |
|    approx_kl            | 0.005040967 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.0001      |
|    loss                 | 2.62e+03    |
|    n_updates            | 506         |
|    policy_gradient_loss | 0.00404     |
|    value_loss           | 5.81e+03    |
-----------------------------------------
Eval num_timesteps=605000, episode_reward=-159.71 +/- 161.08
Episode length: 17.82 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 605000   |
---------------------------------
Eval num_timesteps=605500, episode_reward=-144.12 +/- 156.72
Episode length: 17.04 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 605500   |
---------------------------------
Eval num_timesteps=606000, episode_reward=-223.76 +/- 144.19
Episode length: 16.14 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -224     |
| time/              |          |
|    total_timesteps | 606000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 296      |
|    time_elapsed    | 1249     |
|    total_timesteps | 606208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=606500, episode_reward=-176.19 +/- 123.59
Episode length: 15.86 +/- 4.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -176         |
| time/                   |              |
|    total_timesteps      | 606500       |
| train/                  |              |
|    approx_kl            | 0.0062703863 |
|    clip_fraction        | 0.084        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.253       |
|    explained_variance   | 0.393        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.05e+03     |
|    n_updates            | 507          |
|    policy_gradient_loss | 0.000172     |
|    value_loss           | 7.29e+03     |
------------------------------------------
Eval num_timesteps=607000, episode_reward=-158.16 +/- 149.66
Episode length: 16.74 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 607000   |
---------------------------------
Eval num_timesteps=607500, episode_reward=-194.35 +/- 145.09
Episode length: 16.16 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 607500   |
---------------------------------
Eval num_timesteps=608000, episode_reward=-213.22 +/- 134.55
Episode length: 15.60 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 608000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 297      |
|    time_elapsed    | 1254     |
|    total_timesteps | 608256   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=608500, episode_reward=-198.47 +/- 114.08
Episode length: 15.48 +/- 4.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -198         |
| time/                   |              |
|    total_timesteps      | 608500       |
| train/                  |              |
|    approx_kl            | 0.0046721445 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.488        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.29e+03     |
|    n_updates            | 508          |
|    policy_gradient_loss | 0.0011       |
|    value_loss           | 4.64e+03     |
------------------------------------------
Eval num_timesteps=609000, episode_reward=-195.69 +/- 136.91
Episode length: 17.04 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 609000   |
---------------------------------
Eval num_timesteps=609500, episode_reward=-192.78 +/- 138.35
Episode length: 16.06 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 609500   |
---------------------------------
Eval num_timesteps=610000, episode_reward=-184.22 +/- 126.56
Episode length: 16.26 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 610000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 298      |
|    time_elapsed    | 1258     |
|    total_timesteps | 610304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=610500, episode_reward=-159.05 +/- 141.82
Episode length: 17.38 +/- 5.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | -159         |
| time/                   |              |
|    total_timesteps      | 610500       |
| train/                  |              |
|    approx_kl            | 0.0048544444 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.208       |
|    explained_variance   | 0.394        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.09e+03     |
|    n_updates            | 509          |
|    policy_gradient_loss | 0.000197     |
|    value_loss           | 6.39e+03     |
------------------------------------------
Eval num_timesteps=611000, episode_reward=-184.92 +/- 126.58
Episode length: 16.82 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 611000   |
---------------------------------
Eval num_timesteps=611500, episode_reward=-153.65 +/- 199.84
Episode length: 17.98 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 611500   |
---------------------------------
Eval num_timesteps=612000, episode_reward=-191.90 +/- 132.58
Episode length: 17.26 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 612000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 299      |
|    time_elapsed    | 1262     |
|    total_timesteps | 612352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=612500, episode_reward=-183.88 +/- 131.12
Episode length: 16.76 +/- 5.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | -184        |
| time/                   |             |
|    total_timesteps      | 612500      |
| train/                  |             |
|    approx_kl            | 0.003530113 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.211      |
|    explained_variance   | 0.443       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.5e+03     |
|    n_updates            | 510         |
|    policy_gradient_loss | 0.00204     |
|    value_loss           | 6.62e+03    |
-----------------------------------------
Eval num_timesteps=613000, episode_reward=-183.62 +/- 173.77
Episode length: 16.92 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 613000   |
---------------------------------
Eval num_timesteps=613500, episode_reward=-199.45 +/- 160.54
Episode length: 16.90 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 613500   |
---------------------------------
Eval num_timesteps=614000, episode_reward=-156.08 +/- 141.40
Episode length: 16.66 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 614000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 300      |
|    time_elapsed    | 1266     |
|    total_timesteps | 614400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=614500, episode_reward=-185.39 +/- 120.17
Episode length: 16.10 +/- 4.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -185        |
| time/                   |             |
|    total_timesteps      | 614500      |
| train/                  |             |
|    approx_kl            | 0.004282234 |
|    clip_fraction        | 0.0375      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.259      |
|    explained_variance   | 0.393       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.42e+03    |
|    n_updates            | 511         |
|    policy_gradient_loss | 0.00192     |
|    value_loss           | 7.14e+03    |
-----------------------------------------
Eval num_timesteps=615000, episode_reward=-200.27 +/- 121.99
Episode length: 15.24 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 615000   |
---------------------------------
Eval num_timesteps=615500, episode_reward=-167.23 +/- 128.39
Episode length: 17.38 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 615500   |
---------------------------------
Eval num_timesteps=616000, episode_reward=-143.86 +/- 150.93
Episode length: 17.66 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 616000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 301      |
|    time_elapsed    | 1270     |
|    total_timesteps | 616448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=616500, episode_reward=-185.38 +/- 159.94
Episode length: 14.96 +/- 5.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | -185         |
| time/                   |              |
|    total_timesteps      | 616500       |
| train/                  |              |
|    approx_kl            | 0.0071730404 |
|    clip_fraction        | 0.0647       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.267       |
|    explained_variance   | 0.423        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.59e+03     |
|    n_updates            | 512          |
|    policy_gradient_loss | 0.0138       |
|    value_loss           | 5.1e+03      |
------------------------------------------
Eval num_timesteps=617000, episode_reward=-158.25 +/- 157.26
Episode length: 17.74 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 617000   |
---------------------------------
Eval num_timesteps=617500, episode_reward=-185.20 +/- 137.93
Episode length: 16.36 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 617500   |
---------------------------------
Eval num_timesteps=618000, episode_reward=-185.66 +/- 147.77
Episode length: 16.26 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 618000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 302      |
|    time_elapsed    | 1275     |
|    total_timesteps | 618496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=618500, episode_reward=-153.00 +/- 146.41
Episode length: 16.32 +/- 5.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -153        |
| time/                   |             |
|    total_timesteps      | 618500      |
| train/                  |             |
|    approx_kl            | 0.003329948 |
|    clip_fraction        | 0.0205      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.272      |
|    explained_variance   | 0.387       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.76e+03    |
|    n_updates            | 513         |
|    policy_gradient_loss | 0.00161     |
|    value_loss           | 5.86e+03    |
-----------------------------------------
Eval num_timesteps=619000, episode_reward=-182.06 +/- 145.41
Episode length: 16.98 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 619000   |
---------------------------------
Eval num_timesteps=619500, episode_reward=-154.99 +/- 145.84
Episode length: 16.64 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 619500   |
---------------------------------
Eval num_timesteps=620000, episode_reward=-173.40 +/- 124.34
Episode length: 15.50 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=620500, episode_reward=-185.57 +/- 119.19
Episode length: 15.14 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 620500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 303      |
|    time_elapsed    | 1279     |
|    total_timesteps | 620544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=621000, episode_reward=-159.70 +/- 128.68
Episode length: 17.16 +/- 5.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | -160         |
| time/                   |              |
|    total_timesteps      | 621000       |
| train/                  |              |
|    approx_kl            | 0.0030378702 |
|    clip_fraction        | 0.0304       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.213       |
|    explained_variance   | 0.366        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.4e+03      |
|    n_updates            | 514          |
|    policy_gradient_loss | 0.00156      |
|    value_loss           | 5.61e+03     |
------------------------------------------
Eval num_timesteps=621500, episode_reward=-147.89 +/- 149.32
Episode length: 16.84 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 621500   |
---------------------------------
Eval num_timesteps=622000, episode_reward=-177.72 +/- 140.24
Episode length: 16.02 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 622000   |
---------------------------------
Eval num_timesteps=622500, episode_reward=-178.63 +/- 132.56
Episode length: 16.04 +/- 4.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 622500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 304      |
|    time_elapsed    | 1283     |
|    total_timesteps | 622592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=623000, episode_reward=-165.28 +/- 130.56
Episode length: 15.88 +/- 4.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -165         |
| time/                   |              |
|    total_timesteps      | 623000       |
| train/                  |              |
|    approx_kl            | 0.0043191873 |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.239       |
|    explained_variance   | 0.411        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.11e+03     |
|    n_updates            | 515          |
|    policy_gradient_loss | 0.00634      |
|    value_loss           | 6.15e+03     |
------------------------------------------
Eval num_timesteps=623500, episode_reward=-138.09 +/- 134.95
Episode length: 17.42 +/- 4.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 623500   |
---------------------------------
Eval num_timesteps=624000, episode_reward=-187.30 +/- 153.87
Episode length: 15.46 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 624000   |
---------------------------------
Eval num_timesteps=624500, episode_reward=-193.20 +/- 125.48
Episode length: 15.62 +/- 4.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 624500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 305      |
|    time_elapsed    | 1287     |
|    total_timesteps | 624640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=625000, episode_reward=-158.33 +/- 140.37
Episode length: 15.12 +/- 4.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | -158        |
| time/                   |             |
|    total_timesteps      | 625000      |
| train/                  |             |
|    approx_kl            | 0.006091431 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.244      |
|    explained_variance   | 0.453       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.31e+03    |
|    n_updates            | 516         |
|    policy_gradient_loss | 0.00382     |
|    value_loss           | 6.67e+03    |
-----------------------------------------
Eval num_timesteps=625500, episode_reward=-135.80 +/- 128.31
Episode length: 15.76 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 625500   |
---------------------------------
Eval num_timesteps=626000, episode_reward=-127.23 +/- 178.49
Episode length: 17.36 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 626000   |
---------------------------------
Eval num_timesteps=626500, episode_reward=-111.14 +/- 158.48
Episode length: 16.40 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 626500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -179     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 306      |
|    time_elapsed    | 1292     |
|    total_timesteps | 626688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=627000, episode_reward=-160.36 +/- 106.43
Episode length: 16.12 +/- 4.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -160        |
| time/                   |             |
|    total_timesteps      | 627000      |
| train/                  |             |
|    approx_kl            | 0.006168719 |
|    clip_fraction        | 0.0794      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.21       |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.79e+03    |
|    n_updates            | 517         |
|    policy_gradient_loss | 0.00559     |
|    value_loss           | 6.64e+03    |
-----------------------------------------
Eval num_timesteps=627500, episode_reward=-166.17 +/- 158.67
Episode length: 16.34 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 627500   |
---------------------------------
Eval num_timesteps=628000, episode_reward=-150.68 +/- 164.84
Episode length: 16.92 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 628000   |
---------------------------------
Eval num_timesteps=628500, episode_reward=-135.39 +/- 144.56
Episode length: 16.98 +/- 5.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 628500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -174     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 307      |
|    time_elapsed    | 1296     |
|    total_timesteps | 628736   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=629000, episode_reward=-175.56 +/- 115.59
Episode length: 16.28 +/- 5.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -176        |
| time/                   |             |
|    total_timesteps      | 629000      |
| train/                  |             |
|    approx_kl            | 0.010549409 |
|    clip_fraction        | 0.0853      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.276      |
|    explained_variance   | 0.425       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.03e+03    |
|    n_updates            | 519         |
|    policy_gradient_loss | 0.00328     |
|    value_loss           | 6.49e+03    |
-----------------------------------------
Eval num_timesteps=629500, episode_reward=-176.96 +/- 156.22
Episode length: 17.20 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 629500   |
---------------------------------
Eval num_timesteps=630000, episode_reward=-163.78 +/- 131.37
Episode length: 16.68 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 630000   |
---------------------------------
Eval num_timesteps=630500, episode_reward=-156.72 +/- 142.63
Episode length: 17.56 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 630500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | -146     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 308      |
|    time_elapsed    | 1300     |
|    total_timesteps | 630784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=631000, episode_reward=-153.58 +/- 154.38
Episode length: 16.76 +/- 5.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -154         |
| time/                   |              |
|    total_timesteps      | 631000       |
| train/                  |              |
|    approx_kl            | 0.0048573785 |
|    clip_fraction        | 0.0688       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.235       |
|    explained_variance   | 0.361        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.83e+03     |
|    n_updates            | 520          |
|    policy_gradient_loss | 0.000408     |
|    value_loss           | 6.69e+03     |
------------------------------------------
Eval num_timesteps=631500, episode_reward=-174.85 +/- 136.10
Episode length: 17.16 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 631500   |
---------------------------------
Eval num_timesteps=632000, episode_reward=-167.28 +/- 173.82
Episode length: 17.18 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 632000   |
---------------------------------
Eval num_timesteps=632500, episode_reward=-182.54 +/- 132.14
Episode length: 16.58 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 632500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 309      |
|    time_elapsed    | 1304     |
|    total_timesteps | 632832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=633000, episode_reward=-178.61 +/- 171.49
Episode length: 16.88 +/- 6.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | -179        |
| time/                   |             |
|    total_timesteps      | 633000      |
| train/                  |             |
|    approx_kl            | 0.004929326 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.187      |
|    explained_variance   | 0.444       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.32e+03    |
|    n_updates            | 521         |
|    policy_gradient_loss | 0.0105      |
|    value_loss           | 4.74e+03    |
-----------------------------------------
Eval num_timesteps=633500, episode_reward=-174.45 +/- 145.97
Episode length: 16.16 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 633500   |
---------------------------------
Eval num_timesteps=634000, episode_reward=-190.53 +/- 150.37
Episode length: 16.30 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 634000   |
---------------------------------
Eval num_timesteps=634500, episode_reward=-193.77 +/- 106.64
Episode length: 16.14 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 634500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 310      |
|    time_elapsed    | 1308     |
|    total_timesteps | 634880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=635000, episode_reward=-167.22 +/- 156.44
Episode length: 16.82 +/- 5.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -167         |
| time/                   |              |
|    total_timesteps      | 635000       |
| train/                  |              |
|    approx_kl            | 0.0026517368 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.186       |
|    explained_variance   | 0.433        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.42e+03     |
|    n_updates            | 522          |
|    policy_gradient_loss | 0.00414      |
|    value_loss           | 5.22e+03     |
------------------------------------------
Eval num_timesteps=635500, episode_reward=-191.56 +/- 132.55
Episode length: 15.74 +/- 5.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 635500   |
---------------------------------
Eval num_timesteps=636000, episode_reward=-164.08 +/- 118.03
Episode length: 16.98 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 636000   |
---------------------------------
Eval num_timesteps=636500, episode_reward=-170.12 +/- 120.70
Episode length: 17.20 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 636500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 311      |
|    time_elapsed    | 1312     |
|    total_timesteps | 636928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=637000, episode_reward=-129.30 +/- 139.86
Episode length: 18.36 +/- 4.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.4         |
|    mean_reward          | -129         |
| time/                   |              |
|    total_timesteps      | 637000       |
| train/                  |              |
|    approx_kl            | 0.0028653787 |
|    clip_fraction        | 0.0319       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.193       |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.36e+03     |
|    n_updates            | 523          |
|    policy_gradient_loss | 0.00247      |
|    value_loss           | 5.98e+03     |
------------------------------------------
Eval num_timesteps=637500, episode_reward=-187.71 +/- 155.93
Episode length: 16.92 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 637500   |
---------------------------------
Eval num_timesteps=638000, episode_reward=-158.02 +/- 135.62
Episode length: 17.54 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 638000   |
---------------------------------
Eval num_timesteps=638500, episode_reward=-163.57 +/- 134.42
Episode length: 16.52 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 638500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.1     |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 312      |
|    time_elapsed    | 1317     |
|    total_timesteps | 638976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=639000, episode_reward=-189.24 +/- 130.57
Episode length: 17.14 +/- 5.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.1         |
|    mean_reward          | -189         |
| time/                   |              |
|    total_timesteps      | 639000       |
| train/                  |              |
|    approx_kl            | 0.0045751007 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.175       |
|    explained_variance   | 0.373        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.64e+03     |
|    n_updates            | 524          |
|    policy_gradient_loss | 0.00241      |
|    value_loss           | 6.18e+03     |
------------------------------------------
Eval num_timesteps=639500, episode_reward=-195.64 +/- 175.77
Episode length: 16.88 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 639500   |
---------------------------------
Eval num_timesteps=640000, episode_reward=-176.77 +/- 147.75
Episode length: 16.64 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=640500, episode_reward=-198.98 +/- 161.10
Episode length: 16.98 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 640500   |
---------------------------------
Eval num_timesteps=641000, episode_reward=-175.77 +/- 144.28
Episode length: 17.28 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 641000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 313      |
|    time_elapsed    | 1322     |
|    total_timesteps | 641024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=641500, episode_reward=-202.09 +/- 127.60
Episode length: 16.20 +/- 5.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -202        |
| time/                   |             |
|    total_timesteps      | 641500      |
| train/                  |             |
|    approx_kl            | 0.003770438 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.189      |
|    explained_variance   | 0.415       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.35e+03    |
|    n_updates            | 525         |
|    policy_gradient_loss | 0.0143      |
|    value_loss           | 5.86e+03    |
-----------------------------------------
Eval num_timesteps=642000, episode_reward=-214.49 +/- 127.00
Episode length: 15.94 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 642000   |
---------------------------------
Eval num_timesteps=642500, episode_reward=-200.79 +/- 115.46
Episode length: 16.70 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 642500   |
---------------------------------
Eval num_timesteps=643000, episode_reward=-180.07 +/- 153.09
Episode length: 17.64 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 643000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -210     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 314      |
|    time_elapsed    | 1326     |
|    total_timesteps | 643072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=643500, episode_reward=-128.18 +/- 155.29
Episode length: 19.56 +/- 5.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 19.6         |
|    mean_reward          | -128         |
| time/                   |              |
|    total_timesteps      | 643500       |
| train/                  |              |
|    approx_kl            | 0.0031577912 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.175       |
|    explained_variance   | 0.416        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.89e+03     |
|    n_updates            | 526          |
|    policy_gradient_loss | -0.000864    |
|    value_loss           | 6.04e+03     |
------------------------------------------
Eval num_timesteps=644000, episode_reward=-152.20 +/- 144.64
Episode length: 18.02 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 644000   |
---------------------------------
Eval num_timesteps=644500, episode_reward=-216.31 +/- 129.37
Episode length: 16.78 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 644500   |
---------------------------------
Eval num_timesteps=645000, episode_reward=-188.47 +/- 128.96
Episode length: 16.48 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 645000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 315      |
|    time_elapsed    | 1330     |
|    total_timesteps | 645120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=645500, episode_reward=-174.92 +/- 122.56
Episode length: 16.28 +/- 4.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -175        |
| time/                   |             |
|    total_timesteps      | 645500      |
| train/                  |             |
|    approx_kl            | 0.004436743 |
|    clip_fraction        | 0.0295      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.215      |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.87e+03    |
|    n_updates            | 527         |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 6.03e+03    |
-----------------------------------------
Eval num_timesteps=646000, episode_reward=-156.52 +/- 159.08
Episode length: 17.72 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 646000   |
---------------------------------
Eval num_timesteps=646500, episode_reward=-157.68 +/- 158.68
Episode length: 17.12 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 646500   |
---------------------------------
Eval num_timesteps=647000, episode_reward=-215.19 +/- 123.09
Episode length: 15.82 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 647000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 316      |
|    time_elapsed    | 1334     |
|    total_timesteps | 647168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=647500, episode_reward=-187.12 +/- 142.48
Episode length: 15.54 +/- 5.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -187         |
| time/                   |              |
|    total_timesteps      | 647500       |
| train/                  |              |
|    approx_kl            | 0.0061249277 |
|    clip_fraction        | 0.0491       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.272       |
|    explained_variance   | 0.392        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.01e+03     |
|    n_updates            | 528          |
|    policy_gradient_loss | 0.000934     |
|    value_loss           | 6.68e+03     |
------------------------------------------
Eval num_timesteps=648000, episode_reward=-176.39 +/- 124.61
Episode length: 17.00 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 648000   |
---------------------------------
Eval num_timesteps=648500, episode_reward=-155.39 +/- 158.78
Episode length: 17.68 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 648500   |
---------------------------------
Eval num_timesteps=649000, episode_reward=-185.30 +/- 140.52
Episode length: 17.30 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 649000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 317      |
|    time_elapsed    | 1339     |
|    total_timesteps | 649216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=649500, episode_reward=-198.27 +/- 127.21
Episode length: 15.94 +/- 4.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -198         |
| time/                   |              |
|    total_timesteps      | 649500       |
| train/                  |              |
|    approx_kl            | 0.0052881367 |
|    clip_fraction        | 0.0443       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.255       |
|    explained_variance   | 0.37         |
|    learning_rate        | 0.0001       |
|    loss                 | 4.33e+03     |
|    n_updates            | 529          |
|    policy_gradient_loss | 0.00268      |
|    value_loss           | 7.27e+03     |
------------------------------------------
Eval num_timesteps=650000, episode_reward=-162.67 +/- 147.08
Episode length: 17.20 +/- 4.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 650000   |
---------------------------------
Eval num_timesteps=650500, episode_reward=-184.47 +/- 161.98
Episode length: 17.04 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 650500   |
---------------------------------
Eval num_timesteps=651000, episode_reward=-157.72 +/- 137.39
Episode length: 18.08 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.1     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 651000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 318      |
|    time_elapsed    | 1343     |
|    total_timesteps | 651264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=651500, episode_reward=-145.38 +/- 147.05
Episode length: 16.90 +/- 5.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.9         |
|    mean_reward          | -145         |
| time/                   |              |
|    total_timesteps      | 651500       |
| train/                  |              |
|    approx_kl            | 0.0063879993 |
|    clip_fraction        | 0.066        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.21        |
|    explained_variance   | 0.382        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.45e+03     |
|    n_updates            | 530          |
|    policy_gradient_loss | 0.00186      |
|    value_loss           | 5.86e+03     |
------------------------------------------
Eval num_timesteps=652000, episode_reward=-149.61 +/- 153.70
Episode length: 17.08 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 652000   |
---------------------------------
Eval num_timesteps=652500, episode_reward=-148.91 +/- 134.68
Episode length: 17.42 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 652500   |
---------------------------------
Eval num_timesteps=653000, episode_reward=-205.91 +/- 142.49
Episode length: 15.88 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 653000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -174     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 319      |
|    time_elapsed    | 1347     |
|    total_timesteps | 653312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=653500, episode_reward=-199.05 +/- 149.45
Episode length: 14.88 +/- 4.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | -199        |
| time/                   |             |
|    total_timesteps      | 653500      |
| train/                  |             |
|    approx_kl            | 0.004693417 |
|    clip_fraction        | 0.059       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.208      |
|    explained_variance   | 0.393       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.52e+03    |
|    n_updates            | 531         |
|    policy_gradient_loss | 0.0067      |
|    value_loss           | 5.77e+03    |
-----------------------------------------
Eval num_timesteps=654000, episode_reward=-153.39 +/- 133.94
Episode length: 16.94 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 654000   |
---------------------------------
Eval num_timesteps=654500, episode_reward=-179.35 +/- 114.95
Episode length: 15.24 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 654500   |
---------------------------------
Eval num_timesteps=655000, episode_reward=-147.34 +/- 138.25
Episode length: 16.98 +/- 5.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 655000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 320      |
|    time_elapsed    | 1351     |
|    total_timesteps | 655360   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=655500, episode_reward=-173.04 +/- 135.78
Episode length: 16.02 +/- 4.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -173         |
| time/                   |              |
|    total_timesteps      | 655500       |
| train/                  |              |
|    approx_kl            | 0.0044567203 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.17        |
|    explained_variance   | 0.39         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.6e+03      |
|    n_updates            | 533          |
|    policy_gradient_loss | 0.00368      |
|    value_loss           | 6.26e+03     |
------------------------------------------
Eval num_timesteps=656000, episode_reward=-182.97 +/- 124.30
Episode length: 15.86 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 656000   |
---------------------------------
Eval num_timesteps=656500, episode_reward=-148.74 +/- 135.82
Episode length: 16.64 +/- 5.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 656500   |
---------------------------------
Eval num_timesteps=657000, episode_reward=-153.22 +/- 117.64
Episode length: 15.54 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 657000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 321      |
|    time_elapsed    | 1356     |
|    total_timesteps | 657408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=657500, episode_reward=-161.88 +/- 142.46
Episode length: 16.20 +/- 4.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.2       |
|    mean_reward          | -162       |
| time/                   |            |
|    total_timesteps      | 657500     |
| train/                  |            |
|    approx_kl            | 0.00517042 |
|    clip_fraction        | 0.0841     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.173     |
|    explained_variance   | 0.391      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.18e+03   |
|    n_updates            | 534        |
|    policy_gradient_loss | -0.00184   |
|    value_loss           | 6.22e+03   |
----------------------------------------
Eval num_timesteps=658000, episode_reward=-200.19 +/- 122.04
Episode length: 15.20 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 658000   |
---------------------------------
Eval num_timesteps=658500, episode_reward=-182.18 +/- 145.82
Episode length: 16.98 +/- 5.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 658500   |
---------------------------------
Eval num_timesteps=659000, episode_reward=-200.45 +/- 103.88
Episode length: 14.94 +/- 4.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 659000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 322      |
|    time_elapsed    | 1360     |
|    total_timesteps | 659456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=659500, episode_reward=-164.83 +/- 123.16
Episode length: 17.68 +/- 5.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.7         |
|    mean_reward          | -165         |
| time/                   |              |
|    total_timesteps      | 659500       |
| train/                  |              |
|    approx_kl            | 0.0056195813 |
|    clip_fraction        | 0.0556       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.156       |
|    explained_variance   | 0.428        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.89e+03     |
|    n_updates            | 535          |
|    policy_gradient_loss | 0.00358      |
|    value_loss           | 5.88e+03     |
------------------------------------------
Eval num_timesteps=660000, episode_reward=-162.56 +/- 149.49
Episode length: 16.72 +/- 5.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=660500, episode_reward=-174.69 +/- 145.05
Episode length: 16.46 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 660500   |
---------------------------------
Eval num_timesteps=661000, episode_reward=-209.97 +/- 133.60
Episode length: 14.96 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 661000   |
---------------------------------
Eval num_timesteps=661500, episode_reward=-183.51 +/- 115.84
Episode length: 16.52 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 661500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 323      |
|    time_elapsed    | 1364     |
|    total_timesteps | 661504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=662000, episode_reward=-203.16 +/- 117.74
Episode length: 15.30 +/- 4.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -203         |
| time/                   |              |
|    total_timesteps      | 662000       |
| train/                  |              |
|    approx_kl            | 0.0040641637 |
|    clip_fraction        | 0.0273       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.15        |
|    explained_variance   | 0.378        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.38e+03     |
|    n_updates            | 536          |
|    policy_gradient_loss | 0.00335      |
|    value_loss           | 6.99e+03     |
------------------------------------------
Eval num_timesteps=662500, episode_reward=-165.54 +/- 139.97
Episode length: 16.64 +/- 5.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 662500   |
---------------------------------
Eval num_timesteps=663000, episode_reward=-201.57 +/- 146.46
Episode length: 16.28 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 663000   |
---------------------------------
Eval num_timesteps=663500, episode_reward=-206.50 +/- 137.23
Episode length: 15.26 +/- 4.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 663500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -192     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 324      |
|    time_elapsed    | 1369     |
|    total_timesteps | 663552   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=664000, episode_reward=-157.88 +/- 130.43
Episode length: 17.00 +/- 5.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -158         |
| time/                   |              |
|    total_timesteps      | 664000       |
| train/                  |              |
|    approx_kl            | 0.0065086572 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.152       |
|    explained_variance   | 0.449        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.05e+03     |
|    n_updates            | 538          |
|    policy_gradient_loss | 0.00148      |
|    value_loss           | 5.69e+03     |
------------------------------------------
Eval num_timesteps=664500, episode_reward=-171.93 +/- 112.19
Episode length: 15.52 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 664500   |
---------------------------------
Eval num_timesteps=665000, episode_reward=-174.45 +/- 151.19
Episode length: 17.32 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 665000   |
---------------------------------
Eval num_timesteps=665500, episode_reward=-184.02 +/- 118.64
Episode length: 16.70 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 665500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 325      |
|    time_elapsed    | 1373     |
|    total_timesteps | 665600   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=666000, episode_reward=-194.54 +/- 107.36
Episode length: 15.64 +/- 4.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -195         |
| time/                   |              |
|    total_timesteps      | 666000       |
| train/                  |              |
|    approx_kl            | 0.0046840697 |
|    clip_fraction        | 0.0369       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.127       |
|    explained_variance   | 0.438        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.02e+03     |
|    n_updates            | 540          |
|    policy_gradient_loss | 0.00476      |
|    value_loss           | 5.65e+03     |
------------------------------------------
Eval num_timesteps=666500, episode_reward=-157.72 +/- 119.21
Episode length: 17.10 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 666500   |
---------------------------------
Eval num_timesteps=667000, episode_reward=-187.37 +/- 131.52
Episode length: 16.46 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 667000   |
---------------------------------
Eval num_timesteps=667500, episode_reward=-162.79 +/- 146.41
Episode length: 17.20 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 667500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 326      |
|    time_elapsed    | 1377     |
|    total_timesteps | 667648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=668000, episode_reward=-152.59 +/- 149.74
Episode length: 18.64 +/- 5.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.6         |
|    mean_reward          | -153         |
| time/                   |              |
|    total_timesteps      | 668000       |
| train/                  |              |
|    approx_kl            | 0.0050691157 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.082       |
|    explained_variance   | 0.459        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.93e+03     |
|    n_updates            | 541          |
|    policy_gradient_loss | 0.00815      |
|    value_loss           | 5.38e+03     |
------------------------------------------
Eval num_timesteps=668500, episode_reward=-162.66 +/- 146.99
Episode length: 17.80 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 668500   |
---------------------------------
Eval num_timesteps=669000, episode_reward=-203.04 +/- 129.07
Episode length: 16.46 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 669000   |
---------------------------------
Eval num_timesteps=669500, episode_reward=-189.56 +/- 109.56
Episode length: 16.68 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 669500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 327      |
|    time_elapsed    | 1381     |
|    total_timesteps | 669696   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=670000, episode_reward=-182.48 +/- 130.57
Episode length: 16.78 +/- 5.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | -182        |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.006902724 |
|    clip_fraction        | 0.0184      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.092      |
|    explained_variance   | 0.441       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.26e+03    |
|    n_updates            | 543         |
|    policy_gradient_loss | -0.000321   |
|    value_loss           | 5.65e+03    |
-----------------------------------------
Eval num_timesteps=670500, episode_reward=-166.79 +/- 131.38
Episode length: 17.10 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 670500   |
---------------------------------
Eval num_timesteps=671000, episode_reward=-193.54 +/- 181.33
Episode length: 16.52 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 671000   |
---------------------------------
Eval num_timesteps=671500, episode_reward=-137.32 +/- 148.20
Episode length: 17.64 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 671500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -188     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 328      |
|    time_elapsed    | 1386     |
|    total_timesteps | 671744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=672000, episode_reward=-181.46 +/- 124.92
Episode length: 15.84 +/- 4.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -181         |
| time/                   |              |
|    total_timesteps      | 672000       |
| train/                  |              |
|    approx_kl            | 0.0021662146 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.12        |
|    explained_variance   | 0.416        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.55e+03     |
|    n_updates            | 544          |
|    policy_gradient_loss | 0.00241      |
|    value_loss           | 6.19e+03     |
------------------------------------------
Eval num_timesteps=672500, episode_reward=-152.08 +/- 156.29
Episode length: 17.28 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 672500   |
---------------------------------
Eval num_timesteps=673000, episode_reward=-163.82 +/- 154.49
Episode length: 15.46 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 673000   |
---------------------------------
Eval num_timesteps=673500, episode_reward=-191.95 +/- 149.15
Episode length: 15.92 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 673500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 329      |
|    time_elapsed    | 1390     |
|    total_timesteps | 673792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=674000, episode_reward=-186.80 +/- 124.11
Episode length: 15.50 +/- 4.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -187         |
| time/                   |              |
|    total_timesteps      | 674000       |
| train/                  |              |
|    approx_kl            | 0.0042466796 |
|    clip_fraction        | 0.0402       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.125       |
|    explained_variance   | 0.398        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.62e+03     |
|    n_updates            | 545          |
|    policy_gradient_loss | 0.000979     |
|    value_loss           | 5.91e+03     |
------------------------------------------
Eval num_timesteps=674500, episode_reward=-178.15 +/- 122.41
Episode length: 16.96 +/- 5.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 674500   |
---------------------------------
Eval num_timesteps=675000, episode_reward=-179.28 +/- 142.35
Episode length: 16.66 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 675000   |
---------------------------------
Eval num_timesteps=675500, episode_reward=-127.54 +/- 157.30
Episode length: 18.68 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.7     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 675500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 330      |
|    time_elapsed    | 1394     |
|    total_timesteps | 675840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=676000, episode_reward=-191.67 +/- 139.05
Episode length: 14.90 +/- 4.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | -192        |
| time/                   |             |
|    total_timesteps      | 676000      |
| train/                  |             |
|    approx_kl            | 0.004633084 |
|    clip_fraction        | 0.0361      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0959     |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.4e+03     |
|    n_updates            | 546         |
|    policy_gradient_loss | -0.000398   |
|    value_loss           | 6.19e+03    |
-----------------------------------------
Eval num_timesteps=676500, episode_reward=-138.30 +/- 126.26
Episode length: 16.56 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 676500   |
---------------------------------
Eval num_timesteps=677000, episode_reward=-150.48 +/- 127.51
Episode length: 16.56 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 677000   |
---------------------------------
Eval num_timesteps=677500, episode_reward=-168.55 +/- 132.28
Episode length: 16.64 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 677500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 331      |
|    time_elapsed    | 1398     |
|    total_timesteps | 677888   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=678000, episode_reward=-172.62 +/- 121.13
Episode length: 15.66 +/- 4.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | -173        |
| time/                   |             |
|    total_timesteps      | 678000      |
| train/                  |             |
|    approx_kl            | 0.009556417 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.143      |
|    explained_variance   | 0.437       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.58e+03    |
|    n_updates            | 547         |
|    policy_gradient_loss | 0.0131      |
|    value_loss           | 6.21e+03    |
-----------------------------------------
Eval num_timesteps=678500, episode_reward=-182.60 +/- 131.23
Episode length: 15.70 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 678500   |
---------------------------------
Eval num_timesteps=679000, episode_reward=-190.00 +/- 135.00
Episode length: 15.72 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 679000   |
---------------------------------
Eval num_timesteps=679500, episode_reward=-196.21 +/- 155.88
Episode length: 15.44 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 679500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.6     |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 332      |
|    time_elapsed    | 1402     |
|    total_timesteps | 679936   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=680000, episode_reward=-160.08 +/- 144.03
Episode length: 15.26 +/- 4.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | -160        |
| time/                   |             |
|    total_timesteps      | 680000      |
| train/                  |             |
|    approx_kl            | 0.007971525 |
|    clip_fraction        | 0.0954      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.372       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.71e+03    |
|    n_updates            | 549         |
|    policy_gradient_loss | 0.00359     |
|    value_loss           | 6.22e+03    |
-----------------------------------------
Eval num_timesteps=680500, episode_reward=-162.13 +/- 161.37
Episode length: 16.26 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 680500   |
---------------------------------
Eval num_timesteps=681000, episode_reward=-137.41 +/- 165.41
Episode length: 16.74 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 681000   |
---------------------------------
Eval num_timesteps=681500, episode_reward=-138.55 +/- 171.42
Episode length: 17.06 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 681500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -156     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 333      |
|    time_elapsed    | 1406     |
|    total_timesteps | 681984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=682000, episode_reward=-158.78 +/- 115.25
Episode length: 16.26 +/- 3.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -159         |
| time/                   |              |
|    total_timesteps      | 682000       |
| train/                  |              |
|    approx_kl            | 0.0038522282 |
|    clip_fraction        | 0.0489       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.135       |
|    explained_variance   | 0.31         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.95e+03     |
|    n_updates            | 550          |
|    policy_gradient_loss | 0.00386      |
|    value_loss           | 7.26e+03     |
------------------------------------------
Eval num_timesteps=682500, episode_reward=-170.56 +/- 125.45
Episode length: 16.52 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 682500   |
---------------------------------
Eval num_timesteps=683000, episode_reward=-106.29 +/- 192.47
Episode length: 16.86 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 683000   |
---------------------------------
Eval num_timesteps=683500, episode_reward=-186.80 +/- 139.94
Episode length: 14.98 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 683500   |
---------------------------------
Eval num_timesteps=684000, episode_reward=-156.33 +/- 123.69
Episode length: 15.90 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 334      |
|    time_elapsed    | 1411     |
|    total_timesteps | 684032   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=684500, episode_reward=-183.36 +/- 110.76
Episode length: 14.20 +/- 3.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.2        |
|    mean_reward          | -183        |
| time/                   |             |
|    total_timesteps      | 684500      |
| train/                  |             |
|    approx_kl            | 0.004542253 |
|    clip_fraction        | 0.0281      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0924     |
|    explained_variance   | 0.26        |
|    learning_rate        | 0.0001      |
|    loss                 | 3.9e+03     |
|    n_updates            | 552         |
|    policy_gradient_loss | 0.00372     |
|    value_loss           | 7.74e+03    |
-----------------------------------------
Eval num_timesteps=685000, episode_reward=-171.43 +/- 165.71
Episode length: 15.64 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 685000   |
---------------------------------
Eval num_timesteps=685500, episode_reward=-116.65 +/- 175.67
Episode length: 16.00 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 685500   |
---------------------------------
Eval num_timesteps=686000, episode_reward=-197.61 +/- 128.06
Episode length: 15.00 +/- 3.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 686000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 335      |
|    time_elapsed    | 1415     |
|    total_timesteps | 686080   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=686500, episode_reward=-164.06 +/- 139.68
Episode length: 15.50 +/- 4.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -164         |
| time/                   |              |
|    total_timesteps      | 686500       |
| train/                  |              |
|    approx_kl            | 0.0034075747 |
|    clip_fraction        | 0.0404       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.124       |
|    explained_variance   | 0.342        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.55e+03     |
|    n_updates            | 554          |
|    policy_gradient_loss | 0.00251      |
|    value_loss           | 6.44e+03     |
------------------------------------------
Eval num_timesteps=687000, episode_reward=-157.53 +/- 134.66
Episode length: 15.74 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 687000   |
---------------------------------
Eval num_timesteps=687500, episode_reward=-117.22 +/- 168.42
Episode length: 16.08 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 687500   |
---------------------------------
Eval num_timesteps=688000, episode_reward=-156.96 +/- 168.99
Episode length: 16.02 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 688000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 336      |
|    time_elapsed    | 1419     |
|    total_timesteps | 688128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=688500, episode_reward=-120.80 +/- 169.94
Episode length: 16.36 +/- 5.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -121         |
| time/                   |              |
|    total_timesteps      | 688500       |
| train/                  |              |
|    approx_kl            | 0.0047832504 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0653      |
|    explained_variance   | 0.336        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.78e+03     |
|    n_updates            | 555          |
|    policy_gradient_loss | 0.000277     |
|    value_loss           | 6.45e+03     |
------------------------------------------
Eval num_timesteps=689000, episode_reward=-160.90 +/- 146.42
Episode length: 14.38 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 689000   |
---------------------------------
Eval num_timesteps=689500, episode_reward=-160.06 +/- 145.29
Episode length: 16.40 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 689500   |
---------------------------------
Eval num_timesteps=690000, episode_reward=-139.93 +/- 119.15
Episode length: 15.76 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 690000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 337      |
|    time_elapsed    | 1423     |
|    total_timesteps | 690176   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=690500, episode_reward=-137.81 +/- 180.47
Episode length: 16.60 +/- 5.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -138        |
| time/                   |             |
|    total_timesteps      | 690500      |
| train/                  |             |
|    approx_kl            | 0.016703822 |
|    clip_fraction        | 0.00601     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0364     |
|    explained_variance   | 0.342       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.65e+03    |
|    n_updates            | 558         |
|    policy_gradient_loss | 0.00128     |
|    value_loss           | 6.62e+03    |
-----------------------------------------
Eval num_timesteps=691000, episode_reward=-161.05 +/- 141.60
Episode length: 15.82 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 691000   |
---------------------------------
Eval num_timesteps=691500, episode_reward=-159.10 +/- 146.96
Episode length: 15.70 +/- 4.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 691500   |
---------------------------------
Eval num_timesteps=692000, episode_reward=-169.80 +/- 144.45
Episode length: 15.48 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 692000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 338      |
|    time_elapsed    | 1428     |
|    total_timesteps | 692224   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=692500, episode_reward=-146.85 +/- 122.06
Episode length: 15.72 +/- 4.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -147         |
| time/                   |              |
|    total_timesteps      | 692500       |
| train/                  |              |
|    approx_kl            | 0.0016682895 |
|    clip_fraction        | 0.00277      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.019       |
|    explained_variance   | 0.31         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.95e+03     |
|    n_updates            | 561          |
|    policy_gradient_loss | 0.000227     |
|    value_loss           | 5.58e+03     |
------------------------------------------
Eval num_timesteps=693000, episode_reward=-161.19 +/- 158.93
Episode length: 15.98 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 693000   |
---------------------------------
Eval num_timesteps=693500, episode_reward=-148.96 +/- 143.71
Episode length: 16.12 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 693500   |
---------------------------------
Eval num_timesteps=694000, episode_reward=-127.02 +/- 144.06
Episode length: 16.26 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 694000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 339      |
|    time_elapsed    | 1432     |
|    total_timesteps | 694272   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=694500, episode_reward=-109.66 +/- 156.41
Episode length: 16.04 +/- 6.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -110        |
| time/                   |             |
|    total_timesteps      | 694500      |
| train/                  |             |
|    approx_kl            | 0.005928287 |
|    clip_fraction        | 0.000868    |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.00108    |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.61e+03    |
|    n_updates            | 563         |
|    policy_gradient_loss | 0.000292    |
|    value_loss           | 6.8e+03     |
-----------------------------------------
Eval num_timesteps=695000, episode_reward=-150.91 +/- 172.44
Episode length: 16.12 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 695000   |
---------------------------------
Eval num_timesteps=695500, episode_reward=-160.51 +/- 171.27
Episode length: 15.38 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 695500   |
---------------------------------
Eval num_timesteps=696000, episode_reward=-164.93 +/- 130.54
Episode length: 14.48 +/- 3.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 696000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 340      |
|    time_elapsed    | 1436     |
|    total_timesteps | 696320   |
---------------------------------
Eval num_timesteps=696500, episode_reward=-145.72 +/- 136.63
Episode length: 16.14 +/- 4.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.1      |
|    mean_reward          | -146      |
| time/                   |           |
|    total_timesteps      | 696500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.000187 |
|    explained_variance   | 0.323     |
|    learning_rate        | 0.0001    |
|    loss                 | 3.53e+03  |
|    n_updates            | 573       |
|    policy_gradient_loss | 2.03e-06  |
|    value_loss           | 8.3e+03   |
---------------------------------------
Eval num_timesteps=697000, episode_reward=-175.25 +/- 139.89
Episode length: 15.78 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 697000   |
---------------------------------
Eval num_timesteps=697500, episode_reward=-146.06 +/- 162.20
Episode length: 16.54 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 697500   |
---------------------------------
Eval num_timesteps=698000, episode_reward=-137.89 +/- 141.52
Episode length: 15.54 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 698000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 341      |
|    time_elapsed    | 1441     |
|    total_timesteps | 698368   |
---------------------------------
Eval num_timesteps=698500, episode_reward=-154.19 +/- 140.32
Episode length: 15.82 +/- 4.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -154          |
| time/                   |               |
|    total_timesteps      | 698500        |
| train/                  |               |
|    approx_kl            | 0.00022815797 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000125     |
|    explained_variance   | 0.319         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.22e+03      |
|    n_updates            | 583           |
|    policy_gradient_loss | -7.84e-06     |
|    value_loss           | 5.96e+03      |
-------------------------------------------
Eval num_timesteps=699000, episode_reward=-168.39 +/- 138.91
Episode length: 15.80 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 699000   |
---------------------------------
Eval num_timesteps=699500, episode_reward=-164.02 +/- 155.47
Episode length: 15.62 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 699500   |
---------------------------------
Eval num_timesteps=700000, episode_reward=-150.02 +/- 141.27
Episode length: 15.20 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 700000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 342      |
|    time_elapsed    | 1446     |
|    total_timesteps | 700416   |
---------------------------------
Eval num_timesteps=700500, episode_reward=-196.27 +/- 139.95
Episode length: 15.02 +/- 4.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | -196         |
| time/                   |              |
|    total_timesteps      | 700500       |
| train/                  |              |
|    approx_kl            | 4.656613e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.000202    |
|    explained_variance   | 0.378        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.5e+03      |
|    n_updates            | 593          |
|    policy_gradient_loss | -7.15e-07    |
|    value_loss           | 6.69e+03     |
------------------------------------------
Eval num_timesteps=701000, episode_reward=-147.15 +/- 167.25
Episode length: 16.26 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 701000   |
---------------------------------
Eval num_timesteps=701500, episode_reward=-152.64 +/- 144.83
Episode length: 16.06 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 701500   |
---------------------------------
Eval num_timesteps=702000, episode_reward=-140.34 +/- 157.38
Episode length: 16.10 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 702000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 343      |
|    time_elapsed    | 1450     |
|    total_timesteps | 702464   |
---------------------------------
Eval num_timesteps=702500, episode_reward=-152.02 +/- 178.86
Episode length: 16.54 +/- 5.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.5      |
|    mean_reward          | -152      |
| time/                   |           |
|    total_timesteps      | 702500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.000446 |
|    explained_variance   | 0.302     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.83e+03  |
|    n_updates            | 603       |
|    policy_gradient_loss | 1.86e-06  |
|    value_loss           | 6.92e+03  |
---------------------------------------
Eval num_timesteps=703000, episode_reward=-156.67 +/- 144.64
Episode length: 15.84 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 703000   |
---------------------------------
Eval num_timesteps=703500, episode_reward=-144.93 +/- 174.97
Episode length: 16.34 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 703500   |
---------------------------------
Eval num_timesteps=704000, episode_reward=-167.42 +/- 146.82
Episode length: 15.42 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 704000   |
---------------------------------
Eval num_timesteps=704500, episode_reward=-163.32 +/- 144.22
Episode length: 15.32 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 704500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -145     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 344      |
|    time_elapsed    | 1456     |
|    total_timesteps | 704512   |
---------------------------------
Eval num_timesteps=705000, episode_reward=-119.45 +/- 165.84
Episode length: 16.20 +/- 5.18
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 16.2           |
|    mean_reward          | -119           |
| time/                   |                |
|    total_timesteps      | 705000         |
| train/                  |                |
|    approx_kl            | -3.8999133e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -0.000388      |
|    explained_variance   | 0.369          |
|    learning_rate        | 0.0001         |
|    loss                 | 3.38e+03       |
|    n_updates            | 613            |
|    policy_gradient_loss | 8.97e-07       |
|    value_loss           | 5.29e+03       |
--------------------------------------------
Eval num_timesteps=705500, episode_reward=-126.00 +/- 150.73
Episode length: 16.04 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 705500   |
---------------------------------
Eval num_timesteps=706000, episode_reward=-130.82 +/- 128.01
Episode length: 16.40 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 706000   |
---------------------------------
Eval num_timesteps=706500, episode_reward=-147.93 +/- 172.96
Episode length: 15.88 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 706500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -126     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 345      |
|    time_elapsed    | 1460     |
|    total_timesteps | 706560   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=707000, episode_reward=-172.57 +/- 132.84
Episode length: 16.28 +/- 4.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -173         |
| time/                   |              |
|    total_timesteps      | 707000       |
| train/                  |              |
|    approx_kl            | 0.0010899686 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00849     |
|    explained_variance   | 0.313        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.67e+03     |
|    n_updates            | 616          |
|    policy_gradient_loss | -0.000573    |
|    value_loss           | 8.18e+03     |
------------------------------------------
Eval num_timesteps=707500, episode_reward=-181.45 +/- 143.39
Episode length: 15.04 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 707500   |
---------------------------------
Eval num_timesteps=708000, episode_reward=-150.60 +/- 148.56
Episode length: 16.24 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 708000   |
---------------------------------
Eval num_timesteps=708500, episode_reward=-178.90 +/- 159.33
Episode length: 15.74 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 708500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 346      |
|    time_elapsed    | 1465     |
|    total_timesteps | 708608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=709000, episode_reward=-142.20 +/- 175.90
Episode length: 15.62 +/- 5.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -142         |
| time/                   |              |
|    total_timesteps      | 709000       |
| train/                  |              |
|    approx_kl            | 0.0038417235 |
|    clip_fraction        | 0.0521       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.156       |
|    explained_variance   | 0.358        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.26e+03     |
|    n_updates            | 617          |
|    policy_gradient_loss | 7.88e-05     |
|    value_loss           | 6.4e+03      |
------------------------------------------
Eval num_timesteps=709500, episode_reward=-146.96 +/- 148.35
Episode length: 15.82 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 709500   |
---------------------------------
Eval num_timesteps=710000, episode_reward=-151.47 +/- 154.53
Episode length: 15.28 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 710000   |
---------------------------------
Eval num_timesteps=710500, episode_reward=-118.34 +/- 148.51
Episode length: 16.22 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 710500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 347      |
|    time_elapsed    | 1469     |
|    total_timesteps | 710656   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=711000, episode_reward=-176.39 +/- 132.55
Episode length: 16.12 +/- 5.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -176        |
| time/                   |             |
|    total_timesteps      | 711000      |
| train/                  |             |
|    approx_kl            | 0.009909385 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.365       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.88e+03    |
|    n_updates            | 619         |
|    policy_gradient_loss | 0.0035      |
|    value_loss           | 6.1e+03     |
-----------------------------------------
Eval num_timesteps=711500, episode_reward=-140.74 +/- 125.84
Episode length: 16.28 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 711500   |
---------------------------------
Eval num_timesteps=712000, episode_reward=-144.10 +/- 158.49
Episode length: 16.94 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 712000   |
---------------------------------
Eval num_timesteps=712500, episode_reward=-169.93 +/- 137.10
Episode length: 15.68 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 712500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 348      |
|    time_elapsed    | 1473     |
|    total_timesteps | 712704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=713000, episode_reward=-216.27 +/- 130.65
Episode length: 15.52 +/- 4.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -216         |
| time/                   |              |
|    total_timesteps      | 713000       |
| train/                  |              |
|    approx_kl            | 0.0074138884 |
|    clip_fraction        | 0.113        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.15        |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.57e+03     |
|    n_updates            | 620          |
|    policy_gradient_loss | 0.0136       |
|    value_loss           | 5.81e+03     |
------------------------------------------
Eval num_timesteps=713500, episode_reward=-166.06 +/- 139.73
Episode length: 16.40 +/- 4.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 713500   |
---------------------------------
Eval num_timesteps=714000, episode_reward=-155.08 +/- 176.05
Episode length: 16.82 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 714000   |
---------------------------------
Eval num_timesteps=714500, episode_reward=-148.63 +/- 153.25
Episode length: 16.90 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 714500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 349      |
|    time_elapsed    | 1477     |
|    total_timesteps | 714752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=715000, episode_reward=-167.80 +/- 133.58
Episode length: 16.64 +/- 4.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -168         |
| time/                   |              |
|    total_timesteps      | 715000       |
| train/                  |              |
|    approx_kl            | 0.0039466335 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.419        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.26e+03     |
|    n_updates            | 621          |
|    policy_gradient_loss | 0.00487      |
|    value_loss           | 6.66e+03     |
------------------------------------------
Eval num_timesteps=715500, episode_reward=-206.26 +/- 123.08
Episode length: 15.26 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 715500   |
---------------------------------
Eval num_timesteps=716000, episode_reward=-127.89 +/- 186.00
Episode length: 16.74 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 716000   |
---------------------------------
Eval num_timesteps=716500, episode_reward=-146.68 +/- 137.62
Episode length: 17.26 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 716500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -188     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 350      |
|    time_elapsed    | 1481     |
|    total_timesteps | 716800   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=717000, episode_reward=-183.89 +/- 140.64
Episode length: 16.44 +/- 4.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -184        |
| time/                   |             |
|    total_timesteps      | 717000      |
| train/                  |             |
|    approx_kl            | 0.011869326 |
|    clip_fraction        | 0.0276      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.396       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.6e+03     |
|    n_updates            | 623         |
|    policy_gradient_loss | 0.0014      |
|    value_loss           | 6.15e+03    |
-----------------------------------------
Eval num_timesteps=717500, episode_reward=-172.22 +/- 144.26
Episode length: 16.88 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 717500   |
---------------------------------
Eval num_timesteps=718000, episode_reward=-163.03 +/- 153.69
Episode length: 18.24 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 718000   |
---------------------------------
Eval num_timesteps=718500, episode_reward=-152.84 +/- 137.30
Episode length: 17.72 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 718500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 351      |
|    time_elapsed    | 1486     |
|    total_timesteps | 718848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=719000, episode_reward=-165.71 +/- 132.11
Episode length: 16.86 +/- 4.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.9         |
|    mean_reward          | -166         |
| time/                   |              |
|    total_timesteps      | 719000       |
| train/                  |              |
|    approx_kl            | 0.0019301428 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0966      |
|    explained_variance   | 0.446        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.79e+03     |
|    n_updates            | 624          |
|    policy_gradient_loss | 0.00205      |
|    value_loss           | 5.81e+03     |
------------------------------------------
Eval num_timesteps=719500, episode_reward=-140.21 +/- 141.28
Episode length: 16.40 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 719500   |
---------------------------------
Eval num_timesteps=720000, episode_reward=-138.59 +/- 125.65
Episode length: 16.88 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=720500, episode_reward=-137.52 +/- 137.95
Episode length: 16.82 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 720500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 352      |
|    time_elapsed    | 1490     |
|    total_timesteps | 720896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=721000, episode_reward=-185.92 +/- 159.97
Episode length: 16.02 +/- 5.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -186         |
| time/                   |              |
|    total_timesteps      | 721000       |
| train/                  |              |
|    approx_kl            | 0.0078008138 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.139       |
|    explained_variance   | 0.4          |
|    learning_rate        | 0.0001       |
|    loss                 | 3.97e+03     |
|    n_updates            | 625          |
|    policy_gradient_loss | 0.00525      |
|    value_loss           | 7.06e+03     |
------------------------------------------
Eval num_timesteps=721500, episode_reward=-170.81 +/- 140.88
Episode length: 16.64 +/- 5.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 721500   |
---------------------------------
Eval num_timesteps=722000, episode_reward=-171.92 +/- 135.06
Episode length: 16.04 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 722000   |
---------------------------------
Eval num_timesteps=722500, episode_reward=-151.02 +/- 132.80
Episode length: 17.06 +/- 5.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 722500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -209     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 353      |
|    time_elapsed    | 1494     |
|    total_timesteps | 722944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=723000, episode_reward=-175.64 +/- 132.06
Episode length: 15.18 +/- 5.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -176         |
| time/                   |              |
|    total_timesteps      | 723000       |
| train/                  |              |
|    approx_kl            | 0.0064206542 |
|    clip_fraction        | 0.0844       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.154       |
|    explained_variance   | 0.449        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.42e+03     |
|    n_updates            | 626          |
|    policy_gradient_loss | 0.00938      |
|    value_loss           | 7.26e+03     |
------------------------------------------
Eval num_timesteps=723500, episode_reward=-153.93 +/- 141.77
Episode length: 16.16 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 723500   |
---------------------------------
Eval num_timesteps=724000, episode_reward=-147.15 +/- 146.11
Episode length: 16.92 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 724000   |
---------------------------------
Eval num_timesteps=724500, episode_reward=-174.07 +/- 120.08
Episode length: 15.94 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 724500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 354      |
|    time_elapsed    | 1498     |
|    total_timesteps | 724992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=725000, episode_reward=-163.91 +/- 130.10
Episode length: 15.96 +/- 5.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -164        |
| time/                   |             |
|    total_timesteps      | 725000      |
| train/                  |             |
|    approx_kl            | 0.004880841 |
|    clip_fraction        | 0.0565      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.402       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.81e+03    |
|    n_updates            | 627         |
|    policy_gradient_loss | -0.000699   |
|    value_loss           | 5.92e+03    |
-----------------------------------------
Eval num_timesteps=725500, episode_reward=-161.49 +/- 141.63
Episode length: 16.68 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 725500   |
---------------------------------
Eval num_timesteps=726000, episode_reward=-150.23 +/- 143.57
Episode length: 16.62 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 726000   |
---------------------------------
Eval num_timesteps=726500, episode_reward=-157.37 +/- 139.33
Episode length: 16.58 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 726500   |
---------------------------------
Eval num_timesteps=727000, episode_reward=-186.05 +/- 124.98
Episode length: 15.78 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 727000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 355      |
|    time_elapsed    | 1503     |
|    total_timesteps | 727040   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.05
Eval num_timesteps=727500, episode_reward=-166.25 +/- 114.17
Episode length: 16.00 +/- 4.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -166        |
| time/                   |             |
|    total_timesteps      | 727500      |
| train/                  |             |
|    approx_kl            | 0.011831127 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.182      |
|    explained_variance   | 0.409       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.63e+03    |
|    n_updates            | 629         |
|    policy_gradient_loss | 0.00295     |
|    value_loss           | 5.77e+03    |
-----------------------------------------
Eval num_timesteps=728000, episode_reward=-190.60 +/- 124.79
Episode length: 15.02 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 728000   |
---------------------------------
Eval num_timesteps=728500, episode_reward=-159.67 +/- 136.43
Episode length: 17.38 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 728500   |
---------------------------------
Eval num_timesteps=729000, episode_reward=-174.77 +/- 152.08
Episode length: 16.40 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 729000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -196     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 356      |
|    time_elapsed    | 1507     |
|    total_timesteps | 729088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=729500, episode_reward=-193.53 +/- 149.78
Episode length: 16.32 +/- 5.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -194        |
| time/                   |             |
|    total_timesteps      | 729500      |
| train/                  |             |
|    approx_kl            | 0.009141871 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.36e+03    |
|    n_updates            | 630         |
|    policy_gradient_loss | 0.0118      |
|    value_loss           | 5.22e+03    |
-----------------------------------------
Eval num_timesteps=730000, episode_reward=-190.18 +/- 133.81
Episode length: 16.08 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 730000   |
---------------------------------
Eval num_timesteps=730500, episode_reward=-175.09 +/- 147.10
Episode length: 17.18 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 730500   |
---------------------------------
Eval num_timesteps=731000, episode_reward=-166.37 +/- 147.36
Episode length: 17.74 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 731000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -183     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 357      |
|    time_elapsed    | 1511     |
|    total_timesteps | 731136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=731500, episode_reward=-185.89 +/- 132.77
Episode length: 15.60 +/- 4.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -186         |
| time/                   |              |
|    total_timesteps      | 731500       |
| train/                  |              |
|    approx_kl            | 0.0051954747 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.145       |
|    explained_variance   | 0.434        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.05e+03     |
|    n_updates            | 631          |
|    policy_gradient_loss | 0.00557      |
|    value_loss           | 6.56e+03     |
------------------------------------------
Eval num_timesteps=732000, episode_reward=-166.32 +/- 152.40
Episode length: 16.88 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 732000   |
---------------------------------
Eval num_timesteps=732500, episode_reward=-202.58 +/- 108.76
Episode length: 14.72 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 732500   |
---------------------------------
Eval num_timesteps=733000, episode_reward=-166.81 +/- 124.69
Episode length: 16.78 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 733000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 358      |
|    time_elapsed    | 1515     |
|    total_timesteps | 733184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=733500, episode_reward=-146.71 +/- 135.24
Episode length: 17.02 +/- 5.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -147         |
| time/                   |              |
|    total_timesteps      | 733500       |
| train/                  |              |
|    approx_kl            | 0.0032904437 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.188       |
|    explained_variance   | 0.415        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.46e+03     |
|    n_updates            | 632          |
|    policy_gradient_loss | 0.00439      |
|    value_loss           | 5.63e+03     |
------------------------------------------
Eval num_timesteps=734000, episode_reward=-173.04 +/- 118.60
Episode length: 16.98 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 734000   |
---------------------------------
Eval num_timesteps=734500, episode_reward=-118.13 +/- 144.98
Episode length: 18.22 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 734500   |
---------------------------------
Eval num_timesteps=735000, episode_reward=-185.57 +/- 140.77
Episode length: 16.50 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 735000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 359      |
|    time_elapsed    | 1520     |
|    total_timesteps | 735232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=735500, episode_reward=-162.50 +/- 105.35
Episode length: 16.68 +/- 4.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -163        |
| time/                   |             |
|    total_timesteps      | 735500      |
| train/                  |             |
|    approx_kl            | 0.004301132 |
|    clip_fraction        | 0.0535      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.224      |
|    explained_variance   | 0.411       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.16e+03    |
|    n_updates            | 633         |
|    policy_gradient_loss | 0.00731     |
|    value_loss           | 5.9e+03     |
-----------------------------------------
Eval num_timesteps=736000, episode_reward=-153.98 +/- 129.24
Episode length: 16.72 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 736000   |
---------------------------------
Eval num_timesteps=736500, episode_reward=-142.92 +/- 133.55
Episode length: 16.92 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 736500   |
---------------------------------
Eval num_timesteps=737000, episode_reward=-146.85 +/- 130.65
Episode length: 15.70 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 737000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 360      |
|    time_elapsed    | 1524     |
|    total_timesteps | 737280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=737500, episode_reward=-161.00 +/- 146.54
Episode length: 16.52 +/- 5.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -161        |
| time/                   |             |
|    total_timesteps      | 737500      |
| train/                  |             |
|    approx_kl            | 0.004480597 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.235      |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.18e+03    |
|    n_updates            | 634         |
|    policy_gradient_loss | -0.00336    |
|    value_loss           | 6.82e+03    |
-----------------------------------------
Eval num_timesteps=738000, episode_reward=-171.69 +/- 175.42
Episode length: 16.04 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 738000   |
---------------------------------
Eval num_timesteps=738500, episode_reward=-144.86 +/- 126.73
Episode length: 16.34 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 738500   |
---------------------------------
Eval num_timesteps=739000, episode_reward=-166.12 +/- 135.59
Episode length: 17.28 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 739000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 361      |
|    time_elapsed    | 1528     |
|    total_timesteps | 739328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=739500, episode_reward=-191.17 +/- 108.10
Episode length: 16.14 +/- 4.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -191         |
| time/                   |              |
|    total_timesteps      | 739500       |
| train/                  |              |
|    approx_kl            | 0.0046072016 |
|    clip_fraction        | 0.0578       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | 0.416        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.23e+03     |
|    n_updates            | 635          |
|    policy_gradient_loss | 0.00276      |
|    value_loss           | 5.79e+03     |
------------------------------------------
Eval num_timesteps=740000, episode_reward=-196.03 +/- 149.73
Episode length: 16.18 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=740500, episode_reward=-176.22 +/- 142.58
Episode length: 16.24 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 740500   |
---------------------------------
Eval num_timesteps=741000, episode_reward=-160.69 +/- 127.35
Episode length: 17.06 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 741000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.9     |
|    ep_rew_mean     | -202     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 362      |
|    time_elapsed    | 1532     |
|    total_timesteps | 741376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=741500, episode_reward=-171.75 +/- 168.89
Episode length: 16.54 +/- 5.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -172        |
| time/                   |             |
|    total_timesteps      | 741500      |
| train/                  |             |
|    approx_kl            | 0.006472141 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.441       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.92e+03    |
|    n_updates            | 636         |
|    policy_gradient_loss | 0.000537    |
|    value_loss           | 5.07e+03    |
-----------------------------------------
Eval num_timesteps=742000, episode_reward=-163.23 +/- 133.71
Episode length: 16.46 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 742000   |
---------------------------------
Eval num_timesteps=742500, episode_reward=-192.64 +/- 137.13
Episode length: 16.40 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 742500   |
---------------------------------
Eval num_timesteps=743000, episode_reward=-187.93 +/- 149.87
Episode length: 15.52 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 743000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 363      |
|    time_elapsed    | 1536     |
|    total_timesteps | 743424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=743500, episode_reward=-161.54 +/- 148.60
Episode length: 17.48 +/- 4.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.5         |
|    mean_reward          | -162         |
| time/                   |              |
|    total_timesteps      | 743500       |
| train/                  |              |
|    approx_kl            | 0.0034664213 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.163       |
|    explained_variance   | 0.423        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.01e+03     |
|    n_updates            | 637          |
|    policy_gradient_loss | 0.00123      |
|    value_loss           | 5.85e+03     |
------------------------------------------
Eval num_timesteps=744000, episode_reward=-174.63 +/- 145.39
Episode length: 17.02 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 744000   |
---------------------------------
Eval num_timesteps=744500, episode_reward=-205.46 +/- 122.11
Episode length: 16.16 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 744500   |
---------------------------------
Eval num_timesteps=745000, episode_reward=-181.99 +/- 128.48
Episode length: 15.16 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 745000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 364      |
|    time_elapsed    | 1540     |
|    total_timesteps | 745472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=745500, episode_reward=-165.26 +/- 134.57
Episode length: 17.64 +/- 5.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.6        |
|    mean_reward          | -165        |
| time/                   |             |
|    total_timesteps      | 745500      |
| train/                  |             |
|    approx_kl            | 0.003900251 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.189      |
|    explained_variance   | 0.388       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.93e+03    |
|    n_updates            | 638         |
|    policy_gradient_loss | 0.00589     |
|    value_loss           | 6.31e+03    |
-----------------------------------------
Eval num_timesteps=746000, episode_reward=-143.32 +/- 128.91
Episode length: 17.62 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 746000   |
---------------------------------
Eval num_timesteps=746500, episode_reward=-141.54 +/- 136.13
Episode length: 17.30 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 746500   |
---------------------------------
Eval num_timesteps=747000, episode_reward=-174.99 +/- 124.79
Episode length: 17.00 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
Eval num_timesteps=747500, episode_reward=-158.46 +/- 142.95
Episode length: 17.32 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 747500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 365      |
|    time_elapsed    | 1545     |
|    total_timesteps | 747520   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=748000, episode_reward=-208.59 +/- 107.17
Episode length: 15.42 +/- 4.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | -209        |
| time/                   |             |
|    total_timesteps      | 748000      |
| train/                  |             |
|    approx_kl            | 0.008011504 |
|    clip_fraction        | 0.0282      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.192      |
|    explained_variance   | 0.422       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.25e+03    |
|    n_updates            | 640         |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 5.53e+03    |
-----------------------------------------
Eval num_timesteps=748500, episode_reward=-173.71 +/- 123.82
Episode length: 16.66 +/- 5.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 748500   |
---------------------------------
Eval num_timesteps=749000, episode_reward=-175.55 +/- 108.14
Episode length: 16.20 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 749000   |
---------------------------------
Eval num_timesteps=749500, episode_reward=-179.26 +/- 148.61
Episode length: 16.82 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 749500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 366      |
|    time_elapsed    | 1549     |
|    total_timesteps | 749568   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=750000, episode_reward=-174.95 +/- 125.54
Episode length: 16.68 +/- 4.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -175        |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.004510337 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.395       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.41e+03    |
|    n_updates            | 642         |
|    policy_gradient_loss | 0.0032      |
|    value_loss           | 5.56e+03    |
-----------------------------------------
Eval num_timesteps=750500, episode_reward=-185.28 +/- 135.37
Episode length: 16.22 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 750500   |
---------------------------------
Eval num_timesteps=751000, episode_reward=-141.94 +/- 154.98
Episode length: 17.76 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 751000   |
---------------------------------
Eval num_timesteps=751500, episode_reward=-201.98 +/- 130.89
Episode length: 15.90 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 751500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 367      |
|    time_elapsed    | 1554     |
|    total_timesteps | 751616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=752000, episode_reward=-181.64 +/- 158.89
Episode length: 16.18 +/- 5.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -182        |
| time/                   |             |
|    total_timesteps      | 752000      |
| train/                  |             |
|    approx_kl            | 0.003720117 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.42        |
|    learning_rate        | 0.0001      |
|    loss                 | 3.07e+03    |
|    n_updates            | 643         |
|    policy_gradient_loss | -0.00106    |
|    value_loss           | 5.52e+03    |
-----------------------------------------
Eval num_timesteps=752500, episode_reward=-184.87 +/- 121.05
Episode length: 16.02 +/- 4.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 752500   |
---------------------------------
Eval num_timesteps=753000, episode_reward=-171.88 +/- 134.13
Episode length: 16.50 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 753000   |
---------------------------------
Eval num_timesteps=753500, episode_reward=-150.19 +/- 134.47
Episode length: 17.44 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 753500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 368      |
|    time_elapsed    | 1558     |
|    total_timesteps | 753664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=754000, episode_reward=-180.08 +/- 126.94
Episode length: 15.46 +/- 4.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -180        |
| time/                   |             |
|    total_timesteps      | 754000      |
| train/                  |             |
|    approx_kl            | 0.005014116 |
|    clip_fraction        | 0.0664      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0.372       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.81e+03    |
|    n_updates            | 644         |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 5.31e+03    |
-----------------------------------------
Eval num_timesteps=754500, episode_reward=-161.87 +/- 142.04
Episode length: 16.62 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 754500   |
---------------------------------
Eval num_timesteps=755000, episode_reward=-158.10 +/- 157.84
Episode length: 16.44 +/- 4.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 755000   |
---------------------------------
Eval num_timesteps=755500, episode_reward=-169.82 +/- 137.09
Episode length: 16.48 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 755500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 369      |
|    time_elapsed    | 1562     |
|    total_timesteps | 755712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=756000, episode_reward=-152.29 +/- 151.76
Episode length: 16.92 +/- 5.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | -152        |
| time/                   |             |
|    total_timesteps      | 756000      |
| train/                  |             |
|    approx_kl            | 0.006257575 |
|    clip_fraction        | 0.0979      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.22       |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.49e+03    |
|    n_updates            | 645         |
|    policy_gradient_loss | 7.58e-05    |
|    value_loss           | 6.81e+03    |
-----------------------------------------
Eval num_timesteps=756500, episode_reward=-148.44 +/- 134.66
Episode length: 16.56 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 756500   |
---------------------------------
Eval num_timesteps=757000, episode_reward=-132.83 +/- 162.86
Episode length: 18.28 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.3     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 757000   |
---------------------------------
Eval num_timesteps=757500, episode_reward=-152.93 +/- 134.27
Episode length: 17.30 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 757500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 370      |
|    time_elapsed    | 1566     |
|    total_timesteps | 757760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=758000, episode_reward=-142.23 +/- 146.67
Episode length: 17.46 +/- 6.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.5         |
|    mean_reward          | -142         |
| time/                   |              |
|    total_timesteps      | 758000       |
| train/                  |              |
|    approx_kl            | 0.0061039575 |
|    clip_fraction        | 0.0833       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.257       |
|    explained_variance   | 0.369        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.82e+03     |
|    n_updates            | 646          |
|    policy_gradient_loss | 0.000724     |
|    value_loss           | 6.19e+03     |
------------------------------------------
Eval num_timesteps=758500, episode_reward=-163.96 +/- 137.55
Episode length: 17.52 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 758500   |
---------------------------------
Eval num_timesteps=759000, episode_reward=-195.20 +/- 126.75
Episode length: 15.56 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 759000   |
---------------------------------
Eval num_timesteps=759500, episode_reward=-195.81 +/- 108.93
Episode length: 15.66 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 759500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.1     |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 371      |
|    time_elapsed    | 1570     |
|    total_timesteps | 759808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=760000, episode_reward=-156.70 +/- 130.32
Episode length: 15.74 +/- 4.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -157         |
| time/                   |              |
|    total_timesteps      | 760000       |
| train/                  |              |
|    approx_kl            | 0.0043421797 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.36         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.61e+03     |
|    n_updates            | 647          |
|    policy_gradient_loss | -6.69e-07    |
|    value_loss           | 6.99e+03     |
------------------------------------------
Eval num_timesteps=760500, episode_reward=-158.04 +/- 130.11
Episode length: 16.48 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 760500   |
---------------------------------
Eval num_timesteps=761000, episode_reward=-202.19 +/- 142.38
Episode length: 15.78 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 761000   |
---------------------------------
Eval num_timesteps=761500, episode_reward=-132.42 +/- 156.61
Episode length: 16.54 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 761500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 372      |
|    time_elapsed    | 1574     |
|    total_timesteps | 761856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=762000, episode_reward=-173.49 +/- 116.59
Episode length: 16.26 +/- 4.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -173         |
| time/                   |              |
|    total_timesteps      | 762000       |
| train/                  |              |
|    approx_kl            | 0.0028141313 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.193       |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.66e+03     |
|    n_updates            | 648          |
|    policy_gradient_loss | 0.00102      |
|    value_loss           | 6.71e+03     |
------------------------------------------
Eval num_timesteps=762500, episode_reward=-192.10 +/- 111.08
Episode length: 16.10 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 762500   |
---------------------------------
Eval num_timesteps=763000, episode_reward=-175.50 +/- 151.12
Episode length: 17.08 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 763000   |
---------------------------------
Eval num_timesteps=763500, episode_reward=-170.13 +/- 135.38
Episode length: 16.12 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 763500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -192     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 373      |
|    time_elapsed    | 1579     |
|    total_timesteps | 763904   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=764000, episode_reward=-153.14 +/- 136.98
Episode length: 18.10 +/- 5.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.1         |
|    mean_reward          | -153         |
| time/                   |              |
|    total_timesteps      | 764000       |
| train/                  |              |
|    approx_kl            | 0.0043916474 |
|    clip_fraction        | 0.0303       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.168       |
|    explained_variance   | 0.41         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.52e+03     |
|    n_updates            | 650          |
|    policy_gradient_loss | 0.00258      |
|    value_loss           | 6.52e+03     |
------------------------------------------
Eval num_timesteps=764500, episode_reward=-156.83 +/- 122.69
Episode length: 16.64 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 764500   |
---------------------------------
Eval num_timesteps=765000, episode_reward=-151.36 +/- 157.74
Episode length: 17.22 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 765000   |
---------------------------------
Eval num_timesteps=765500, episode_reward=-177.27 +/- 145.86
Episode length: 16.52 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 765500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -179     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 374      |
|    time_elapsed    | 1583     |
|    total_timesteps | 765952   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=766000, episode_reward=-151.24 +/- 144.30
Episode length: 16.98 +/- 5.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -151         |
| time/                   |              |
|    total_timesteps      | 766000       |
| train/                  |              |
|    approx_kl            | 0.0028911512 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.158       |
|    explained_variance   | 0.442        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.62e+03     |
|    n_updates            | 651          |
|    policy_gradient_loss | 0.00112      |
|    value_loss           | 6.58e+03     |
------------------------------------------
Eval num_timesteps=766500, episode_reward=-174.40 +/- 131.34
Episode length: 16.74 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 766500   |
---------------------------------
Eval num_timesteps=767000, episode_reward=-182.99 +/- 131.57
Episode length: 15.68 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 767000   |
---------------------------------
Eval num_timesteps=767500, episode_reward=-182.25 +/- 141.31
Episode length: 16.14 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 767500   |
---------------------------------
Eval num_timesteps=768000, episode_reward=-207.18 +/- 137.07
Episode length: 16.00 +/- 4.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 375      |
|    time_elapsed    | 1588     |
|    total_timesteps | 768000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=768500, episode_reward=-118.23 +/- 122.35
Episode length: 17.88 +/- 5.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.9        |
|    mean_reward          | -118        |
| time/                   |             |
|    total_timesteps      | 768500      |
| train/                  |             |
|    approx_kl            | 0.007521527 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.422       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.77e+03    |
|    n_updates            | 652         |
|    policy_gradient_loss | 0.00276     |
|    value_loss           | 6.36e+03    |
-----------------------------------------
Eval num_timesteps=769000, episode_reward=-174.26 +/- 130.39
Episode length: 15.20 +/- 4.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 769000   |
---------------------------------
Eval num_timesteps=769500, episode_reward=-176.63 +/- 143.48
Episode length: 15.86 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 769500   |
---------------------------------
Eval num_timesteps=770000, episode_reward=-171.26 +/- 141.56
Episode length: 15.88 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 770000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 376      |
|    time_elapsed    | 1592     |
|    total_timesteps | 770048   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=770500, episode_reward=-188.86 +/- 128.52
Episode length: 15.34 +/- 4.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.3       |
|    mean_reward          | -189       |
| time/                   |            |
|    total_timesteps      | 770500     |
| train/                  |            |
|    approx_kl            | 0.00359347 |
|    clip_fraction        | 0.0493     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.162     |
|    explained_variance   | 0.44       |
|    learning_rate        | 0.0001     |
|    loss                 | 3.13e+03   |
|    n_updates            | 653        |
|    policy_gradient_loss | 0.000559   |
|    value_loss           | 5.94e+03   |
----------------------------------------
Eval num_timesteps=771000, episode_reward=-170.24 +/- 160.48
Episode length: 16.60 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 771000   |
---------------------------------
Eval num_timesteps=771500, episode_reward=-165.84 +/- 171.66
Episode length: 16.30 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 771500   |
---------------------------------
Eval num_timesteps=772000, episode_reward=-171.73 +/- 133.90
Episode length: 16.26 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 772000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 377      |
|    time_elapsed    | 1596     |
|    total_timesteps | 772096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=772500, episode_reward=-137.51 +/- 172.33
Episode length: 15.76 +/- 5.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -138         |
| time/                   |              |
|    total_timesteps      | 772500       |
| train/                  |              |
|    approx_kl            | 0.0053291423 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.174       |
|    explained_variance   | 0.421        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.15e+03     |
|    n_updates            | 654          |
|    policy_gradient_loss | 0.00139      |
|    value_loss           | 5.48e+03     |
------------------------------------------
Eval num_timesteps=773000, episode_reward=-108.54 +/- 187.50
Episode length: 17.22 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 773000   |
---------------------------------
Eval num_timesteps=773500, episode_reward=-151.35 +/- 152.32
Episode length: 16.36 +/- 5.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 773500   |
---------------------------------
Eval num_timesteps=774000, episode_reward=-117.85 +/- 116.88
Episode length: 16.52 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 774000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 378      |
|    time_elapsed    | 1600     |
|    total_timesteps | 774144   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=774500, episode_reward=-144.43 +/- 153.43
Episode length: 16.92 +/- 5.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | -144        |
| time/                   |             |
|    total_timesteps      | 774500      |
| train/                  |             |
|    approx_kl            | 0.008488994 |
|    clip_fraction        | 0.0429      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.427       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.36e+03    |
|    n_updates            | 656         |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 5.25e+03    |
-----------------------------------------
Eval num_timesteps=775000, episode_reward=-160.48 +/- 124.03
Episode length: 15.62 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 775000   |
---------------------------------
Eval num_timesteps=775500, episode_reward=-157.22 +/- 117.76
Episode length: 16.86 +/- 4.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 775500   |
---------------------------------
Eval num_timesteps=776000, episode_reward=-169.09 +/- 147.60
Episode length: 16.54 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 776000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 379      |
|    time_elapsed    | 1604     |
|    total_timesteps | 776192   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=776500, episode_reward=-147.84 +/- 141.47
Episode length: 17.20 +/- 5.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | -148         |
| time/                   |              |
|    total_timesteps      | 776500       |
| train/                  |              |
|    approx_kl            | 0.0035938188 |
|    clip_fraction        | 0.0605       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.168       |
|    explained_variance   | 0.374        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.99e+03     |
|    n_updates            | 657          |
|    policy_gradient_loss | 0.000668     |
|    value_loss           | 6.11e+03     |
------------------------------------------
Eval num_timesteps=777000, episode_reward=-191.92 +/- 140.39
Episode length: 15.38 +/- 4.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 777000   |
---------------------------------
Eval num_timesteps=777500, episode_reward=-157.18 +/- 135.95
Episode length: 17.44 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 777500   |
---------------------------------
Eval num_timesteps=778000, episode_reward=-161.66 +/- 156.49
Episode length: 16.64 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 778000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 380      |
|    time_elapsed    | 1608     |
|    total_timesteps | 778240   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=778500, episode_reward=-167.98 +/- 160.34
Episode length: 16.00 +/- 5.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -168         |
| time/                   |              |
|    total_timesteps      | 778500       |
| train/                  |              |
|    approx_kl            | 0.0031693203 |
|    clip_fraction        | 0.0431       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | 0.396        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.85e+03     |
|    n_updates            | 659          |
|    policy_gradient_loss | 0.000358     |
|    value_loss           | 6.03e+03     |
------------------------------------------
Eval num_timesteps=779000, episode_reward=-196.51 +/- 120.52
Episode length: 14.90 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 779000   |
---------------------------------
Eval num_timesteps=779500, episode_reward=-142.25 +/- 152.54
Episode length: 16.40 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 779500   |
---------------------------------
Eval num_timesteps=780000, episode_reward=-194.82 +/- 161.72
Episode length: 16.44 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 780000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 381      |
|    time_elapsed    | 1613     |
|    total_timesteps | 780288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=780500, episode_reward=-163.65 +/- 124.38
Episode length: 15.40 +/- 4.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -164         |
| time/                   |              |
|    total_timesteps      | 780500       |
| train/                  |              |
|    approx_kl            | 0.0052961065 |
|    clip_fraction        | 0.0813       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.173       |
|    explained_variance   | 0.446        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.99e+03     |
|    n_updates            | 660          |
|    policy_gradient_loss | 0.00456      |
|    value_loss           | 5.2e+03      |
------------------------------------------
Eval num_timesteps=781000, episode_reward=-133.08 +/- 113.74
Episode length: 16.58 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 781000   |
---------------------------------
Eval num_timesteps=781500, episode_reward=-118.68 +/- 170.12
Episode length: 17.00 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 781500   |
---------------------------------
Eval num_timesteps=782000, episode_reward=-135.47 +/- 180.87
Episode length: 16.66 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 782000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 382      |
|    time_elapsed    | 1617     |
|    total_timesteps | 782336   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=782500, episode_reward=-164.58 +/- 139.72
Episode length: 16.44 +/- 5.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -165        |
| time/                   |             |
|    total_timesteps      | 782500      |
| train/                  |             |
|    approx_kl            | 0.004513655 |
|    clip_fraction        | 0.0534      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.38e+03    |
|    n_updates            | 661         |
|    policy_gradient_loss | 0.00866     |
|    value_loss           | 6.94e+03    |
-----------------------------------------
Eval num_timesteps=783000, episode_reward=-160.22 +/- 122.72
Episode length: 17.14 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 783000   |
---------------------------------
Eval num_timesteps=783500, episode_reward=-142.08 +/- 148.18
Episode length: 16.76 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 783500   |
---------------------------------
Eval num_timesteps=784000, episode_reward=-196.12 +/- 130.98
Episode length: 15.56 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 784000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 383      |
|    time_elapsed    | 1621     |
|    total_timesteps | 784384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=784500, episode_reward=-177.21 +/- 131.72
Episode length: 14.02 +/- 4.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14          |
|    mean_reward          | -177        |
| time/                   |             |
|    total_timesteps      | 784500      |
| train/                  |             |
|    approx_kl            | 0.003212468 |
|    clip_fraction        | 0.032       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.206      |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.05e+03    |
|    n_updates            | 662         |
|    policy_gradient_loss | 0.000652    |
|    value_loss           | 5.72e+03    |
-----------------------------------------
Eval num_timesteps=785000, episode_reward=-158.65 +/- 139.67
Episode length: 15.64 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 785000   |
---------------------------------
Eval num_timesteps=785500, episode_reward=-149.96 +/- 148.63
Episode length: 15.60 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 785500   |
---------------------------------
Eval num_timesteps=786000, episode_reward=-157.08 +/- 170.42
Episode length: 16.10 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 786000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -146     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 384      |
|    time_elapsed    | 1625     |
|    total_timesteps | 786432   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=786500, episode_reward=-136.60 +/- 164.60
Episode length: 16.02 +/- 5.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -137        |
| time/                   |             |
|    total_timesteps      | 786500      |
| train/                  |             |
|    approx_kl            | 0.008004445 |
|    clip_fraction        | 0.0549      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.173      |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.11e+03    |
|    n_updates            | 664         |
|    policy_gradient_loss | 0.000256    |
|    value_loss           | 5.75e+03    |
-----------------------------------------
Eval num_timesteps=787000, episode_reward=-105.85 +/- 179.46
Episode length: 16.94 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 787000   |
---------------------------------
Eval num_timesteps=787500, episode_reward=-166.56 +/- 143.64
Episode length: 14.60 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 787500   |
---------------------------------
Eval num_timesteps=788000, episode_reward=-149.54 +/- 143.08
Episode length: 15.78 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 788000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 385      |
|    time_elapsed    | 1629     |
|    total_timesteps | 788480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=788500, episode_reward=-129.10 +/- 169.99
Episode length: 16.46 +/- 5.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -129         |
| time/                   |              |
|    total_timesteps      | 788500       |
| train/                  |              |
|    approx_kl            | 0.0023514952 |
|    clip_fraction        | 0.0286       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.16        |
|    explained_variance   | 0.37         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.9e+03      |
|    n_updates            | 665          |
|    policy_gradient_loss | -0.000738    |
|    value_loss           | 6.18e+03     |
------------------------------------------
Eval num_timesteps=789000, episode_reward=-120.96 +/- 113.29
Episode length: 16.72 +/- 4.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 789000   |
---------------------------------
Eval num_timesteps=789500, episode_reward=-149.29 +/- 144.34
Episode length: 15.54 +/- 4.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 789500   |
---------------------------------
Eval num_timesteps=790000, episode_reward=-176.21 +/- 117.30
Episode length: 15.12 +/- 4.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 790000   |
---------------------------------
Eval num_timesteps=790500, episode_reward=-148.86 +/- 168.33
Episode length: 16.22 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 790500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 386      |
|    time_elapsed    | 1634     |
|    total_timesteps | 790528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=791000, episode_reward=-176.38 +/- 116.96
Episode length: 15.02 +/- 3.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | -176         |
| time/                   |              |
|    total_timesteps      | 791000       |
| train/                  |              |
|    approx_kl            | 0.0037963092 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.1         |
|    explained_variance   | 0.363        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.45e+03     |
|    n_updates            | 666          |
|    policy_gradient_loss | 0.0027       |
|    value_loss           | 5.66e+03     |
------------------------------------------
Eval num_timesteps=791500, episode_reward=-145.77 +/- 194.85
Episode length: 16.70 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 791500   |
---------------------------------
Eval num_timesteps=792000, episode_reward=-146.86 +/- 178.36
Episode length: 15.66 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 792000   |
---------------------------------
Eval num_timesteps=792500, episode_reward=-147.09 +/- 128.47
Episode length: 15.66 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 792500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 387      |
|    time_elapsed    | 1638     |
|    total_timesteps | 792576   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=793000, episode_reward=-130.10 +/- 317.84
Episode length: 15.72 +/- 6.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -130         |
| time/                   |              |
|    total_timesteps      | 793000       |
| train/                  |              |
|    approx_kl            | 0.0057671806 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0934      |
|    explained_variance   | 0.355        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.08e+03     |
|    n_updates            | 668          |
|    policy_gradient_loss | 0.00381      |
|    value_loss           | 6.13e+03     |
------------------------------------------
Eval num_timesteps=793500, episode_reward=-171.21 +/- 177.28
Episode length: 15.68 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 793500   |
---------------------------------
Eval num_timesteps=794000, episode_reward=-153.95 +/- 152.80
Episode length: 15.40 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 794000   |
---------------------------------
Eval num_timesteps=794500, episode_reward=-149.16 +/- 113.83
Episode length: 15.52 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 794500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 388      |
|    time_elapsed    | 1642     |
|    total_timesteps | 794624   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=795000, episode_reward=-136.71 +/- 169.58
Episode length: 15.88 +/- 5.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -137         |
| time/                   |              |
|    total_timesteps      | 795000       |
| train/                  |              |
|    approx_kl            | 0.0034974008 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.083       |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.82e+03     |
|    n_updates            | 669          |
|    policy_gradient_loss | 0.00273      |
|    value_loss           | 6.54e+03     |
------------------------------------------
Eval num_timesteps=795500, episode_reward=-141.14 +/- 164.88
Episode length: 16.18 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 795500   |
---------------------------------
Eval num_timesteps=796000, episode_reward=-143.94 +/- 135.85
Episode length: 15.56 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 796000   |
---------------------------------
Eval num_timesteps=796500, episode_reward=-165.83 +/- 121.47
Episode length: 15.30 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 796500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 389      |
|    time_elapsed    | 1646     |
|    total_timesteps | 796672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=797000, episode_reward=-116.36 +/- 161.56
Episode length: 17.28 +/- 5.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | -116        |
| time/                   |             |
|    total_timesteps      | 797000      |
| train/                  |             |
|    approx_kl            | 0.004571306 |
|    clip_fraction        | 0.0252      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.387       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.73e+03    |
|    n_updates            | 670         |
|    policy_gradient_loss | 0.003       |
|    value_loss           | 6.05e+03    |
-----------------------------------------
Eval num_timesteps=797500, episode_reward=-118.98 +/- 157.24
Episode length: 16.18 +/- 5.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 797500   |
---------------------------------
Eval num_timesteps=798000, episode_reward=-137.34 +/- 151.50
Episode length: 16.48 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 798000   |
---------------------------------
Eval num_timesteps=798500, episode_reward=-174.96 +/- 136.64
Episode length: 15.80 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 798500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 390      |
|    time_elapsed    | 1650     |
|    total_timesteps | 798720   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=799000, episode_reward=-118.23 +/- 145.42
Episode length: 16.28 +/- 5.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -118        |
| time/                   |             |
|    total_timesteps      | 799000      |
| train/                  |             |
|    approx_kl            | 0.008904235 |
|    clip_fraction        | 0.0234      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.07e+03    |
|    n_updates            | 672         |
|    policy_gradient_loss | 0.00312     |
|    value_loss           | 5.62e+03    |
-----------------------------------------
Eval num_timesteps=799500, episode_reward=-151.27 +/- 114.69
Episode length: 15.02 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 799500   |
---------------------------------
Eval num_timesteps=800000, episode_reward=-147.31 +/- 124.96
Episode length: 15.44 +/- 4.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=800500, episode_reward=-142.10 +/- 132.02
Episode length: 16.56 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 800500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 391      |
|    time_elapsed    | 1654     |
|    total_timesteps | 800768   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=801000, episode_reward=-169.47 +/- 132.47
Episode length: 15.64 +/- 4.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -169         |
| time/                   |              |
|    total_timesteps      | 801000       |
| train/                  |              |
|    approx_kl            | 0.0023463462 |
|    clip_fraction        | 0.0026       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.186       |
|    explained_variance   | 0.354        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.66e+03     |
|    n_updates            | 673          |
|    policy_gradient_loss | 0.00361      |
|    value_loss           | 6.22e+03     |
------------------------------------------
Eval num_timesteps=801500, episode_reward=-164.82 +/- 146.06
Episode length: 15.34 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 801500   |
---------------------------------
Eval num_timesteps=802000, episode_reward=-150.36 +/- 155.03
Episode length: 16.38 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 802000   |
---------------------------------
Eval num_timesteps=802500, episode_reward=-133.08 +/- 140.46
Episode length: 16.30 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 802500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 392      |
|    time_elapsed    | 1659     |
|    total_timesteps | 802816   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.53
Eval num_timesteps=803000, episode_reward=-160.07 +/- 139.77
Episode length: 15.60 +/- 4.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.6       |
|    mean_reward          | -160       |
| time/                   |            |
|    total_timesteps      | 803000     |
| train/                  |            |
|    approx_kl            | 0.02089876 |
|    clip_fraction        | 0.037      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.195     |
|    explained_variance   | 0.346      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.62e+03   |
|    n_updates            | 674        |
|    policy_gradient_loss | 0.0296     |
|    value_loss           | 6.56e+03   |
----------------------------------------
Eval num_timesteps=803500, episode_reward=-154.84 +/- 153.97
Episode length: 16.38 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 803500   |
---------------------------------
Eval num_timesteps=804000, episode_reward=-166.83 +/- 116.86
Episode length: 15.50 +/- 3.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 804000   |
---------------------------------
Eval num_timesteps=804500, episode_reward=-163.34 +/- 145.37
Episode length: 15.70 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 804500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 393      |
|    time_elapsed    | 1663     |
|    total_timesteps | 804864   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=805000, episode_reward=-161.54 +/- 132.14
Episode length: 15.46 +/- 4.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -162        |
| time/                   |             |
|    total_timesteps      | 805000      |
| train/                  |             |
|    approx_kl            | 0.005371223 |
|    clip_fraction        | 0.0844      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.25       |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.97e+03    |
|    n_updates            | 675         |
|    policy_gradient_loss | 0.00116     |
|    value_loss           | 5.96e+03    |
-----------------------------------------
Eval num_timesteps=805500, episode_reward=-158.32 +/- 121.11
Episode length: 15.60 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 805500   |
---------------------------------
Eval num_timesteps=806000, episode_reward=-205.86 +/- 130.06
Episode length: 13.94 +/- 3.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.9     |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 806000   |
---------------------------------
Eval num_timesteps=806500, episode_reward=-187.69 +/- 128.80
Episode length: 14.68 +/- 3.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 806500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 394      |
|    time_elapsed    | 1667     |
|    total_timesteps | 806912   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=807000, episode_reward=-177.70 +/- 167.28
Episode length: 15.04 +/- 4.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15         |
|    mean_reward          | -178       |
| time/                   |            |
|    total_timesteps      | 807000     |
| train/                  |            |
|    approx_kl            | 0.00529608 |
|    clip_fraction        | 0.026      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.119     |
|    explained_variance   | 0.374      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.48e+03   |
|    n_updates            | 676        |
|    policy_gradient_loss | 0.00358    |
|    value_loss           | 5.6e+03    |
----------------------------------------
Eval num_timesteps=807500, episode_reward=-151.03 +/- 153.68
Episode length: 16.08 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 807500   |
---------------------------------
Eval num_timesteps=808000, episode_reward=-144.51 +/- 152.30
Episode length: 16.04 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 808000   |
---------------------------------
Eval num_timesteps=808500, episode_reward=-180.47 +/- 134.08
Episode length: 15.10 +/- 3.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 808500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 395      |
|    time_elapsed    | 1671     |
|    total_timesteps | 808960   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=809000, episode_reward=-152.40 +/- 158.57
Episode length: 16.00 +/- 5.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -152        |
| time/                   |             |
|    total_timesteps      | 809000      |
| train/                  |             |
|    approx_kl            | 0.004250653 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0972     |
|    explained_variance   | 0.319       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.88e+03    |
|    n_updates            | 678         |
|    policy_gradient_loss | 0.00325     |
|    value_loss           | 6.97e+03    |
-----------------------------------------
Eval num_timesteps=809500, episode_reward=-161.67 +/- 192.55
Episode length: 15.92 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 809500   |
---------------------------------
Eval num_timesteps=810000, episode_reward=-109.52 +/- 157.34
Episode length: 17.42 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 810000   |
---------------------------------
Eval num_timesteps=810500, episode_reward=-141.52 +/- 162.95
Episode length: 16.22 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 810500   |
---------------------------------
Eval num_timesteps=811000, episode_reward=-161.73 +/- 158.98
Episode length: 16.08 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 811000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 396      |
|    time_elapsed    | 1676     |
|    total_timesteps | 811008   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=811500, episode_reward=-119.78 +/- 168.57
Episode length: 16.84 +/- 5.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | -120        |
| time/                   |             |
|    total_timesteps      | 811500      |
| train/                  |             |
|    approx_kl            | 0.009741602 |
|    clip_fraction        | 0.0326      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.174      |
|    explained_variance   | 0.331       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.72e+03    |
|    n_updates            | 680         |
|    policy_gradient_loss | 0.00487     |
|    value_loss           | 5.9e+03     |
-----------------------------------------
Eval num_timesteps=812000, episode_reward=-131.60 +/- 184.95
Episode length: 16.48 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 812000   |
---------------------------------
Eval num_timesteps=812500, episode_reward=-109.10 +/- 130.97
Episode length: 16.64 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 812500   |
---------------------------------
Eval num_timesteps=813000, episode_reward=-140.35 +/- 154.52
Episode length: 16.30 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 813000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 397      |
|    time_elapsed    | 1680     |
|    total_timesteps | 813056   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=813500, episode_reward=-138.73 +/- 125.76
Episode length: 15.98 +/- 4.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -139        |
| time/                   |             |
|    total_timesteps      | 813500      |
| train/                  |             |
|    approx_kl            | 0.005798443 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.362       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.12e+03    |
|    n_updates            | 682         |
|    policy_gradient_loss | 0.00208     |
|    value_loss           | 6.54e+03    |
-----------------------------------------
Eval num_timesteps=814000, episode_reward=-144.36 +/- 179.67
Episode length: 16.66 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 814000   |
---------------------------------
Eval num_timesteps=814500, episode_reward=-155.23 +/- 152.55
Episode length: 15.98 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 814500   |
---------------------------------
Eval num_timesteps=815000, episode_reward=-135.56 +/- 159.15
Episode length: 15.62 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 815000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 398      |
|    time_elapsed    | 1684     |
|    total_timesteps | 815104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=815500, episode_reward=-167.10 +/- 160.71
Episode length: 15.72 +/- 4.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -167         |
| time/                   |              |
|    total_timesteps      | 815500       |
| train/                  |              |
|    approx_kl            | 0.0054830015 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0733      |
|    explained_variance   | 0.372        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.15e+03     |
|    n_updates            | 683          |
|    policy_gradient_loss | -0.00304     |
|    value_loss           | 6.13e+03     |
------------------------------------------
Eval num_timesteps=816000, episode_reward=-178.51 +/- 153.88
Episode length: 15.26 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 816000   |
---------------------------------
Eval num_timesteps=816500, episode_reward=-162.77 +/- 165.89
Episode length: 15.40 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 816500   |
---------------------------------
Eval num_timesteps=817000, episode_reward=-146.40 +/- 131.05
Episode length: 15.32 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 817000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -147     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 399      |
|    time_elapsed    | 1688     |
|    total_timesteps | 817152   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=817500, episode_reward=-135.37 +/- 189.16
Episode length: 17.44 +/- 6.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.4        |
|    mean_reward          | -135        |
| time/                   |             |
|    total_timesteps      | 817500      |
| train/                  |             |
|    approx_kl            | 0.004965979 |
|    clip_fraction        | 0.01        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0229     |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.34e+03    |
|    n_updates            | 685         |
|    policy_gradient_loss | 0.00194     |
|    value_loss           | 5.75e+03    |
-----------------------------------------
Eval num_timesteps=818000, episode_reward=-150.41 +/- 111.14
Episode length: 16.00 +/- 4.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 818000   |
---------------------------------
Eval num_timesteps=818500, episode_reward=-101.91 +/- 177.92
Episode length: 17.70 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -102     |
| time/              |          |
|    total_timesteps | 818500   |
---------------------------------
Eval num_timesteps=819000, episode_reward=-167.24 +/- 132.11
Episode length: 14.74 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 819000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 400      |
|    time_elapsed    | 1692     |
|    total_timesteps | 819200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=819500, episode_reward=-169.18 +/- 178.26
Episode length: 14.60 +/- 5.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | -169         |
| time/                   |              |
|    total_timesteps      | 819500       |
| train/                  |              |
|    approx_kl            | 0.0020902194 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.037       |
|    explained_variance   | 0.385        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.14e+03     |
|    n_updates            | 686          |
|    policy_gradient_loss | 0.00123      |
|    value_loss           | 6.4e+03      |
------------------------------------------
Eval num_timesteps=820000, episode_reward=-147.23 +/- 153.55
Episode length: 15.94 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=820500, episode_reward=-167.41 +/- 150.39
Episode length: 16.10 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 820500   |
---------------------------------
Eval num_timesteps=821000, episode_reward=-149.54 +/- 141.18
Episode length: 15.50 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 821000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 401      |
|    time_elapsed    | 1696     |
|    total_timesteps | 821248   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=821500, episode_reward=-133.05 +/- 146.15
Episode length: 16.38 +/- 4.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -133         |
| time/                   |              |
|    total_timesteps      | 821500       |
| train/                  |              |
|    approx_kl            | 0.0023704355 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0685      |
|    explained_variance   | 0.385        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.56e+03     |
|    n_updates            | 687          |
|    policy_gradient_loss | 0.00151      |
|    value_loss           | 6.11e+03     |
------------------------------------------
Eval num_timesteps=822000, episode_reward=-111.37 +/- 145.60
Episode length: 17.18 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 822000   |
---------------------------------
Eval num_timesteps=822500, episode_reward=-153.82 +/- 161.69
Episode length: 15.72 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 822500   |
---------------------------------
Eval num_timesteps=823000, episode_reward=-136.87 +/- 152.39
Episode length: 16.34 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 823000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 402      |
|    time_elapsed    | 1701     |
|    total_timesteps | 823296   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
Eval num_timesteps=823500, episode_reward=-132.30 +/- 146.35
Episode length: 17.00 +/- 4.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -132         |
| time/                   |              |
|    total_timesteps      | 823500       |
| train/                  |              |
|    approx_kl            | 0.0012478984 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0206      |
|    explained_variance   | 0.369        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.23e+03     |
|    n_updates            | 691          |
|    policy_gradient_loss | 0.000371     |
|    value_loss           | 6.65e+03     |
------------------------------------------
Eval num_timesteps=824000, episode_reward=-184.77 +/- 146.27
Episode length: 15.52 +/- 3.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 824000   |
---------------------------------
Eval num_timesteps=824500, episode_reward=-171.97 +/- 130.28
Episode length: 14.94 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 824500   |
---------------------------------
Eval num_timesteps=825000, episode_reward=-178.57 +/- 117.60
Episode length: 15.46 +/- 4.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 825000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -142     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 403      |
|    time_elapsed    | 1705     |
|    total_timesteps | 825344   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=825500, episode_reward=-164.16 +/- 143.52
Episode length: 14.38 +/- 4.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.4         |
|    mean_reward          | -164         |
| time/                   |              |
|    total_timesteps      | 825500       |
| train/                  |              |
|    approx_kl            | 0.0036934575 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0726      |
|    explained_variance   | 0.372        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.6e+03      |
|    n_updates            | 692          |
|    policy_gradient_loss | 0.00473      |
|    value_loss           | 5.83e+03     |
------------------------------------------
Eval num_timesteps=826000, episode_reward=-147.21 +/- 141.89
Episode length: 15.32 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 826000   |
---------------------------------
Eval num_timesteps=826500, episode_reward=-161.01 +/- 133.99
Episode length: 15.18 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 826500   |
---------------------------------
Eval num_timesteps=827000, episode_reward=-175.35 +/- 131.47
Episode length: 15.36 +/- 4.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 827000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 404      |
|    time_elapsed    | 1709     |
|    total_timesteps | 827392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=827500, episode_reward=-140.31 +/- 184.25
Episode length: 16.12 +/- 5.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -140         |
| time/                   |              |
|    total_timesteps      | 827500       |
| train/                  |              |
|    approx_kl            | 0.0021473428 |
|    clip_fraction        | 0.00417      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0133      |
|    explained_variance   | 0.314        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.12e+03     |
|    n_updates            | 693          |
|    policy_gradient_loss | -0.000158    |
|    value_loss           | 6.26e+03     |
------------------------------------------
Eval num_timesteps=828000, episode_reward=-206.04 +/- 164.13
Episode length: 15.26 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 828000   |
---------------------------------
Eval num_timesteps=828500, episode_reward=-149.94 +/- 157.98
Episode length: 15.52 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 828500   |
---------------------------------
Eval num_timesteps=829000, episode_reward=-157.96 +/- 143.11
Episode length: 16.06 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 829000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 405      |
|    time_elapsed    | 1713     |
|    total_timesteps | 829440   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=829500, episode_reward=-149.81 +/- 132.65
Episode length: 15.84 +/- 5.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -150         |
| time/                   |              |
|    total_timesteps      | 829500       |
| train/                  |              |
|    approx_kl            | 0.0008167851 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00501     |
|    explained_variance   | 0.329        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.37e+03     |
|    n_updates            | 695          |
|    policy_gradient_loss | 0.000591     |
|    value_loss           | 7.37e+03     |
------------------------------------------
Eval num_timesteps=830000, episode_reward=-134.43 +/- 131.02
Episode length: 16.92 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 830000   |
---------------------------------
Eval num_timesteps=830500, episode_reward=-171.13 +/- 119.66
Episode length: 15.68 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 830500   |
---------------------------------
Eval num_timesteps=831000, episode_reward=-175.17 +/- 115.01
Episode length: 15.36 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 831000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 406      |
|    time_elapsed    | 1717     |
|    total_timesteps | 831488   |
---------------------------------
Eval num_timesteps=831500, episode_reward=-150.88 +/- 126.36
Episode length: 15.66 +/- 4.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.7          |
|    mean_reward          | -151          |
| time/                   |               |
|    total_timesteps      | 831500        |
| train/                  |               |
|    approx_kl            | 0.00045728983 |
|    clip_fraction        | 0.000439      |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000905     |
|    explained_variance   | 0.301         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.93e+03      |
|    n_updates            | 705           |
|    policy_gradient_loss | -0.000224     |
|    value_loss           | 6.63e+03      |
-------------------------------------------
Eval num_timesteps=832000, episode_reward=-161.71 +/- 140.15
Episode length: 15.54 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 832000   |
---------------------------------
Eval num_timesteps=832500, episode_reward=-180.71 +/- 127.30
Episode length: 15.42 +/- 4.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 832500   |
---------------------------------
Eval num_timesteps=833000, episode_reward=-185.52 +/- 166.68
Episode length: 15.24 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 833000   |
---------------------------------
Eval num_timesteps=833500, episode_reward=-160.82 +/- 143.66
Episode length: 15.28 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 833500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -156     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 407      |
|    time_elapsed    | 1722     |
|    total_timesteps | 833536   |
---------------------------------
Eval num_timesteps=834000, episode_reward=-189.93 +/- 188.97
Episode length: 15.22 +/- 5.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | -190          |
| time/                   |               |
|    total_timesteps      | 834000        |
| train/                  |               |
|    approx_kl            | 1.2223609e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -7.47e-05     |
|    explained_variance   | 0.446         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.35e+03      |
|    n_updates            | 715           |
|    policy_gradient_loss | 2.79e-06      |
|    value_loss           | 5.39e+03      |
-------------------------------------------
Eval num_timesteps=834500, episode_reward=-156.51 +/- 111.04
Episode length: 15.10 +/- 3.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 834500   |
---------------------------------
Eval num_timesteps=835000, episode_reward=-107.59 +/- 132.78
Episode length: 16.84 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 835000   |
---------------------------------
Eval num_timesteps=835500, episode_reward=-165.67 +/- 139.71
Episode length: 15.10 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 835500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 408      |
|    time_elapsed    | 1727     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=836000, episode_reward=-107.03 +/- 159.97
Episode length: 17.26 +/- 5.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 17.3      |
|    mean_reward          | -107      |
| time/                   |           |
|    total_timesteps      | 836000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.95e-05 |
|    explained_variance   | 0.366     |
|    learning_rate        | 0.0001    |
|    loss                 | 3.38e+03  |
|    n_updates            | 725       |
|    policy_gradient_loss | -3.28e-07 |
|    value_loss           | 6.4e+03   |
---------------------------------------
Eval num_timesteps=836500, episode_reward=-123.67 +/- 148.98
Episode length: 16.52 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 836500   |
---------------------------------
Eval num_timesteps=837000, episode_reward=-189.13 +/- 127.94
Episode length: 15.12 +/- 4.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 837000   |
---------------------------------
Eval num_timesteps=837500, episode_reward=-159.40 +/- 132.52
Episode length: 16.26 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 837500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 409      |
|    time_elapsed    | 1732     |
|    total_timesteps | 837632   |
---------------------------------
Eval num_timesteps=838000, episode_reward=-119.51 +/- 155.52
Episode length: 16.68 +/- 5.67
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 16.7           |
|    mean_reward          | -120           |
| time/                   |                |
|    total_timesteps      | 838000         |
| train/                  |                |
|    approx_kl            | -1.4842954e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -5.12e-05      |
|    explained_variance   | 0.418          |
|    learning_rate        | 0.0001         |
|    loss                 | 3.3e+03        |
|    n_updates            | 735            |
|    policy_gradient_loss | -2.76e-07      |
|    value_loss           | 6.21e+03       |
--------------------------------------------
Eval num_timesteps=838500, episode_reward=-155.58 +/- 143.15
Episode length: 15.00 +/- 4.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 838500   |
---------------------------------
Eval num_timesteps=839000, episode_reward=-147.69 +/- 144.42
Episode length: 16.08 +/- 4.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 839000   |
---------------------------------
Eval num_timesteps=839500, episode_reward=-165.15 +/- 131.69
Episode length: 15.12 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 839500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 410      |
|    time_elapsed    | 1736     |
|    total_timesteps | 839680   |
---------------------------------
Eval num_timesteps=840000, episode_reward=-135.42 +/- 159.08
Episode length: 16.02 +/- 4.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -135          |
| time/                   |               |
|    total_timesteps      | 840000        |
| train/                  |               |
|    approx_kl            | 6.9849193e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00158      |
|    explained_variance   | 0.367         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.37e+03      |
|    n_updates            | 745           |
|    policy_gradient_loss | -4.96e-06     |
|    value_loss           | 5.93e+03      |
-------------------------------------------
Eval num_timesteps=840500, episode_reward=-160.49 +/- 184.16
Episode length: 16.04 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 840500   |
---------------------------------
Eval num_timesteps=841000, episode_reward=-164.83 +/- 136.87
Episode length: 15.56 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 841000   |
---------------------------------
Eval num_timesteps=841500, episode_reward=-117.72 +/- 150.29
Episode length: 17.48 +/- 4.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 841500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 411      |
|    time_elapsed    | 1741     |
|    total_timesteps | 841728   |
---------------------------------
Eval num_timesteps=842000, episode_reward=-111.28 +/- 137.76
Episode length: 16.24 +/- 4.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.2      |
|    mean_reward          | -111      |
| time/                   |           |
|    total_timesteps      | 842000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.000144 |
|    explained_variance   | 0.368     |
|    learning_rate        | 0.0001    |
|    loss                 | 3.78e+03  |
|    n_updates            | 755       |
|    policy_gradient_loss | 7.98e-07  |
|    value_loss           | 6.72e+03  |
---------------------------------------
Eval num_timesteps=842500, episode_reward=-167.36 +/- 142.11
Episode length: 15.34 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 842500   |
---------------------------------
Eval num_timesteps=843000, episode_reward=-146.35 +/- 155.96
Episode length: 15.78 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 843000   |
---------------------------------
Eval num_timesteps=843500, episode_reward=-179.11 +/- 128.02
Episode length: 16.14 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 843500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 412      |
|    time_elapsed    | 1746     |
|    total_timesteps | 843776   |
---------------------------------
Eval num_timesteps=844000, episode_reward=-166.01 +/- 116.07
Episode length: 15.70 +/- 4.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.7          |
|    mean_reward          | -166          |
| time/                   |               |
|    total_timesteps      | 844000        |
| train/                  |               |
|    approx_kl            | -8.440111e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000327     |
|    explained_variance   | 0.348         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.84e+03      |
|    n_updates            | 765           |
|    policy_gradient_loss | -8.07e-07     |
|    value_loss           | 6.43e+03      |
-------------------------------------------
Eval num_timesteps=844500, episode_reward=-137.92 +/- 172.21
Episode length: 16.04 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 844500   |
---------------------------------
Eval num_timesteps=845000, episode_reward=-188.59 +/- 135.99
Episode length: 15.34 +/- 4.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 845000   |
---------------------------------
Eval num_timesteps=845500, episode_reward=-133.84 +/- 170.57
Episode length: 17.02 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 845500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 413      |
|    time_elapsed    | 1750     |
|    total_timesteps | 845824   |
---------------------------------
Eval num_timesteps=846000, episode_reward=-151.03 +/- 186.57
Episode length: 16.06 +/- 5.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.1          |
|    mean_reward          | -151          |
| time/                   |               |
|    total_timesteps      | 846000        |
| train/                  |               |
|    approx_kl            | -5.529728e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000425     |
|    explained_variance   | 0.305         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.72e+03      |
|    n_updates            | 775           |
|    policy_gradient_loss | -1.38e-06     |
|    value_loss           | 7.25e+03      |
-------------------------------------------
Eval num_timesteps=846500, episode_reward=-126.81 +/- 142.62
Episode length: 15.80 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 846500   |
---------------------------------
Eval num_timesteps=847000, episode_reward=-148.56 +/- 152.23
Episode length: 16.38 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 847000   |
---------------------------------
Eval num_timesteps=847500, episode_reward=-193.04 +/- 127.15
Episode length: 15.56 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 847500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 414      |
|    time_elapsed    | 1755     |
|    total_timesteps | 847872   |
---------------------------------
Eval num_timesteps=848000, episode_reward=-130.67 +/- 142.47
Episode length: 15.48 +/- 4.53
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 15.5           |
|    mean_reward          | -131           |
| time/                   |                |
|    total_timesteps      | 848000         |
| train/                  |                |
|    approx_kl            | -6.1409082e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    clip_range_vf        | 0.2            |
|    entropy_loss         | -0.000255      |
|    explained_variance   | 0.303          |
|    learning_rate        | 0.0001         |
|    loss                 | 2.82e+03       |
|    n_updates            | 785            |
|    policy_gradient_loss | -5.56e-07      |
|    value_loss           | 6.26e+03       |
--------------------------------------------
Eval num_timesteps=848500, episode_reward=-175.26 +/- 138.78
Episode length: 15.26 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 848500   |
---------------------------------
Eval num_timesteps=849000, episode_reward=-160.95 +/- 121.14
Episode length: 15.94 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 849000   |
---------------------------------
Eval num_timesteps=849500, episode_reward=-139.32 +/- 166.72
Episode length: 16.34 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 849500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 415      |
|    time_elapsed    | 1760     |
|    total_timesteps | 849920   |
---------------------------------
Eval num_timesteps=850000, episode_reward=-177.70 +/- 132.32
Episode length: 15.60 +/- 4.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 15.6      |
|    mean_reward          | -178      |
| time/                   |           |
|    total_timesteps      | 850000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0.421     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.32e+03  |
|    n_updates            | 795       |
|    policy_gradient_loss | -3.24e-08 |
|    value_loss           | 5.5e+03   |
---------------------------------------
Eval num_timesteps=850500, episode_reward=-148.99 +/- 145.86
Episode length: 16.28 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 850500   |
---------------------------------
Eval num_timesteps=851000, episode_reward=-145.64 +/- 172.45
Episode length: 16.22 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 851000   |
---------------------------------
Eval num_timesteps=851500, episode_reward=-175.24 +/- 139.76
Episode length: 14.84 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 851500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -147     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 416      |
|    time_elapsed    | 1764     |
|    total_timesteps | 851968   |
---------------------------------
Eval num_timesteps=852000, episode_reward=-169.11 +/- 146.31
Episode length: 15.04 +/- 3.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | -169          |
| time/                   |               |
|    total_timesteps      | 852000        |
| train/                  |               |
|    approx_kl            | -4.656613e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -2.7e-05      |
|    explained_variance   | 0.363         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.32e+03      |
|    n_updates            | 805           |
|    policy_gradient_loss | 8.43e-08      |
|    value_loss           | 6.22e+03      |
-------------------------------------------
Eval num_timesteps=852500, episode_reward=-144.14 +/- 147.96
Episode length: 16.18 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 852500   |
---------------------------------
Eval num_timesteps=853000, episode_reward=-149.90 +/- 156.14
Episode length: 15.88 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 853000   |
---------------------------------
Eval num_timesteps=853500, episode_reward=-154.00 +/- 164.72
Episode length: 16.44 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 853500   |
---------------------------------
Eval num_timesteps=854000, episode_reward=-160.37 +/- 157.18
Episode length: 15.98 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 417      |
|    time_elapsed    | 1770     |
|    total_timesteps | 854016   |
---------------------------------
Eval num_timesteps=854500, episode_reward=-102.35 +/- 166.96
Episode length: 16.84 +/- 5.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -102         |
| time/                   |              |
|    total_timesteps      | 854500       |
| train/                  |              |
|    approx_kl            | 0.0037002265 |
|    clip_fraction        | 0.00303      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0411      |
|    explained_variance   | 0.399        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.52e+03     |
|    n_updates            | 815          |
|    policy_gradient_loss | -0.00028     |
|    value_loss           | 4.94e+03     |
------------------------------------------
Eval num_timesteps=855000, episode_reward=-135.58 +/- 130.85
Episode length: 15.80 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 855000   |
---------------------------------
Eval num_timesteps=855500, episode_reward=-187.79 +/- 122.29
Episode length: 14.14 +/- 3.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.1     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 855500   |
---------------------------------
Eval num_timesteps=856000, episode_reward=-139.46 +/- 167.44
Episode length: 17.04 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 856000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | -211     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 418      |
|    time_elapsed    | 1774     |
|    total_timesteps | 856064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=856500, episode_reward=-166.11 +/- 152.38
Episode length: 15.36 +/- 4.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | -166        |
| time/                   |             |
|    total_timesteps      | 856500      |
| train/                  |             |
|    approx_kl            | 0.009105105 |
|    clip_fraction        | 0.0556      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.395      |
|    explained_variance   | 0.257       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.77e+03    |
|    n_updates            | 816         |
|    policy_gradient_loss | 0.00398     |
|    value_loss           | 9.88e+03    |
-----------------------------------------
Eval num_timesteps=857000, episode_reward=-131.57 +/- 139.34
Episode length: 16.26 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 857000   |
---------------------------------
Eval num_timesteps=857500, episode_reward=-156.88 +/- 114.62
Episode length: 15.66 +/- 3.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 857500   |
---------------------------------
Eval num_timesteps=858000, episode_reward=-175.57 +/- 125.22
Episode length: 15.72 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 858000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 419      |
|    time_elapsed    | 1778     |
|    total_timesteps | 858112   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.14
Eval num_timesteps=858500, episode_reward=-155.26 +/- 141.50
Episode length: 14.96 +/- 4.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | -155        |
| time/                   |             |
|    total_timesteps      | 858500      |
| train/                  |             |
|    approx_kl            | 0.013706028 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.83e+03    |
|    n_updates            | 817         |
|    policy_gradient_loss | 0.00745     |
|    value_loss           | 7.21e+03    |
-----------------------------------------
Eval num_timesteps=859000, episode_reward=-137.03 +/- 128.65
Episode length: 16.08 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 859000   |
---------------------------------
Eval num_timesteps=859500, episode_reward=-136.87 +/- 146.26
Episode length: 16.18 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 859500   |
---------------------------------
Eval num_timesteps=860000, episode_reward=-172.47 +/- 118.20
Episode length: 14.70 +/- 4.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 860000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -179     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 420      |
|    time_elapsed    | 1782     |
|    total_timesteps | 860160   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=860500, episode_reward=-163.68 +/- 193.88
Episode length: 15.80 +/- 5.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -164        |
| time/                   |             |
|    total_timesteps      | 860500      |
| train/                  |             |
|    approx_kl            | 0.013979135 |
|    clip_fraction        | 0.0391      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.44       |
|    explained_variance   | 0.307       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.27e+03    |
|    n_updates            | 818         |
|    policy_gradient_loss | 0.000617    |
|    value_loss           | 7.49e+03    |
-----------------------------------------
Eval num_timesteps=861000, episode_reward=-131.85 +/- 149.14
Episode length: 16.58 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 861000   |
---------------------------------
Eval num_timesteps=861500, episode_reward=-175.18 +/- 143.76
Episode length: 15.26 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 861500   |
---------------------------------
Eval num_timesteps=862000, episode_reward=-177.94 +/- 157.41
Episode length: 15.70 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 862000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 421      |
|    time_elapsed    | 1786     |
|    total_timesteps | 862208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=862500, episode_reward=-154.12 +/- 160.91
Episode length: 15.56 +/- 4.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -154        |
| time/                   |             |
|    total_timesteps      | 862500      |
| train/                  |             |
|    approx_kl            | 0.007132293 |
|    clip_fraction        | 0.0521      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.603      |
|    explained_variance   | 0.293       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.15e+03    |
|    n_updates            | 819         |
|    policy_gradient_loss | 0.00596     |
|    value_loss           | 9e+03       |
-----------------------------------------
Eval num_timesteps=863000, episode_reward=-92.94 +/- 163.02
Episode length: 17.58 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -92.9    |
| time/              |          |
|    total_timesteps | 863000   |
---------------------------------
Eval num_timesteps=863500, episode_reward=-147.22 +/- 178.41
Episode length: 16.42 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 863500   |
---------------------------------
Eval num_timesteps=864000, episode_reward=-149.02 +/- 147.65
Episode length: 15.72 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 864000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 422      |
|    time_elapsed    | 1791     |
|    total_timesteps | 864256   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=864500, episode_reward=-129.52 +/- 195.31
Episode length: 16.08 +/- 5.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -130        |
| time/                   |             |
|    total_timesteps      | 864500      |
| train/                  |             |
|    approx_kl            | 0.010335088 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.734      |
|    explained_variance   | 0.261       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.81e+03    |
|    n_updates            | 820         |
|    policy_gradient_loss | 0.0152      |
|    value_loss           | 7.83e+03    |
-----------------------------------------
Eval num_timesteps=865000, episode_reward=-130.06 +/- 129.85
Episode length: 16.50 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 865000   |
---------------------------------
Eval num_timesteps=865500, episode_reward=-130.84 +/- 141.93
Episode length: 15.94 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 865500   |
---------------------------------
Eval num_timesteps=866000, episode_reward=-148.62 +/- 134.69
Episode length: 15.98 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 866000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -246     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 423      |
|    time_elapsed    | 1795     |
|    total_timesteps | 866304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=866500, episode_reward=-157.06 +/- 142.88
Episode length: 15.28 +/- 4.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | -157        |
| time/                   |             |
|    total_timesteps      | 866500      |
| train/                  |             |
|    approx_kl            | 0.006074018 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.791      |
|    explained_variance   | 0.267       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.37e+03    |
|    n_updates            | 821         |
|    policy_gradient_loss | 0.00282     |
|    value_loss           | 1.04e+04    |
-----------------------------------------
Eval num_timesteps=867000, episode_reward=-139.29 +/- 150.93
Episode length: 16.04 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 867000   |
---------------------------------
Eval num_timesteps=867500, episode_reward=-100.49 +/- 194.51
Episode length: 17.56 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -100     |
| time/              |          |
|    total_timesteps | 867500   |
---------------------------------
Eval num_timesteps=868000, episode_reward=-122.30 +/- 185.60
Episode length: 16.76 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 868000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 424      |
|    time_elapsed    | 1799     |
|    total_timesteps | 868352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=868500, episode_reward=-177.71 +/- 139.31
Episode length: 15.52 +/- 4.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -178        |
| time/                   |             |
|    total_timesteps      | 868500      |
| train/                  |             |
|    approx_kl            | 0.006565548 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.734      |
|    explained_variance   | 0.295       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.84e+03    |
|    n_updates            | 822         |
|    policy_gradient_loss | 0.0018      |
|    value_loss           | 6.82e+03    |
-----------------------------------------
Eval num_timesteps=869000, episode_reward=-150.84 +/- 151.17
Episode length: 16.06 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 869000   |
---------------------------------
Eval num_timesteps=869500, episode_reward=-153.20 +/- 157.47
Episode length: 16.54 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 869500   |
---------------------------------
Eval num_timesteps=870000, episode_reward=-153.20 +/- 152.11
Episode length: 15.08 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 870000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -227     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 425      |
|    time_elapsed    | 1803     |
|    total_timesteps | 870400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=870500, episode_reward=-251.53 +/- 104.34
Episode length: 15.26 +/- 4.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | -252        |
| time/                   |             |
|    total_timesteps      | 870500      |
| train/                  |             |
|    approx_kl            | 0.009612684 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.747      |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.68e+03    |
|    n_updates            | 823         |
|    policy_gradient_loss | 0.0236      |
|    value_loss           | 6.74e+03    |
-----------------------------------------
Eval num_timesteps=871000, episode_reward=-225.95 +/- 133.75
Episode length: 15.08 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -226     |
| time/              |          |
|    total_timesteps | 871000   |
---------------------------------
Eval num_timesteps=871500, episode_reward=-173.23 +/- 129.78
Episode length: 16.38 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 871500   |
---------------------------------
Eval num_timesteps=872000, episode_reward=-191.94 +/- 127.99
Episode length: 16.90 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 872000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -222     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 426      |
|    time_elapsed    | 1807     |
|    total_timesteps | 872448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=872500, episode_reward=-178.48 +/- 127.11
Episode length: 16.46 +/- 5.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -178         |
| time/                   |              |
|    total_timesteps      | 872500       |
| train/                  |              |
|    approx_kl            | 0.0063378736 |
|    clip_fraction        | 0.0719       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.718       |
|    explained_variance   | 0.311        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.95e+03     |
|    n_updates            | 824          |
|    policy_gradient_loss | -0.000567    |
|    value_loss           | 6.58e+03     |
------------------------------------------
Eval num_timesteps=873000, episode_reward=-215.99 +/- 129.81
Episode length: 16.30 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 873000   |
---------------------------------
Eval num_timesteps=873500, episode_reward=-195.96 +/- 130.73
Episode length: 17.26 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 873500   |
---------------------------------
Eval num_timesteps=874000, episode_reward=-180.52 +/- 134.97
Episode length: 17.00 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 874000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -231     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 427      |
|    time_elapsed    | 1811     |
|    total_timesteps | 874496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=874500, episode_reward=-198.01 +/- 134.25
Episode length: 16.10 +/- 5.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -198         |
| time/                   |              |
|    total_timesteps      | 874500       |
| train/                  |              |
|    approx_kl            | 0.0073269773 |
|    clip_fraction        | 0.115        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.732       |
|    explained_variance   | 0.345        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.72e+03     |
|    n_updates            | 825          |
|    policy_gradient_loss | 0.00668      |
|    value_loss           | 6.52e+03     |
------------------------------------------
Eval num_timesteps=875000, episode_reward=-219.63 +/- 104.88
Episode length: 14.70 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 875000   |
---------------------------------
Eval num_timesteps=875500, episode_reward=-247.19 +/- 118.31
Episode length: 14.64 +/- 3.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | -247     |
| time/              |          |
|    total_timesteps | 875500   |
---------------------------------
Eval num_timesteps=876000, episode_reward=-212.11 +/- 121.11
Episode length: 15.80 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 876000   |
---------------------------------
Eval num_timesteps=876500, episode_reward=-230.48 +/- 114.27
Episode length: 15.58 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -230     |
| time/              |          |
|    total_timesteps | 876500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -227     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 428      |
|    time_elapsed    | 1816     |
|    total_timesteps | 876544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=877000, episode_reward=-161.60 +/- 154.71
Episode length: 15.06 +/- 5.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.1         |
|    mean_reward          | -162         |
| time/                   |              |
|    total_timesteps      | 877000       |
| train/                  |              |
|    approx_kl            | 0.0062805233 |
|    clip_fraction        | 0.0938       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.591       |
|    explained_variance   | 0.36         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.25e+03     |
|    n_updates            | 826          |
|    policy_gradient_loss | -0.000934    |
|    value_loss           | 5.6e+03      |
------------------------------------------
Eval num_timesteps=877500, episode_reward=-207.49 +/- 127.16
Episode length: 16.12 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 877500   |
---------------------------------
Eval num_timesteps=878000, episode_reward=-170.47 +/- 164.84
Episode length: 16.50 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 878000   |
---------------------------------
Eval num_timesteps=878500, episode_reward=-216.69 +/- 108.72
Episode length: 14.20 +/- 3.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | -217     |
| time/              |          |
|    total_timesteps | 878500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -213     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 429      |
|    time_elapsed    | 1820     |
|    total_timesteps | 878592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=879000, episode_reward=-225.53 +/- 104.28
Episode length: 14.76 +/- 4.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | -226        |
| time/                   |             |
|    total_timesteps      | 879000      |
| train/                  |             |
|    approx_kl            | 0.007774148 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.552      |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.06e+03    |
|    n_updates            | 827         |
|    policy_gradient_loss | 0.0123      |
|    value_loss           | 6.02e+03    |
-----------------------------------------
Eval num_timesteps=879500, episode_reward=-182.24 +/- 126.81
Episode length: 17.12 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 879500   |
---------------------------------
Eval num_timesteps=880000, episode_reward=-206.82 +/- 120.16
Episode length: 15.60 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=880500, episode_reward=-170.28 +/- 123.90
Episode length: 17.20 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 880500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -207     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 430      |
|    time_elapsed    | 1824     |
|    total_timesteps | 880640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=881000, episode_reward=-150.22 +/- 143.87
Episode length: 16.80 +/- 5.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | -150        |
| time/                   |             |
|    total_timesteps      | 881000      |
| train/                  |             |
|    approx_kl            | 0.009506341 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.452      |
|    explained_variance   | 0.356       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.47e+03    |
|    n_updates            | 828         |
|    policy_gradient_loss | 0.00661     |
|    value_loss           | 6.86e+03    |
-----------------------------------------
Eval num_timesteps=881500, episode_reward=-157.85 +/- 161.45
Episode length: 15.80 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 881500   |
---------------------------------
Eval num_timesteps=882000, episode_reward=-170.50 +/- 151.49
Episode length: 16.30 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 882000   |
---------------------------------
Eval num_timesteps=882500, episode_reward=-161.46 +/- 141.35
Episode length: 16.68 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 882500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 431      |
|    time_elapsed    | 1828     |
|    total_timesteps | 882688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=883000, episode_reward=-149.20 +/- 173.19
Episode length: 15.72 +/- 5.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | -149        |
| time/                   |             |
|    total_timesteps      | 883000      |
| train/                  |             |
|    approx_kl            | 0.010428001 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.394      |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.11e+03    |
|    n_updates            | 829         |
|    policy_gradient_loss | 0.0124      |
|    value_loss           | 6.45e+03    |
-----------------------------------------
Eval num_timesteps=883500, episode_reward=-152.83 +/- 132.29
Episode length: 16.10 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 883500   |
---------------------------------
Eval num_timesteps=884000, episode_reward=-144.00 +/- 150.27
Episode length: 15.74 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 884000   |
---------------------------------
Eval num_timesteps=884500, episode_reward=-151.88 +/- 130.23
Episode length: 15.72 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 884500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 432      |
|    time_elapsed    | 1832     |
|    total_timesteps | 884736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=885000, episode_reward=-161.00 +/- 116.43
Episode length: 16.40 +/- 4.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -161         |
| time/                   |              |
|    total_timesteps      | 885000       |
| train/                  |              |
|    approx_kl            | 0.0055066296 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.325       |
|    explained_variance   | 0.384        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.51e+03     |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.000132    |
|    value_loss           | 6.92e+03     |
------------------------------------------
Eval num_timesteps=885500, episode_reward=-200.04 +/- 124.85
Episode length: 16.10 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 885500   |
---------------------------------
Eval num_timesteps=886000, episode_reward=-177.44 +/- 169.70
Episode length: 15.92 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 886000   |
---------------------------------
Eval num_timesteps=886500, episode_reward=-203.86 +/- 128.68
Episode length: 16.04 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 886500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 433      |
|    time_elapsed    | 1836     |
|    total_timesteps | 886784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=887000, episode_reward=-148.28 +/- 120.23
Episode length: 16.96 +/- 4.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | -148        |
| time/                   |             |
|    total_timesteps      | 887000      |
| train/                  |             |
|    approx_kl            | 0.011775834 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.361      |
|    explained_variance   | 0.398       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.4e+03     |
|    n_updates            | 831         |
|    policy_gradient_loss | 0.00875     |
|    value_loss           | 5.11e+03    |
-----------------------------------------
Eval num_timesteps=887500, episode_reward=-135.63 +/- 147.43
Episode length: 17.30 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 887500   |
---------------------------------
Eval num_timesteps=888000, episode_reward=-141.98 +/- 153.56
Episode length: 17.46 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 888000   |
---------------------------------
Eval num_timesteps=888500, episode_reward=-144.62 +/- 155.24
Episode length: 17.54 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 888500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -189     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 434      |
|    time_elapsed    | 1841     |
|    total_timesteps | 888832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=889000, episode_reward=-181.51 +/- 145.36
Episode length: 15.26 +/- 4.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -182         |
| time/                   |              |
|    total_timesteps      | 889000       |
| train/                  |              |
|    approx_kl            | 0.0055359867 |
|    clip_fraction        | 0.0844       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.374       |
|    explained_variance   | 0.401        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.53e+03     |
|    n_updates            | 832          |
|    policy_gradient_loss | 0.0111       |
|    value_loss           | 6.35e+03     |
------------------------------------------
Eval num_timesteps=889500, episode_reward=-130.78 +/- 126.79
Episode length: 18.22 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 889500   |
---------------------------------
Eval num_timesteps=890000, episode_reward=-143.94 +/- 154.17
Episode length: 17.14 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 890000   |
---------------------------------
Eval num_timesteps=890500, episode_reward=-145.71 +/- 143.40
Episode length: 16.90 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 890500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 435      |
|    time_elapsed    | 1845     |
|    total_timesteps | 890880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=891000, episode_reward=-150.83 +/- 119.26
Episode length: 16.90 +/- 5.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | -151        |
| time/                   |             |
|    total_timesteps      | 891000      |
| train/                  |             |
|    approx_kl            | 0.008710035 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.325      |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.92e+03    |
|    n_updates            | 833         |
|    policy_gradient_loss | 0.00537     |
|    value_loss           | 5.95e+03    |
-----------------------------------------
Eval num_timesteps=891500, episode_reward=-180.40 +/- 123.52
Episode length: 16.42 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 891500   |
---------------------------------
Eval num_timesteps=892000, episode_reward=-194.92 +/- 147.15
Episode length: 15.92 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 892000   |
---------------------------------
Eval num_timesteps=892500, episode_reward=-178.59 +/- 139.55
Episode length: 16.74 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 892500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 436      |
|    time_elapsed    | 1849     |
|    total_timesteps | 892928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=893000, episode_reward=-196.71 +/- 144.15
Episode length: 16.06 +/- 5.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -197        |
| time/                   |             |
|    total_timesteps      | 893000      |
| train/                  |             |
|    approx_kl            | 0.007421238 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.308      |
|    explained_variance   | 0.412       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.61e+03    |
|    n_updates            | 834         |
|    policy_gradient_loss | 0.00182     |
|    value_loss           | 5.98e+03    |
-----------------------------------------
Eval num_timesteps=893500, episode_reward=-172.50 +/- 123.57
Episode length: 17.22 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 893500   |
---------------------------------
Eval num_timesteps=894000, episode_reward=-168.92 +/- 115.79
Episode length: 16.94 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 894000   |
---------------------------------
Eval num_timesteps=894500, episode_reward=-182.58 +/- 131.83
Episode length: 16.48 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 894500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -179     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 437      |
|    time_elapsed    | 1853     |
|    total_timesteps | 894976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=895000, episode_reward=-136.76 +/- 126.94
Episode length: 16.50 +/- 4.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -137         |
| time/                   |              |
|    total_timesteps      | 895000       |
| train/                  |              |
|    approx_kl            | 0.0066781775 |
|    clip_fraction        | 0.0527       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.234       |
|    explained_variance   | 0.411        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.64e+03     |
|    n_updates            | 835          |
|    policy_gradient_loss | 0.00369      |
|    value_loss           | 5.85e+03     |
------------------------------------------
Eval num_timesteps=895500, episode_reward=-209.65 +/- 142.23
Episode length: 15.62 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 895500   |
---------------------------------
Eval num_timesteps=896000, episode_reward=-137.54 +/- 141.58
Episode length: 17.18 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 896000   |
---------------------------------
Eval num_timesteps=896500, episode_reward=-170.88 +/- 138.07
Episode length: 16.62 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 896500   |
---------------------------------
Eval num_timesteps=897000, episode_reward=-158.19 +/- 131.71
Episode length: 15.88 +/- 4.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 897000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 438      |
|    time_elapsed    | 1858     |
|    total_timesteps | 897024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=897500, episode_reward=-178.42 +/- 124.50
Episode length: 16.10 +/- 4.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -178         |
| time/                   |              |
|    total_timesteps      | 897500       |
| train/                  |              |
|    approx_kl            | 0.0029631346 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.23        |
|    explained_variance   | 0.398        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.27e+03     |
|    n_updates            | 836          |
|    policy_gradient_loss | 0.00127      |
|    value_loss           | 5.55e+03     |
------------------------------------------
Eval num_timesteps=898000, episode_reward=-178.01 +/- 162.54
Episode length: 17.30 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 898000   |
---------------------------------
Eval num_timesteps=898500, episode_reward=-143.05 +/- 142.72
Episode length: 17.26 +/- 5.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 898500   |
---------------------------------
Eval num_timesteps=899000, episode_reward=-154.35 +/- 177.40
Episode length: 16.18 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 899000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 439      |
|    time_elapsed    | 1862     |
|    total_timesteps | 899072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=899500, episode_reward=-173.86 +/- 130.74
Episode length: 15.90 +/- 4.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -174         |
| time/                   |              |
|    total_timesteps      | 899500       |
| train/                  |              |
|    approx_kl            | 0.0048153005 |
|    clip_fraction        | 0.0536       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.211       |
|    explained_variance   | 0.449        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.42e+03     |
|    n_updates            | 837          |
|    policy_gradient_loss | 0.00563      |
|    value_loss           | 5.78e+03     |
------------------------------------------
Eval num_timesteps=900000, episode_reward=-125.17 +/- 128.77
Episode length: 18.20 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=900500, episode_reward=-172.95 +/- 123.33
Episode length: 16.42 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 900500   |
---------------------------------
Eval num_timesteps=901000, episode_reward=-147.79 +/- 134.78
Episode length: 16.64 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 901000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 440      |
|    time_elapsed    | 1866     |
|    total_timesteps | 901120   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=901500, episode_reward=-184.27 +/- 102.62
Episode length: 16.26 +/- 4.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -184         |
| time/                   |              |
|    total_timesteps      | 901500       |
| train/                  |              |
|    approx_kl            | 0.0037226852 |
|    clip_fraction        | 0.0465       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.208       |
|    explained_variance   | 0.419        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.24e+03     |
|    n_updates            | 839          |
|    policy_gradient_loss | 0.00136      |
|    value_loss           | 5.91e+03     |
------------------------------------------
Eval num_timesteps=902000, episode_reward=-166.87 +/- 155.27
Episode length: 17.00 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 902000   |
---------------------------------
Eval num_timesteps=902500, episode_reward=-149.82 +/- 157.31
Episode length: 18.36 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.4     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 902500   |
---------------------------------
Eval num_timesteps=903000, episode_reward=-155.68 +/- 127.84
Episode length: 17.56 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 903000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -199     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 441      |
|    time_elapsed    | 1871     |
|    total_timesteps | 903168   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=903500, episode_reward=-171.08 +/- 135.51
Episode length: 16.08 +/- 5.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.1       |
|    mean_reward          | -171       |
| time/                   |            |
|    total_timesteps      | 903500     |
| train/                  |            |
|    approx_kl            | 0.00447491 |
|    clip_fraction        | 0.028      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.215     |
|    explained_variance   | 0.454      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.55e+03   |
|    n_updates            | 841        |
|    policy_gradient_loss | 0.00578    |
|    value_loss           | 5.52e+03   |
----------------------------------------
Eval num_timesteps=904000, episode_reward=-153.34 +/- 111.89
Episode length: 16.80 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 904000   |
---------------------------------
Eval num_timesteps=904500, episode_reward=-173.84 +/- 151.51
Episode length: 15.80 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 904500   |
---------------------------------
Eval num_timesteps=905000, episode_reward=-179.70 +/- 171.72
Episode length: 16.88 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 905000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -189     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 442      |
|    time_elapsed    | 1875     |
|    total_timesteps | 905216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=905500, episode_reward=-185.84 +/- 153.68
Episode length: 16.06 +/- 4.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -186        |
| time/                   |             |
|    total_timesteps      | 905500      |
| train/                  |             |
|    approx_kl            | 0.008438986 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.194      |
|    explained_variance   | 0.404       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.95e+03    |
|    n_updates            | 842         |
|    policy_gradient_loss | 0.00189     |
|    value_loss           | 7e+03       |
-----------------------------------------
Eval num_timesteps=906000, episode_reward=-156.61 +/- 140.08
Episode length: 17.20 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 906000   |
---------------------------------
Eval num_timesteps=906500, episode_reward=-171.51 +/- 143.33
Episode length: 16.74 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 906500   |
---------------------------------
Eval num_timesteps=907000, episode_reward=-174.27 +/- 122.04
Episode length: 16.26 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 907000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 443      |
|    time_elapsed    | 1879     |
|    total_timesteps | 907264   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=907500, episode_reward=-165.38 +/- 133.29
Episode length: 15.90 +/- 4.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | -165        |
| time/                   |             |
|    total_timesteps      | 907500      |
| train/                  |             |
|    approx_kl            | 0.007238819 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.03e+03    |
|    n_updates            | 843         |
|    policy_gradient_loss | 0.00663     |
|    value_loss           | 6.2e+03     |
-----------------------------------------
Eval num_timesteps=908000, episode_reward=-160.07 +/- 129.71
Episode length: 15.10 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 908000   |
---------------------------------
Eval num_timesteps=908500, episode_reward=-144.68 +/- 162.20
Episode length: 16.72 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 908500   |
---------------------------------
Eval num_timesteps=909000, episode_reward=-129.95 +/- 188.60
Episode length: 16.86 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 909000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 444      |
|    time_elapsed    | 1883     |
|    total_timesteps | 909312   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=909500, episode_reward=-129.53 +/- 136.95
Episode length: 16.48 +/- 4.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -130        |
| time/                   |             |
|    total_timesteps      | 909500      |
| train/                  |             |
|    approx_kl            | 0.006199043 |
|    clip_fraction        | 0.0418      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.381       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.17e+03    |
|    n_updates            | 846         |
|    policy_gradient_loss | 0.00345     |
|    value_loss           | 5.73e+03    |
-----------------------------------------
Eval num_timesteps=910000, episode_reward=-175.78 +/- 116.41
Episode length: 14.90 +/- 4.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 910000   |
---------------------------------
Eval num_timesteps=910500, episode_reward=-129.18 +/- 167.32
Episode length: 16.44 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 910500   |
---------------------------------
Eval num_timesteps=911000, episode_reward=-126.66 +/- 202.46
Episode length: 16.64 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 911000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 445      |
|    time_elapsed    | 1888     |
|    total_timesteps | 911360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=911500, episode_reward=-164.01 +/- 141.60
Episode length: 15.26 +/- 3.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -164         |
| time/                   |              |
|    total_timesteps      | 911500       |
| train/                  |              |
|    approx_kl            | 0.0042802854 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0875      |
|    explained_variance   | 0.382        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.3e+03      |
|    n_updates            | 847          |
|    policy_gradient_loss | 0.00573      |
|    value_loss           | 6.25e+03     |
------------------------------------------
Eval num_timesteps=912000, episode_reward=-136.03 +/- 143.75
Episode length: 15.70 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 912000   |
---------------------------------
Eval num_timesteps=912500, episode_reward=-180.84 +/- 126.38
Episode length: 15.18 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 912500   |
---------------------------------
Eval num_timesteps=913000, episode_reward=-98.45 +/- 199.35
Episode length: 17.28 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -98.4    |
| time/              |          |
|    total_timesteps | 913000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 446      |
|    time_elapsed    | 1892     |
|    total_timesteps | 913408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=913500, episode_reward=-139.86 +/- 141.57
Episode length: 17.28 +/- 5.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | -140       |
| time/                   |            |
|    total_timesteps      | 913500     |
| train/                  |            |
|    approx_kl            | 0.00331445 |
|    clip_fraction        | 0.00601    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.0508    |
|    explained_variance   | 0.369      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.95e+03   |
|    n_updates            | 848        |
|    policy_gradient_loss | 0.00443    |
|    value_loss           | 6.17e+03   |
----------------------------------------
Eval num_timesteps=914000, episode_reward=-159.84 +/- 158.29
Episode length: 15.62 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 914000   |
---------------------------------
Eval num_timesteps=914500, episode_reward=-141.39 +/- 185.43
Episode length: 16.04 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 914500   |
---------------------------------
Eval num_timesteps=915000, episode_reward=-160.69 +/- 119.42
Episode length: 15.58 +/- 3.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 915000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 447      |
|    time_elapsed    | 1896     |
|    total_timesteps | 915456   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=915500, episode_reward=-149.69 +/- 146.02
Episode length: 16.12 +/- 4.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -150         |
| time/                   |              |
|    total_timesteps      | 915500       |
| train/                  |              |
|    approx_kl            | 0.0023702893 |
|    clip_fraction        | 0.00196      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0128      |
|    explained_variance   | 0.34         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.59e+03     |
|    n_updates            | 855          |
|    policy_gradient_loss | 0.000359     |
|    value_loss           | 6.65e+03     |
------------------------------------------
Eval num_timesteps=916000, episode_reward=-161.58 +/- 178.16
Episode length: 15.96 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 916000   |
---------------------------------
Eval num_timesteps=916500, episode_reward=-177.74 +/- 135.73
Episode length: 14.66 +/- 4.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 916500   |
---------------------------------
Eval num_timesteps=917000, episode_reward=-138.39 +/- 176.17
Episode length: 16.52 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 917000   |
---------------------------------
Eval num_timesteps=917500, episode_reward=-144.57 +/- 167.43
Episode length: 15.84 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 917500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 448      |
|    time_elapsed    | 1901     |
|    total_timesteps | 917504   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=918000, episode_reward=-112.96 +/- 169.42
Episode length: 17.74 +/- 5.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | -113        |
| time/                   |             |
|    total_timesteps      | 918000      |
| train/                  |             |
|    approx_kl            | 0.004993188 |
|    clip_fraction        | 0.00591     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0304     |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.6e+03     |
|    n_updates            | 857         |
|    policy_gradient_loss | 0.00133     |
|    value_loss           | 6.47e+03    |
-----------------------------------------
Eval num_timesteps=918500, episode_reward=-147.37 +/- 142.26
Episode length: 16.40 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 918500   |
---------------------------------
Eval num_timesteps=919000, episode_reward=-145.63 +/- 136.26
Episode length: 15.86 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 919000   |
---------------------------------
Eval num_timesteps=919500, episode_reward=-129.22 +/- 160.11
Episode length: 16.70 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 919500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 449      |
|    time_elapsed    | 1905     |
|    total_timesteps | 919552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=920000, episode_reward=-122.41 +/- 148.69
Episode length: 15.76 +/- 4.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -122         |
| time/                   |              |
|    total_timesteps      | 920000       |
| train/                  |              |
|    approx_kl            | 0.0080444375 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0905      |
|    explained_variance   | 0.363        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.25e+03     |
|    n_updates            | 858          |
|    policy_gradient_loss | 0.00937      |
|    value_loss           | 5.59e+03     |
------------------------------------------
Eval num_timesteps=920500, episode_reward=-160.89 +/- 133.56
Episode length: 14.88 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 920500   |
---------------------------------
Eval num_timesteps=921000, episode_reward=-125.16 +/- 165.25
Episode length: 16.62 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 921000   |
---------------------------------
Eval num_timesteps=921500, episode_reward=-151.12 +/- 147.86
Episode length: 16.04 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 921500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 450      |
|    time_elapsed    | 1909     |
|    total_timesteps | 921600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=922000, episode_reward=-180.25 +/- 117.48
Episode length: 14.62 +/- 4.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | -180         |
| time/                   |              |
|    total_timesteps      | 922000       |
| train/                  |              |
|    approx_kl            | 0.0040621753 |
|    clip_fraction        | 0.0383       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.181       |
|    explained_variance   | 0.41         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.55e+03     |
|    n_updates            | 859          |
|    policy_gradient_loss | 0.00407      |
|    value_loss           | 5.73e+03     |
------------------------------------------
Eval num_timesteps=922500, episode_reward=-96.38 +/- 170.16
Episode length: 17.80 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -96.4    |
| time/              |          |
|    total_timesteps | 922500   |
---------------------------------
Eval num_timesteps=923000, episode_reward=-139.20 +/- 171.13
Episode length: 15.96 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 923000   |
---------------------------------
Eval num_timesteps=923500, episode_reward=-150.40 +/- 147.16
Episode length: 15.70 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 923500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 451      |
|    time_elapsed    | 1914     |
|    total_timesteps | 923648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=924000, episode_reward=-146.24 +/- 179.08
Episode length: 15.72 +/- 4.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -146         |
| time/                   |              |
|    total_timesteps      | 924000       |
| train/                  |              |
|    approx_kl            | 0.0053021433 |
|    clip_fraction        | 0.0332       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.109       |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.75e+03     |
|    n_updates            | 860          |
|    policy_gradient_loss | 0.00326      |
|    value_loss           | 6.65e+03     |
------------------------------------------
Eval num_timesteps=924500, episode_reward=-155.93 +/- 136.55
Episode length: 15.00 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 924500   |
---------------------------------
Eval num_timesteps=925000, episode_reward=-131.16 +/- 160.60
Episode length: 15.86 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 925000   |
---------------------------------
Eval num_timesteps=925500, episode_reward=-117.53 +/- 146.57
Episode length: 16.44 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 925500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -143     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 452      |
|    time_elapsed    | 1918     |
|    total_timesteps | 925696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=926000, episode_reward=-166.11 +/- 179.99
Episode length: 15.90 +/- 5.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | -166        |
| time/                   |             |
|    total_timesteps      | 926000      |
| train/                  |             |
|    approx_kl            | 0.004270026 |
|    clip_fraction        | 0.00694     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0549     |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.02e+03    |
|    n_updates            | 861         |
|    policy_gradient_loss | 0.00208     |
|    value_loss           | 7.04e+03    |
-----------------------------------------
Eval num_timesteps=926500, episode_reward=-199.37 +/- 132.93
Episode length: 14.54 +/- 3.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 926500   |
---------------------------------
Eval num_timesteps=927000, episode_reward=-154.58 +/- 123.34
Episode length: 15.70 +/- 3.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 927000   |
---------------------------------
Eval num_timesteps=927500, episode_reward=-176.89 +/- 147.45
Episode length: 16.08 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 927500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -114     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 453      |
|    time_elapsed    | 1922     |
|    total_timesteps | 927744   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=928000, episode_reward=-178.67 +/- 138.01
Episode length: 15.14 +/- 4.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.1         |
|    mean_reward          | -179         |
| time/                   |              |
|    total_timesteps      | 928000       |
| train/                  |              |
|    approx_kl            | 0.0019711023 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0247      |
|    explained_variance   | 0.318        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.91e+03     |
|    n_updates            | 864          |
|    policy_gradient_loss | 0.000163     |
|    value_loss           | 6.02e+03     |
------------------------------------------
Eval num_timesteps=928500, episode_reward=-176.63 +/- 132.93
Episode length: 15.50 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 928500   |
---------------------------------
Eval num_timesteps=929000, episode_reward=-164.69 +/- 120.58
Episode length: 14.90 +/- 3.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 929000   |
---------------------------------
Eval num_timesteps=929500, episode_reward=-121.24 +/- 177.05
Episode length: 16.90 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 929500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 454      |
|    time_elapsed    | 1926     |
|    total_timesteps | 929792   |
---------------------------------
Eval num_timesteps=930000, episode_reward=-168.97 +/- 131.43
Episode length: 15.38 +/- 4.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | -169          |
| time/                   |               |
|    total_timesteps      | 930000        |
| train/                  |               |
|    approx_kl            | 1.1404045e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000842     |
|    explained_variance   | 0.372         |
|    learning_rate        | 0.0001        |
|    loss                 | 4e+03         |
|    n_updates            | 874           |
|    policy_gradient_loss | -9.45e-05     |
|    value_loss           | 5.95e+03      |
-------------------------------------------
Eval num_timesteps=930500, episode_reward=-154.98 +/- 124.09
Episode length: 15.18 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 930500   |
---------------------------------
Eval num_timesteps=931000, episode_reward=-99.48 +/- 166.64
Episode length: 18.10 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.1     |
|    mean_reward     | -99.5    |
| time/              |          |
|    total_timesteps | 931000   |
---------------------------------
Eval num_timesteps=931500, episode_reward=-133.33 +/- 180.59
Episode length: 16.64 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 931500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -142     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 455      |
|    time_elapsed    | 1931     |
|    total_timesteps | 931840   |
---------------------------------
Eval num_timesteps=932000, episode_reward=-165.68 +/- 119.92
Episode length: 15.10 +/- 3.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.1         |
|    mean_reward          | -166         |
| time/                   |              |
|    total_timesteps      | 932000       |
| train/                  |              |
|    approx_kl            | 3.410969e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.00147     |
|    explained_variance   | 0.388        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.42e+03     |
|    n_updates            | 884          |
|    policy_gradient_loss | 2.38e-06     |
|    value_loss           | 6.26e+03     |
------------------------------------------
Eval num_timesteps=932500, episode_reward=-146.41 +/- 167.47
Episode length: 16.38 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 932500   |
---------------------------------
Eval num_timesteps=933000, episode_reward=-90.40 +/- 167.68
Episode length: 17.66 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -90.4    |
| time/              |          |
|    total_timesteps | 933000   |
---------------------------------
Eval num_timesteps=933500, episode_reward=-150.83 +/- 158.53
Episode length: 16.80 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 933500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 456      |
|    time_elapsed    | 1935     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=934000, episode_reward=-164.16 +/- 151.00
Episode length: 16.26 +/- 4.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.3          |
|    mean_reward          | -164          |
| time/                   |               |
|    total_timesteps      | 934000        |
| train/                  |               |
|    approx_kl            | 2.4097972e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000266     |
|    explained_variance   | 0.427         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.68e+03      |
|    n_updates            | 894           |
|    policy_gradient_loss | -1.41e-06     |
|    value_loss           | 5.09e+03      |
-------------------------------------------
Eval num_timesteps=934500, episode_reward=-127.90 +/- 127.18
Episode length: 16.30 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 934500   |
---------------------------------
Eval num_timesteps=935000, episode_reward=-144.62 +/- 142.40
Episode length: 16.26 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 935000   |
---------------------------------
Eval num_timesteps=935500, episode_reward=-154.48 +/- 157.80
Episode length: 15.30 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 935500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 457      |
|    time_elapsed    | 1940     |
|    total_timesteps | 935936   |
---------------------------------
Eval num_timesteps=936000, episode_reward=-107.98 +/- 157.85
Episode length: 16.92 +/- 5.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.9      |
|    mean_reward          | -108      |
| time/                   |           |
|    total_timesteps      | 936000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.000232 |
|    explained_variance   | 0.307     |
|    learning_rate        | 0.0001    |
|    loss                 | 3.31e+03  |
|    n_updates            | 904       |
|    policy_gradient_loss | 8.3e-07   |
|    value_loss           | 7.3e+03   |
---------------------------------------
Eval num_timesteps=936500, episode_reward=-148.87 +/- 146.25
Episode length: 15.72 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 936500   |
---------------------------------
Eval num_timesteps=937000, episode_reward=-156.28 +/- 177.51
Episode length: 16.12 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 937000   |
---------------------------------
Eval num_timesteps=937500, episode_reward=-146.82 +/- 135.55
Episode length: 16.54 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 937500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 458      |
|    time_elapsed    | 1945     |
|    total_timesteps | 937984   |
---------------------------------
Eval num_timesteps=938000, episode_reward=-183.55 +/- 130.41
Episode length: 15.60 +/- 3.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | -184          |
| time/                   |               |
|    total_timesteps      | 938000        |
| train/                  |               |
|    approx_kl            | 1.7462298e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000216     |
|    explained_variance   | 0.33          |
|    learning_rate        | 0.0001        |
|    loss                 | 2.18e+03      |
|    n_updates            | 914           |
|    policy_gradient_loss | 6.57e-07      |
|    value_loss           | 7.03e+03      |
-------------------------------------------
Eval num_timesteps=938500, episode_reward=-146.26 +/- 174.15
Episode length: 16.70 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 938500   |
---------------------------------
Eval num_timesteps=939000, episode_reward=-178.99 +/- 131.15
Episode length: 15.48 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 939000   |
---------------------------------
Eval num_timesteps=939500, episode_reward=-147.18 +/- 139.82
Episode length: 16.48 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 939500   |
---------------------------------
Eval num_timesteps=940000, episode_reward=-178.44 +/- 126.54
Episode length: 14.92 +/- 3.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 459      |
|    time_elapsed    | 1950     |
|    total_timesteps | 940032   |
---------------------------------
Eval num_timesteps=940500, episode_reward=-131.80 +/- 132.58
Episode length: 16.80 +/- 4.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | -132          |
| time/                   |               |
|    total_timesteps      | 940500        |
| train/                  |               |
|    approx_kl            | -9.109499e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000183     |
|    explained_variance   | 0.352         |
|    learning_rate        | 0.0001        |
|    loss                 | 3.2e+03       |
|    n_updates            | 924           |
|    policy_gradient_loss | -1.24e-07     |
|    value_loss           | 6.26e+03      |
-------------------------------------------
Eval num_timesteps=941000, episode_reward=-130.32 +/- 167.84
Episode length: 16.22 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 941000   |
---------------------------------
Eval num_timesteps=941500, episode_reward=-139.43 +/- 179.47
Episode length: 16.92 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 941500   |
---------------------------------
Eval num_timesteps=942000, episode_reward=-173.88 +/- 138.79
Episode length: 14.52 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 942000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 460      |
|    time_elapsed    | 1955     |
|    total_timesteps | 942080   |
---------------------------------
Eval num_timesteps=942500, episode_reward=-160.72 +/- 150.14
Episode length: 16.04 +/- 5.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -161          |
| time/                   |               |
|    total_timesteps      | 942500        |
| train/                  |               |
|    approx_kl            | -7.945346e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.000259     |
|    explained_variance   | 0.347         |
|    learning_rate        | 0.0001        |
|    loss                 | 4.28e+03      |
|    n_updates            | 934           |
|    policy_gradient_loss | -1.2e-07      |
|    value_loss           | 6.81e+03      |
-------------------------------------------
Eval num_timesteps=943000, episode_reward=-180.37 +/- 130.79
Episode length: 15.50 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 943000   |
---------------------------------
Eval num_timesteps=943500, episode_reward=-181.80 +/- 121.93
Episode length: 15.26 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 943500   |
---------------------------------
Eval num_timesteps=944000, episode_reward=-172.33 +/- 141.25
Episode length: 15.18 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 944000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 461      |
|    time_elapsed    | 1959     |
|    total_timesteps | 944128   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 4.60
Eval num_timesteps=944500, episode_reward=-172.83 +/- 174.51
Episode length: 15.90 +/- 5.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 15.9      |
|    mean_reward          | -173      |
| time/                   |           |
|    total_timesteps      | 944500    |
| train/                  |           |
|    approx_kl            | 0.6571974 |
|    clip_fraction        | 0.0012    |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.00278  |
|    explained_variance   | 0.363     |
|    learning_rate        | 0.0001    |
|    loss                 | 3.22e+03  |
|    n_updates            | 936       |
|    policy_gradient_loss | 0.000299  |
|    value_loss           | 6.04e+03  |
---------------------------------------
Eval num_timesteps=945000, episode_reward=-125.94 +/- 172.65
Episode length: 16.12 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 945000   |
---------------------------------
Eval num_timesteps=945500, episode_reward=-138.30 +/- 113.99
Episode length: 16.42 +/- 4.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 945500   |
---------------------------------
Eval num_timesteps=946000, episode_reward=-161.11 +/- 174.23
Episode length: 15.48 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 946000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 462      |
|    time_elapsed    | 1964     |
|    total_timesteps | 946176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.11
Eval num_timesteps=946500, episode_reward=-151.01 +/- 126.78
Episode length: 16.38 +/- 4.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -151        |
| time/                   |             |
|    total_timesteps      | 946500      |
| train/                  |             |
|    approx_kl            | 0.007936479 |
|    clip_fraction        | 0.00112     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0244     |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.0001      |
|    loss                 | 3.27e+03    |
|    n_updates            | 937         |
|    policy_gradient_loss | 4.54e-05    |
|    value_loss           | 6.87e+03    |
-----------------------------------------
Eval num_timesteps=947000, episode_reward=-147.14 +/- 150.69
Episode length: 15.54 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 947000   |
---------------------------------
Eval num_timesteps=947500, episode_reward=-169.21 +/- 138.07
Episode length: 15.80 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 947500   |
---------------------------------
Eval num_timesteps=948000, episode_reward=-178.86 +/- 132.49
Episode length: 15.98 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 948000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -126     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 463      |
|    time_elapsed    | 1968     |
|    total_timesteps | 948224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=948500, episode_reward=-122.68 +/- 136.03
Episode length: 16.66 +/- 4.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -123         |
| time/                   |              |
|    total_timesteps      | 948500       |
| train/                  |              |
|    approx_kl            | 0.0048835855 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0956      |
|    explained_variance   | 0.388        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.34e+03     |
|    n_updates            | 938          |
|    policy_gradient_loss | 0.00514      |
|    value_loss           | 5.77e+03     |
------------------------------------------
Eval num_timesteps=949000, episode_reward=-148.38 +/- 122.83
Episode length: 16.36 +/- 4.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 949000   |
---------------------------------
Eval num_timesteps=949500, episode_reward=-164.22 +/- 148.86
Episode length: 15.64 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 949500   |
---------------------------------
Eval num_timesteps=950000, episode_reward=-155.28 +/- 133.16
Episode length: 16.22 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 950000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 464      |
|    time_elapsed    | 1972     |
|    total_timesteps | 950272   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=950500, episode_reward=-137.09 +/- 133.82
Episode length: 15.66 +/- 4.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -137         |
| time/                   |              |
|    total_timesteps      | 950500       |
| train/                  |              |
|    approx_kl            | 0.0031963752 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.161       |
|    explained_variance   | 0.317        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.67e+03     |
|    n_updates            | 939          |
|    policy_gradient_loss | 0.00751      |
|    value_loss           | 6.71e+03     |
------------------------------------------
Eval num_timesteps=951000, episode_reward=-127.76 +/- 138.88
Episode length: 16.82 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 951000   |
---------------------------------
Eval num_timesteps=951500, episode_reward=-147.26 +/- 160.37
Episode length: 16.32 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 951500   |
---------------------------------
Eval num_timesteps=952000, episode_reward=-140.56 +/- 167.81
Episode length: 16.14 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 952000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 465      |
|    time_elapsed    | 1976     |
|    total_timesteps | 952320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=952500, episode_reward=-158.15 +/- 167.66
Episode length: 16.40 +/- 5.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -158         |
| time/                   |              |
|    total_timesteps      | 952500       |
| train/                  |              |
|    approx_kl            | 0.0047320356 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.17        |
|    explained_variance   | 0.354        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.73e+03     |
|    n_updates            | 940          |
|    policy_gradient_loss | 0.00273      |
|    value_loss           | 6.08e+03     |
------------------------------------------
Eval num_timesteps=953000, episode_reward=-158.04 +/- 140.53
Episode length: 15.82 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 953000   |
---------------------------------
Eval num_timesteps=953500, episode_reward=-172.90 +/- 141.94
Episode length: 15.28 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 953500   |
---------------------------------
Eval num_timesteps=954000, episode_reward=-142.11 +/- 144.37
Episode length: 15.32 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 954000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 466      |
|    time_elapsed    | 1980     |
|    total_timesteps | 954368   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=954500, episode_reward=-149.50 +/- 154.53
Episode length: 16.24 +/- 4.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -150        |
| time/                   |             |
|    total_timesteps      | 954500      |
| train/                  |             |
|    approx_kl            | 0.012783001 |
|    clip_fraction        | 0.0521      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.262      |
|    explained_variance   | 0.35        |
|    learning_rate        | 0.0001      |
|    loss                 | 3.1e+03     |
|    n_updates            | 941         |
|    policy_gradient_loss | 0.019       |
|    value_loss           | 7.28e+03    |
-----------------------------------------
Eval num_timesteps=955000, episode_reward=-137.68 +/- 142.68
Episode length: 16.86 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 955000   |
---------------------------------
Eval num_timesteps=955500, episode_reward=-167.69 +/- 135.10
Episode length: 15.24 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 955500   |
---------------------------------
Eval num_timesteps=956000, episode_reward=-161.32 +/- 180.21
Episode length: 15.10 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 956000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 467      |
|    time_elapsed    | 1984     |
|    total_timesteps | 956416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=956500, episode_reward=-128.21 +/- 172.82
Episode length: 16.66 +/- 5.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -128        |
| time/                   |             |
|    total_timesteps      | 956500      |
| train/                  |             |
|    approx_kl            | 0.012740885 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.331      |
|    explained_variance   | 0.362       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.42e+03    |
|    n_updates            | 942         |
|    policy_gradient_loss | 0.0198      |
|    value_loss           | 6.48e+03    |
-----------------------------------------
Eval num_timesteps=957000, episode_reward=-155.88 +/- 167.44
Episode length: 15.60 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 957000   |
---------------------------------
Eval num_timesteps=957500, episode_reward=-179.87 +/- 149.81
Episode length: 14.72 +/- 4.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 957500   |
---------------------------------
Eval num_timesteps=958000, episode_reward=-175.24 +/- 184.36
Episode length: 15.92 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 958000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 468      |
|    time_elapsed    | 1988     |
|    total_timesteps | 958464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=958500, episode_reward=-167.66 +/- 136.31
Episode length: 17.70 +/- 5.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | -168        |
| time/                   |             |
|    total_timesteps      | 958500      |
| train/                  |             |
|    approx_kl            | 0.009797751 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.458      |
|    explained_variance   | 0.419       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.51e+03    |
|    n_updates            | 943         |
|    policy_gradient_loss | 0.0239      |
|    value_loss           | 4.4e+03     |
-----------------------------------------
Eval num_timesteps=959000, episode_reward=-204.66 +/- 140.79
Episode length: 16.12 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 959000   |
---------------------------------
Eval num_timesteps=959500, episode_reward=-179.59 +/- 164.52
Episode length: 16.66 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 959500   |
---------------------------------
Eval num_timesteps=960000, episode_reward=-172.29 +/- 123.66
Episode length: 17.16 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=960500, episode_reward=-181.22 +/- 143.65
Episode length: 16.06 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 960500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -181     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 469      |
|    time_elapsed    | 1993     |
|    total_timesteps | 960512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=961000, episode_reward=-156.64 +/- 130.71
Episode length: 16.66 +/- 4.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -157        |
| time/                   |             |
|    total_timesteps      | 961000      |
| train/                  |             |
|    approx_kl            | 0.010294128 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.518      |
|    explained_variance   | 0.414       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.91e+03    |
|    n_updates            | 944         |
|    policy_gradient_loss | -0.00163    |
|    value_loss           | 6.03e+03    |
-----------------------------------------
Eval num_timesteps=961500, episode_reward=-171.22 +/- 127.14
Episode length: 16.20 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 961500   |
---------------------------------
Eval num_timesteps=962000, episode_reward=-194.63 +/- 154.17
Episode length: 15.28 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 962000   |
---------------------------------
Eval num_timesteps=962500, episode_reward=-164.85 +/- 160.12
Episode length: 16.30 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 962500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 470      |
|    time_elapsed    | 1997     |
|    total_timesteps | 962560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=963000, episode_reward=-178.01 +/- 132.20
Episode length: 16.32 +/- 5.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -178        |
| time/                   |             |
|    total_timesteps      | 963000      |
| train/                  |             |
|    approx_kl            | 0.006606226 |
|    clip_fraction        | 0.0724      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.482      |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.99e+03    |
|    n_updates            | 945         |
|    policy_gradient_loss | 0.0142      |
|    value_loss           | 6.47e+03    |
-----------------------------------------
Eval num_timesteps=963500, episode_reward=-159.02 +/- 132.20
Episode length: 16.90 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 963500   |
---------------------------------
Eval num_timesteps=964000, episode_reward=-167.27 +/- 128.94
Episode length: 15.42 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 964000   |
---------------------------------
Eval num_timesteps=964500, episode_reward=-205.40 +/- 146.38
Episode length: 15.08 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 964500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 471      |
|    time_elapsed    | 2001     |
|    total_timesteps | 964608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=965000, episode_reward=-180.33 +/- 127.38
Episode length: 17.00 +/- 4.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -180         |
| time/                   |              |
|    total_timesteps      | 965000       |
| train/                  |              |
|    approx_kl            | 0.0064026895 |
|    clip_fraction        | 0.0903       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.425       |
|    explained_variance   | 0.391        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.36e+03     |
|    n_updates            | 946          |
|    policy_gradient_loss | 0.00237      |
|    value_loss           | 6.21e+03     |
------------------------------------------
Eval num_timesteps=965500, episode_reward=-194.44 +/- 110.08
Episode length: 15.48 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 965500   |
---------------------------------
Eval num_timesteps=966000, episode_reward=-174.63 +/- 146.56
Episode length: 16.58 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 966000   |
---------------------------------
Eval num_timesteps=966500, episode_reward=-200.42 +/- 135.79
Episode length: 15.74 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 966500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -216     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 472      |
|    time_elapsed    | 2006     |
|    total_timesteps | 966656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=967000, episode_reward=-198.46 +/- 135.82
Episode length: 15.34 +/- 4.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -198         |
| time/                   |              |
|    total_timesteps      | 967000       |
| train/                  |              |
|    approx_kl            | 0.0078916205 |
|    clip_fraction        | 0.135        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.488       |
|    explained_variance   | 0.421        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.26e+03     |
|    n_updates            | 947          |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 6.03e+03     |
------------------------------------------
Eval num_timesteps=967500, episode_reward=-181.43 +/- 146.49
Episode length: 15.84 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 967500   |
---------------------------------
Eval num_timesteps=968000, episode_reward=-151.34 +/- 126.59
Episode length: 16.68 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 968000   |
---------------------------------
Eval num_timesteps=968500, episode_reward=-161.50 +/- 144.86
Episode length: 16.40 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 968500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 473      |
|    time_elapsed    | 2010     |
|    total_timesteps | 968704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=969000, episode_reward=-151.84 +/- 157.14
Episode length: 16.38 +/- 4.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -152        |
| time/                   |             |
|    total_timesteps      | 969000      |
| train/                  |             |
|    approx_kl            | 0.008884596 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.326      |
|    explained_variance   | 0.389       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.63e+03    |
|    n_updates            | 948         |
|    policy_gradient_loss | 0.0037      |
|    value_loss           | 5.68e+03    |
-----------------------------------------
Eval num_timesteps=969500, episode_reward=-148.75 +/- 165.03
Episode length: 15.42 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 969500   |
---------------------------------
Eval num_timesteps=970000, episode_reward=-132.07 +/- 179.38
Episode length: 16.00 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 970000   |
---------------------------------
Eval num_timesteps=970500, episode_reward=-143.05 +/- 165.03
Episode length: 16.10 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 970500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 474      |
|    time_elapsed    | 2014     |
|    total_timesteps | 970752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=971000, episode_reward=-153.29 +/- 135.76
Episode length: 16.50 +/- 5.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -153         |
| time/                   |              |
|    total_timesteps      | 971000       |
| train/                  |              |
|    approx_kl            | 0.0069784266 |
|    clip_fraction        | 0.0873       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.328       |
|    explained_variance   | 0.402        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.71e+03     |
|    n_updates            | 949          |
|    policy_gradient_loss | 0.00246      |
|    value_loss           | 5.48e+03     |
------------------------------------------
Eval num_timesteps=971500, episode_reward=-172.79 +/- 139.93
Episode length: 17.02 +/- 5.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 971500   |
---------------------------------
Eval num_timesteps=972000, episode_reward=-176.46 +/- 134.29
Episode length: 16.32 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 972000   |
---------------------------------
Eval num_timesteps=972500, episode_reward=-171.81 +/- 120.71
Episode length: 16.26 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 972500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 475      |
|    time_elapsed    | 2018     |
|    total_timesteps | 972800   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=973000, episode_reward=-187.47 +/- 145.71
Episode length: 15.00 +/- 4.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | -187        |
| time/                   |             |
|    total_timesteps      | 973000      |
| train/                  |             |
|    approx_kl            | 0.026538061 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.41       |
|    explained_variance   | 0.394       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.31e+03    |
|    n_updates            | 951         |
|    policy_gradient_loss | 0.00864     |
|    value_loss           | 6.01e+03    |
-----------------------------------------
Eval num_timesteps=973500, episode_reward=-160.74 +/- 162.53
Episode length: 15.88 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 973500   |
---------------------------------
Eval num_timesteps=974000, episode_reward=-168.87 +/- 126.27
Episode length: 14.96 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 974000   |
---------------------------------
Eval num_timesteps=974500, episode_reward=-128.27 +/- 143.15
Episode length: 16.42 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 974500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 476      |
|    time_elapsed    | 2022     |
|    total_timesteps | 974848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=975000, episode_reward=-166.84 +/- 145.06
Episode length: 16.18 +/- 4.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -167         |
| time/                   |              |
|    total_timesteps      | 975000       |
| train/                  |              |
|    approx_kl            | 0.0062910565 |
|    clip_fraction        | 0.1          |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.352       |
|    explained_variance   | 0.398        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.86e+03     |
|    n_updates            | 952          |
|    policy_gradient_loss | 0.003        |
|    value_loss           | 7e+03        |
------------------------------------------
Eval num_timesteps=975500, episode_reward=-154.07 +/- 159.15
Episode length: 15.92 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 975500   |
---------------------------------
Eval num_timesteps=976000, episode_reward=-156.55 +/- 143.17
Episode length: 15.60 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 976000   |
---------------------------------
Eval num_timesteps=976500, episode_reward=-139.58 +/- 148.46
Episode length: 15.84 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 976500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 477      |
|    time_elapsed    | 2026     |
|    total_timesteps | 976896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=977000, episode_reward=-167.28 +/- 181.49
Episode length: 15.78 +/- 5.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -167        |
| time/                   |             |
|    total_timesteps      | 977000      |
| train/                  |             |
|    approx_kl            | 0.008808853 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.274      |
|    explained_variance   | 0.437       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.96e+03    |
|    n_updates            | 953         |
|    policy_gradient_loss | 0.000473    |
|    value_loss           | 6.72e+03    |
-----------------------------------------
Eval num_timesteps=977500, episode_reward=-153.34 +/- 161.23
Episode length: 15.58 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 977500   |
---------------------------------
Eval num_timesteps=978000, episode_reward=-155.23 +/- 150.19
Episode length: 15.20 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 978000   |
---------------------------------
Eval num_timesteps=978500, episode_reward=-158.85 +/- 194.47
Episode length: 16.18 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 978500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 478      |
|    time_elapsed    | 2030     |
|    total_timesteps | 978944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=979000, episode_reward=-168.57 +/- 155.37
Episode length: 15.76 +/- 4.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -169        |
| time/                   |             |
|    total_timesteps      | 979000      |
| train/                  |             |
|    approx_kl            | 0.005236198 |
|    clip_fraction        | 0.0349      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.365       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.47e+03    |
|    n_updates            | 954         |
|    policy_gradient_loss | 0.00389     |
|    value_loss           | 5.94e+03    |
-----------------------------------------
Eval num_timesteps=979500, episode_reward=-136.51 +/- 140.50
Episode length: 16.54 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 979500   |
---------------------------------
Eval num_timesteps=980000, episode_reward=-146.00 +/- 163.56
Episode length: 16.20 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=980500, episode_reward=-154.18 +/- 146.33
Episode length: 15.98 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 980500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 479      |
|    time_elapsed    | 2035     |
|    total_timesteps | 980992   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.23
Eval num_timesteps=981000, episode_reward=-151.10 +/- 147.58
Episode length: 15.30 +/- 4.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.3       |
|    mean_reward          | -151       |
| time/                   |            |
|    total_timesteps      | 981000     |
| train/                  |            |
|    approx_kl            | 0.06559814 |
|    clip_fraction        | 0.0395     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.156     |
|    explained_variance   | 0.418      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.5e+03    |
|    n_updates            | 956        |
|    policy_gradient_loss | 0.0146     |
|    value_loss           | 5.63e+03   |
----------------------------------------
Eval num_timesteps=981500, episode_reward=-117.28 +/- 168.15
Episode length: 16.52 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 981500   |
---------------------------------
Eval num_timesteps=982000, episode_reward=-193.72 +/- 144.70
Episode length: 15.30 +/- 4.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 982000   |
---------------------------------
Eval num_timesteps=982500, episode_reward=-183.05 +/- 151.47
Episode length: 15.54 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 982500   |
---------------------------------
Eval num_timesteps=983000, episode_reward=-159.48 +/- 120.39
Episode length: 16.00 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 983000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 480      |
|    time_elapsed    | 2039     |
|    total_timesteps | 983040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=983500, episode_reward=-175.83 +/- 156.78
Episode length: 15.40 +/- 4.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -176         |
| time/                   |              |
|    total_timesteps      | 983500       |
| train/                  |              |
|    approx_kl            | 0.0044350354 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.195       |
|    explained_variance   | 0.367        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.66e+03     |
|    n_updates            | 957          |
|    policy_gradient_loss | 0.0013       |
|    value_loss           | 5.88e+03     |
------------------------------------------
Eval num_timesteps=984000, episode_reward=-174.99 +/- 144.10
Episode length: 15.62 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 984000   |
---------------------------------
Eval num_timesteps=984500, episode_reward=-179.08 +/- 132.70
Episode length: 15.84 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 984500   |
---------------------------------
Eval num_timesteps=985000, episode_reward=-148.49 +/- 146.12
Episode length: 16.62 +/- 4.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 985000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 481      |
|    time_elapsed    | 2043     |
|    total_timesteps | 985088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=985500, episode_reward=-163.27 +/- 152.36
Episode length: 15.24 +/- 4.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | -163        |
| time/                   |             |
|    total_timesteps      | 985500      |
| train/                  |             |
|    approx_kl            | 0.005287055 |
|    clip_fraction        | 0.0163      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.5e+03     |
|    n_updates            | 958         |
|    policy_gradient_loss | 0.00932     |
|    value_loss           | 5.83e+03    |
-----------------------------------------
Eval num_timesteps=986000, episode_reward=-140.42 +/- 156.63
Episode length: 16.12 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 986000   |
---------------------------------
Eval num_timesteps=986500, episode_reward=-147.08 +/- 166.78
Episode length: 16.08 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 986500   |
---------------------------------
Eval num_timesteps=987000, episode_reward=-121.36 +/- 159.84
Episode length: 17.62 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 987000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -143     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 482      |
|    time_elapsed    | 2048     |
|    total_timesteps | 987136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=987500, episode_reward=-165.89 +/- 166.43
Episode length: 16.34 +/- 5.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -166         |
| time/                   |              |
|    total_timesteps      | 987500       |
| train/                  |              |
|    approx_kl            | 0.0034013195 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.186       |
|    explained_variance   | 0.351        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.01e+03     |
|    n_updates            | 959          |
|    policy_gradient_loss | -0.000398    |
|    value_loss           | 6.27e+03     |
------------------------------------------
Eval num_timesteps=988000, episode_reward=-172.70 +/- 172.68
Episode length: 15.12 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 988000   |
---------------------------------
Eval num_timesteps=988500, episode_reward=-150.90 +/- 163.01
Episode length: 15.74 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 988500   |
---------------------------------
Eval num_timesteps=989000, episode_reward=-130.68 +/- 200.45
Episode length: 16.26 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 989000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 483      |
|    time_elapsed    | 2052     |
|    total_timesteps | 989184   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=989500, episode_reward=-161.76 +/- 140.75
Episode length: 15.66 +/- 4.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | -162        |
| time/                   |             |
|    total_timesteps      | 989500      |
| train/                  |             |
|    approx_kl            | 0.008299675 |
|    clip_fraction        | 0.016       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.28e+03    |
|    n_updates            | 961         |
|    policy_gradient_loss | 0.00269     |
|    value_loss           | 5.41e+03    |
-----------------------------------------
Eval num_timesteps=990000, episode_reward=-146.66 +/- 146.68
Episode length: 16.36 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 990000   |
---------------------------------
Eval num_timesteps=990500, episode_reward=-124.31 +/- 161.47
Episode length: 16.88 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 990500   |
---------------------------------
Eval num_timesteps=991000, episode_reward=-121.77 +/- 127.82
Episode length: 16.20 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 991000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 484      |
|    time_elapsed    | 2056     |
|    total_timesteps | 991232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=991500, episode_reward=-168.84 +/- 141.69
Episode length: 15.76 +/- 4.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -169        |
| time/                   |             |
|    total_timesteps      | 991500      |
| train/                  |             |
|    approx_kl            | 0.002334765 |
|    clip_fraction        | 0.00595     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0298     |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.18e+03    |
|    n_updates            | 962         |
|    policy_gradient_loss | 0.00104     |
|    value_loss           | 6.65e+03    |
-----------------------------------------
Eval num_timesteps=992000, episode_reward=-171.98 +/- 179.30
Episode length: 15.76 +/- 5.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 992000   |
---------------------------------
Eval num_timesteps=992500, episode_reward=-162.64 +/- 149.27
Episode length: 16.10 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 992500   |
---------------------------------
Eval num_timesteps=993000, episode_reward=-120.39 +/- 144.87
Episode length: 17.52 +/- 5.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 993000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 485      |
|    time_elapsed    | 2060     |
|    total_timesteps | 993280   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 2.97
Eval num_timesteps=993500, episode_reward=-128.02 +/- 161.84
Episode length: 16.20 +/- 5.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.2      |
|    mean_reward          | -128      |
| time/                   |           |
|    total_timesteps      | 993500    |
| train/                  |           |
|    approx_kl            | 0.4265122 |
|    clip_fraction        | 0.00361   |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.0104   |
|    explained_variance   | 0.387     |
|    learning_rate        | 0.0001    |
|    loss                 | 3.41e+03  |
|    n_updates            | 969       |
|    policy_gradient_loss | -0.000393 |
|    value_loss           | 6.56e+03  |
---------------------------------------
Eval num_timesteps=994000, episode_reward=-189.86 +/- 172.44
Episode length: 14.94 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 994000   |
---------------------------------
Eval num_timesteps=994500, episode_reward=-166.74 +/- 144.32
Episode length: 14.90 +/- 4.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 994500   |
---------------------------------
Eval num_timesteps=995000, episode_reward=-145.20 +/- 140.14
Episode length: 15.88 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 995000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 486      |
|    time_elapsed    | 2065     |
|    total_timesteps | 995328   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
Eval num_timesteps=995500, episode_reward=-94.01 +/- 191.35
Episode length: 17.06 +/- 6.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.1       |
|    mean_reward          | -94        |
| time/                   |            |
|    total_timesteps      | 995500     |
| train/                  |            |
|    approx_kl            | 0.00961701 |
|    clip_fraction        | 0.046      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.164     |
|    explained_variance   | 0.369      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.72e+03   |
|    n_updates            | 971        |
|    policy_gradient_loss | 0.00324    |
|    value_loss           | 7.16e+03   |
----------------------------------------
Eval num_timesteps=996000, episode_reward=-127.14 +/- 128.82
Episode length: 16.48 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 996000   |
---------------------------------
Eval num_timesteps=996500, episode_reward=-152.46 +/- 134.73
Episode length: 15.62 +/- 4.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 996500   |
---------------------------------
Eval num_timesteps=997000, episode_reward=-139.69 +/- 156.59
Episode length: 15.96 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 997000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -149     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 487      |
|    time_elapsed    | 2069     |
|    total_timesteps | 997376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=997500, episode_reward=-142.28 +/- 166.85
Episode length: 16.28 +/- 5.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -142        |
| time/                   |             |
|    total_timesteps      | 997500      |
| train/                  |             |
|    approx_kl            | 0.004085529 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.39e+03    |
|    n_updates            | 972         |
|    policy_gradient_loss | 0.00154     |
|    value_loss           | 6.75e+03    |
-----------------------------------------
Eval num_timesteps=998000, episode_reward=-135.92 +/- 176.72
Episode length: 16.84 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 998000   |
---------------------------------
Eval num_timesteps=998500, episode_reward=-142.28 +/- 151.97
Episode length: 17.28 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 998500   |
---------------------------------
Eval num_timesteps=999000, episode_reward=-165.81 +/- 173.65
Episode length: 15.86 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 999000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 488      |
|    time_elapsed    | 2073     |
|    total_timesteps | 999424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=999500, episode_reward=-187.59 +/- 131.60
Episode length: 16.60 +/- 4.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -188        |
| time/                   |             |
|    total_timesteps      | 999500      |
| train/                  |             |
|    approx_kl            | 0.007915841 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.408       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.22e+03    |
|    n_updates            | 973         |
|    policy_gradient_loss | 0.00965     |
|    value_loss           | 5.79e+03    |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=-175.25 +/- 135.02
Episode length: 17.42 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1000500, episode_reward=-210.62 +/- 119.29
Episode length: 15.50 +/- 4.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 1000500  |
---------------------------------
Eval num_timesteps=1001000, episode_reward=-190.01 +/- 132.11
Episode length: 16.44 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 1001000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 489      |
|    time_elapsed    | 2077     |
|    total_timesteps | 1001472  |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-extra-last/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Early stopping at step 0 due to reaching max kl: 0.03
