/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-534.48 +/- 69.95
Episode length: 53.32 +/- 17.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-517.20 +/- 72.46
Episode length: 53.40 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=1500, episode_reward=-509.22 +/- 67.86
Episode length: 51.18 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-525.00 +/- 68.25
Episode length: 53.30 +/- 19.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.7     |
|    ep_rew_mean     | -347     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=-522.14 +/- 75.71
Episode length: 48.90 +/- 16.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.9        |
|    mean_reward          | -522        |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.012364844 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | 0.000122    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.9e+03     |
|    n_updates            | 202         |
|    policy_gradient_loss | -0.00951    |
|    value_loss           | 3.83e+03    |
-----------------------------------------
Eval num_timesteps=3000, episode_reward=-517.34 +/- 78.67
Episode length: 47.64 +/- 15.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-515.54 +/- 79.48
Episode length: 50.66 +/- 20.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=-520.33 +/- 75.72
Episode length: 49.16 +/- 18.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.2     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.3     |
|    ep_rew_mean     | -368     |
| time/              |          |
|    fps             | 226      |
|    iterations      | 2        |
|    time_elapsed    | 18       |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=745.28 +/- 553.84
Episode length: 35.68 +/- 5.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.7        |
|    mean_reward          | 745         |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.009742057 |
|    clip_fraction        | 0.0992      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.05       |
|    explained_variance   | 0.000304    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.85e+03    |
|    n_updates            | 204         |
|    policy_gradient_loss | -0.00586    |
|    value_loss           | 6.18e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=5000, episode_reward=766.34 +/- 675.98
Episode length: 34.52 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
Eval num_timesteps=5500, episode_reward=872.08 +/- 717.89
Episode length: 35.24 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
New best mean reward!
Eval num_timesteps=6000, episode_reward=721.62 +/- 668.44
Episode length: 32.94 +/- 8.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.6     |
|    ep_rew_mean     | -351     |
| time/              |          |
|    fps             | 247      |
|    iterations      | 3        |
|    time_elapsed    | 24       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=6500, episode_reward=841.39 +/- 724.83
Episode length: 34.84 +/- 7.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.8        |
|    mean_reward          | 841         |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.011037775 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2          |
|    explained_variance   | 0.000576    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.45e+03    |
|    n_updates            | 206         |
|    policy_gradient_loss | -0.00527    |
|    value_loss           | 4.72e+03    |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=925.72 +/- 696.35
Episode length: 36.42 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 926      |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
New best mean reward!
Eval num_timesteps=7500, episode_reward=590.03 +/- 580.47
Episode length: 32.86 +/- 7.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 590      |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=971.54 +/- 775.17
Episode length: 35.20 +/- 8.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.5     |
|    ep_rew_mean     | -330     |
| time/              |          |
|    fps             | 259      |
|    iterations      | 4        |
|    time_elapsed    | 31       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=8500, episode_reward=956.11 +/- 699.19
Episode length: 37.12 +/- 5.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.1        |
|    mean_reward          | 956         |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.005192671 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.94       |
|    explained_variance   | -5.54e-05   |
|    learning_rate        | 0.0001      |
|    loss                 | 3.66e+03    |
|    n_updates            | 207         |
|    policy_gradient_loss | -0.000804   |
|    value_loss           | 6.09e+03    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=712.11 +/- 620.72
Episode length: 34.62 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=727.95 +/- 616.44
Episode length: 34.70 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=897.86 +/- 701.06
Episode length: 35.66 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 65       |
|    ep_rew_mean     | -282     |
| time/              |          |
|    fps             | 267      |
|    iterations      | 5        |
|    time_elapsed    | 38       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=10500, episode_reward=927.87 +/- 740.59
Episode length: 35.36 +/- 6.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | 928         |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.004976834 |
|    clip_fraction        | 0.0608      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.86       |
|    explained_variance   | 0.00095     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.81e+03    |
|    n_updates            | 208         |
|    policy_gradient_loss | -0.00811    |
|    value_loss           | 4.93e+03    |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=669.65 +/- 587.90
Episode length: 34.08 +/- 7.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=646.60 +/- 588.41
Episode length: 33.42 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=895.84 +/- 740.90
Episode length: 35.34 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.9     |
|    ep_rew_mean     | -226     |
| time/              |          |
|    fps             | 274      |
|    iterations      | 6        |
|    time_elapsed    | 44       |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=12500, episode_reward=851.05 +/- 688.93
Episode length: 35.70 +/- 6.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.7       |
|    mean_reward          | 851        |
| time/                   |            |
|    total_timesteps      | 12500      |
| train/                  |            |
|    approx_kl            | 0.00650056 |
|    clip_fraction        | 0.0801     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.74      |
|    explained_variance   | 0.00139    |
|    learning_rate        | 0.0001     |
|    loss                 | 2.5e+03    |
|    n_updates            | 209        |
|    policy_gradient_loss | -0.00323   |
|    value_loss           | 6.82e+03   |
----------------------------------------
Eval num_timesteps=13000, episode_reward=753.39 +/- 629.39
Episode length: 34.98 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=925.17 +/- 699.45
Episode length: 36.20 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 925      |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=789.76 +/- 678.99
Episode length: 34.54 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.3     |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 278      |
|    iterations      | 7        |
|    time_elapsed    | 51       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=14500, episode_reward=886.75 +/- 716.74
Episode length: 35.16 +/- 6.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 887          |
| time/                   |              |
|    total_timesteps      | 14500        |
| train/                  |              |
|    approx_kl            | 0.0070386315 |
|    clip_fraction        | 0.0723       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.61        |
|    explained_variance   | 0.00118      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.35e+03     |
|    n_updates            | 210          |
|    policy_gradient_loss | 0.00167      |
|    value_loss           | 8.62e+03     |
------------------------------------------
Eval num_timesteps=15000, episode_reward=842.96 +/- 669.84
Episode length: 35.80 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=793.31 +/- 698.46
Episode length: 34.28 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=943.78 +/- 727.56
Episode length: 35.98 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 944      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 281      |
|    iterations      | 8        |
|    time_elapsed    | 58       |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=16500, episode_reward=831.70 +/- 667.96
Episode length: 35.86 +/- 6.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 832          |
| time/                   |              |
|    total_timesteps      | 16500        |
| train/                  |              |
|    approx_kl            | 0.0070415027 |
|    clip_fraction        | 0.099        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.46        |
|    explained_variance   | 0.000175     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.78e+03     |
|    n_updates            | 211          |
|    policy_gradient_loss | 0.00354      |
|    value_loss           | 8.4e+03      |
------------------------------------------
Eval num_timesteps=17000, episode_reward=684.75 +/- 635.27
Episode length: 34.12 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=818.61 +/- 667.56
Episode length: 35.10 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=973.02 +/- 718.23
Episode length: 36.26 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | -55      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 9        |
|    time_elapsed    | 65       |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=18500, episode_reward=845.28 +/- 657.82
Episode length: 35.78 +/- 6.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.8       |
|    mean_reward          | 845        |
| time/                   |            |
|    total_timesteps      | 18500      |
| train/                  |            |
|    approx_kl            | 0.00624302 |
|    clip_fraction        | 0.0684     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.33      |
|    explained_variance   | 0.000449   |
|    learning_rate        | 0.0001     |
|    loss                 | 4.75e+03   |
|    n_updates            | 212        |
|    policy_gradient_loss | 0.00175    |
|    value_loss           | 7.76e+03   |
----------------------------------------
Eval num_timesteps=19000, episode_reward=655.61 +/- 580.25
Episode length: 33.70 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 656      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=856.90 +/- 738.23
Episode length: 34.54 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=1012.60 +/- 780.15
Episode length: 35.78 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 8.53     |
| time/              |          |
|    fps             | 285      |
|    iterations      | 10       |
|    time_elapsed    | 71       |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=20500, episode_reward=830.18 +/- 663.87
Episode length: 35.32 +/- 6.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 830          |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0050571747 |
|    clip_fraction        | 0.0744       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.00309      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.06e+03     |
|    n_updates            | 213          |
|    policy_gradient_loss | -0.00263     |
|    value_loss           | 8.04e+03     |
------------------------------------------
Eval num_timesteps=21000, episode_reward=883.59 +/- 744.10
Episode length: 34.82 +/- 7.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=693.99 +/- 652.95
Episode length: 33.32 +/- 7.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=730.86 +/- 616.85
Episode length: 34.76 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=913.70 +/- 783.75
Episode length: 34.68 +/- 8.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.3     |
|    ep_rew_mean     | 43.2     |
| time/              |          |
|    fps             | 282      |
|    iterations      | 11       |
|    time_elapsed    | 79       |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=23000, episode_reward=842.69 +/- 644.35
Episode length: 36.18 +/- 5.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 36.2       |
|    mean_reward          | 843        |
| time/                   |            |
|    total_timesteps      | 23000      |
| train/                  |            |
|    approx_kl            | 0.01123019 |
|    clip_fraction        | 0.0643     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -1.08      |
|    explained_variance   | 0.00269    |
|    learning_rate        | 0.0001     |
|    loss                 | 4.28e+03   |
|    n_updates            | 215        |
|    policy_gradient_loss | 0.000718   |
|    value_loss           | 9.5e+03    |
----------------------------------------
Eval num_timesteps=23500, episode_reward=1002.42 +/- 734.79
Episode length: 37.10 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=956.13 +/- 684.93
Episode length: 37.64 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.6     |
|    mean_reward     | 956      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=765.81 +/- 633.64
Episode length: 35.16 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.7     |
|    ep_rew_mean     | 79.1     |
| time/              |          |
|    fps             | 284      |
|    iterations      | 12       |
|    time_elapsed    | 86       |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=25000, episode_reward=766.18 +/- 709.52
Episode length: 33.50 +/- 7.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.5        |
|    mean_reward          | 766         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.007965619 |
|    clip_fraction        | 0.0743      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.803      |
|    explained_variance   | 0.000548    |
|    learning_rate        | 0.0001      |
|    loss                 | 1.04e+04    |
|    n_updates            | 217         |
|    policy_gradient_loss | -0.00175    |
|    value_loss           | 1.28e+04    |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=869.16 +/- 667.58
Episode length: 36.16 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=762.12 +/- 639.56
Episode length: 34.54 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=823.84 +/- 708.10
Episode length: 35.00 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.4     |
|    ep_rew_mean     | 132      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 13       |
|    time_elapsed    | 93       |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=27000, episode_reward=684.30 +/- 602.44
Episode length: 34.00 +/- 7.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 684          |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0039673443 |
|    clip_fraction        | 0.054        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.645       |
|    explained_variance   | 0.00235      |
|    learning_rate        | 0.0001       |
|    loss                 | 5.77e+03     |
|    n_updates            | 218          |
|    policy_gradient_loss | -0.000955    |
|    value_loss           | 1.19e+04     |
------------------------------------------
Eval num_timesteps=27500, episode_reward=719.47 +/- 598.44
Episode length: 34.92 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=691.72 +/- 579.29
Episode length: 34.44 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 692      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=729.38 +/- 597.60
Episode length: 35.40 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 729      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.3     |
|    ep_rew_mean     | 223      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 14       |
|    time_elapsed    | 100      |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=29000, episode_reward=788.21 +/- 663.05
Episode length: 35.12 +/- 6.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 788          |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0045885993 |
|    clip_fraction        | 0.0452       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.49        |
|    explained_variance   | 0.00132      |
|    learning_rate        | 0.0001       |
|    loss                 | 6.4e+03      |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00178     |
|    value_loss           | 2.27e+04     |
------------------------------------------
Eval num_timesteps=29500, episode_reward=848.50 +/- 714.07
Episode length: 35.44 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=699.42 +/- 574.18
Episode length: 34.90 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=774.48 +/- 593.36
Episode length: 35.50 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.9     |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 15       |
|    time_elapsed    | 107      |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=31000, episode_reward=817.58 +/- 656.88
Episode length: 35.16 +/- 6.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 818          |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0037634901 |
|    clip_fraction        | 0.0489       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.331       |
|    explained_variance   | 0.000602     |
|    learning_rate        | 0.0001       |
|    loss                 | 1.46e+04     |
|    n_updates            | 222          |
|    policy_gradient_loss | -0.00361     |
|    value_loss           | 2.92e+04     |
------------------------------------------
Eval num_timesteps=31500, episode_reward=871.20 +/- 711.99
Episode length: 35.34 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=820.85 +/- 681.96
Episode length: 34.80 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=972.37 +/- 696.22
Episode length: 37.28 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.9     |
|    ep_rew_mean     | 445      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 16       |
|    time_elapsed    | 114      |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=33000, episode_reward=853.97 +/- 654.68
Episode length: 35.76 +/- 5.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 854          |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0033954657 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.223       |
|    explained_variance   | 0.000273     |
|    learning_rate        | 0.0001       |
|    loss                 | 1.89e+04     |
|    n_updates            | 223          |
|    policy_gradient_loss | -0.00141     |
|    value_loss           | 3.89e+04     |
------------------------------------------
Eval num_timesteps=33500, episode_reward=733.93 +/- 614.04
Episode length: 34.12 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=988.67 +/- 720.00
Episode length: 36.64 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=638.72 +/- 569.58
Episode length: 33.86 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 639      |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 652      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 17       |
|    time_elapsed    | 120      |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=35000, episode_reward=1045.81 +/- 759.56
Episode length: 35.90 +/- 6.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.9        |
|    mean_reward          | 1.05e+03    |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.020021472 |
|    clip_fraction        | 0.0142      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | -5.96e-06   |
|    learning_rate        | 0.0001      |
|    loss                 | 4.36e+04    |
|    n_updates            | 226         |
|    policy_gradient_loss | -0.00168    |
|    value_loss           | 6.32e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=35500, episode_reward=749.20 +/- 603.64
Episode length: 35.10 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=861.59 +/- 708.23
Episode length: 35.66 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=900.86 +/- 717.24
Episode length: 35.74 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 18       |
|    time_elapsed    | 127      |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=37000, episode_reward=790.01 +/- 651.91
Episode length: 35.84 +/- 7.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 790          |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 0.0037145994 |
|    clip_fraction        | 0.0064       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0444      |
|    explained_variance   | 5.19e-06     |
|    learning_rate        | 0.0001       |
|    loss                 | 3.36e+04     |
|    n_updates            | 232          |
|    policy_gradient_loss | -0.000548    |
|    value_loss           | 7.88e+04     |
------------------------------------------
Eval num_timesteps=37500, episode_reward=716.02 +/- 578.27
Episode length: 35.14 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=881.56 +/- 714.11
Episode length: 35.68 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=939.48 +/- 726.32
Episode length: 36.16 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 19       |
|    time_elapsed    | 134      |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=39000, episode_reward=762.72 +/- 739.63
Episode length: 33.54 +/- 7.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.5        |
|    mean_reward          | 763         |
| time/                   |             |
|    total_timesteps      | 39000       |
| train/                  |             |
|    approx_kl            | 0.000967795 |
|    clip_fraction        | 0.00218     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0122     |
|    explained_variance   | 6.19e-05    |
|    learning_rate        | 0.0001      |
|    loss                 | 2.1e+04     |
|    n_updates            | 235         |
|    policy_gradient_loss | -0.000252   |
|    value_loss           | 5.44e+04    |
-----------------------------------------
Eval num_timesteps=39500, episode_reward=815.30 +/- 688.67
Episode length: 34.80 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=834.78 +/- 651.28
Episode length: 35.86 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=770.41 +/- 635.34
Episode length: 35.52 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 747      |
| time/              |          |
|    fps             | 289      |
|    iterations      | 20       |
|    time_elapsed    | 141      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=839.92 +/- 672.24
Episode length: 35.56 +/- 6.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 840           |
| time/                   |               |
|    total_timesteps      | 41000         |
| train/                  |               |
|    approx_kl            | 0.00020248629 |
|    clip_fraction        | 0.000928      |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00538      |
|    explained_variance   | 0.000153      |
|    learning_rate        | 0.0001        |
|    loss                 | 1.88e+04      |
|    n_updates            | 245           |
|    policy_gradient_loss | -0.000203     |
|    value_loss           | 5.2e+04       |
-------------------------------------------
Eval num_timesteps=41500, episode_reward=722.45 +/- 647.37
Episode length: 34.40 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=974.24 +/- 830.69
Episode length: 34.78 +/- 8.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 974      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=847.36 +/- 714.55
Episode length: 34.88 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=900.17 +/- 742.31
Episode length: 34.70 +/- 7.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 900      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 764      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 21       |
|    time_elapsed    | 149      |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=43500, episode_reward=822.85 +/- 709.32
Episode length: 35.02 +/- 6.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 823          |
| time/                   |              |
|    total_timesteps      | 43500        |
| train/                  |              |
|    approx_kl            | 0.0029605515 |
|    clip_fraction        | 0.000613     |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0103      |
|    explained_variance   | 0.00045      |
|    learning_rate        | 0.0001       |
|    loss                 | 2.95e+04     |
|    n_updates            | 247          |
|    policy_gradient_loss | -2.4e-05     |
|    value_loss           | 6.89e+04     |
------------------------------------------
Eval num_timesteps=44000, episode_reward=954.93 +/- 742.66
Episode length: 35.88 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 955      |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=727.54 +/- 605.03
Episode length: 34.98 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=877.55 +/- 660.87
Episode length: 36.52 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 22       |
|    time_elapsed    | 156      |
|    total_timesteps | 45056    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=45500, episode_reward=913.12 +/- 732.74
Episode length: 35.20 +/- 7.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 913           |
| time/                   |               |
|    total_timesteps      | 45500         |
| train/                  |               |
|    approx_kl            | 0.00076089066 |
|    clip_fraction        | 0.00149       |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.0101       |
|    explained_variance   | 0.000669      |
|    learning_rate        | 0.0001        |
|    loss                 | 1.61e+04      |
|    n_updates            | 250           |
|    policy_gradient_loss | -0.000119     |
|    value_loss           | 6.37e+04      |
-------------------------------------------
Eval num_timesteps=46000, episode_reward=935.30 +/- 748.97
Episode length: 35.62 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 935      |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=883.48 +/- 733.34
Episode length: 34.92 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=872.19 +/- 716.70
Episode length: 35.34 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 288      |
|    iterations      | 23       |
|    time_elapsed    | 163      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=768.34 +/- 639.17
Episode length: 34.58 +/- 6.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 768           |
| time/                   |               |
|    total_timesteps      | 47500         |
| train/                  |               |
|    approx_kl            | 4.8428774e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    clip_range_vf        | 0.2           |
|    entropy_loss         | -0.00198      |
|    explained_variance   | 0.000877      |
|    learning_rate        | 0.0001        |
|    loss                 | 2.8e+04       |
|    n_updates            | 260           |
|    policy_gradient_loss | 1.05e-05      |
|    value_loss           | 5.48e+04      |
-------------------------------------------
Eval num_timesteps=48000, episode_reward=811.53 +/- 615.47
Episode length: 36.26 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=1006.83 +/- 720.99
Episode length: 36.90 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=699.04 +/- 607.79
Episode length: 33.88 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 24       |
|    time_elapsed    | 170      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=704.72 +/- 574.20
Episode length: 35.10 +/- 6.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 705          |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 9.837095e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.000774    |
|    explained_variance   | 0.000858     |
|    learning_rate        | 0.0001       |
|    loss                 | 3.27e+04     |
|    n_updates            | 270          |
|    policy_gradient_loss | 3.94e-06     |
|    value_loss           | 5.13e+04     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=874.43 +/- 736.03
Episode length: 34.68 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=880.91 +/- 735.14
Episode length: 35.52 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=749.42 +/- 691.06
Episode length: 33.78 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 838      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 25       |
|    time_elapsed    | 177      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=733.29 +/- 676.99
Episode length: 34.16 +/- 6.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 733       |
| time/                   |           |
|    total_timesteps      | 51500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.000128 |
|    explained_variance   | 0.0015    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.26e+04  |
|    n_updates            | 280       |
|    policy_gradient_loss | 2.33e-06  |
|    value_loss           | 6.17e+04  |
---------------------------------------
Eval num_timesteps=52000, episode_reward=820.20 +/- 681.11
Episode length: 34.34 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=849.99 +/- 710.97
Episode length: 35.64 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=634.09 +/- 537.44
Episode length: 33.96 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 634      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 896      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 26       |
|    time_elapsed    | 185      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=862.18 +/- 701.93
Episode length: 35.10 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 862       |
| time/                   |           |
|    total_timesteps      | 53500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.33e-06 |
|    explained_variance   | 0.00198   |
|    learning_rate        | 0.0001    |
|    loss                 | 3.27e+04  |
|    n_updates            | 290       |
|    policy_gradient_loss | 3.79e-07  |
|    value_loss           | 7.24e+04  |
---------------------------------------
Eval num_timesteps=54000, episode_reward=828.46 +/- 655.71
Episode length: 35.46 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=860.48 +/- 690.95
Episode length: 35.90 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=842.32 +/- 690.64
Episode length: 35.14 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 27       |
|    time_elapsed    | 192      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=881.15 +/- 660.48
Episode length: 36.50 +/- 6.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 881       |
| time/                   |           |
|    total_timesteps      | 55500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.82e-07 |
|    explained_variance   | 0.00194   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.57e+04  |
|    n_updates            | 300       |
|    policy_gradient_loss | 1.16e-08  |
|    value_loss           | 4.3e+04   |
---------------------------------------
Eval num_timesteps=56000, episode_reward=889.94 +/- 698.40
Episode length: 35.58 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=836.64 +/- 699.45
Episode length: 35.16 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=894.47 +/- 720.63
Episode length: 35.46 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 799      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 28       |
|    time_elapsed    | 199      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=1122.64 +/- 720.92
Episode length: 37.74 +/- 5.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.7      |
|    mean_reward          | 1.12e+03  |
| time/                   |           |
|    total_timesteps      | 57500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-09 |
|    explained_variance   | 0.00349   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.42e+04  |
|    n_updates            | 310       |
|    policy_gradient_loss | 3.57e-09  |
|    value_loss           | 4.79e+04  |
---------------------------------------
New best mean reward!
Eval num_timesteps=58000, episode_reward=897.15 +/- 698.84
Episode length: 35.92 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=724.84 +/- 732.16
Episode length: 33.08 +/- 8.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=866.75 +/- 731.66
Episode length: 34.74 +/- 7.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 29       |
|    time_elapsed    | 206      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=949.69 +/- 684.76
Episode length: 36.32 +/- 5.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 950       |
| time/                   |           |
|    total_timesteps      | 59500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.48e-12 |
|    explained_variance   | 0.00424   |
|    learning_rate        | 0.0001    |
|    loss                 | 9.26e+03  |
|    n_updates            | 320       |
|    policy_gradient_loss | -3.52e-10 |
|    value_loss           | 4.2e+04   |
---------------------------------------
Eval num_timesteps=60000, episode_reward=979.87 +/- 757.99
Episode length: 35.80 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=811.63 +/- 703.28
Episode length: 34.42 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=796.97 +/- 692.18
Episode length: 34.48 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 658      |
| time/              |          |
|    fps             | 287      |
|    iterations      | 30       |
|    time_elapsed    | 213      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=970.21 +/- 714.07
Episode length: 36.30 +/- 6.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 970       |
| time/                   |           |
|    total_timesteps      | 61500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.53e-10 |
|    explained_variance   | 0.00104   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.41e+04  |
|    n_updates            | 330       |
|    policy_gradient_loss | 1.1e-09   |
|    value_loss           | 2.75e+04  |
---------------------------------------
Eval num_timesteps=62000, episode_reward=841.25 +/- 659.10
Episode length: 35.26 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=604.54 +/- 582.09
Episode length: 33.28 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 605      |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=772.17 +/- 591.76
Episode length: 35.82 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 772      |
| time/              |          |
|    fps             | 286      |
|    iterations      | 31       |
|    time_elapsed    | 221      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=620.46 +/- 564.02
Episode length: 33.50 +/- 6.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 620       |
| time/                   |           |
|    total_timesteps      | 63500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.26e-12 |
|    explained_variance   | 0.00574   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.54e+04  |
|    n_updates            | 340       |
|    policy_gradient_loss | 9.87e-10  |
|    value_loss           | 5.57e+04  |
---------------------------------------
Eval num_timesteps=64000, episode_reward=896.95 +/- 738.08
Episode length: 35.38 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=830.80 +/- 686.17
Episode length: 35.44 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=739.81 +/- 634.00
Episode length: 34.86 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=817.24 +/- 725.60
Episode length: 34.46 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 850      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 32       |
|    time_elapsed    | 229      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=890.14 +/- 707.41
Episode length: 35.62 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 890       |
| time/                   |           |
|    total_timesteps      | 66000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.67e-16 |
|    explained_variance   | 0.00452   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.47e+04  |
|    n_updates            | 350       |
|    policy_gradient_loss | -2.6e-10  |
|    value_loss           | 3.45e+04  |
---------------------------------------
Eval num_timesteps=66500, episode_reward=790.37 +/- 621.72
Episode length: 35.50 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=901.83 +/- 753.23
Episode length: 34.98 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=833.61 +/- 711.37
Episode length: 34.42 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 33       |
|    time_elapsed    | 236      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=835.99 +/- 721.31
Episode length: 34.20 +/- 6.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 836       |
| time/                   |           |
|    total_timesteps      | 68000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.44e-20 |
|    explained_variance   | 0.00395   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.62e+04  |
|    n_updates            | 360       |
|    policy_gradient_loss | -5.24e-10 |
|    value_loss           | 3.38e+04  |
---------------------------------------
Eval num_timesteps=68500, episode_reward=826.81 +/- 677.00
Episode length: 35.10 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=849.74 +/- 750.20
Episode length: 35.14 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=715.17 +/- 654.65
Episode length: 33.34 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 808      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 34       |
|    time_elapsed    | 244      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=789.58 +/- 702.16
Episode length: 34.26 +/- 7.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 790       |
| time/                   |           |
|    total_timesteps      | 70000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.07e-26 |
|    explained_variance   | 0.00933   |
|    learning_rate        | 0.0001    |
|    loss                 | 2e+04     |
|    n_updates            | 370       |
|    policy_gradient_loss | -8.64e-10 |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=70500, episode_reward=750.37 +/- 609.56
Episode length: 34.80 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=765.09 +/- 709.13
Episode length: 33.64 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=802.19 +/- 717.76
Episode length: 34.48 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 35       |
|    time_elapsed    | 251      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=959.22 +/- 674.87
Episode length: 36.46 +/- 7.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 959       |
| time/                   |           |
|    total_timesteps      | 72000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.52e-24 |
|    explained_variance   | 0.0011    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.18e+04  |
|    n_updates            | 380       |
|    policy_gradient_loss | -6.88e-10 |
|    value_loss           | 5.48e+04  |
---------------------------------------
Eval num_timesteps=72500, episode_reward=812.34 +/- 646.85
Episode length: 35.44 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=829.81 +/- 678.54
Episode length: 35.26 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=1038.41 +/- 769.55
Episode length: 36.54 +/- 7.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 36       |
|    time_elapsed    | 258      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=827.21 +/- 686.74
Episode length: 35.18 +/- 6.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 827       |
| time/                   |           |
|    total_timesteps      | 74000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.68e-18 |
|    explained_variance   | 0.00518   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.23e+04  |
|    n_updates            | 390       |
|    policy_gradient_loss | -7.38e-10 |
|    value_loss           | 3.55e+04  |
---------------------------------------
Eval num_timesteps=74500, episode_reward=852.60 +/- 697.75
Episode length: 35.54 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=610.65 +/- 526.12
Episode length: 33.88 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 611      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=853.63 +/- 713.12
Episode length: 35.14 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 745      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 37       |
|    time_elapsed    | 265      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=829.33 +/- 654.09
Episode length: 36.10 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 829       |
| time/                   |           |
|    total_timesteps      | 76000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.45e-20 |
|    explained_variance   | 0.00613   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.49e+04  |
|    n_updates            | 400       |
|    policy_gradient_loss | -1.57e-10 |
|    value_loss           | 3.17e+04  |
---------------------------------------
Eval num_timesteps=76500, episode_reward=862.32 +/- 700.50
Episode length: 35.58 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=694.39 +/- 553.28
Episode length: 34.68 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=693.16 +/- 599.73
Episode length: 33.80 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 782      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 38       |
|    time_elapsed    | 273      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=838.05 +/- 753.55
Episode length: 34.06 +/- 7.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 78000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.56e-18 |
|    explained_variance   | 0.0103    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.95e+04  |
|    n_updates            | 410       |
|    policy_gradient_loss | -1.89e-10 |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=78500, episode_reward=670.13 +/- 560.10
Episode length: 34.60 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=833.29 +/- 689.67
Episode length: 34.54 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=840.21 +/- 716.03
Episode length: 35.10 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 844      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 39       |
|    time_elapsed    | 280      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=844.71 +/- 656.30
Episode length: 35.66 +/- 7.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 845       |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.38e-20 |
|    explained_variance   | 0.00667   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.21e+04  |
|    n_updates            | 420       |
|    policy_gradient_loss | -2.53e-10 |
|    value_loss           | 3.81e+04  |
---------------------------------------
Eval num_timesteps=80500, episode_reward=731.40 +/- 636.09
Episode length: 34.14 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=831.21 +/- 647.56
Episode length: 36.36 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=1060.77 +/- 750.88
Episode length: 36.98 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 879      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 40       |
|    time_elapsed    | 287      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=857.65 +/- 672.16
Episode length: 36.12 +/- 5.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 858       |
| time/                   |           |
|    total_timesteps      | 82000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.87e-18 |
|    explained_variance   | 0.0074    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.08e+04  |
|    n_updates            | 430       |
|    policy_gradient_loss | 2.56e-10  |
|    value_loss           | 3.79e+04  |
---------------------------------------
Eval num_timesteps=82500, episode_reward=863.38 +/- 712.36
Episode length: 35.44 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=1019.85 +/- 712.01
Episode length: 36.48 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=766.11 +/- 589.22
Episode length: 36.00 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 847      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 41       |
|    time_elapsed    | 295      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=1020.92 +/- 732.93
Episode length: 36.54 +/- 6.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 1.02e+03  |
| time/                   |           |
|    total_timesteps      | 84000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.14e-20 |
|    explained_variance   | 0.00735   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.2e+04   |
|    n_updates            | 440       |
|    policy_gradient_loss | -3.83e-10 |
|    value_loss           | 3.84e+04  |
---------------------------------------
Eval num_timesteps=84500, episode_reward=866.50 +/- 689.95
Episode length: 36.06 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=814.20 +/- 708.25
Episode length: 34.40 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=808.27 +/- 716.42
Episode length: 34.84 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=658.86 +/- 624.07
Episode length: 33.26 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 809      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 42       |
|    time_elapsed    | 303      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=899.15 +/- 703.82
Episode length: 35.96 +/- 6.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 899       |
| time/                   |           |
|    total_timesteps      | 86500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.43e-18 |
|    explained_variance   | 0.0114    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.68e+04  |
|    n_updates            | 450       |
|    policy_gradient_loss | 1.99e-09  |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=87000, episode_reward=568.30 +/- 516.38
Episode length: 33.36 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 568      |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=843.76 +/- 724.67
Episode length: 35.18 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=903.29 +/- 699.45
Episode length: 36.10 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 43       |
|    time_elapsed    | 310      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=824.95 +/- 627.29
Episode length: 36.22 +/- 6.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 825       |
| time/                   |           |
|    total_timesteps      | 88500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.2e-20  |
|    explained_variance   | 0.00761   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.83e+04  |
|    n_updates            | 460       |
|    policy_gradient_loss | -3.45e-10 |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=89000, episode_reward=814.53 +/- 669.43
Episode length: 35.28 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=590.62 +/- 520.70
Episode length: 33.20 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 591      |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=982.86 +/- 680.86
Episode length: 36.92 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 983      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 756      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 44       |
|    time_elapsed    | 318      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=821.94 +/- 662.41
Episode length: 35.18 +/- 6.81
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.2     |
|    mean_reward          | 822      |
| time/                   |          |
|    total_timesteps      | 90500    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -7e-18   |
|    explained_variance   | 0.00564  |
|    learning_rate        | 0.0001   |
|    loss                 | 2.07e+04 |
|    n_updates            | 470      |
|    policy_gradient_loss | 1.66e-09 |
|    value_loss           | 3.65e+04 |
--------------------------------------
Eval num_timesteps=91000, episode_reward=794.44 +/- 730.87
Episode length: 34.42 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=866.15 +/- 720.96
Episode length: 35.48 +/- 7.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=711.39 +/- 601.35
Episode length: 34.32 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.3     |
|    ep_rew_mean     | 649      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 45       |
|    time_elapsed    | 325      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=838.25 +/- 675.54
Episode length: 35.36 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 92500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.99e-20 |
|    explained_variance   | 0.00425   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.8e+04   |
|    n_updates            | 480       |
|    policy_gradient_loss | -1.33e-09 |
|    value_loss           | 3.28e+04  |
---------------------------------------
Eval num_timesteps=93000, episode_reward=804.56 +/- 680.88
Episode length: 34.54 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=860.02 +/- 730.89
Episode length: 34.50 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=755.35 +/- 659.35
Episode length: 34.54 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 806      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 46       |
|    time_elapsed    | 333      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=822.04 +/- 710.87
Episode length: 34.26 +/- 6.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 822       |
| time/                   |           |
|    total_timesteps      | 94500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.62e-27 |
|    explained_variance   | 0.0092    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.84e+04  |
|    n_updates            | 490       |
|    policy_gradient_loss | -6.69e-10 |
|    value_loss           | 4.02e+04  |
---------------------------------------
Eval num_timesteps=95000, episode_reward=901.17 +/- 763.07
Episode length: 34.88 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=774.16 +/- 634.38
Episode length: 34.84 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=807.22 +/- 676.70
Episode length: 35.14 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 812      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 47       |
|    time_elapsed    | 340      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=832.60 +/- 677.27
Episode length: 35.18 +/- 6.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 833       |
| time/                   |           |
|    total_timesteps      | 96500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.88e-24 |
|    explained_variance   | 0.00498   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.8e+04   |
|    n_updates            | 500       |
|    policy_gradient_loss | 2.11e-09  |
|    value_loss           | 5.23e+04  |
---------------------------------------
Eval num_timesteps=97000, episode_reward=828.57 +/- 715.18
Episode length: 34.28 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=851.99 +/- 597.53
Episode length: 37.10 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=876.87 +/- 752.69
Episode length: 34.60 +/- 7.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 830      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 48       |
|    time_elapsed    | 347      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=873.79 +/- 709.95
Episode length: 35.28 +/- 6.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 874       |
| time/                   |           |
|    total_timesteps      | 98500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.02e-17 |
|    explained_variance   | 0.00994   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.83e+04  |
|    n_updates            | 510       |
|    policy_gradient_loss | -1.19e-09 |
|    value_loss           | 3.99e+04  |
---------------------------------------
Eval num_timesteps=99000, episode_reward=801.81 +/- 657.77
Episode length: 34.94 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=906.91 +/- 685.84
Episode length: 36.58 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=929.38 +/- 731.53
Episode length: 35.74 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 744      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 49       |
|    time_elapsed    | 354      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=804.24 +/- 667.95
Episode length: 34.84 +/- 6.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 804       |
| time/                   |           |
|    total_timesteps      | 100500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.52e-20 |
|    explained_variance   | 0.00311   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.88e+04  |
|    n_updates            | 520       |
|    policy_gradient_loss | 8.5e-10   |
|    value_loss           | 3.19e+04  |
---------------------------------------
Eval num_timesteps=101000, episode_reward=1085.95 +/- 735.03
Episode length: 37.18 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=877.95 +/- 735.33
Episode length: 35.36 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=689.13 +/- 599.04
Episode length: 34.06 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 865      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 50       |
|    time_elapsed    | 362      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=953.16 +/- 746.76
Episode length: 35.84 +/- 7.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 953       |
| time/                   |           |
|    total_timesteps      | 102500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.03e-17 |
|    explained_variance   | 0.0129    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.52e+04  |
|    n_updates            | 530       |
|    policy_gradient_loss | -1.08e-10 |
|    value_loss           | 4.02e+04  |
---------------------------------------
Eval num_timesteps=103000, episode_reward=861.66 +/- 716.86
Episode length: 34.98 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=674.76 +/- 613.33
Episode length: 33.56 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=905.71 +/- 690.43
Episode length: 36.42 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 913      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 51       |
|    time_elapsed    | 369      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=770.82 +/- 673.85
Episode length: 34.62 +/- 6.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 771       |
| time/                   |           |
|    total_timesteps      | 104500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.04e-19 |
|    explained_variance   | 0.0095    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.36e+04  |
|    n_updates            | 540       |
|    policy_gradient_loss | 9.9e-11   |
|    value_loss           | 4.09e+04  |
---------------------------------------
Eval num_timesteps=105000, episode_reward=783.09 +/- 714.32
Episode length: 33.62 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=821.76 +/- 655.88
Episode length: 35.42 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=765.62 +/- 591.54
Episode length: 35.80 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 890      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 52       |
|    time_elapsed    | 376      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=890.61 +/- 664.01
Episode length: 36.22 +/- 5.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 891       |
| time/                   |           |
|    total_timesteps      | 106500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.93e-17 |
|    explained_variance   | 0.00841   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.34e+04  |
|    n_updates            | 550       |
|    policy_gradient_loss | 1.98e-10  |
|    value_loss           | 3.65e+04  |
---------------------------------------
Eval num_timesteps=107000, episode_reward=820.94 +/- 680.24
Episode length: 35.14 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=1012.54 +/- 727.03
Episode length: 36.92 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=852.73 +/- 655.49
Episode length: 36.16 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=834.89 +/- 703.46
Episode length: 35.20 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 768      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 53       |
|    time_elapsed    | 385      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=858.73 +/- 650.62
Episode length: 36.48 +/- 5.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 859       |
| time/                   |           |
|    total_timesteps      | 109000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.13e-19 |
|    explained_variance   | 0.0057    |
|    learning_rate        | 0.0001    |
|    loss                 | 9.29e+03  |
|    n_updates            | 560       |
|    policy_gradient_loss | -9.01e-10 |
|    value_loss           | 3.66e+04  |
---------------------------------------
Eval num_timesteps=109500, episode_reward=766.69 +/- 717.13
Episode length: 33.90 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=823.79 +/- 686.29
Episode length: 35.24 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=705.02 +/- 636.61
Episode length: 33.74 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 682      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 54       |
|    time_elapsed    | 392      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=887.97 +/- 700.99
Episode length: 35.60 +/- 6.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 888       |
| time/                   |           |
|    total_timesteps      | 111000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.11e-17 |
|    explained_variance   | 0.00363   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.73e+04  |
|    n_updates            | 570       |
|    policy_gradient_loss | -7.63e-10 |
|    value_loss           | 3.66e+04  |
---------------------------------------
Eval num_timesteps=111500, episode_reward=892.62 +/- 706.82
Episode length: 35.44 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=828.53 +/- 680.88
Episode length: 34.90 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=995.95 +/- 742.14
Episode length: 36.48 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 55       |
|    time_elapsed    | 400      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=949.60 +/- 689.05
Episode length: 36.96 +/- 5.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37        |
|    mean_reward          | 950       |
| time/                   |           |
|    total_timesteps      | 113000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.01e-19 |
|    explained_variance   | 0.00921   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.26e+04  |
|    n_updates            | 580       |
|    policy_gradient_loss | 8.66e-10  |
|    value_loss           | 3.6e+04   |
---------------------------------------
Eval num_timesteps=113500, episode_reward=767.36 +/- 706.62
Episode length: 34.60 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=885.02 +/- 693.22
Episode length: 35.82 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=693.82 +/- 562.66
Episode length: 34.62 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 56       |
|    time_elapsed    | 407      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=633.98 +/- 534.39
Episode length: 33.78 +/- 6.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 634       |
| time/                   |           |
|    total_timesteps      | 115000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.2e-17  |
|    explained_variance   | 0.00202   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.66e+04  |
|    n_updates            | 590       |
|    policy_gradient_loss | -3.36e-10 |
|    value_loss           | 3.81e+04  |
---------------------------------------
Eval num_timesteps=115500, episode_reward=878.18 +/- 710.08
Episode length: 35.56 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=720.79 +/- 620.79
Episode length: 34.34 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=909.67 +/- 695.04
Episode length: 36.12 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 910      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 741      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 57       |
|    time_elapsed    | 414      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=664.48 +/- 575.34
Episode length: 33.98 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 664       |
| time/                   |           |
|    total_timesteps      | 117000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.1e-19  |
|    explained_variance   | 0.00746   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.94e+04  |
|    n_updates            | 600       |
|    policy_gradient_loss | -1.53e-09 |
|    value_loss           | 3.55e+04  |
---------------------------------------
Eval num_timesteps=117500, episode_reward=659.40 +/- 571.06
Episode length: 33.88 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=933.85 +/- 735.96
Episode length: 35.30 +/- 7.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=852.26 +/- 681.24
Episode length: 36.10 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 689      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 58       |
|    time_elapsed    | 421      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=799.30 +/- 710.50
Episode length: 34.54 +/- 7.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 799       |
| time/                   |           |
|    total_timesteps      | 119000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.39e-17 |
|    explained_variance   | 0.0038    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.74e+04  |
|    n_updates            | 610       |
|    policy_gradient_loss | -2.74e-10 |
|    value_loss           | 3.66e+04  |
---------------------------------------
Eval num_timesteps=119500, episode_reward=975.74 +/- 720.79
Episode length: 35.92 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 976      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=917.21 +/- 765.77
Episode length: 35.32 +/- 7.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=679.00 +/- 587.75
Episode length: 33.84 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 716      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 59       |
|    time_elapsed    | 429      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=903.76 +/- 777.75
Episode length: 34.86 +/- 7.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 904       |
| time/                   |           |
|    total_timesteps      | 121000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.13e-19 |
|    explained_variance   | 0.00787   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.46e+04  |
|    n_updates            | 620       |
|    policy_gradient_loss | 7.01e-10  |
|    value_loss           | 3.17e+04  |
---------------------------------------
Eval num_timesteps=121500, episode_reward=833.41 +/- 693.16
Episode length: 35.68 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=864.72 +/- 677.88
Episode length: 36.12 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=829.31 +/- 632.90
Episode length: 35.78 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 704      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 60       |
|    time_elapsed    | 436      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=752.27 +/- 694.61
Episode length: 33.66 +/- 6.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 752       |
| time/                   |           |
|    total_timesteps      | 123000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.51e-17 |
|    explained_variance   | 0.00534   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.17e+04  |
|    n_updates            | 630       |
|    policy_gradient_loss | -3.77e-10 |
|    value_loss           | 3.64e+04  |
---------------------------------------
Eval num_timesteps=123500, episode_reward=746.41 +/- 604.52
Episode length: 35.20 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=613.03 +/- 488.40
Episode length: 34.16 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 613      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=891.26 +/- 701.55
Episode length: 35.78 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 828      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 61       |
|    time_elapsed    | 443      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=994.28 +/- 741.35
Episode length: 36.24 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 994       |
| time/                   |           |
|    total_timesteps      | 125000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.22e-19 |
|    explained_variance   | 0.0117    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.2e+04   |
|    n_updates            | 640       |
|    policy_gradient_loss | 8.44e-11  |
|    value_loss           | 4.14e+04  |
---------------------------------------
Eval num_timesteps=125500, episode_reward=809.47 +/- 644.98
Episode length: 35.50 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=897.44 +/- 714.81
Episode length: 35.66 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 897      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=1055.92 +/- 769.60
Episode length: 36.10 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 882      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 62       |
|    time_elapsed    | 450      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=1082.80 +/- 739.43
Episode length: 37.08 +/- 6.32
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 37.1     |
|    mean_reward          | 1.08e+03 |
| time/                   |          |
|    total_timesteps      | 127000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.5e-17 |
|    explained_variance   | 0.00866  |
|    learning_rate        | 0.0001   |
|    loss                 | 2.37e+04 |
|    n_updates            | 650      |
|    policy_gradient_loss | 2.07e-10 |
|    value_loss           | 4.06e+04 |
--------------------------------------
Eval num_timesteps=127500, episode_reward=716.75 +/- 632.49
Episode length: 34.02 +/- 8.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=715.85 +/- 591.29
Episode length: 34.92 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=1006.99 +/- 751.88
Episode length: 36.30 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=974.55 +/- 706.65
Episode length: 37.52 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 773      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 63       |
|    time_elapsed    | 459      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=723.82 +/- 569.93
Episode length: 35.26 +/- 5.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 724       |
| time/                   |           |
|    total_timesteps      | 129500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.15e-19 |
|    explained_variance   | 0.00478   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.74e+04  |
|    n_updates            | 660       |
|    policy_gradient_loss | 1.02e-10  |
|    value_loss           | 3.56e+04  |
---------------------------------------
Eval num_timesteps=130000, episode_reward=607.19 +/- 469.83
Episode length: 34.42 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 607      |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=760.42 +/- 655.03
Episode length: 34.96 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=721.64 +/- 630.79
Episode length: 34.58 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 64       |
|    time_elapsed    | 466      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=771.98 +/- 660.39
Episode length: 34.92 +/- 6.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 772       |
| time/                   |           |
|    total_timesteps      | 131500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.41e-17 |
|    explained_variance   | 0.00743   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.36e+04  |
|    n_updates            | 670       |
|    policy_gradient_loss | -5.11e-10 |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=132000, episode_reward=706.37 +/- 646.10
Episode length: 33.82 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=822.04 +/- 636.31
Episode length: 35.62 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=883.70 +/- 693.80
Episode length: 35.82 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 820      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 65       |
|    time_elapsed    | 474      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=781.94 +/- 662.34
Episode length: 34.98 +/- 5.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 782       |
| time/                   |           |
|    total_timesteps      | 133500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.26e-19 |
|    explained_variance   | 0.0076    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.38e+04  |
|    n_updates            | 680       |
|    policy_gradient_loss | -2.3e-10  |
|    value_loss           | 3.41e+04  |
---------------------------------------
Eval num_timesteps=134000, episode_reward=866.24 +/- 638.26
Episode length: 36.24 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=924.87 +/- 715.27
Episode length: 36.04 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 925      |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=880.59 +/- 719.52
Episode length: 35.28 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 66       |
|    time_elapsed    | 481      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=1097.01 +/- 756.99
Episode length: 36.80 +/- 7.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 1.1e+03   |
| time/                   |           |
|    total_timesteps      | 135500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.56e-17 |
|    explained_variance   | 0.014     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.82e+04  |
|    n_updates            | 690       |
|    policy_gradient_loss | 9.59e-10  |
|    value_loss           | 3.97e+04  |
---------------------------------------
Eval num_timesteps=136000, episode_reward=831.96 +/- 658.52
Episode length: 35.60 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=802.23 +/- 675.07
Episode length: 34.62 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=839.40 +/- 649.07
Episode length: 35.84 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 906      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 67       |
|    time_elapsed    | 488      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=962.95 +/- 706.38
Episode length: 36.60 +/- 6.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 963       |
| time/                   |           |
|    total_timesteps      | 137500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.17e-19 |
|    explained_variance   | 0.0104    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.33e+04  |
|    n_updates            | 700       |
|    policy_gradient_loss | 5.65e-10  |
|    value_loss           | 3.65e+04  |
---------------------------------------
Eval num_timesteps=138000, episode_reward=850.18 +/- 659.62
Episode length: 35.88 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=950.77 +/- 747.33
Episode length: 35.42 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 951      |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=728.28 +/- 677.54
Episode length: 33.30 +/- 7.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 891      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 68       |
|    time_elapsed    | 496      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=984.34 +/- 727.96
Episode length: 36.00 +/- 6.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 984       |
| time/                   |           |
|    total_timesteps      | 139500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.46e-22 |
|    explained_variance   | 0.0122    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.93e+04  |
|    n_updates            | 710       |
|    policy_gradient_loss | 1.4e-10   |
|    value_loss           | 3.84e+04  |
---------------------------------------
Eval num_timesteps=140000, episode_reward=907.89 +/- 753.58
Episode length: 35.08 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=891.93 +/- 754.78
Episode length: 35.06 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=836.01 +/- 659.14
Episode length: 35.90 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 798      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 69       |
|    time_elapsed    | 503      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=905.26 +/- 732.70
Episode length: 35.72 +/- 6.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 905       |
| time/                   |           |
|    total_timesteps      | 141500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.93e-20 |
|    explained_variance   | 0.00515   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.96e+04  |
|    n_updates            | 720       |
|    policy_gradient_loss | -1.73e-09 |
|    value_loss           | 4.29e+04  |
---------------------------------------
Eval num_timesteps=142000, episode_reward=1007.05 +/- 814.74
Episode length: 35.14 +/- 8.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=819.58 +/- 688.11
Episode length: 34.56 +/- 7.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=1049.52 +/- 742.37
Episode length: 37.26 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 70       |
|    time_elapsed    | 510      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=925.81 +/- 680.15
Episode length: 36.76 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 926       |
| time/                   |           |
|    total_timesteps      | 143500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.03e-14 |
|    explained_variance   | 0.0114    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.68e+04  |
|    n_updates            | 730       |
|    policy_gradient_loss | -6.64e-10 |
|    value_loss           | 3.64e+04  |
---------------------------------------
Eval num_timesteps=144000, episode_reward=664.48 +/- 597.35
Episode length: 34.32 +/- 7.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 664      |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=826.77 +/- 671.58
Episode length: 35.92 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=832.90 +/- 658.46
Episode length: 36.16 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 854      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 71       |
|    time_elapsed    | 517      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=775.60 +/- 655.41
Episode length: 34.24 +/- 7.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 776       |
| time/                   |           |
|    total_timesteps      | 145500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-16 |
|    explained_variance   | 0.0121    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.08e+04  |
|    n_updates            | 740       |
|    policy_gradient_loss | 7.89e-10  |
|    value_loss           | 4.44e+04  |
---------------------------------------
Eval num_timesteps=146000, episode_reward=1053.20 +/- 753.79
Episode length: 36.62 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=741.24 +/- 605.91
Episode length: 35.30 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=914.02 +/- 704.68
Episode length: 36.18 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 951      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 72       |
|    time_elapsed    | 525      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=911.81 +/- 697.53
Episode length: 36.26 +/- 6.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 912       |
| time/                   |           |
|    total_timesteps      | 147500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.77e-22 |
|    explained_variance   | 0.0108    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.58e+04  |
|    n_updates            | 750       |
|    policy_gradient_loss | 8.54e-10  |
|    value_loss           | 3.74e+04  |
---------------------------------------
Eval num_timesteps=148000, episode_reward=767.26 +/- 711.08
Episode length: 33.62 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=851.21 +/- 655.58
Episode length: 36.22 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=847.79 +/- 642.50
Episode length: 36.08 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=851.68 +/- 710.39
Episode length: 35.28 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 801      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 73       |
|    time_elapsed    | 533      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=852.20 +/- 682.51
Episode length: 35.74 +/- 6.02
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 852       |
| time/                   |           |
|    total_timesteps      | 150000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.13e-20 |
|    explained_variance   | 0.0054    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.78e+04  |
|    n_updates            | 760       |
|    policy_gradient_loss | 1.14e-09  |
|    value_loss           | 4.52e+04  |
---------------------------------------
Eval num_timesteps=150500, episode_reward=702.90 +/- 651.10
Episode length: 33.48 +/- 7.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=921.26 +/- 729.63
Episode length: 35.70 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=728.82 +/- 677.25
Episode length: 33.72 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 729      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 753      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 74       |
|    time_elapsed    | 541      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=951.48 +/- 699.69
Episode length: 36.42 +/- 6.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 951       |
| time/                   |           |
|    total_timesteps      | 152000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.01e-21 |
|    explained_variance   | 0.00439   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.88e+04  |
|    n_updates            | 770       |
|    policy_gradient_loss | -9.04e-10 |
|    value_loss           | 3.54e+04  |
---------------------------------------
Eval num_timesteps=152500, episode_reward=834.82 +/- 673.39
Episode length: 35.74 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=771.90 +/- 668.04
Episode length: 35.02 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=858.15 +/- 647.19
Episode length: 36.50 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 741      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 75       |
|    time_elapsed    | 548      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=864.22 +/- 673.49
Episode length: 35.88 +/- 6.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 864       |
| time/                   |           |
|    total_timesteps      | 154000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.4e-15  |
|    explained_variance   | 0.00455   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.67e+04  |
|    n_updates            | 780       |
|    policy_gradient_loss | -8.82e-10 |
|    value_loss           | 3.33e+04  |
---------------------------------------
Eval num_timesteps=154500, episode_reward=724.31 +/- 630.06
Episode length: 34.36 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=833.53 +/- 700.19
Episode length: 35.26 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=849.83 +/- 645.65
Episode length: 36.40 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 786      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 76       |
|    time_elapsed    | 555      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=867.46 +/- 679.65
Episode length: 35.92 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 867       |
| time/                   |           |
|    total_timesteps      | 156000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.94e-18 |
|    explained_variance   | 0.0101    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.02e+04  |
|    n_updates            | 790       |
|    policy_gradient_loss | 3.23e-10  |
|    value_loss           | 3.86e+04  |
---------------------------------------
Eval num_timesteps=156500, episode_reward=846.50 +/- 710.62
Episode length: 35.08 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=707.40 +/- 578.54
Episode length: 34.44 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=936.12 +/- 670.61
Episode length: 37.22 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 867      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 77       |
|    time_elapsed    | 562      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=754.56 +/- 661.05
Episode length: 34.52 +/- 7.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 755       |
| time/                   |           |
|    total_timesteps      | 158000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.51e-15 |
|    explained_variance   | 0.00715   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.39e+04  |
|    n_updates            | 800       |
|    policy_gradient_loss | -6.66e-10 |
|    value_loss           | 3.82e+04  |
---------------------------------------
Eval num_timesteps=158500, episode_reward=934.52 +/- 724.27
Episode length: 35.66 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 935      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=825.30 +/- 682.65
Episode length: 35.88 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=837.92 +/- 716.65
Episode length: 35.10 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 78       |
|    time_elapsed    | 570      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=826.04 +/- 672.03
Episode length: 35.22 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 826       |
| time/                   |           |
|    total_timesteps      | 160000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.38e-18 |
|    explained_variance   | 0.0065    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.3e+04   |
|    n_updates            | 810       |
|    policy_gradient_loss | 4.57e-10  |
|    value_loss           | 3.4e+04   |
---------------------------------------
Eval num_timesteps=160500, episode_reward=728.35 +/- 643.26
Episode length: 33.92 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=803.82 +/- 613.13
Episode length: 36.10 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=782.40 +/- 666.86
Episode length: 34.80 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 890      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 79       |
|    time_elapsed    | 577      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=898.79 +/- 681.07
Episode length: 36.30 +/- 5.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 899       |
| time/                   |           |
|    total_timesteps      | 162000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.83e-23 |
|    explained_variance   | 0.0158    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.06e+04  |
|    n_updates            | 820       |
|    policy_gradient_loss | -6.91e-10 |
|    value_loss           | 4.29e+04  |
---------------------------------------
Eval num_timesteps=162500, episode_reward=750.07 +/- 601.90
Episode length: 35.04 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=744.83 +/- 669.92
Episode length: 33.96 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=662.00 +/- 578.14
Episode length: 34.08 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 80       |
|    time_elapsed    | 584      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=872.72 +/- 719.51
Episode length: 35.22 +/- 6.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 873       |
| time/                   |           |
|    total_timesteps      | 164000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.84e-21 |
|    explained_variance   | 0.0061    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.26e+04  |
|    n_updates            | 830       |
|    policy_gradient_loss | 5.78e-10  |
|    value_loss           | 4.63e+04  |
---------------------------------------
Eval num_timesteps=164500, episode_reward=879.67 +/- 768.11
Episode length: 34.82 +/- 7.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=682.70 +/- 574.53
Episode length: 34.96 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=760.43 +/- 672.91
Episode length: 33.96 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 81       |
|    time_elapsed    | 591      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=785.00 +/- 689.11
Episode length: 33.88 +/- 6.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 785       |
| time/                   |           |
|    total_timesteps      | 166000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.05e-23 |
|    explained_variance   | 0.0133    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.27e+04  |
|    n_updates            | 840       |
|    policy_gradient_loss | -4.77e-10 |
|    value_loss           | 3.64e+04  |
---------------------------------------
Eval num_timesteps=166500, episode_reward=710.42 +/- 687.97
Episode length: 33.32 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 710      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=852.19 +/- 713.17
Episode length: 34.80 +/- 7.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=894.58 +/- 683.97
Episode length: 35.86 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 713      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 82       |
|    time_elapsed    | 599      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=726.83 +/- 680.45
Episode length: 33.56 +/- 7.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.6      |
|    mean_reward          | 727       |
| time/                   |           |
|    total_timesteps      | 168000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.95e-21 |
|    explained_variance   | -0.000742 |
|    learning_rate        | 0.0001    |
|    loss                 | 2.13e+04  |
|    n_updates            | 850       |
|    policy_gradient_loss | 7.51e-10  |
|    value_loss           | 4.9e+04   |
---------------------------------------
Eval num_timesteps=168500, episode_reward=967.08 +/- 732.76
Episode length: 35.98 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=967.25 +/- 731.23
Episode length: 36.18 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 967      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=881.51 +/- 726.22
Episode length: 34.96 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 740      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 83       |
|    time_elapsed    | 606      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=923.72 +/- 763.76
Episode length: 35.26 +/- 7.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 924       |
| time/                   |           |
|    total_timesteps      | 170000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.61e-23 |
|    explained_variance   | 0.00969   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.48e+04  |
|    n_updates            | 860       |
|    policy_gradient_loss | 3.41e-10  |
|    value_loss           | 3.78e+04  |
---------------------------------------
Eval num_timesteps=170500, episode_reward=1002.23 +/- 739.44
Episode length: 36.56 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=993.68 +/- 710.26
Episode length: 37.78 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=719.06 +/- 616.58
Episode length: 34.50 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=821.84 +/- 645.46
Episode length: 35.52 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 897      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 84       |
|    time_elapsed    | 615      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=817.88 +/- 658.62
Episode length: 35.26 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 818       |
| time/                   |           |
|    total_timesteps      | 172500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.22e-21 |
|    explained_variance   | 0.0138    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.71e+04  |
|    n_updates            | 870       |
|    policy_gradient_loss | -2.47e-10 |
|    value_loss           | 4.59e+04  |
---------------------------------------
Eval num_timesteps=173000, episode_reward=1044.34 +/- 711.25
Episode length: 36.58 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=705.55 +/- 641.49
Episode length: 34.18 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=846.45 +/- 661.93
Episode length: 35.92 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 85       |
|    time_elapsed    | 622      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=1067.53 +/- 759.51
Episode length: 36.92 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 1.07e+03  |
| time/                   |           |
|    total_timesteps      | 174500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.81e-15 |
|    explained_variance   | 0.00761   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.5e+04   |
|    n_updates            | 880       |
|    policy_gradient_loss | 1.5e-10   |
|    value_loss           | 3.63e+04  |
---------------------------------------
Eval num_timesteps=175000, episode_reward=920.79 +/- 739.50
Episode length: 35.46 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=655.58 +/- 593.04
Episode length: 34.44 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 656      |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=811.11 +/- 687.18
Episode length: 34.66 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 902      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 86       |
|    time_elapsed    | 630      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=888.30 +/- 731.07
Episode length: 34.76 +/- 7.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 888       |
| time/                   |           |
|    total_timesteps      | 176500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.07e-17 |
|    explained_variance   | 0.0119    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.42e+04  |
|    n_updates            | 890       |
|    policy_gradient_loss | -9.14e-10 |
|    value_loss           | 4.99e+04  |
---------------------------------------
Eval num_timesteps=177000, episode_reward=820.20 +/- 666.57
Episode length: 35.52 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=872.67 +/- 637.51
Episode length: 36.66 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=690.86 +/- 618.07
Episode length: 34.06 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 87       |
|    time_elapsed    | 637      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=699.44 +/- 669.60
Episode length: 33.46 +/- 7.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 699       |
| time/                   |           |
|    total_timesteps      | 178500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.34e-15 |
|    explained_variance   | 0.00617   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.18e+04  |
|    n_updates            | 900       |
|    policy_gradient_loss | 9.02e-11  |
|    value_loss           | 3.7e+04   |
---------------------------------------
Eval num_timesteps=179000, episode_reward=830.04 +/- 648.67
Episode length: 35.62 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=1090.90 +/- 750.16
Episode length: 37.34 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=832.99 +/- 707.67
Episode length: 35.18 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 791      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 88       |
|    time_elapsed    | 644      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=815.68 +/- 691.28
Episode length: 34.30 +/- 6.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 816       |
| time/                   |           |
|    total_timesteps      | 180500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-17 |
|    explained_variance   | 0.00994   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.34e+04  |
|    n_updates            | 910       |
|    policy_gradient_loss | -2.84e-10 |
|    value_loss           | 4.08e+04  |
---------------------------------------
Eval num_timesteps=181000, episode_reward=883.22 +/- 673.77
Episode length: 36.04 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=719.44 +/- 582.16
Episode length: 34.82 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=813.66 +/- 700.56
Episode length: 34.66 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 814      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 833      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 89       |
|    time_elapsed    | 652      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=1101.82 +/- 740.42
Episode length: 37.54 +/- 5.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.5      |
|    mean_reward          | 1.1e+03   |
| time/                   |           |
|    total_timesteps      | 182500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.46e-15 |
|    explained_variance   | 0.0147    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.7e+04   |
|    n_updates            | 920       |
|    policy_gradient_loss | 9.28e-10  |
|    value_loss           | 3.76e+04  |
---------------------------------------
Eval num_timesteps=183000, episode_reward=816.28 +/- 666.53
Episode length: 35.26 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=1023.12 +/- 720.58
Episode length: 36.84 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=931.15 +/- 724.91
Episode length: 35.94 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 902      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 90       |
|    time_elapsed    | 659      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=827.11 +/- 652.04
Episode length: 35.28 +/- 6.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 827       |
| time/                   |           |
|    total_timesteps      | 184500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.1e-17  |
|    explained_variance   | 0.0132    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.84e+04  |
|    n_updates            | 930       |
|    policy_gradient_loss | -5.18e-10 |
|    value_loss           | 4.55e+04  |
---------------------------------------
Eval num_timesteps=185000, episode_reward=889.61 +/- 737.32
Episode length: 35.12 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 890      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=752.17 +/- 584.99
Episode length: 35.54 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 752      |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=621.88 +/- 670.32
Episode length: 31.82 +/- 7.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 31.8     |
|    mean_reward     | 622      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 860      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 91       |
|    time_elapsed    | 666      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=894.78 +/- 651.08
Episode length: 37.06 +/- 5.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.1      |
|    mean_reward          | 895       |
| time/                   |           |
|    total_timesteps      | 186500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.66e-15 |
|    explained_variance   | 0.00681   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.94e+04  |
|    n_updates            | 940       |
|    policy_gradient_loss | 5.41e-10  |
|    value_loss           | 3.55e+04  |
---------------------------------------
Eval num_timesteps=187000, episode_reward=839.01 +/- 680.54
Episode length: 35.58 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=724.74 +/- 667.10
Episode length: 33.58 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=878.22 +/- 721.37
Episode length: 35.30 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 92       |
|    time_elapsed    | 674      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=660.05 +/- 612.01
Episode length: 33.46 +/- 6.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 660       |
| time/                   |           |
|    total_timesteps      | 188500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.51e-17 |
|    explained_variance   | 0.0125    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.62e+04  |
|    n_updates            | 950       |
|    policy_gradient_loss | 9.9e-11   |
|    value_loss           | 3.84e+04  |
---------------------------------------
Eval num_timesteps=189000, episode_reward=730.78 +/- 621.05
Episode length: 35.34 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=799.44 +/- 622.87
Episode length: 35.72 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=725.67 +/- 613.59
Episode length: 34.36 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 911      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 93       |
|    time_elapsed    | 681      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=670.86 +/- 628.03
Episode length: 33.74 +/- 6.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 671       |
| time/                   |           |
|    total_timesteps      | 190500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.67e-15 |
|    explained_variance   | 0.0187    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.02e+04  |
|    n_updates            | 960       |
|    policy_gradient_loss | 6.55e-11  |
|    value_loss           | 4.17e+04  |
---------------------------------------
Eval num_timesteps=191000, episode_reward=1114.60 +/- 727.90
Episode length: 37.78 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=707.00 +/- 603.31
Episode length: 33.94 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=1080.71 +/- 747.71
Episode length: 37.30 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=745.70 +/- 682.30
Episode length: 34.28 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 878      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 94       |
|    time_elapsed    | 690      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=622.49 +/- 539.29
Episode length: 33.80 +/- 5.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.8      |
|    mean_reward          | 622       |
| time/                   |           |
|    total_timesteps      | 193000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.38e-17 |
|    explained_variance   | 0.0114    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.83e+04  |
|    n_updates            | 970       |
|    policy_gradient_loss | -6.17e-10 |
|    value_loss           | 4.1e+04   |
---------------------------------------
Eval num_timesteps=193500, episode_reward=893.65 +/- 756.73
Episode length: 34.94 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=635.46 +/- 613.54
Episode length: 32.72 +/- 7.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.7     |
|    mean_reward     | 635      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=826.38 +/- 706.21
Episode length: 34.82 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 892      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 95       |
|    time_elapsed    | 697      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=743.15 +/- 646.44
Episode length: 34.58 +/- 6.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 743       |
| time/                   |           |
|    total_timesteps      | 195000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.88e-15 |
|    explained_variance   | 0.0143    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.98e+04  |
|    n_updates            | 980       |
|    policy_gradient_loss | -1.13e-09 |
|    value_loss           | 3.6e+04   |
---------------------------------------
Eval num_timesteps=195500, episode_reward=830.78 +/- 683.35
Episode length: 34.96 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=893.70 +/- 700.25
Episode length: 36.00 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=779.93 +/- 613.07
Episode length: 35.26 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 863      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 96       |
|    time_elapsed    | 704      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=665.95 +/- 531.40
Episode length: 35.12 +/- 6.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 666       |
| time/                   |           |
|    total_timesteps      | 197000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-17 |
|    explained_variance   | 0.00961   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.15e+04  |
|    n_updates            | 990       |
|    policy_gradient_loss | -9.05e-10 |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=197500, episode_reward=793.56 +/- 603.18
Episode length: 36.42 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=906.79 +/- 710.08
Episode length: 36.18 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=806.88 +/- 737.17
Episode length: 33.92 +/- 7.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 829      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 97       |
|    time_elapsed    | 712      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=786.45 +/- 611.81
Episode length: 35.76 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 786       |
| time/                   |           |
|    total_timesteps      | 199000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.52e-15 |
|    explained_variance   | 0.0152    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.88e+04  |
|    n_updates            | 1000      |
|    policy_gradient_loss | -1.11e-09 |
|    value_loss           | 3.84e+04  |
---------------------------------------
Eval num_timesteps=199500, episode_reward=825.96 +/- 650.53
Episode length: 35.44 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=715.85 +/- 558.13
Episode length: 35.48 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 716      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=628.34 +/- 588.75
Episode length: 33.50 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 628      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 894      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 98       |
|    time_elapsed    | 719      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=935.73 +/- 696.91
Episode length: 36.64 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 936       |
| time/                   |           |
|    total_timesteps      | 201000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-17 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.96e+04  |
|    n_updates            | 1010      |
|    policy_gradient_loss | -1.4e-09  |
|    value_loss           | 4.02e+04  |
---------------------------------------
Eval num_timesteps=201500, episode_reward=746.38 +/- 596.64
Episode length: 34.90 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=698.36 +/- 625.71
Episode length: 33.98 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=833.12 +/- 608.99
Episode length: 36.26 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 910      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 99       |
|    time_elapsed    | 726      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=875.90 +/- 683.28
Episode length: 35.72 +/- 5.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 876       |
| time/                   |           |
|    total_timesteps      | 203000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.51e-15 |
|    explained_variance   | 0.0128    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.89e+04  |
|    n_updates            | 1020      |
|    policy_gradient_loss | 7.13e-11  |
|    value_loss           | 3.96e+04  |
---------------------------------------
Eval num_timesteps=203500, episode_reward=744.64 +/- 631.09
Episode length: 34.30 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=839.07 +/- 711.72
Episode length: 34.62 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=772.53 +/- 639.15
Episode length: 34.62 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 803      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 100      |
|    time_elapsed    | 733      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=656.67 +/- 639.61
Episode length: 33.38 +/- 7.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.4      |
|    mean_reward          | 657       |
| time/                   |           |
|    total_timesteps      | 205000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.37e-17 |
|    explained_variance   | 0.00626   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.05e+04  |
|    n_updates            | 1030      |
|    policy_gradient_loss | 3.25e-10  |
|    value_loss           | 3.43e+04  |
---------------------------------------
Eval num_timesteps=205500, episode_reward=737.25 +/- 627.06
Episode length: 35.12 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=722.12 +/- 615.97
Episode length: 34.36 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=789.76 +/- 725.37
Episode length: 34.20 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 820      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 101      |
|    time_elapsed    | 740      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=677.75 +/- 585.47
Episode length: 34.34 +/- 6.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 678       |
| time/                   |           |
|    total_timesteps      | 207000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.47e-15 |
|    explained_variance   | 0.0117    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.98e+04  |
|    n_updates            | 1040      |
|    policy_gradient_loss | 6.26e-10  |
|    value_loss           | 4.18e+04  |
---------------------------------------
Eval num_timesteps=207500, episode_reward=870.28 +/- 735.10
Episode length: 34.62 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 870      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=731.94 +/- 591.10
Episode length: 34.86 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=758.66 +/- 632.50
Episode length: 34.64 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 837      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 102      |
|    time_elapsed    | 748      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=930.92 +/- 703.06
Episode length: 36.50 +/- 6.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 931       |
| time/                   |           |
|    total_timesteps      | 209000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.28e-17 |
|    explained_variance   | 0.00851   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.98e+04  |
|    n_updates            | 1050      |
|    policy_gradient_loss | -7.77e-10 |
|    value_loss           | 3.68e+04  |
---------------------------------------
Eval num_timesteps=209500, episode_reward=895.01 +/- 723.54
Episode length: 35.64 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=812.61 +/- 688.27
Episode length: 34.64 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=850.61 +/- 653.85
Episode length: 36.34 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 702      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 103      |
|    time_elapsed    | 755      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=746.43 +/- 614.72
Episode length: 34.58 +/- 6.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 746       |
| time/                   |           |
|    total_timesteps      | 211000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.33e-15 |
|    explained_variance   | 0.00422   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.35e+04  |
|    n_updates            | 1060      |
|    policy_gradient_loss | 3.62e-10  |
|    value_loss           | 3.57e+04  |
---------------------------------------
Eval num_timesteps=211500, episode_reward=791.17 +/- 633.62
Episode length: 35.42 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=782.76 +/- 674.78
Episode length: 34.16 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=842.59 +/- 673.96
Episode length: 35.86 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 819      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 104      |
|    time_elapsed    | 762      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=863.43 +/- 747.24
Episode length: 34.46 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 863       |
| time/                   |           |
|    total_timesteps      | 213000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.24e-17 |
|    explained_variance   | 0.0129    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.55e+04  |
|    n_updates            | 1070      |
|    policy_gradient_loss | -8.16e-10 |
|    value_loss           | 4.16e+04  |
---------------------------------------
Eval num_timesteps=213500, episode_reward=835.63 +/- 661.14
Episode length: 35.44 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=972.24 +/- 740.83
Episode length: 36.20 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=775.31 +/- 645.12
Episode length: 34.86 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=878.64 +/- 700.51
Episode length: 34.90 +/- 7.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 829      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 105      |
|    time_elapsed    | 771      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=927.79 +/- 746.52
Episode length: 35.58 +/- 6.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 928       |
| time/                   |           |
|    total_timesteps      | 215500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.56e-15 |
|    explained_variance   | 0.00837   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.17e+04  |
|    n_updates            | 1080      |
|    policy_gradient_loss | -1.35e-09 |
|    value_loss           | 3.62e+04  |
---------------------------------------
Eval num_timesteps=216000, episode_reward=900.75 +/- 674.15
Episode length: 35.84 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=797.08 +/- 692.74
Episode length: 34.46 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=891.94 +/- 663.31
Episode length: 36.68 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 869      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 106      |
|    time_elapsed    | 778      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=935.45 +/- 714.21
Episode length: 36.10 +/- 6.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 935       |
| time/                   |           |
|    total_timesteps      | 217500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.44e-17 |
|    explained_variance   | 0.012     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.4e+04   |
|    n_updates            | 1090      |
|    policy_gradient_loss | 9.58e-10  |
|    value_loss           | 3.96e+04  |
---------------------------------------
Eval num_timesteps=218000, episode_reward=821.71 +/- 687.21
Episode length: 34.86 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=1038.50 +/- 707.13
Episode length: 37.26 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=828.74 +/- 662.49
Episode length: 35.94 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 829      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 107      |
|    time_elapsed    | 786      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=1009.84 +/- 784.34
Episode length: 35.72 +/- 7.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 219500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.61e-15 |
|    explained_variance   | 0.00398   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.94e+04  |
|    n_updates            | 1100      |
|    policy_gradient_loss | -1.29e-09 |
|    value_loss           | 3.53e+04  |
---------------------------------------
Eval num_timesteps=220000, episode_reward=693.95 +/- 600.19
Episode length: 34.10 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=834.65 +/- 717.03
Episode length: 34.30 +/- 7.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=735.52 +/- 656.19
Episode length: 34.38 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 732      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 108      |
|    time_elapsed    | 793      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=679.33 +/- 596.46
Episode length: 34.18 +/- 6.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 679       |
| time/                   |           |
|    total_timesteps      | 221500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-17 |
|    explained_variance   | 0.00777   |
|    learning_rate        | 0.0001    |
|    loss                 | 8.6e+03   |
|    n_updates            | 1110      |
|    policy_gradient_loss | -8.88e-10 |
|    value_loss           | 3.46e+04  |
---------------------------------------
Eval num_timesteps=222000, episode_reward=660.78 +/- 553.85
Episode length: 34.60 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=827.71 +/- 662.33
Episode length: 35.80 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 828      |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=798.79 +/- 661.29
Episode length: 35.14 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 796      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 109      |
|    time_elapsed    | 800      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=936.64 +/- 719.08
Episode length: 35.74 +/- 6.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 937       |
| time/                   |           |
|    total_timesteps      | 223500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.63e-15 |
|    explained_variance   | 0.0148    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.29e+04  |
|    n_updates            | 1120      |
|    policy_gradient_loss | 2.07e-10  |
|    value_loss           | 3.97e+04  |
---------------------------------------
Eval num_timesteps=224000, episode_reward=683.25 +/- 618.63
Episode length: 33.34 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=1005.76 +/- 741.83
Episode length: 36.40 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=894.37 +/- 724.70
Episode length: 35.58 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 877      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 110      |
|    time_elapsed    | 807      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=1035.80 +/- 737.56
Episode length: 36.80 +/- 5.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.8      |
|    mean_reward          | 1.04e+03  |
| time/                   |           |
|    total_timesteps      | 225500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.13e-17 |
|    explained_variance   | 0.00888   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.69e+04  |
|    n_updates            | 1130      |
|    policy_gradient_loss | -8.29e-11 |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=226000, episode_reward=783.73 +/- 650.59
Episode length: 34.88 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=971.81 +/- 779.58
Episode length: 35.88 +/- 7.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=816.46 +/- 652.53
Episode length: 35.64 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 111      |
|    time_elapsed    | 815      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=849.22 +/- 685.72
Episode length: 35.74 +/- 6.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 849       |
| time/                   |           |
|    total_timesteps      | 227500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.37e-23 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.24e+04  |
|    n_updates            | 1140      |
|    policy_gradient_loss | 9.55e-10  |
|    value_loss           | 3.96e+04  |
---------------------------------------
Eval num_timesteps=228000, episode_reward=815.56 +/- 705.91
Episode length: 34.94 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=937.44 +/- 752.90
Episode length: 35.26 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 937      |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=856.56 +/- 734.55
Episode length: 34.58 +/- 7.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 857      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 835      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 112      |
|    time_elapsed    | 822      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=935.06 +/- 699.49
Episode length: 36.00 +/- 5.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 935       |
| time/                   |           |
|    total_timesteps      | 229500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.17e-21 |
|    explained_variance   | 0.0061    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.78e+04  |
|    n_updates            | 1150      |
|    policy_gradient_loss | -5.65e-10 |
|    value_loss           | 4.95e+04  |
---------------------------------------
Eval num_timesteps=230000, episode_reward=791.97 +/- 643.58
Episode length: 35.62 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=786.06 +/- 676.31
Episode length: 34.30 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=840.26 +/- 646.63
Episode length: 36.08 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 883      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 113      |
|    time_elapsed    | 829      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=724.83 +/- 586.66
Episode length: 35.42 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 725       |
| time/                   |           |
|    total_timesteps      | 231500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.41e-23 |
|    explained_variance   | 0.0187    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.34e+04  |
|    n_updates            | 1160      |
|    policy_gradient_loss | -4.03e-10 |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=232000, episode_reward=861.42 +/- 641.73
Episode length: 36.26 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=857.50 +/- 675.20
Episode length: 35.88 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 858      |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=925.69 +/- 756.16
Episode length: 35.58 +/- 7.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 926      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 883      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 114      |
|    time_elapsed    | 837      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=786.01 +/- 711.74
Episode length: 34.18 +/- 7.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 786       |
| time/                   |           |
|    total_timesteps      | 233500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.96e-21 |
|    explained_variance   | 0.00615   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.67e+04  |
|    n_updates            | 1170      |
|    policy_gradient_loss | 2.05e-09  |
|    value_loss           | 4.8e+04   |
---------------------------------------
Eval num_timesteps=234000, episode_reward=914.32 +/- 722.90
Episode length: 35.72 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=794.50 +/- 681.14
Episode length: 34.72 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 795      |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=838.02 +/- 682.94
Episode length: 35.52 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=689.65 +/- 585.91
Episode length: 34.54 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 690      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 115      |
|    time_elapsed    | 845      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=708.32 +/- 620.16
Episode length: 34.40 +/- 6.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 708       |
| time/                   |           |
|    total_timesteps      | 236000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.2e-15  |
|    explained_variance   | 0.0127    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.64e+04  |
|    n_updates            | 1180      |
|    policy_gradient_loss | -8.61e-10 |
|    value_loss           | 4.08e+04  |
---------------------------------------
Eval num_timesteps=236500, episode_reward=903.42 +/- 700.61
Episode length: 35.92 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=928.80 +/- 715.56
Episode length: 35.78 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=967.94 +/- 759.11
Episode length: 35.82 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 723      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 116      |
|    time_elapsed    | 852      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=825.28 +/- 731.14
Episode length: 34.02 +/- 7.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 825       |
| time/                   |           |
|    total_timesteps      | 238000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.57e-17 |
|    explained_variance   | 0.0108    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.7e+04   |
|    n_updates            | 1190      |
|    policy_gradient_loss | -2.33e-10 |
|    value_loss           | 3.52e+04  |
---------------------------------------
Eval num_timesteps=238500, episode_reward=889.42 +/- 691.38
Episode length: 36.22 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=615.69 +/- 552.65
Episode length: 33.62 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 616      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=773.37 +/- 653.53
Episode length: 35.16 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 117      |
|    time_elapsed    | 860      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=974.70 +/- 689.46
Episode length: 37.70 +/- 5.54
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.7      |
|    mean_reward          | 975       |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.86e-16 |
|    explained_variance   | 0.00869   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.65e+04  |
|    n_updates            | 1200      |
|    policy_gradient_loss | -6.27e-10 |
|    value_loss           | 3.68e+04  |
---------------------------------------
Eval num_timesteps=240500, episode_reward=723.02 +/- 628.07
Episode length: 34.72 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=782.09 +/- 699.92
Episode length: 34.06 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=578.90 +/- 511.58
Episode length: 33.50 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 579      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 792      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 118      |
|    time_elapsed    | 867      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=733.12 +/- 622.99
Episode length: 34.48 +/- 5.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 733       |
| time/                   |           |
|    total_timesteps      | 242000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.02e-18 |
|    explained_variance   | 0.0101    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.52e+04  |
|    n_updates            | 1210      |
|    policy_gradient_loss | -1.41e-10 |
|    value_loss           | 3.86e+04  |
---------------------------------------
Eval num_timesteps=242500, episode_reward=919.78 +/- 685.20
Episode length: 36.72 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=689.72 +/- 577.61
Episode length: 34.82 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 690      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=1033.65 +/- 726.74
Episode length: 36.98 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 788      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 119      |
|    time_elapsed    | 874      |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=864.54 +/- 648.03
Episode length: 36.32 +/- 6.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 865       |
| time/                   |           |
|    total_timesteps      | 244000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.56e-16 |
|    explained_variance   | 0.0103    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.42e+04  |
|    n_updates            | 1220      |
|    policy_gradient_loss | -4.66e-11 |
|    value_loss           | 3.78e+04  |
---------------------------------------
Eval num_timesteps=244500, episode_reward=746.50 +/- 661.13
Episode length: 34.06 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=997.54 +/- 715.79
Episode length: 37.26 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=906.67 +/- 738.28
Episode length: 35.44 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 892      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 120      |
|    time_elapsed    | 882      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=732.02 +/- 587.41
Episode length: 35.04 +/- 6.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 732       |
| time/                   |           |
|    total_timesteps      | 246000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.74e-18 |
|    explained_variance   | 0.0155    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.09e+04  |
|    n_updates            | 1230      |
|    policy_gradient_loss | -1.43e-09 |
|    value_loss           | 4.28e+04  |
---------------------------------------
Eval num_timesteps=246500, episode_reward=778.83 +/- 638.05
Episode length: 35.10 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=852.99 +/- 647.34
Episode length: 36.56 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=906.42 +/- 733.65
Episode length: 34.74 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 882      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 121      |
|    time_elapsed    | 889      |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=941.47 +/- 706.17
Episode length: 36.46 +/- 6.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 941       |
| time/                   |           |
|    total_timesteps      | 248000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.98e-16 |
|    explained_variance   | 0.0107    |
|    learning_rate        | 0.0001    |
|    loss                 | 2e+04     |
|    n_updates            | 1240      |
|    policy_gradient_loss | 6.18e-10  |
|    value_loss           | 3.56e+04  |
---------------------------------------
Eval num_timesteps=248500, episode_reward=726.43 +/- 579.60
Episode length: 34.78 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=980.30 +/- 730.56
Episode length: 36.56 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=686.65 +/- 591.53
Episode length: 34.66 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 828      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 122      |
|    time_elapsed    | 897      |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=814.90 +/- 675.15
Episode length: 34.92 +/- 5.78
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 815       |
| time/                   |           |
|    total_timesteps      | 250000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.9e-11  |
|    explained_variance   | 0.00975   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.37e+04  |
|    n_updates            | 1250      |
|    policy_gradient_loss | -6.13e-10 |
|    value_loss           | 3.49e+04  |
---------------------------------------
Eval num_timesteps=250500, episode_reward=761.80 +/- 672.40
Episode length: 33.64 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=796.37 +/- 680.07
Episode length: 34.96 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=974.87 +/- 743.13
Episode length: 36.06 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 838      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 123      |
|    time_elapsed    | 904      |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=934.06 +/- 727.64
Episode length: 35.44 +/- 6.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 934       |
| time/                   |           |
|    total_timesteps      | 252000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.68e-13 |
|    explained_variance   | 0.00875   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.5e+04   |
|    n_updates            | 1260      |
|    policy_gradient_loss | -6.05e-10 |
|    value_loss           | 4e+04     |
---------------------------------------
Eval num_timesteps=252500, episode_reward=1014.24 +/- 776.73
Episode length: 35.62 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=760.44 +/- 605.59
Episode length: 34.92 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=718.36 +/- 668.62
Episode length: 33.54 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 124      |
|    time_elapsed    | 911      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=728.50 +/- 634.21
Episode length: 34.56 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 729       |
| time/                   |           |
|    total_timesteps      | 254000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.12e-18 |
|    explained_variance   | 0.0082    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.23e+04  |
|    n_updates            | 1270      |
|    policy_gradient_loss | -6.81e-10 |
|    value_loss           | 3.52e+04  |
---------------------------------------
Eval num_timesteps=254500, episode_reward=673.63 +/- 585.86
Episode length: 34.22 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 674      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=814.93 +/- 674.38
Episode length: 34.52 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=1019.82 +/- 708.15
Episode length: 37.26 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=693.47 +/- 559.97
Episode length: 34.50 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 808      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 125      |
|    time_elapsed    | 920      |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=911.80 +/- 707.10
Episode length: 36.30 +/- 6.73
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 36.3     |
|    mean_reward          | 912      |
| time/                   |          |
|    total_timesteps      | 256500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -9e-16   |
|    explained_variance   | 0.0113   |
|    learning_rate        | 0.0001   |
|    loss                 | 2.04e+04 |
|    n_updates            | 1280     |
|    policy_gradient_loss | -5.3e-10 |
|    value_loss           | 4.04e+04 |
--------------------------------------
Eval num_timesteps=257000, episode_reward=762.42 +/- 670.44
Episode length: 34.34 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=783.22 +/- 632.61
Episode length: 35.26 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=824.03 +/- 677.14
Episode length: 35.18 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 749      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 126      |
|    time_elapsed    | 927      |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=825.96 +/- 665.92
Episode length: 35.26 +/- 6.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 826       |
| time/                   |           |
|    total_timesteps      | 258500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.74e-11 |
|    explained_variance   | 0.0078    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.81e+04  |
|    n_updates            | 1290      |
|    policy_gradient_loss | -3.19e-10 |
|    value_loss           | 3.34e+04  |
---------------------------------------
Eval num_timesteps=259000, episode_reward=887.59 +/- 646.51
Episode length: 36.62 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=972.92 +/- 686.53
Episode length: 36.52 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=614.77 +/- 592.46
Episode length: 32.86 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 615      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 836      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 127      |
|    time_elapsed    | 934      |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=861.26 +/- 712.60
Episode length: 35.00 +/- 7.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 861       |
| time/                   |           |
|    total_timesteps      | 260500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.78e-13 |
|    explained_variance   | 0.00924   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.57e+04  |
|    n_updates            | 1300      |
|    policy_gradient_loss | 8.91e-10  |
|    value_loss           | 4.55e+04  |
---------------------------------------
Eval num_timesteps=261000, episode_reward=719.01 +/- 570.92
Episode length: 34.72 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=1000.40 +/- 735.70
Episode length: 37.02 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=1038.85 +/- 765.08
Episode length: 36.02 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 128      |
|    time_elapsed    | 941      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=838.16 +/- 644.24
Episode length: 35.82 +/- 6.57
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.8     |
|    mean_reward          | 838      |
| time/                   |          |
|    total_timesteps      | 262500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -6.2e-18 |
|    explained_variance   | 0.012    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.58e+04 |
|    n_updates            | 1310     |
|    policy_gradient_loss | 4.87e-10 |
|    value_loss           | 3.83e+04 |
--------------------------------------
Eval num_timesteps=263000, episode_reward=821.37 +/- 702.61
Episode length: 34.60 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=769.91 +/- 683.93
Episode length: 34.60 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=816.39 +/- 638.38
Episode length: 35.82 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 892      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 129      |
|    time_elapsed    | 949      |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=896.93 +/- 722.30
Episode length: 35.68 +/- 6.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 897       |
| time/                   |           |
|    total_timesteps      | 264500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.36e-22 |
|    explained_variance   | 0.0185    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.04e+04  |
|    n_updates            | 1320      |
|    policy_gradient_loss | 6.69e-11  |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=265000, episode_reward=829.86 +/- 658.28
Episode length: 35.56 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=887.84 +/- 717.84
Episode length: 35.12 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=826.24 +/- 695.87
Episode length: 35.06 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 846      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 130      |
|    time_elapsed    | 956      |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=879.11 +/- 729.14
Episode length: 35.34 +/- 6.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 879       |
| time/                   |           |
|    total_timesteps      | 266500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.68e-19 |
|    explained_variance   | 0.00885   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.53e+04  |
|    n_updates            | 1330      |
|    policy_gradient_loss | -2.1e-09  |
|    value_loss           | 4.67e+04  |
---------------------------------------
Eval num_timesteps=267000, episode_reward=757.33 +/- 594.76
Episode length: 35.66 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=1095.45 +/- 709.80
Episode length: 38.06 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.1     |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=739.93 +/- 595.73
Episode length: 35.56 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 747      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 131      |
|    time_elapsed    | 963      |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=906.29 +/- 724.09
Episode length: 35.90 +/- 7.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 906       |
| time/                   |           |
|    total_timesteps      | 268500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.94e-14 |
|    explained_variance   | 0.0121    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.58e+04  |
|    n_updates            | 1340      |
|    policy_gradient_loss | 1.37e-10  |
|    value_loss           | 3.62e+04  |
---------------------------------------
Eval num_timesteps=269000, episode_reward=797.70 +/- 610.78
Episode length: 35.82 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=726.45 +/- 622.08
Episode length: 34.10 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=871.80 +/- 703.86
Episode length: 35.62 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 824      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 132      |
|    time_elapsed    | 970      |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=710.58 +/- 539.57
Episode length: 35.32 +/- 5.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 711       |
| time/                   |           |
|    total_timesteps      | 270500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.41e-16 |
|    explained_variance   | 0.0143    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.27e+04  |
|    n_updates            | 1350      |
|    policy_gradient_loss | -5.95e-10 |
|    value_loss           | 4.54e+04  |
---------------------------------------
Eval num_timesteps=271000, episode_reward=947.70 +/- 734.63
Episode length: 36.12 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 948      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=789.28 +/- 620.66
Episode length: 34.94 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=981.16 +/- 751.65
Episode length: 36.06 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 981      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 870      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 133      |
|    time_elapsed    | 978      |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=870.77 +/- 673.09
Episode length: 36.02 +/- 7.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 871       |
| time/                   |           |
|    total_timesteps      | 272500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.14e-22 |
|    explained_variance   | 0.0118    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.8e+04   |
|    n_updates            | 1360      |
|    policy_gradient_loss | 5.94e-10  |
|    value_loss           | 3.62e+04  |
---------------------------------------
Eval num_timesteps=273000, episode_reward=920.71 +/- 712.53
Episode length: 35.92 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=703.71 +/- 618.21
Episode length: 33.90 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=898.23 +/- 702.50
Episode length: 35.58 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 803      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 134      |
|    time_elapsed    | 985      |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=794.69 +/- 666.23
Episode length: 35.54 +/- 6.51
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.5     |
|    mean_reward          | 795      |
| time/                   |          |
|    total_timesteps      | 274500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -1.7e-19 |
|    explained_variance   | 0.00518  |
|    learning_rate        | 0.0001   |
|    loss                 | 1.96e+04 |
|    n_updates            | 1370     |
|    policy_gradient_loss | 5.82e-12 |
|    value_loss           | 4.45e+04 |
--------------------------------------
Eval num_timesteps=275000, episode_reward=800.71 +/- 714.34
Episode length: 34.14 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=646.63 +/- 616.85
Episode length: 33.52 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=824.71 +/- 725.06
Episode length: 34.54 +/- 7.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 800      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 135      |
|    time_elapsed    | 992      |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=847.93 +/- 732.62
Episode length: 34.60 +/- 7.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 848       |
| time/                   |           |
|    total_timesteps      | 276500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.58e-22 |
|    explained_variance   | 0.0168    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.46e+04  |
|    n_updates            | 1380      |
|    policy_gradient_loss | -2.27e-10 |
|    value_loss           | 4.12e+04  |
---------------------------------------
Eval num_timesteps=277000, episode_reward=773.49 +/- 702.79
Episode length: 34.42 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=865.93 +/- 699.81
Episode length: 35.86 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=688.86 +/- 624.36
Episode length: 34.20 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=770.15 +/- 661.95
Episode length: 34.22 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 136      |
|    time_elapsed    | 1001     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=653.73 +/- 640.02
Episode length: 33.10 +/- 6.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.1      |
|    mean_reward          | 654       |
| time/                   |           |
|    total_timesteps      | 279000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.88e-19 |
|    explained_variance   | 0.0124    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.75e+04  |
|    n_updates            | 1390      |
|    policy_gradient_loss | -1.3e-09  |
|    value_loss           | 4.63e+04  |
---------------------------------------
Eval num_timesteps=279500, episode_reward=890.90 +/- 727.22
Episode length: 35.70 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=920.41 +/- 703.79
Episode length: 36.06 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=909.18 +/- 704.45
Episode length: 36.42 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 909      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 838      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 137      |
|    time_elapsed    | 1008     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=969.05 +/- 777.68
Episode length: 35.58 +/- 7.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 969       |
| time/                   |           |
|    total_timesteps      | 281000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.42e-14 |
|    explained_variance   | 0.0173    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.62e+04  |
|    n_updates            | 1400      |
|    policy_gradient_loss | -5.34e-10 |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=281500, episode_reward=891.23 +/- 683.81
Episode length: 36.20 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=821.45 +/- 686.62
Episode length: 35.66 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=785.55 +/- 643.97
Episode length: 34.86 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 885      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 138      |
|    time_elapsed    | 1016     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=905.76 +/- 707.44
Episode length: 35.24 +/- 7.16
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.2     |
|    mean_reward          | 906      |
| time/                   |          |
|    total_timesteps      | 283000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -6.9e-17 |
|    explained_variance   | 0.0141   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.78e+04 |
|    n_updates            | 1410     |
|    policy_gradient_loss | 1.73e-10 |
|    value_loss           | 4.22e+04 |
--------------------------------------
Eval num_timesteps=283500, episode_reward=876.82 +/- 679.24
Episode length: 35.74 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=735.57 +/- 578.91
Episode length: 35.14 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=816.71 +/- 642.60
Episode length: 35.90 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 917      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 139      |
|    time_elapsed    | 1023     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=725.29 +/- 597.67
Episode length: 34.98 +/- 6.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 725       |
| time/                   |           |
|    total_timesteps      | 285000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.53e-14 |
|    explained_variance   | 0.0159    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.02e+04  |
|    n_updates            | 1420      |
|    policy_gradient_loss | -1.11e-10 |
|    value_loss           | 4.11e+04  |
---------------------------------------
Eval num_timesteps=285500, episode_reward=1008.16 +/- 727.56
Episode length: 36.58 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=792.05 +/- 645.00
Episode length: 35.44 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=753.05 +/- 630.51
Episode length: 34.64 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 845      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 140      |
|    time_elapsed    | 1030     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=983.00 +/- 752.69
Episode length: 36.54 +/- 7.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 983       |
| time/                   |           |
|    total_timesteps      | 287000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.47e-11 |
|    explained_variance   | 0.011     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.56e+04  |
|    n_updates            | 1430      |
|    policy_gradient_loss | 5.21e-10  |
|    value_loss           | 3.7e+04   |
---------------------------------------
Eval num_timesteps=287500, episode_reward=978.64 +/- 760.58
Episode length: 35.70 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 979      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=791.05 +/- 707.76
Episode length: 34.14 +/- 7.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=846.88 +/- 699.49
Episode length: 35.56 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 847      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 141      |
|    time_elapsed    | 1037     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=803.42 +/- 656.52
Episode length: 35.02 +/- 6.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 803       |
| time/                   |           |
|    total_timesteps      | 289000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.54e-13 |
|    explained_variance   | 0.0117    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.7e+04   |
|    n_updates            | 1440      |
|    policy_gradient_loss | -1.27e-09 |
|    value_loss           | 4.83e+04  |
---------------------------------------
Eval num_timesteps=289500, episode_reward=685.47 +/- 611.66
Episode length: 34.10 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 685      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=837.37 +/- 687.82
Episode length: 35.16 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=701.40 +/- 595.14
Episode length: 34.14 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 902      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 142      |
|    time_elapsed    | 1045     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=937.02 +/- 747.02
Episode length: 35.80 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 937       |
| time/                   |           |
|    total_timesteps      | 291000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-17 |
|    explained_variance   | 0.014     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.97e+04  |
|    n_updates            | 1450      |
|    policy_gradient_loss | -6.75e-10 |
|    value_loss           | 4.21e+04  |
---------------------------------------
Eval num_timesteps=291500, episode_reward=764.33 +/- 648.13
Episode length: 34.60 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=730.61 +/- 683.76
Episode length: 33.66 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=802.33 +/- 668.94
Episode length: 34.90 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 908      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 143      |
|    time_elapsed    | 1052     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=874.04 +/- 653.45
Episode length: 36.08 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 874       |
| time/                   |           |
|    total_timesteps      | 293000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.75e-15 |
|    explained_variance   | 0.0214    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.23e+04  |
|    n_updates            | 1460      |
|    policy_gradient_loss | -6.65e-10 |
|    value_loss           | 4.14e+04  |
---------------------------------------
Eval num_timesteps=293500, episode_reward=583.92 +/- 473.49
Episode length: 33.78 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 584      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=840.06 +/- 731.46
Episode length: 34.18 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=899.23 +/- 663.34
Episode length: 36.24 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 899      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 144      |
|    time_elapsed    | 1059     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=919.11 +/- 694.04
Episode length: 36.12 +/- 6.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 919       |
| time/                   |           |
|    total_timesteps      | 295000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.51e-17 |
|    explained_variance   | 0.0111    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.18e+04  |
|    n_updates            | 1470      |
|    policy_gradient_loss | -1.53e-09 |
|    value_loss           | 4.08e+04  |
---------------------------------------
Eval num_timesteps=295500, episode_reward=801.86 +/- 684.40
Episode length: 33.82 +/- 7.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=893.06 +/- 712.90
Episode length: 35.32 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=1017.83 +/- 744.44
Episode length: 36.90 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 817      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 145      |
|    time_elapsed    | 1067     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=616.35 +/- 593.14
Episode length: 33.04 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33        |
|    mean_reward          | 616       |
| time/                   |           |
|    total_timesteps      | 297000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.97e-15 |
|    explained_variance   | 0.0152    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.22e+04  |
|    n_updates            | 1480      |
|    policy_gradient_loss | -7.68e-10 |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=297500, episode_reward=748.23 +/- 679.03
Episode length: 33.94 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=803.42 +/- 672.41
Episode length: 35.18 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=1006.12 +/- 754.48
Episode length: 36.08 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=829.43 +/- 635.03
Episode length: 36.08 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 799      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 146      |
|    time_elapsed    | 1075     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=887.92 +/- 712.73
Episode length: 35.46 +/- 6.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 888       |
| time/                   |           |
|    total_timesteps      | 299500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.51e-17 |
|    explained_variance   | 0.0104    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.34e+04  |
|    n_updates            | 1490      |
|    policy_gradient_loss | -2.33e-10 |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=300000, episode_reward=819.41 +/- 691.45
Episode length: 35.20 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=970.33 +/- 728.16
Episode length: 36.08 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=782.68 +/- 669.32
Episode length: 35.12 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 785      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 147      |
|    time_elapsed    | 1082     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=780.81 +/- 691.27
Episode length: 34.32 +/- 6.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 781       |
| time/                   |           |
|    total_timesteps      | 301500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.13e-15 |
|    explained_variance   | 0.00933   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.57e+04  |
|    n_updates            | 1500      |
|    policy_gradient_loss | -1.31e-09 |
|    value_loss           | 4.16e+04  |
---------------------------------------
Eval num_timesteps=302000, episode_reward=791.29 +/- 718.80
Episode length: 33.92 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=720.83 +/- 629.24
Episode length: 34.42 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=829.29 +/- 684.14
Episode length: 35.60 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 828      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 148      |
|    time_elapsed    | 1089     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=1001.56 +/- 729.80
Episode length: 36.56 +/- 6.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 1e+03     |
| time/                   |           |
|    total_timesteps      | 303500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.74e-17 |
|    explained_variance   | 0.0132    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.84e+04  |
|    n_updates            | 1510      |
|    policy_gradient_loss | 2.33e-10  |
|    value_loss           | 4.23e+04  |
---------------------------------------
Eval num_timesteps=304000, episode_reward=953.95 +/- 690.87
Episode length: 36.62 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 954      |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=973.71 +/- 721.56
Episode length: 36.66 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 974      |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=770.29 +/- 651.41
Episode length: 35.00 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 921      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 149      |
|    time_elapsed    | 1097     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=920.69 +/- 727.87
Episode length: 35.56 +/- 6.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 921       |
| time/                   |           |
|    total_timesteps      | 305500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.71e-23 |
|    explained_variance   | 0.0168    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.11e+04  |
|    n_updates            | 1520      |
|    policy_gradient_loss | 4.09e-10  |
|    value_loss           | 4e+04     |
---------------------------------------
Eval num_timesteps=306000, episode_reward=786.20 +/- 641.73
Episode length: 35.66 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=920.73 +/- 657.25
Episode length: 36.90 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=544.33 +/- 464.88
Episode length: 32.88 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 544      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 896      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 150      |
|    time_elapsed    | 1104     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=768.26 +/- 666.21
Episode length: 34.38 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 768       |
| time/                   |           |
|    total_timesteps      | 307500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.11e-20 |
|    explained_variance   | 0.0135    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.26e+04  |
|    n_updates            | 1530      |
|    policy_gradient_loss | -6.78e-10 |
|    value_loss           | 5.07e+04  |
---------------------------------------
Eval num_timesteps=308000, episode_reward=907.22 +/- 700.93
Episode length: 35.46 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=832.26 +/- 664.49
Episode length: 35.78 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=760.38 +/- 629.44
Episode length: 34.92 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 875      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 151      |
|    time_elapsed    | 1111     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=997.12 +/- 749.91
Episode length: 36.28 +/- 6.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 997       |
| time/                   |           |
|    total_timesteps      | 309500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.86e-23 |
|    explained_variance   | 0.0154    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.89e+04  |
|    n_updates            | 1540      |
|    policy_gradient_loss | -5.38e-10 |
|    value_loss           | 3.64e+04  |
---------------------------------------
Eval num_timesteps=310000, episode_reward=747.18 +/- 622.41
Episode length: 34.90 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=1101.86 +/- 741.29
Episode length: 36.68 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=800.04 +/- 660.08
Episode length: 35.04 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 849      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 152      |
|    time_elapsed    | 1119     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=724.64 +/- 629.48
Episode length: 34.60 +/- 5.96
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.6     |
|    mean_reward          | 725      |
| time/                   |          |
|    total_timesteps      | 311500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -6e-20   |
|    explained_variance   | 0.0121   |
|    learning_rate        | 0.0001   |
|    loss                 | 3.03e+04 |
|    n_updates            | 1550     |
|    policy_gradient_loss | 1.12e-09 |
|    value_loss           | 4.94e+04 |
--------------------------------------
Eval num_timesteps=312000, episode_reward=854.20 +/- 733.92
Episode length: 34.56 +/- 7.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=725.06 +/- 643.93
Episode length: 34.88 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=801.03 +/- 674.08
Episode length: 34.88 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 153      |
|    time_elapsed    | 1126     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=929.59 +/- 706.98
Episode length: 36.48 +/- 5.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 930       |
| time/                   |           |
|    total_timesteps      | 313500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-14 |
|    explained_variance   | 0.0144    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.08e+04  |
|    n_updates            | 1560      |
|    policy_gradient_loss | -2.15e-10 |
|    value_loss           | 3.89e+04  |
---------------------------------------
Eval num_timesteps=314000, episode_reward=772.26 +/- 612.82
Episode length: 35.64 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=683.77 +/- 593.02
Episode length: 34.28 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=817.50 +/- 659.02
Episode length: 35.14 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 736      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 154      |
|    time_elapsed    | 1133     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=669.46 +/- 631.88
Episode length: 33.68 +/- 7.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 669       |
| time/                   |           |
|    total_timesteps      | 315500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.78e-17 |
|    explained_variance   | 0.00863   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.04e+04  |
|    n_updates            | 1570      |
|    policy_gradient_loss | 1.72e-10  |
|    value_loss           | 3.16e+04  |
---------------------------------------
Eval num_timesteps=316000, episode_reward=573.60 +/- 497.15
Episode length: 32.92 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 574      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=698.12 +/- 537.90
Episode length: 35.26 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=714.80 +/- 591.99
Episode length: 34.76 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 715      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 793      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 155      |
|    time_elapsed    | 1140     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=917.60 +/- 738.05
Episode length: 36.04 +/- 6.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 918       |
| time/                   |           |
|    total_timesteps      | 317500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.58e-20 |
|    explained_variance   | 0.0162    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.06e+04  |
|    n_updates            | 1580      |
|    policy_gradient_loss | 4.28e-10  |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=318000, episode_reward=833.49 +/- 674.96
Episode length: 35.94 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=755.78 +/- 616.49
Episode length: 34.72 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=973.40 +/- 718.26
Episode length: 36.26 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 803      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 156      |
|    time_elapsed    | 1148     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=797.91 +/- 686.33
Episode length: 34.60 +/- 6.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 798       |
| time/                   |           |
|    total_timesteps      | 319500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.22e-15 |
|    explained_variance   | 0.0166    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.97e+04  |
|    n_updates            | 1590      |
|    policy_gradient_loss | -2.14e-10 |
|    value_loss           | 4.16e+04  |
---------------------------------------
Eval num_timesteps=320000, episode_reward=872.33 +/- 677.60
Episode length: 35.58 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=810.83 +/- 658.41
Episode length: 35.76 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=810.87 +/- 684.58
Episode length: 35.16 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=850.60 +/- 669.33
Episode length: 35.66 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 823      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 157      |
|    time_elapsed    | 1156     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=977.88 +/- 777.21
Episode length: 35.66 +/- 6.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 978       |
| time/                   |           |
|    total_timesteps      | 322000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.02e-18 |
|    explained_variance   | 0.0147    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.75e+04  |
|    n_updates            | 1600      |
|    policy_gradient_loss | 2.9e-10   |
|    value_loss           | 3.92e+04  |
---------------------------------------
Eval num_timesteps=322500, episode_reward=905.27 +/- 670.91
Episode length: 37.06 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=846.15 +/- 725.72
Episode length: 34.86 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=795.58 +/- 757.37
Episode length: 33.62 +/- 7.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 731      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 158      |
|    time_elapsed    | 1164     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=850.53 +/- 722.42
Episode length: 34.58 +/- 6.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 851       |
| time/                   |           |
|    total_timesteps      | 324000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.21e-15 |
|    explained_variance   | 0.00299   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.87e+04  |
|    n_updates            | 1610      |
|    policy_gradient_loss | -1.04e-09 |
|    value_loss           | 3.45e+04  |
---------------------------------------
Eval num_timesteps=324500, episode_reward=914.14 +/- 680.80
Episode length: 37.06 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 914      |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=970.34 +/- 782.42
Episode length: 35.70 +/- 8.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=698.60 +/- 668.20
Episode length: 33.20 +/- 7.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 706      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 159      |
|    time_elapsed    | 1171     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=836.08 +/- 667.96
Episode length: 35.52 +/- 6.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 836       |
| time/                   |           |
|    total_timesteps      | 326000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.05e-17 |
|    explained_variance   | 0.0113    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.01e+04  |
|    n_updates            | 1620      |
|    policy_gradient_loss | 9.72e-10  |
|    value_loss           | 3.32e+04  |
---------------------------------------
Eval num_timesteps=326500, episode_reward=722.23 +/- 565.65
Episode length: 35.10 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=876.34 +/- 675.85
Episode length: 35.80 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=754.53 +/- 602.16
Episode length: 35.86 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 807      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 160      |
|    time_elapsed    | 1178     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=879.66 +/- 721.13
Episode length: 35.20 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 880       |
| time/                   |           |
|    total_timesteps      | 328000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.53e-21 |
|    explained_variance   | 0.0139    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.26e+04  |
|    n_updates            | 1630      |
|    policy_gradient_loss | -6.46e-10 |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=328500, episode_reward=907.44 +/- 662.76
Episode length: 36.54 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=880.82 +/- 687.14
Episode length: 35.74 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=916.75 +/- 742.10
Episode length: 35.24 +/- 7.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 864      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 161      |
|    time_elapsed    | 1186     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=779.70 +/- 648.45
Episode length: 35.32 +/- 5.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 780       |
| time/                   |           |
|    total_timesteps      | 330000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.89e-19 |
|    explained_variance   | 0.0139    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.04e+04  |
|    n_updates            | 1640      |
|    policy_gradient_loss | -2.06e-09 |
|    value_loss           | 4.44e+04  |
---------------------------------------
Eval num_timesteps=330500, episode_reward=779.40 +/- 635.69
Episode length: 35.54 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=722.88 +/- 593.80
Episode length: 34.62 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=891.74 +/- 739.18
Episode length: 35.16 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 984      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 162      |
|    time_elapsed    | 1193     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=716.95 +/- 627.75
Episode length: 34.62 +/- 6.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 717       |
| time/                   |           |
|    total_timesteps      | 332000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.22e-16 |
|    explained_variance   | 0.0261    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.99e+04  |
|    n_updates            | 1650      |
|    policy_gradient_loss | 2.55e-10  |
|    value_loss           | 4.54e+04  |
---------------------------------------
Eval num_timesteps=332500, episode_reward=911.25 +/- 759.86
Episode length: 34.96 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=852.47 +/- 693.42
Episode length: 35.22 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=688.86 +/- 662.90
Episode length: 32.94 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 689      |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 163      |
|    time_elapsed    | 1200     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=829.82 +/- 693.17
Episode length: 35.06 +/- 6.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 830       |
| time/                   |           |
|    total_timesteps      | 334000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.14e-11 |
|    explained_variance   | 0.01      |
|    learning_rate        | 0.0001    |
|    loss                 | 2.21e+04  |
|    n_updates            | 1660      |
|    policy_gradient_loss | -4.21e-10 |
|    value_loss           | 3.45e+04  |
---------------------------------------
Eval num_timesteps=334500, episode_reward=771.26 +/- 673.31
Episode length: 34.62 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 771      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=983.92 +/- 771.17
Episode length: 35.48 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 984      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=780.37 +/- 626.42
Episode length: 35.58 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 813      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 164      |
|    time_elapsed    | 1208     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=821.14 +/- 685.55
Episode length: 35.30 +/- 6.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 821       |
| time/                   |           |
|    total_timesteps      | 336000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.62e-14 |
|    explained_variance   | 0.0172    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.74e+04  |
|    n_updates            | 1670      |
|    policy_gradient_loss | -9.9e-11  |
|    value_loss           | 4.86e+04  |
---------------------------------------
Eval num_timesteps=336500, episode_reward=694.36 +/- 624.79
Episode length: 33.68 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=830.38 +/- 708.11
Episode length: 34.08 +/- 7.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=682.48 +/- 581.60
Episode length: 34.06 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 916      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 165      |
|    time_elapsed    | 1215     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=883.61 +/- 705.25
Episode length: 35.98 +/- 7.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 884       |
| time/                   |           |
|    total_timesteps      | 338000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.56e-18 |
|    explained_variance   | 0.0186    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.85e+04  |
|    n_updates            | 1680      |
|    policy_gradient_loss | -1.72e-10 |
|    value_loss           | 3.88e+04  |
---------------------------------------
Eval num_timesteps=338500, episode_reward=831.72 +/- 745.30
Episode length: 34.30 +/- 7.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=797.03 +/- 641.28
Episode length: 35.12 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=920.69 +/- 730.43
Episode length: 35.96 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 883      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 166      |
|    time_elapsed    | 1222     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=767.12 +/- 697.24
Episode length: 34.08 +/- 6.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 767       |
| time/                   |           |
|    total_timesteps      | 340000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.37e-16 |
|    explained_variance   | 0.0195    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.65e+04  |
|    n_updates            | 1690      |
|    policy_gradient_loss | 1.3e-09   |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=340500, episode_reward=808.92 +/- 691.54
Episode length: 35.00 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=782.00 +/- 699.47
Episode length: 34.54 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=833.31 +/- 714.53
Episode length: 34.40 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=791.71 +/- 664.40
Episode length: 35.06 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 887      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 167      |
|    time_elapsed    | 1230     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=882.83 +/- 726.13
Episode length: 35.34 +/- 6.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 883       |
| time/                   |           |
|    total_timesteps      | 342500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.47e-11 |
|    explained_variance   | 0.0176    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.38e+04  |
|    n_updates            | 1700      |
|    policy_gradient_loss | 2.04e-10  |
|    value_loss           | 4.12e+04  |
---------------------------------------
Eval num_timesteps=343000, episode_reward=795.98 +/- 669.39
Episode length: 34.80 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=679.61 +/- 626.45
Episode length: 33.64 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 680      |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=854.36 +/- 648.24
Episode length: 35.78 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 168      |
|    time_elapsed    | 1238     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=692.80 +/- 580.36
Episode length: 34.26 +/- 6.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 693       |
| time/                   |           |
|    total_timesteps      | 344500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.13e-14 |
|    explained_variance   | 0.012     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.28e+04  |
|    n_updates            | 1710      |
|    policy_gradient_loss | 9.31e-11  |
|    value_loss           | 3.69e+04  |
---------------------------------------
Eval num_timesteps=345000, episode_reward=808.30 +/- 687.67
Episode length: 35.02 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=923.35 +/- 734.49
Episode length: 35.64 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=811.14 +/- 658.32
Episode length: 34.90 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 900      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 169      |
|    time_elapsed    | 1245     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=706.07 +/- 689.01
Episode length: 33.38 +/- 7.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.4      |
|    mean_reward          | 706       |
| time/                   |           |
|    total_timesteps      | 346500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.68e-18 |
|    explained_variance   | 0.0189    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.03e+04  |
|    n_updates            | 1720      |
|    policy_gradient_loss | 6.05e-10  |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=347000, episode_reward=971.42 +/- 731.05
Episode length: 36.20 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=813.06 +/- 659.45
Episode length: 35.32 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=799.40 +/- 679.34
Episode length: 35.44 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 777      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 170      |
|    time_elapsed    | 1252     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=897.56 +/- 745.56
Episode length: 34.66 +/- 7.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 898       |
| time/                   |           |
|    total_timesteps      | 348500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.73e-16 |
|    explained_variance   | 0.0126    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.03e+04  |
|    n_updates            | 1730      |
|    policy_gradient_loss | 2.4e-09   |
|    value_loss           | 3.89e+04  |
---------------------------------------
Eval num_timesteps=349000, episode_reward=822.94 +/- 669.29
Episode length: 35.28 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=847.98 +/- 687.92
Episode length: 35.30 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=968.97 +/- 717.53
Episode length: 36.38 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 969      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 748      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 171      |
|    time_elapsed    | 1260     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=880.04 +/- 681.61
Episode length: 36.22 +/- 6.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 880       |
| time/                   |           |
|    total_timesteps      | 350500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.19e-18 |
|    explained_variance   | 0.0126    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.9e+04   |
|    n_updates            | 1740      |
|    policy_gradient_loss | -1.2e-09  |
|    value_loss           | 3.48e+04  |
---------------------------------------
Eval num_timesteps=351000, episode_reward=747.41 +/- 672.18
Episode length: 33.90 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=838.05 +/- 643.49
Episode length: 35.98 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=879.79 +/- 717.19
Episode length: 35.50 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 755      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 172      |
|    time_elapsed    | 1267     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=974.72 +/- 730.73
Episode length: 35.86 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 975       |
| time/                   |           |
|    total_timesteps      | 352500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.22e-16 |
|    explained_variance   | 0.00793   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.23e+04  |
|    n_updates            | 1750      |
|    policy_gradient_loss | -1.18e-09 |
|    value_loss           | 3.7e+04   |
---------------------------------------
Eval num_timesteps=353000, episode_reward=921.39 +/- 660.81
Episode length: 36.56 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=859.52 +/- 722.30
Episode length: 35.06 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=755.81 +/- 664.70
Episode length: 34.68 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 839      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 173      |
|    time_elapsed    | 1274     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=930.06 +/- 723.29
Episode length: 36.22 +/- 6.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 930       |
| time/                   |           |
|    total_timesteps      | 354500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.02e-18 |
|    explained_variance   | 0.0176    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.97e+04  |
|    n_updates            | 1760      |
|    policy_gradient_loss | -1.11e-09 |
|    value_loss           | 3.92e+04  |
---------------------------------------
Eval num_timesteps=355000, episode_reward=908.49 +/- 716.71
Episode length: 35.36 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=891.41 +/- 720.40
Episode length: 35.30 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=941.61 +/- 682.97
Episode length: 36.66 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 174      |
|    time_elapsed    | 1282     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=792.23 +/- 614.64
Episode length: 35.94 +/- 5.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 792       |
| time/                   |           |
|    total_timesteps      | 356500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.71e-16 |
|    explained_variance   | 0.00846   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.43e+04  |
|    n_updates            | 1770      |
|    policy_gradient_loss | 1.16e-11  |
|    value_loss           | 3.95e+04  |
---------------------------------------
Eval num_timesteps=357000, episode_reward=923.30 +/- 739.12
Episode length: 35.34 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=750.36 +/- 668.22
Episode length: 34.38 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=702.77 +/- 622.67
Episode length: 34.44 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 835      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 175      |
|    time_elapsed    | 1289     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=882.17 +/- 728.45
Episode length: 35.06 +/- 6.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 882       |
| time/                   |           |
|    total_timesteps      | 358500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.46e-18 |
|    explained_variance   | 0.0152    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.92e+04  |
|    n_updates            | 1780      |
|    policy_gradient_loss | 6.04e-10  |
|    value_loss           | 3.66e+04  |
---------------------------------------
Eval num_timesteps=359000, episode_reward=756.91 +/- 684.50
Episode length: 33.94 +/- 7.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=616.29 +/- 576.08
Episode length: 32.72 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.7     |
|    mean_reward     | 616      |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=1124.92 +/- 741.66
Episode length: 37.76 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 857      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 176      |
|    time_elapsed    | 1296     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=700.43 +/- 587.50
Episode length: 34.98 +/- 6.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 700       |
| time/                   |           |
|    total_timesteps      | 360500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.12e-15 |
|    explained_variance   | 0.0155    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.38e+04  |
|    n_updates            | 1790      |
|    policy_gradient_loss | 1.62e-09  |
|    value_loss           | 4.04e+04  |
---------------------------------------
Eval num_timesteps=361000, episode_reward=941.07 +/- 681.15
Episode length: 36.88 +/- 5.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 941      |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=969.60 +/- 749.81
Episode length: 35.56 +/- 7.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=956.23 +/- 665.08
Episode length: 37.16 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 956      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 177      |
|    time_elapsed    | 1304     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=806.52 +/- 624.11
Episode length: 36.12 +/- 5.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 807       |
| time/                   |           |
|    total_timesteps      | 362500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.22e-18 |
|    explained_variance   | 0.0163    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.89e+04  |
|    n_updates            | 1800      |
|    policy_gradient_loss | 3.09e-10  |
|    value_loss           | 3.58e+04  |
---------------------------------------
Eval num_timesteps=363000, episode_reward=901.10 +/- 711.36
Episode length: 35.36 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=917.27 +/- 719.26
Episode length: 36.12 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=907.27 +/- 706.66
Episode length: 35.54 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=737.87 +/- 597.64
Episode length: 34.96 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 877      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 178      |
|    time_elapsed    | 1312     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=740.68 +/- 677.70
Episode length: 33.86 +/- 7.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 741       |
| time/                   |           |
|    total_timesteps      | 365000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.09e-15 |
|    explained_variance   | 0.0228    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.37e+04  |
|    n_updates            | 1810      |
|    policy_gradient_loss | 2.45e-09  |
|    value_loss           | 4.15e+04  |
---------------------------------------
Eval num_timesteps=365500, episode_reward=958.99 +/- 694.76
Episode length: 36.18 +/- 7.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 959      |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=805.32 +/- 643.51
Episode length: 35.36 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=809.10 +/- 628.75
Episode length: 35.38 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 908      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 179      |
|    time_elapsed    | 1320     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=820.43 +/- 669.36
Episode length: 35.12 +/- 7.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 820       |
| time/                   |           |
|    total_timesteps      | 367000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.34e-18 |
|    explained_variance   | 0.0185    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.57e+04  |
|    n_updates            | 1820      |
|    policy_gradient_loss | -2.71e-10 |
|    value_loss           | 3.96e+04  |
---------------------------------------
Eval num_timesteps=367500, episode_reward=820.37 +/- 725.16
Episode length: 34.16 +/- 7.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=756.35 +/- 668.02
Episode length: 34.38 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=877.74 +/- 693.64
Episode length: 35.50 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 841      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 180      |
|    time_elapsed    | 1327     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=825.53 +/- 628.87
Episode length: 36.06 +/- 5.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 826       |
| time/                   |           |
|    total_timesteps      | 369000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-15 |
|    explained_variance   | 0.0158    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.64e+04  |
|    n_updates            | 1830      |
|    policy_gradient_loss | -1.85e-10 |
|    value_loss           | 4.05e+04  |
---------------------------------------
Eval num_timesteps=369500, episode_reward=931.33 +/- 706.28
Episode length: 36.06 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=883.89 +/- 707.65
Episode length: 35.60 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=633.12 +/- 625.53
Episode length: 32.12 +/- 7.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.1     |
|    mean_reward     | 633      |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 853      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 181      |
|    time_elapsed    | 1334     |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=732.36 +/- 593.13
Episode length: 34.88 +/- 6.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 732       |
| time/                   |           |
|    total_timesteps      | 371000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.19e-18 |
|    explained_variance   | 0.0154    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.84e+04  |
|    n_updates            | 1840      |
|    policy_gradient_loss | 3.81e-10  |
|    value_loss           | 4.07e+04  |
---------------------------------------
Eval num_timesteps=371500, episode_reward=652.45 +/- 576.10
Episode length: 33.54 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=723.29 +/- 616.62
Episode length: 34.36 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=713.41 +/- 605.61
Episode length: 34.96 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 713      |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 794      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 182      |
|    time_elapsed    | 1341     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=985.14 +/- 717.58
Episode length: 36.26 +/- 6.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 985       |
| time/                   |           |
|    total_timesteps      | 373000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.4e-15  |
|    explained_variance   | 0.0178    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.77e+04  |
|    n_updates            | 1850      |
|    policy_gradient_loss | -3.75e-10 |
|    value_loss           | 4.07e+04  |
---------------------------------------
Eval num_timesteps=373500, episode_reward=983.20 +/- 759.54
Episode length: 36.08 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 983      |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=739.55 +/- 643.93
Episode length: 34.30 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 740      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=898.10 +/- 714.26
Episode length: 35.14 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 673      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 183      |
|    time_elapsed    | 1348     |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=1006.80 +/- 746.33
Episode length: 36.72 +/- 6.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 375000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.58e-11 |
|    explained_variance   | 0.0085    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.94e+04  |
|    n_updates            | 1860      |
|    policy_gradient_loss | 4.95e-10  |
|    value_loss           | 3.17e+04  |
---------------------------------------
Eval num_timesteps=375500, episode_reward=928.04 +/- 722.71
Episode length: 35.88 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=879.38 +/- 700.07
Episode length: 35.46 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=659.87 +/- 558.66
Episode length: 34.44 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 660      |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 672      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 184      |
|    time_elapsed    | 1356     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=686.89 +/- 583.72
Episode length: 34.62 +/- 6.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 687       |
| time/                   |           |
|    total_timesteps      | 377000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.52e-13 |
|    explained_variance   | 0.0178    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.1e+04   |
|    n_updates            | 1870      |
|    policy_gradient_loss | -9.2e-10  |
|    value_loss           | 4.01e+04  |
---------------------------------------
Eval num_timesteps=377500, episode_reward=760.67 +/- 627.95
Episode length: 34.98 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=874.10 +/- 628.97
Episode length: 36.42 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=835.54 +/- 664.96
Episode length: 35.82 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 836      |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 833      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 185      |
|    time_elapsed    | 1363     |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=781.32 +/- 698.86
Episode length: 35.02 +/- 7.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 781       |
| time/                   |           |
|    total_timesteps      | 379000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.92e-18 |
|    explained_variance   | 0.0142    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.73e+04  |
|    n_updates            | 1880      |
|    policy_gradient_loss | 2.76e-10  |
|    value_loss           | 3.53e+04  |
---------------------------------------
Eval num_timesteps=379500, episode_reward=881.83 +/- 703.46
Episode length: 35.98 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=891.63 +/- 739.58
Episode length: 35.60 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=936.31 +/- 733.42
Episode length: 35.62 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 882      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 186      |
|    time_elapsed    | 1370     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=907.68 +/- 763.41
Episode length: 35.20 +/- 7.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 908       |
| time/                   |           |
|    total_timesteps      | 381000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.64e-15 |
|    explained_variance   | 0.0214    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.88e+04  |
|    n_updates            | 1890      |
|    policy_gradient_loss | -2.69e-10 |
|    value_loss           | 3.96e+04  |
---------------------------------------
Eval num_timesteps=381500, episode_reward=884.05 +/- 730.29
Episode length: 35.64 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=928.16 +/- 737.15
Episode length: 35.58 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=887.37 +/- 718.13
Episode length: 35.60 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 887      |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 865      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 187      |
|    time_elapsed    | 1378     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=692.34 +/- 634.80
Episode length: 33.70 +/- 6.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.7      |
|    mean_reward          | 692       |
| time/                   |           |
|    total_timesteps      | 383000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.19e-18 |
|    explained_variance   | 0.0191    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.99e+04  |
|    n_updates            | 1900      |
|    policy_gradient_loss | 1.48e-09  |
|    value_loss           | 3.7e+04   |
---------------------------------------
Eval num_timesteps=383500, episode_reward=758.11 +/- 667.88
Episode length: 34.08 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=767.81 +/- 678.70
Episode length: 33.80 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=734.95 +/- 662.71
Episode length: 34.34 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=846.15 +/- 665.26
Episode length: 35.58 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 841      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 188      |
|    time_elapsed    | 1386     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=744.78 +/- 670.17
Episode length: 34.54 +/- 7.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 745       |
| time/                   |           |
|    total_timesteps      | 385500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.83e-15 |
|    explained_variance   | 0.0229    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.11e+04  |
|    n_updates            | 1910      |
|    policy_gradient_loss | -4.54e-10 |
|    value_loss           | 4.16e+04  |
---------------------------------------
Eval num_timesteps=386000, episode_reward=668.38 +/- 618.28
Episode length: 33.84 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=710.76 +/- 604.07
Episode length: 34.44 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=970.49 +/- 736.49
Episode length: 36.20 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 778      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 189      |
|    time_elapsed    | 1394     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=720.23 +/- 604.29
Episode length: 34.18 +/- 6.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 720       |
| time/                   |           |
|    total_timesteps      | 387500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.06e-17 |
|    explained_variance   | 0.0092    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.11e+04  |
|    n_updates            | 1920      |
|    policy_gradient_loss | 1.77e-09  |
|    value_loss           | 2.96e+04  |
---------------------------------------
Eval num_timesteps=388000, episode_reward=877.58 +/- 662.44
Episode length: 35.92 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=879.39 +/- 737.69
Episode length: 35.40 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=841.92 +/- 706.30
Episode length: 35.26 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 766      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 190      |
|    time_elapsed    | 1401     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=852.65 +/- 704.21
Episode length: 35.74 +/- 6.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 853       |
| time/                   |           |
|    total_timesteps      | 389500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.89e-15 |
|    explained_variance   | 0.0156    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.99e+04  |
|    n_updates            | 1930      |
|    policy_gradient_loss | -8.73e-11 |
|    value_loss           | 4.02e+04  |
---------------------------------------
Eval num_timesteps=390000, episode_reward=719.65 +/- 624.17
Episode length: 34.02 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=682.33 +/- 655.10
Episode length: 33.46 +/- 7.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=825.24 +/- 698.00
Episode length: 35.26 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 890      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 191      |
|    time_elapsed    | 1408     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=795.69 +/- 683.90
Episode length: 34.24 +/- 7.03
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.2     |
|    mean_reward          | 796      |
| time/                   |          |
|    total_timesteps      | 391500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -9.4e-18 |
|    explained_variance   | 0.0236   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.66e+04 |
|    n_updates            | 1940     |
|    policy_gradient_loss | -1.6e-10 |
|    value_loss           | 4.46e+04 |
--------------------------------------
Eval num_timesteps=392000, episode_reward=781.32 +/- 659.95
Episode length: 34.30 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=788.20 +/- 651.83
Episode length: 35.34 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 788      |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=727.06 +/- 663.20
Episode length: 34.10 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    fps             | 277      |
|    iterations      | 192      |
|    time_elapsed    | 1415     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=787.72 +/- 621.45
Episode length: 35.72 +/- 5.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 788       |
| time/                   |           |
|    total_timesteps      | 393500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.14e-23 |
|    explained_variance   | 0.0315    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.04e+04  |
|    n_updates            | 1950      |
|    policy_gradient_loss | -3.93e-10 |
|    value_loss           | 4.01e+04  |
---------------------------------------
Eval num_timesteps=394000, episode_reward=817.63 +/- 735.37
Episode length: 33.90 +/- 8.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=669.29 +/- 577.99
Episode length: 34.08 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=904.13 +/- 770.68
Episode length: 34.92 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.9     |
|    ep_rew_mean     | 994      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 193      |
|    time_elapsed    | 1423     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=848.86 +/- 655.64
Episode length: 36.24 +/- 5.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 849       |
| time/                   |           |
|    total_timesteps      | 395500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.19e-20 |
|    explained_variance   | 0.0197    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.64e+04  |
|    n_updates            | 1960      |
|    policy_gradient_loss | 9.76e-10  |
|    value_loss           | 4.84e+04  |
---------------------------------------
Eval num_timesteps=396000, episode_reward=859.58 +/- 747.13
Episode length: 34.42 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=843.67 +/- 680.66
Episode length: 35.42 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=858.99 +/- 759.92
Episode length: 34.34 +/- 7.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 774      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 194      |
|    time_elapsed    | 1430     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=799.58 +/- 619.41
Episode length: 35.86 +/- 5.39
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.9     |
|    mean_reward          | 800      |
| time/                   |          |
|    total_timesteps      | 397500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.4e-15 |
|    explained_variance   | 0.013    |
|    learning_rate        | 0.0001   |
|    loss                 | 1.72e+04 |
|    n_updates            | 1970     |
|    policy_gradient_loss | 9.55e-10 |
|    value_loss           | 3.89e+04 |
--------------------------------------
Eval num_timesteps=398000, episode_reward=906.79 +/- 696.10
Episode length: 36.26 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=876.03 +/- 725.84
Episode length: 34.92 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=867.26 +/- 664.83
Episode length: 36.36 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 867      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 866      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 195      |
|    time_elapsed    | 1438     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=911.45 +/- 733.81
Episode length: 35.48 +/- 6.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 911       |
| time/                   |           |
|    total_timesteps      | 399500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.37e-17 |
|    explained_variance   | 0.0209    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.7e+04   |
|    n_updates            | 1980      |
|    policy_gradient_loss | 3.83e-10  |
|    value_loss           | 4.11e+04  |
---------------------------------------
Eval num_timesteps=400000, episode_reward=822.57 +/- 662.03
Episode length: 35.82 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=681.02 +/- 629.76
Episode length: 33.82 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=743.69 +/- 642.66
Episode length: 34.56 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 981      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 196      |
|    time_elapsed    | 1445     |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=401500, episode_reward=841.51 +/- 684.23
Episode length: 35.02 +/- 6.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 842       |
| time/                   |           |
|    total_timesteps      | 401500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.73e-15 |
|    explained_variance   | 0.0202    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.79e+04  |
|    n_updates            | 1990      |
|    policy_gradient_loss | -9.34e-10 |
|    value_loss           | 4.09e+04  |
---------------------------------------
Eval num_timesteps=402000, episode_reward=628.08 +/- 550.65
Episode length: 33.36 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 628      |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=724.28 +/- 604.79
Episode length: 35.20 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=860.70 +/- 708.17
Episode length: 35.18 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 861      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 197      |
|    time_elapsed    | 1452     |
|    total_timesteps | 403456   |
---------------------------------
Eval num_timesteps=403500, episode_reward=675.35 +/- 611.78
Episode length: 33.40 +/- 6.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.4      |
|    mean_reward          | 675       |
| time/                   |           |
|    total_timesteps      | 403500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.62e-17 |
|    explained_variance   | 0.015     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.69e+04  |
|    n_updates            | 2000      |
|    policy_gradient_loss | 4.07e-10  |
|    value_loss           | 3.88e+04  |
---------------------------------------
Eval num_timesteps=404000, episode_reward=873.76 +/- 670.26
Episode length: 35.78 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=995.75 +/- 710.78
Episode length: 37.00 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=993.03 +/- 694.04
Episode length: 37.32 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 993      |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=808.39 +/- 629.49
Episode length: 34.96 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 807      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 198      |
|    time_elapsed    | 1461     |
|    total_timesteps | 405504   |
---------------------------------
Eval num_timesteps=406000, episode_reward=927.37 +/- 699.82
Episode length: 36.12 +/- 5.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 927       |
| time/                   |           |
|    total_timesteps      | 406000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.95e-15 |
|    explained_variance   | 0.00829   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.47e+04  |
|    n_updates            | 2010      |
|    policy_gradient_loss | 7.64e-10  |
|    value_loss           | 3.69e+04  |
---------------------------------------
Eval num_timesteps=406500, episode_reward=792.50 +/- 736.43
Episode length: 33.94 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=771.63 +/- 643.42
Episode length: 35.18 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 772      |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=646.88 +/- 570.28
Episode length: 34.28 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 870      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 199      |
|    time_elapsed    | 1469     |
|    total_timesteps | 407552   |
---------------------------------
Eval num_timesteps=408000, episode_reward=884.75 +/- 723.45
Episode length: 34.96 +/- 7.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 885       |
| time/                   |           |
|    total_timesteps      | 408000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.58e-17 |
|    explained_variance   | 0.0208    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.8e+04   |
|    n_updates            | 2020      |
|    policy_gradient_loss | -3.42e-10 |
|    value_loss           | 4.22e+04  |
---------------------------------------
Eval num_timesteps=408500, episode_reward=768.84 +/- 695.77
Episode length: 33.44 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=749.96 +/- 693.11
Episode length: 34.04 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=927.27 +/- 681.71
Episode length: 36.76 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 927      |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 200      |
|    time_elapsed    | 1476     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=410000, episode_reward=756.24 +/- 691.91
Episode length: 34.16 +/- 6.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 756       |
| time/                   |           |
|    total_timesteps      | 410000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.16e-15 |
|    explained_variance   | 0.0132    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.82e+04  |
|    n_updates            | 2030      |
|    policy_gradient_loss | 4.83e-10  |
|    value_loss           | 4.03e+04  |
---------------------------------------
Eval num_timesteps=410500, episode_reward=868.77 +/- 722.07
Episode length: 35.26 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=759.17 +/- 691.24
Episode length: 33.92 +/- 7.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=758.27 +/- 640.92
Episode length: 34.60 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 733      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 201      |
|    time_elapsed    | 1483     |
|    total_timesteps | 411648   |
---------------------------------
Eval num_timesteps=412000, episode_reward=687.18 +/- 622.10
Episode length: 33.52 +/- 6.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.5      |
|    mean_reward          | 687       |
| time/                   |           |
|    total_timesteps      | 412000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.19e-13 |
|    explained_variance   | 0.0138    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.17e+04  |
|    n_updates            | 2040      |
|    policy_gradient_loss | -4.39e-10 |
|    value_loss           | 3.48e+04  |
---------------------------------------
Eval num_timesteps=412500, episode_reward=722.46 +/- 604.49
Episode length: 35.14 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=732.26 +/- 608.69
Episode length: 35.26 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=654.41 +/- 579.09
Episode length: 33.22 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 654      |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 738      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 202      |
|    time_elapsed    | 1490     |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=414000, episode_reward=686.12 +/- 609.68
Episode length: 34.74 +/- 7.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 686       |
| time/                   |           |
|    total_timesteps      | 414000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.38e-15 |
|    explained_variance   | 0.0159    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.82e+04  |
|    n_updates            | 2050      |
|    policy_gradient_loss | -2.88e-10 |
|    value_loss           | 3.52e+04  |
---------------------------------------
Eval num_timesteps=414500, episode_reward=718.76 +/- 662.30
Episode length: 33.48 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=767.25 +/- 695.58
Episode length: 33.80 +/- 7.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=677.75 +/- 592.82
Episode length: 33.98 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 761      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 203      |
|    time_elapsed    | 1497     |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=738.56 +/- 625.11
Episode length: 34.30 +/- 7.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 739       |
| time/                   |           |
|    total_timesteps      | 416000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.86e-13 |
|    explained_variance   | 0.0176    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.6e+04   |
|    n_updates            | 2060      |
|    policy_gradient_loss | -1.04e-09 |
|    value_loss           | 3.53e+04  |
---------------------------------------
Eval num_timesteps=416500, episode_reward=730.89 +/- 596.92
Episode length: 35.10 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=936.19 +/- 743.39
Episode length: 36.06 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=736.59 +/- 602.95
Episode length: 35.02 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 733      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 204      |
|    time_elapsed    | 1505     |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=758.57 +/- 608.29
Episode length: 35.12 +/- 6.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 759       |
| time/                   |           |
|    total_timesteps      | 418000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.3e-15  |
|    explained_variance   | 0.0146    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.38e+04  |
|    n_updates            | 2070      |
|    policy_gradient_loss | -7.39e-10 |
|    value_loss           | 3.34e+04  |
---------------------------------------
Eval num_timesteps=418500, episode_reward=734.38 +/- 593.42
Episode length: 35.38 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=788.86 +/- 658.75
Episode length: 35.24 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=730.96 +/- 620.77
Episode length: 34.58 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 731      |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 863      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 205      |
|    time_elapsed    | 1512     |
|    total_timesteps | 419840   |
---------------------------------
Eval num_timesteps=420000, episode_reward=802.42 +/- 648.79
Episode length: 35.70 +/- 6.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 802       |
| time/                   |           |
|    total_timesteps      | 420000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.52e-19 |
|    explained_variance   | 0.0203    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.2e+04   |
|    n_updates            | 2080      |
|    policy_gradient_loss | 1.01e-09  |
|    value_loss           | 3.83e+04  |
---------------------------------------
Eval num_timesteps=420500, episode_reward=953.79 +/- 743.45
Episode length: 35.88 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 954      |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=804.18 +/- 679.01
Episode length: 35.16 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=884.47 +/- 675.41
Episode length: 35.78 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 206      |
|    time_elapsed    | 1519     |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=422000, episode_reward=883.22 +/- 701.71
Episode length: 35.82 +/- 7.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 883       |
| time/                   |           |
|    total_timesteps      | 422000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.96e-17 |
|    explained_variance   | 0.0129    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.57e+04  |
|    n_updates            | 2090      |
|    policy_gradient_loss | -4.92e-10 |
|    value_loss           | 4.38e+04  |
---------------------------------------
Eval num_timesteps=422500, episode_reward=877.19 +/- 729.11
Episode length: 35.46 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 877      |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=887.55 +/- 712.03
Episode length: 35.26 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 888      |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=933.16 +/- 750.52
Episode length: 35.46 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 763      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 207      |
|    time_elapsed    | 1527     |
|    total_timesteps | 423936   |
---------------------------------
Eval num_timesteps=424000, episode_reward=808.50 +/- 665.40
Episode length: 35.36 +/- 7.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 808       |
| time/                   |           |
|    total_timesteps      | 424000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.22e-19 |
|    explained_variance   | 0.019     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.85e+04  |
|    n_updates            | 2100      |
|    policy_gradient_loss | 6.43e-10  |
|    value_loss           | 3.72e+04  |
---------------------------------------
Eval num_timesteps=424500, episode_reward=863.51 +/- 717.49
Episode length: 35.38 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=757.96 +/- 666.67
Episode length: 34.44 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=636.34 +/- 583.81
Episode length: 33.60 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 636      |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 964      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 208      |
|    time_elapsed    | 1534     |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=426000, episode_reward=996.07 +/- 718.64
Episode length: 36.32 +/- 6.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 996       |
| time/                   |           |
|    total_timesteps      | 426000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.64e-17 |
|    explained_variance   | 0.0337    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.32e+04  |
|    n_updates            | 2110      |
|    policy_gradient_loss | 1.14e-09  |
|    value_loss           | 4.15e+04  |
---------------------------------------
Eval num_timesteps=426500, episode_reward=821.87 +/- 678.84
Episode length: 35.32 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=938.18 +/- 770.57
Episode length: 35.46 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 938      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=898.17 +/- 725.50
Episode length: 35.74 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=847.75 +/- 683.49
Episode length: 35.68 +/- 7.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 209      |
|    time_elapsed    | 1543     |
|    total_timesteps | 428032   |
---------------------------------
Eval num_timesteps=428500, episode_reward=839.43 +/- 636.28
Episode length: 36.24 +/- 6.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 839       |
| time/                   |           |
|    total_timesteps      | 428500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.96e-19 |
|    explained_variance   | 0.0255    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.87e+04  |
|    n_updates            | 2120      |
|    policy_gradient_loss | -7.71e-10 |
|    value_loss           | 3.73e+04  |
---------------------------------------
Eval num_timesteps=429000, episode_reward=837.38 +/- 673.35
Episode length: 35.84 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=891.89 +/- 735.68
Episode length: 35.70 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=746.80 +/- 652.23
Episode length: 34.42 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 873      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 210      |
|    time_elapsed    | 1550     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430500, episode_reward=842.21 +/- 639.40
Episode length: 36.16 +/- 6.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 842       |
| time/                   |           |
|    total_timesteps      | 430500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.67e-17 |
|    explained_variance   | 0.0229    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.34e+04  |
|    n_updates            | 2130      |
|    policy_gradient_loss | -9.58e-10 |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=431000, episode_reward=821.12 +/- 671.94
Episode length: 35.34 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=760.11 +/- 662.62
Episode length: 34.24 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=953.52 +/- 743.99
Episode length: 35.82 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 954      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 842      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 211      |
|    time_elapsed    | 1557     |
|    total_timesteps | 432128   |
---------------------------------
Eval num_timesteps=432500, episode_reward=801.16 +/- 657.95
Episode length: 35.32 +/- 6.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 801       |
| time/                   |           |
|    total_timesteps      | 432500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.78e-12 |
|    explained_variance   | 0.0256    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.39e+04  |
|    n_updates            | 2140      |
|    policy_gradient_loss | 4.02e-10  |
|    value_loss           | 3.61e+04  |
---------------------------------------
Eval num_timesteps=433000, episode_reward=703.75 +/- 646.28
Episode length: 33.38 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=825.42 +/- 690.45
Episode length: 34.62 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=754.62 +/- 605.33
Episode length: 35.14 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 685      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 212      |
|    time_elapsed    | 1565     |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=434500, episode_reward=852.80 +/- 695.80
Episode length: 35.46 +/- 6.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 853       |
| time/                   |           |
|    total_timesteps      | 434500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.15e-14 |
|    explained_variance   | 0.00909   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.09e+04  |
|    n_updates            | 2150      |
|    policy_gradient_loss | -2.85e-10 |
|    value_loss           | 2.9e+04   |
---------------------------------------
Eval num_timesteps=435000, episode_reward=655.81 +/- 614.48
Episode length: 33.42 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 656      |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=862.61 +/- 703.18
Episode length: 35.50 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=910.90 +/- 761.83
Episode length: 35.00 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 723      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 213      |
|    time_elapsed    | 1573     |
|    total_timesteps | 436224   |
---------------------------------
Eval num_timesteps=436500, episode_reward=902.10 +/- 702.31
Episode length: 35.70 +/- 5.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 902       |
| time/                   |           |
|    total_timesteps      | 436500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.85e-13 |
|    explained_variance   | 0.0168    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.36e+04  |
|    n_updates            | 2160      |
|    policy_gradient_loss | 1.34e-10  |
|    value_loss           | 3.33e+04  |
---------------------------------------
Eval num_timesteps=437000, episode_reward=1001.21 +/- 706.20
Episode length: 37.26 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=834.12 +/- 696.63
Episode length: 35.60 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=960.81 +/- 718.25
Episode length: 36.44 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 961      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 877      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 214      |
|    time_elapsed    | 1580     |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=438500, episode_reward=990.84 +/- 728.62
Episode length: 36.66 +/- 6.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 991       |
| time/                   |           |
|    total_timesteps      | 438500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.58e-15 |
|    explained_variance   | 0.0219    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.55e+04  |
|    n_updates            | 2170      |
|    policy_gradient_loss | -1.2e-09  |
|    value_loss           | 4.41e+04  |
---------------------------------------
Eval num_timesteps=439000, episode_reward=837.21 +/- 652.50
Episode length: 35.60 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=774.58 +/- 597.98
Episode length: 36.02 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=765.73 +/- 661.72
Episode length: 34.86 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 803      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 215      |
|    time_elapsed    | 1588     |
|    total_timesteps | 440320   |
---------------------------------
Eval num_timesteps=440500, episode_reward=838.42 +/- 658.62
Episode length: 35.60 +/- 5.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 838       |
| time/                   |           |
|    total_timesteps      | 440500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.75e-13 |
|    explained_variance   | 0.0139    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.17e+04  |
|    n_updates            | 2180      |
|    policy_gradient_loss | -8.73e-12 |
|    value_loss           | 3.56e+04  |
---------------------------------------
Eval num_timesteps=441000, episode_reward=1039.66 +/- 724.88
Episode length: 37.18 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.2     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=824.49 +/- 673.69
Episode length: 35.06 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=905.18 +/- 759.28
Episode length: 35.44 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 817      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 216      |
|    time_elapsed    | 1595     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=442500, episode_reward=767.89 +/- 623.62
Episode length: 35.16 +/- 5.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 768       |
| time/                   |           |
|    total_timesteps      | 442500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.19e-15 |
|    explained_variance   | 0.0193    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.53e+04  |
|    n_updates            | 2190      |
|    policy_gradient_loss | 2.75e-10  |
|    value_loss           | 3.96e+04  |
---------------------------------------
Eval num_timesteps=443000, episode_reward=800.10 +/- 627.18
Episode length: 35.88 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=986.80 +/- 757.41
Episode length: 36.04 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 987      |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=755.53 +/- 651.63
Episode length: 34.24 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 876      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 217      |
|    time_elapsed    | 1602     |
|    total_timesteps | 444416   |
---------------------------------
Eval num_timesteps=444500, episode_reward=966.37 +/- 707.86
Episode length: 36.58 +/- 5.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 966       |
| time/                   |           |
|    total_timesteps      | 444500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.23e-19 |
|    explained_variance   | 0.0221    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.7e+04   |
|    n_updates            | 2200      |
|    policy_gradient_loss | 6.85e-10  |
|    value_loss           | 3.77e+04  |
---------------------------------------
Eval num_timesteps=445000, episode_reward=801.73 +/- 648.68
Episode length: 35.14 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=818.50 +/- 677.66
Episode length: 34.38 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=778.98 +/- 621.70
Episode length: 35.24 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 218      |
|    time_elapsed    | 1610     |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=446500, episode_reward=864.16 +/- 659.64
Episode length: 36.48 +/- 5.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 864       |
| time/                   |           |
|    total_timesteps      | 446500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.29e-17 |
|    explained_variance   | 0.00721   |
|    learning_rate        | 0.0001    |
|    loss                 | 2.32e+04  |
|    n_updates            | 2210      |
|    policy_gradient_loss | -1.54e-10 |
|    value_loss           | 4.49e+04  |
---------------------------------------
Eval num_timesteps=447000, episode_reward=700.13 +/- 652.43
Episode length: 33.56 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 700      |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=751.68 +/- 630.63
Episode length: 34.88 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 752      |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=738.34 +/- 616.88
Episode length: 34.96 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=813.16 +/- 660.45
Episode length: 35.18 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 803      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 219      |
|    time_elapsed    | 1618     |
|    total_timesteps | 448512   |
---------------------------------
Eval num_timesteps=449000, episode_reward=694.31 +/- 621.16
Episode length: 34.56 +/- 6.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 694       |
| time/                   |           |
|    total_timesteps      | 449000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.52e-19 |
|    explained_variance   | 0.0273    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.04e+04  |
|    n_updates            | 2220      |
|    policy_gradient_loss | -2.91e-12 |
|    value_loss           | 4.21e+04  |
---------------------------------------
Eval num_timesteps=449500, episode_reward=908.53 +/- 754.72
Episode length: 35.06 +/- 7.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 909      |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=851.18 +/- 690.34
Episode length: 35.98 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=911.88 +/- 701.00
Episode length: 35.96 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 933      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 220      |
|    time_elapsed    | 1625     |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=451000, episode_reward=894.14 +/- 703.68
Episode length: 35.98 +/- 6.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 894       |
| time/                   |           |
|    total_timesteps      | 451000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.93e-17 |
|    explained_variance   | 0.0271    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.01e+04  |
|    n_updates            | 2230      |
|    policy_gradient_loss | 1.69e-09  |
|    value_loss           | 4.15e+04  |
---------------------------------------
Eval num_timesteps=451500, episode_reward=884.80 +/- 668.12
Episode length: 36.62 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=822.67 +/- 691.95
Episode length: 35.00 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 823      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=848.26 +/- 720.65
Episode length: 35.06 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 820      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 221      |
|    time_elapsed    | 1633     |
|    total_timesteps | 452608   |
---------------------------------
Eval num_timesteps=453000, episode_reward=875.97 +/- 709.90
Episode length: 35.22 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 876       |
| time/                   |           |
|    total_timesteps      | 453000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.87e-19 |
|    explained_variance   | 0.0211    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.71e+04  |
|    n_updates            | 2240      |
|    policy_gradient_loss | -7.28e-12 |
|    value_loss           | 3.69e+04  |
---------------------------------------
Eval num_timesteps=453500, episode_reward=969.34 +/- 751.13
Episode length: 35.94 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 969      |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=757.32 +/- 635.52
Episode length: 34.66 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=815.96 +/- 712.44
Episode length: 34.52 +/- 7.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 850      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 222      |
|    time_elapsed    | 1640     |
|    total_timesteps | 454656   |
---------------------------------
Eval num_timesteps=455000, episode_reward=908.00 +/- 700.29
Episode length: 35.92 +/- 7.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 908       |
| time/                   |           |
|    total_timesteps      | 455000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.77e-17 |
|    explained_variance   | 0.0186    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.49e+04  |
|    n_updates            | 2250      |
|    policy_gradient_loss | 1.17e-09  |
|    value_loss           | 4.11e+04  |
---------------------------------------
Eval num_timesteps=455500, episode_reward=936.34 +/- 742.66
Episode length: 35.56 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=854.23 +/- 668.33
Episode length: 36.22 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=887.07 +/- 709.36
Episode length: 35.60 +/- 7.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 887      |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 857      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 223      |
|    time_elapsed    | 1647     |
|    total_timesteps | 456704   |
---------------------------------
Eval num_timesteps=457000, episode_reward=882.72 +/- 759.96
Episode length: 34.66 +/- 7.24
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.7     |
|    mean_reward          | 883      |
| time/                   |          |
|    total_timesteps      | 457000   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -2.2e-12 |
|    explained_variance   | 0.0187   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.73e+04 |
|    n_updates            | 2260     |
|    policy_gradient_loss | 6.58e-10 |
|    value_loss           | 3.66e+04 |
--------------------------------------
Eval num_timesteps=457500, episode_reward=865.72 +/- 629.36
Episode length: 36.62 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 866      |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=874.64 +/- 688.00
Episode length: 36.36 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=733.70 +/- 655.73
Episode length: 34.38 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 891      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 224      |
|    time_elapsed    | 1655     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=459000, episode_reward=911.98 +/- 696.91
Episode length: 36.38 +/- 6.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 912       |
| time/                   |           |
|    total_timesteps      | 459000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.16e-14 |
|    explained_variance   | 0.023     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.41e+04  |
|    n_updates            | 2270      |
|    policy_gradient_loss | -3.62e-10 |
|    value_loss           | 4.22e+04  |
---------------------------------------
Eval num_timesteps=459500, episode_reward=863.07 +/- 649.86
Episode length: 36.60 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=945.97 +/- 718.07
Episode length: 36.34 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=852.96 +/- 645.09
Episode length: 35.54 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 908      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 225      |
|    time_elapsed    | 1662     |
|    total_timesteps | 460800   |
---------------------------------
Eval num_timesteps=461000, episode_reward=750.58 +/- 620.36
Episode length: 34.52 +/- 6.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 751       |
| time/                   |           |
|    total_timesteps      | 461000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.24e-12 |
|    explained_variance   | 0.0322    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.17e+04  |
|    n_updates            | 2280      |
|    policy_gradient_loss | 1.23e-09  |
|    value_loss           | 4.27e+04  |
---------------------------------------
Eval num_timesteps=461500, episode_reward=726.99 +/- 626.42
Episode length: 34.42 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=882.72 +/- 706.19
Episode length: 35.64 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=992.72 +/- 744.22
Episode length: 36.08 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 993      |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 810      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 226      |
|    time_elapsed    | 1669     |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=463000, episode_reward=908.08 +/- 763.86
Episode length: 35.82 +/- 6.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 908       |
| time/                   |           |
|    total_timesteps      | 463000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.44e-14 |
|    explained_variance   | 0.0174    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.45e+04  |
|    n_updates            | 2290      |
|    policy_gradient_loss | 6.48e-11  |
|    value_loss           | 3.59e+04  |
---------------------------------------
Eval num_timesteps=463500, episode_reward=947.59 +/- 745.37
Episode length: 36.10 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 948      |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=1083.30 +/- 729.72
Episode length: 37.36 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=904.78 +/- 718.03
Episode length: 36.14 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 836      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 227      |
|    time_elapsed    | 1677     |
|    total_timesteps | 464896   |
---------------------------------
Eval num_timesteps=465000, episode_reward=987.01 +/- 735.27
Episode length: 36.26 +/- 6.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.3      |
|    mean_reward          | 987       |
| time/                   |           |
|    total_timesteps      | 465000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.66e-19 |
|    explained_variance   | 0.027     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.95e+04  |
|    n_updates            | 2300      |
|    policy_gradient_loss | -2.18e-11 |
|    value_loss           | 3.89e+04  |
---------------------------------------
Eval num_timesteps=465500, episode_reward=877.52 +/- 689.31
Episode length: 35.40 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=800.80 +/- 688.33
Episode length: 34.48 +/- 7.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=779.37 +/- 655.36
Episode length: 34.76 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 783      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 228      |
|    time_elapsed    | 1684     |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=467000, episode_reward=870.85 +/- 670.61
Episode length: 36.16 +/- 5.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 871       |
| time/                   |           |
|    total_timesteps      | 467000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.04e-16 |
|    explained_variance   | 0.00985   |
|    learning_rate        | 0.0001    |
|    loss                 | 1.97e+04  |
|    n_updates            | 2310      |
|    policy_gradient_loss | -1.11e-09 |
|    value_loss           | 4.42e+04  |
---------------------------------------
Eval num_timesteps=467500, episode_reward=720.44 +/- 673.09
Episode length: 34.22 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=648.79 +/- 596.04
Episode length: 33.50 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=881.56 +/- 684.70
Episode length: 36.00 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 781      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 229      |
|    time_elapsed    | 1691     |
|    total_timesteps | 468992   |
---------------------------------
Eval num_timesteps=469000, episode_reward=758.60 +/- 680.60
Episode length: 33.94 +/- 6.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 33.9      |
|    mean_reward          | 759       |
| time/                   |           |
|    total_timesteps      | 469000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.36e-13 |
|    explained_variance   | 0.0207    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.73e+04  |
|    n_updates            | 2320      |
|    policy_gradient_loss | 6.11e-11  |
|    value_loss           | 3.5e+04   |
---------------------------------------
Eval num_timesteps=469500, episode_reward=789.17 +/- 661.14
Episode length: 35.16 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=840.25 +/- 661.24
Episode length: 36.10 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=804.96 +/- 649.00
Episode length: 35.22 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=818.64 +/- 717.76
Episode length: 34.36 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 230      |
|    time_elapsed    | 1700     |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=471500, episode_reward=776.75 +/- 687.32
Episode length: 34.42 +/- 7.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 777       |
| time/                   |           |
|    total_timesteps      | 471500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.84e-13 |
|    explained_variance   | 0.0195    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.79e+04  |
|    n_updates            | 2330      |
|    policy_gradient_loss | -7.87e-10 |
|    value_loss           | 3.71e+04  |
---------------------------------------
Eval num_timesteps=472000, episode_reward=873.22 +/- 685.49
Episode length: 35.24 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 873      |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=931.88 +/- 722.79
Episode length: 35.96 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=774.43 +/- 651.02
Episode length: 34.54 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 858      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 231      |
|    time_elapsed    | 1707     |
|    total_timesteps | 473088   |
---------------------------------
Eval num_timesteps=473500, episode_reward=752.34 +/- 695.35
Episode length: 34.00 +/- 7.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 752       |
| time/                   |           |
|    total_timesteps      | 473500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.09e-15 |
|    explained_variance   | 0.0298    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.47e+04  |
|    n_updates            | 2340      |
|    policy_gradient_loss | -1.55e-09 |
|    value_loss           | 4.35e+04  |
---------------------------------------
Eval num_timesteps=474000, episode_reward=786.40 +/- 635.09
Episode length: 34.94 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=801.45 +/- 684.15
Episode length: 34.46 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=709.42 +/- 642.05
Episode length: 34.88 +/- 7.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 709      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 232      |
|    time_elapsed    | 1714     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=475500, episode_reward=757.98 +/- 634.93
Episode length: 34.64 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 758       |
| time/                   |           |
|    total_timesteps      | 475500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.85e-13 |
|    explained_variance   | 0.0201    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.94e+04  |
|    n_updates            | 2350      |
|    policy_gradient_loss | 1.39e-09  |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=476000, episode_reward=1019.34 +/- 761.09
Episode length: 36.28 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=801.63 +/- 634.62
Episode length: 35.30 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=918.17 +/- 726.74
Episode length: 36.08 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 918      |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 832      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 233      |
|    time_elapsed    | 1722     |
|    total_timesteps | 477184   |
---------------------------------
Eval num_timesteps=477500, episode_reward=825.53 +/- 653.75
Episode length: 35.34 +/- 6.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 826       |
| time/                   |           |
|    total_timesteps      | 477500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.65e-15 |
|    explained_variance   | 0.0256    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.83e+04  |
|    n_updates            | 2360      |
|    policy_gradient_loss | 2.27e-10  |
|    value_loss           | 4.1e+04   |
---------------------------------------
Eval num_timesteps=478000, episode_reward=719.22 +/- 673.87
Episode length: 33.34 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=1003.31 +/- 719.27
Episode length: 36.98 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=860.17 +/- 686.26
Episode length: 35.46 +/- 7.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 860      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 872      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 234      |
|    time_elapsed    | 1729     |
|    total_timesteps | 479232   |
---------------------------------
Eval num_timesteps=479500, episode_reward=775.64 +/- 625.51
Episode length: 35.14 +/- 6.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 776       |
| time/                   |           |
|    total_timesteps      | 479500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.74e-20 |
|    explained_variance   | 0.021     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.79e+04  |
|    n_updates            | 2370      |
|    policy_gradient_loss | 4.13e-10  |
|    value_loss           | 3.43e+04  |
---------------------------------------
Eval num_timesteps=480000, episode_reward=886.40 +/- 724.90
Episode length: 35.58 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=938.49 +/- 698.46
Episode length: 36.68 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 938      |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=765.16 +/- 672.17
Episode length: 34.44 +/- 7.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 821      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 235      |
|    time_elapsed    | 1736     |
|    total_timesteps | 481280   |
---------------------------------
Eval num_timesteps=481500, episode_reward=1022.93 +/- 701.44
Episode length: 37.84 +/- 5.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.8      |
|    mean_reward          | 1.02e+03  |
| time/                   |           |
|    total_timesteps      | 481500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.56e-17 |
|    explained_variance   | 0.0238    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.93e+04  |
|    n_updates            | 2380      |
|    policy_gradient_loss | 1.38e-09  |
|    value_loss           | 4.55e+04  |
---------------------------------------
Eval num_timesteps=482000, episode_reward=727.34 +/- 649.27
Episode length: 34.30 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=901.98 +/- 694.78
Episode length: 35.44 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=761.17 +/- 654.27
Episode length: 34.68 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 912      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 236      |
|    time_elapsed    | 1743     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=483500, episode_reward=735.43 +/- 651.97
Episode length: 34.54 +/- 6.33
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.5      |
|    mean_reward          | 735       |
| time/                   |           |
|    total_timesteps      | 483500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.43e-20 |
|    explained_variance   | 0.0253    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.19e+04  |
|    n_updates            | 2390      |
|    policy_gradient_loss | 2.52e-10  |
|    value_loss           | 3.59e+04  |
---------------------------------------
Eval num_timesteps=484000, episode_reward=925.88 +/- 803.92
Episode length: 33.94 +/- 8.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 926      |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=699.47 +/- 576.48
Episode length: 35.12 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=691.15 +/- 618.30
Episode length: 33.58 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 922      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 237      |
|    time_elapsed    | 1751     |
|    total_timesteps | 485376   |
---------------------------------
Eval num_timesteps=485500, episode_reward=788.70 +/- 719.60
Episode length: 34.10 +/- 7.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 789       |
| time/                   |           |
|    total_timesteps      | 485500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.79e-17 |
|    explained_variance   | 0.028     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.87e+04  |
|    n_updates            | 2400      |
|    policy_gradient_loss | -7.28e-10 |
|    value_loss           | 4.65e+04  |
---------------------------------------
Eval num_timesteps=486000, episode_reward=884.41 +/- 669.61
Episode length: 36.58 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=705.27 +/- 572.92
Episode length: 34.82 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 705      |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=746.41 +/- 621.60
Episode length: 34.72 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 807      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 238      |
|    time_elapsed    | 1758     |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=487500, episode_reward=759.68 +/- 660.79
Episode length: 34.56 +/- 7.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 760       |
| time/                   |           |
|    total_timesteps      | 487500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.23e-13 |
|    explained_variance   | 0.0194    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.06e+04  |
|    n_updates            | 2410      |
|    policy_gradient_loss | 4.41e-10  |
|    value_loss           | 3.63e+04  |
---------------------------------------
Eval num_timesteps=488000, episode_reward=897.90 +/- 696.78
Episode length: 35.70 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=803.77 +/- 671.81
Episode length: 34.78 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=729.19 +/- 645.12
Episode length: 34.40 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 729      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 803      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 239      |
|    time_elapsed    | 1765     |
|    total_timesteps | 489472   |
---------------------------------
Eval num_timesteps=489500, episode_reward=937.10 +/- 664.41
Episode length: 37.08 +/- 6.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 37.1      |
|    mean_reward          | 937       |
| time/                   |           |
|    total_timesteps      | 489500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.08e-15 |
|    explained_variance   | 0.0196    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.66e+04  |
|    n_updates            | 2420      |
|    policy_gradient_loss | -8.59e-11 |
|    value_loss           | 3.56e+04  |
---------------------------------------
Eval num_timesteps=490000, episode_reward=835.48 +/- 691.58
Episode length: 35.32 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=733.57 +/- 633.46
Episode length: 34.50 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=848.44 +/- 705.45
Episode length: 35.10 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=741.56 +/- 642.61
Episode length: 34.70 +/- 7.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 804      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 240      |
|    time_elapsed    | 1774     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=492000, episode_reward=824.64 +/- 669.69
Episode length: 35.16 +/- 6.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 825       |
| time/                   |           |
|    total_timesteps      | 492000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.27e-13 |
|    explained_variance   | 0.0217    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.69e+04  |
|    n_updates            | 2430      |
|    policy_gradient_loss | 4.23e-10  |
|    value_loss           | 3.39e+04  |
---------------------------------------
Eval num_timesteps=492500, episode_reward=762.58 +/- 656.87
Episode length: 35.00 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=907.62 +/- 746.57
Episode length: 34.72 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=706.55 +/- 603.08
Episode length: 34.54 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 241      |
|    time_elapsed    | 1781     |
|    total_timesteps | 493568   |
---------------------------------
Eval num_timesteps=494000, episode_reward=834.13 +/- 659.00
Episode length: 35.90 +/- 5.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 834       |
| time/                   |           |
|    total_timesteps      | 494000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.36e-15 |
|    explained_variance   | 0.0251    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.62e+04  |
|    n_updates            | 2440      |
|    policy_gradient_loss | 6.98e-10  |
|    value_loss           | 3.74e+04  |
---------------------------------------
Eval num_timesteps=494500, episode_reward=999.05 +/- 734.89
Episode length: 36.74 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 999      |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=800.86 +/- 608.93
Episode length: 35.78 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=986.96 +/- 740.54
Episode length: 36.26 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 987      |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 837      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 242      |
|    time_elapsed    | 1789     |
|    total_timesteps | 495616   |
---------------------------------
Eval num_timesteps=496000, episode_reward=950.55 +/- 690.57
Episode length: 36.68 +/- 5.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.7      |
|    mean_reward          | 951       |
| time/                   |           |
|    total_timesteps      | 496000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.02e-12 |
|    explained_variance   | 0.0247    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.44e+04  |
|    n_updates            | 2450      |
|    policy_gradient_loss | 4.73e-10  |
|    value_loss           | 3.83e+04  |
---------------------------------------
Eval num_timesteps=496500, episode_reward=893.00 +/- 726.21
Episode length: 35.42 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=640.73 +/- 538.01
Episode length: 34.82 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=785.70 +/- 708.30
Episode length: 34.06 +/- 7.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 243      |
|    time_elapsed    | 1796     |
|    total_timesteps | 497664   |
---------------------------------
Eval num_timesteps=498000, episode_reward=779.66 +/- 617.18
Episode length: 35.60 +/- 5.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 780       |
| time/                   |           |
|    total_timesteps      | 498000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.44e-15 |
|    explained_variance   | 0.0255    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.55e+04  |
|    n_updates            | 2460      |
|    policy_gradient_loss | -3.96e-10 |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=498500, episode_reward=849.51 +/- 677.05
Episode length: 35.80 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=968.26 +/- 732.66
Episode length: 35.46 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=712.33 +/- 621.72
Episode length: 34.14 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 712      |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 979      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 244      |
|    time_elapsed    | 1803     |
|    total_timesteps | 499712   |
---------------------------------
Eval num_timesteps=500000, episode_reward=702.62 +/- 613.81
Episode length: 34.62 +/- 6.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 703       |
| time/                   |           |
|    total_timesteps      | 500000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.34e-19 |
|    explained_variance   | 0.0349    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.43e+04  |
|    n_updates            | 2470      |
|    policy_gradient_loss | 6.68e-10  |
|    value_loss           | 4.42e+04  |
---------------------------------------
Eval num_timesteps=500500, episode_reward=898.91 +/- 691.66
Episode length: 35.86 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=840.08 +/- 700.79
Episode length: 35.20 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=766.91 +/- 589.52
Episode length: 35.86 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 845      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 245      |
|    time_elapsed    | 1810     |
|    total_timesteps | 501760   |
---------------------------------
Eval num_timesteps=502000, episode_reward=929.58 +/- 723.04
Episode length: 35.90 +/- 7.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 930       |
| time/                   |           |
|    total_timesteps      | 502000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.64e-17 |
|    explained_variance   | 0.0175    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.01e+04  |
|    n_updates            | 2480      |
|    policy_gradient_loss | -4.55e-10 |
|    value_loss           | 4.36e+04  |
---------------------------------------
Eval num_timesteps=502500, episode_reward=681.46 +/- 629.25
Episode length: 33.72 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 502500   |
---------------------------------
Eval num_timesteps=503000, episode_reward=647.26 +/- 591.49
Episode length: 33.02 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 503000   |
---------------------------------
Eval num_timesteps=503500, episode_reward=836.88 +/- 700.34
Episode length: 35.54 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 503500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 787      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 246      |
|    time_elapsed    | 1818     |
|    total_timesteps | 503808   |
---------------------------------
Eval num_timesteps=504000, episode_reward=796.82 +/- 698.36
Episode length: 34.66 +/- 7.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 797       |
| time/                   |           |
|    total_timesteps      | 504000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.75e-12 |
|    explained_variance   | 0.0261    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.5e+04   |
|    n_updates            | 2490      |
|    policy_gradient_loss | -1.31e-10 |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=504500, episode_reward=724.38 +/- 630.06
Episode length: 35.08 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 504500   |
---------------------------------
Eval num_timesteps=505000, episode_reward=783.85 +/- 617.19
Episode length: 35.50 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
Eval num_timesteps=505500, episode_reward=929.81 +/- 690.54
Episode length: 36.74 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 930      |
| time/              |          |
|    total_timesteps | 505500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 695      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 247      |
|    time_elapsed    | 1825     |
|    total_timesteps | 505856   |
---------------------------------
Eval num_timesteps=506000, episode_reward=875.22 +/- 685.28
Episode length: 36.08 +/- 6.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 875       |
| time/                   |           |
|    total_timesteps      | 506000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.53e-09 |
|    explained_variance   | 0.017     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.48e+04  |
|    n_updates            | 2500      |
|    policy_gradient_loss | 2.91e-12  |
|    value_loss           | 3.3e+04   |
---------------------------------------
Eval num_timesteps=506500, episode_reward=749.01 +/- 607.94
Episode length: 35.18 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 506500   |
---------------------------------
Eval num_timesteps=507000, episode_reward=745.91 +/- 610.29
Episode length: 34.92 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 746      |
| time/              |          |
|    total_timesteps | 507000   |
---------------------------------
Eval num_timesteps=507500, episode_reward=719.34 +/- 612.32
Episode length: 34.62 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 507500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 762      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 248      |
|    time_elapsed    | 1833     |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=508000, episode_reward=701.65 +/- 581.70
Episode length: 34.36 +/- 6.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 702       |
| time/                   |           |
|    total_timesteps      | 508000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.84e-11 |
|    explained_variance   | 0.0228    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.48e+04  |
|    n_updates            | 2510      |
|    policy_gradient_loss | 1.43e-09  |
|    value_loss           | 4.73e+04  |
---------------------------------------
Eval num_timesteps=508500, episode_reward=906.41 +/- 675.39
Episode length: 36.82 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 508500   |
---------------------------------
Eval num_timesteps=509000, episode_reward=951.86 +/- 767.36
Episode length: 35.72 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 509000   |
---------------------------------
Eval num_timesteps=509500, episode_reward=734.54 +/- 615.31
Episode length: 35.36 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 509500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 834      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 249      |
|    time_elapsed    | 1840     |
|    total_timesteps | 509952   |
---------------------------------
Eval num_timesteps=510000, episode_reward=912.05 +/- 694.83
Episode length: 36.06 +/- 6.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.1      |
|    mean_reward          | 912       |
| time/                   |           |
|    total_timesteps      | 510000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.56e-15 |
|    explained_variance   | 0.0235    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.93e+04  |
|    n_updates            | 2520      |
|    policy_gradient_loss | 1.02e-09  |
|    value_loss           | 3.74e+04  |
---------------------------------------
Eval num_timesteps=510500, episode_reward=798.98 +/- 682.50
Episode length: 34.48 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 510500   |
---------------------------------
Eval num_timesteps=511000, episode_reward=906.75 +/- 674.74
Episode length: 36.90 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 511000   |
---------------------------------
Eval num_timesteps=511500, episode_reward=792.28 +/- 614.72
Episode length: 36.08 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 511500   |
---------------------------------
Eval num_timesteps=512000, episode_reward=998.99 +/- 716.06
Episode length: 36.82 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 999      |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 911      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 250      |
|    time_elapsed    | 1849     |
|    total_timesteps | 512000   |
---------------------------------
Eval num_timesteps=512500, episode_reward=874.05 +/- 718.71
Episode length: 35.22 +/- 6.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 874       |
| time/                   |           |
|    total_timesteps      | 512500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.21e-19 |
|    explained_variance   | 0.0348    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.13e+04  |
|    n_updates            | 2530      |
|    policy_gradient_loss | 1.02e-10  |
|    value_loss           | 4.13e+04  |
---------------------------------------
Eval num_timesteps=513000, episode_reward=971.53 +/- 748.93
Episode length: 35.66 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 513000   |
---------------------------------
Eval num_timesteps=513500, episode_reward=919.09 +/- 706.91
Episode length: 35.80 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 513500   |
---------------------------------
Eval num_timesteps=514000, episode_reward=809.57 +/- 693.07
Episode length: 34.86 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 983      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 251      |
|    time_elapsed    | 1856     |
|    total_timesteps | 514048   |
---------------------------------
Eval num_timesteps=514500, episode_reward=880.08 +/- 712.78
Episode length: 35.28 +/- 6.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 880       |
| time/                   |           |
|    total_timesteps      | 514500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.46e-17 |
|    explained_variance   | 0.0306    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.9e+04   |
|    n_updates            | 2540      |
|    policy_gradient_loss | -4e-10    |
|    value_loss           | 4.25e+04  |
---------------------------------------
Eval num_timesteps=515000, episode_reward=717.84 +/- 623.35
Episode length: 34.16 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 718      |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
Eval num_timesteps=515500, episode_reward=782.35 +/- 665.62
Episode length: 34.78 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 515500   |
---------------------------------
Eval num_timesteps=516000, episode_reward=679.24 +/- 512.93
Episode length: 35.22 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 679      |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 989      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 252      |
|    time_elapsed    | 1864     |
|    total_timesteps | 516096   |
---------------------------------
Eval num_timesteps=516500, episode_reward=841.42 +/- 682.76
Episode length: 35.28 +/- 6.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 841       |
| time/                   |           |
|    total_timesteps      | 516500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.92e-19 |
|    explained_variance   | 0.041     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.88e+04  |
|    n_updates            | 2550      |
|    policy_gradient_loss | -1.69e-09 |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=517000, episode_reward=909.34 +/- 735.77
Episode length: 35.16 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 909      |
| time/              |          |
|    total_timesteps | 517000   |
---------------------------------
Eval num_timesteps=517500, episode_reward=577.65 +/- 496.86
Episode length: 34.00 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 578      |
| time/              |          |
|    total_timesteps | 517500   |
---------------------------------
Eval num_timesteps=518000, episode_reward=731.86 +/- 608.94
Episode length: 34.64 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 732      |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 253      |
|    time_elapsed    | 1871     |
|    total_timesteps | 518144   |
---------------------------------
Eval num_timesteps=518500, episode_reward=903.27 +/- 687.48
Episode length: 36.22 +/- 5.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 903       |
| time/                   |           |
|    total_timesteps      | 518500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.22e-17 |
|    explained_variance   | 0.0278    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.71e+04  |
|    n_updates            | 2560      |
|    policy_gradient_loss | -1.57e-09 |
|    value_loss           | 4.49e+04  |
---------------------------------------
Eval num_timesteps=519000, episode_reward=766.70 +/- 621.11
Episode length: 35.20 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 519000   |
---------------------------------
Eval num_timesteps=519500, episode_reward=1084.32 +/- 789.20
Episode length: 36.42 +/- 7.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 519500   |
---------------------------------
Eval num_timesteps=520000, episode_reward=784.60 +/- 583.18
Episode length: 36.32 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 785      |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 895      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 254      |
|    time_elapsed    | 1878     |
|    total_timesteps | 520192   |
---------------------------------
Eval num_timesteps=520500, episode_reward=741.03 +/- 619.54
Episode length: 35.16 +/- 6.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 741       |
| time/                   |           |
|    total_timesteps      | 520500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.18e-12 |
|    explained_variance   | 0.0314    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.82e+04  |
|    n_updates            | 2570      |
|    policy_gradient_loss | -1.79e-10 |
|    value_loss           | 3.88e+04  |
---------------------------------------
Eval num_timesteps=521000, episode_reward=640.88 +/- 590.88
Episode length: 32.92 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 521000   |
---------------------------------
Eval num_timesteps=521500, episode_reward=893.35 +/- 707.89
Episode length: 35.72 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 521500   |
---------------------------------
Eval num_timesteps=522000, episode_reward=1109.21 +/- 728.31
Episode length: 37.80 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 930      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 255      |
|    time_elapsed    | 1886     |
|    total_timesteps | 522240   |
---------------------------------
Eval num_timesteps=522500, episode_reward=749.58 +/- 631.42
Episode length: 34.70 +/- 6.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 750       |
| time/                   |           |
|    total_timesteps      | 522500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.42e-14 |
|    explained_variance   | 0.0277    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.95e+04  |
|    n_updates            | 2580      |
|    policy_gradient_loss | -4.42e-10 |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=523000, episode_reward=834.55 +/- 644.80
Episode length: 35.62 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 523000   |
---------------------------------
Eval num_timesteps=523500, episode_reward=861.55 +/- 707.98
Episode length: 35.22 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 523500   |
---------------------------------
Eval num_timesteps=524000, episode_reward=817.10 +/- 687.95
Episode length: 34.56 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 817      |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 752      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 256      |
|    time_elapsed    | 1893     |
|    total_timesteps | 524288   |
---------------------------------
Eval num_timesteps=524500, episode_reward=779.16 +/- 671.21
Episode length: 34.76 +/- 7.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 779       |
| time/                   |           |
|    total_timesteps      | 524500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.99e-13 |
|    explained_variance   | 0.0104    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.94e+04  |
|    n_updates            | 2590      |
|    policy_gradient_loss | -9.42e-10 |
|    value_loss           | 3.48e+04  |
---------------------------------------
Eval num_timesteps=525000, episode_reward=906.82 +/- 745.74
Episode length: 34.68 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 525000   |
---------------------------------
Eval num_timesteps=525500, episode_reward=738.33 +/- 592.32
Episode length: 35.58 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 738      |
| time/              |          |
|    total_timesteps | 525500   |
---------------------------------
Eval num_timesteps=526000, episode_reward=839.13 +/- 683.54
Episode length: 35.30 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 739      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 257      |
|    time_elapsed    | 1900     |
|    total_timesteps | 526336   |
---------------------------------
Eval num_timesteps=526500, episode_reward=911.02 +/- 744.38
Episode length: 35.30 +/- 7.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.3      |
|    mean_reward          | 911       |
| time/                   |           |
|    total_timesteps      | 526500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -5.28e-15 |
|    explained_variance   | 0.0253    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.02e+04  |
|    n_updates            | 2600      |
|    policy_gradient_loss | -8.44e-10 |
|    value_loss           | 3.8e+04   |
---------------------------------------
Eval num_timesteps=527000, episode_reward=933.33 +/- 716.10
Episode length: 35.94 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 527000   |
---------------------------------
Eval num_timesteps=527500, episode_reward=999.07 +/- 729.54
Episode length: 36.50 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 999      |
| time/              |          |
|    total_timesteps | 527500   |
---------------------------------
Eval num_timesteps=528000, episode_reward=892.97 +/- 689.52
Episode length: 35.86 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 966      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 258      |
|    time_elapsed    | 1908     |
|    total_timesteps | 528384   |
---------------------------------
Eval num_timesteps=528500, episode_reward=823.53 +/- 686.61
Episode length: 34.58 +/- 6.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 824       |
| time/                   |           |
|    total_timesteps      | 528500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.33e-19 |
|    explained_variance   | 0.0394    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.23e+04  |
|    n_updates            | 2610      |
|    policy_gradient_loss | -1.41e-09 |
|    value_loss           | 4.23e+04  |
---------------------------------------
Eval num_timesteps=529000, episode_reward=795.68 +/- 673.80
Episode length: 34.62 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 529000   |
---------------------------------
Eval num_timesteps=529500, episode_reward=749.04 +/- 638.59
Episode length: 34.58 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 529500   |
---------------------------------
Eval num_timesteps=530000, episode_reward=793.36 +/- 670.56
Episode length: 34.92 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 864      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 259      |
|    time_elapsed    | 1915     |
|    total_timesteps | 530432   |
---------------------------------
Eval num_timesteps=530500, episode_reward=874.39 +/- 748.33
Episode length: 35.18 +/- 7.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 874       |
| time/                   |           |
|    total_timesteps      | 530500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.87e-17 |
|    explained_variance   | 0.021     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.63e+04  |
|    n_updates            | 2620      |
|    policy_gradient_loss | 5.53e-10  |
|    value_loss           | 4.51e+04  |
---------------------------------------
Eval num_timesteps=531000, episode_reward=864.05 +/- 704.21
Episode length: 34.84 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 531000   |
---------------------------------
Eval num_timesteps=531500, episode_reward=863.85 +/- 740.93
Episode length: 35.00 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 531500   |
---------------------------------
Eval num_timesteps=532000, episode_reward=886.47 +/- 757.66
Episode length: 34.58 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 886      |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 850      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 260      |
|    time_elapsed    | 1922     |
|    total_timesteps | 532480   |
---------------------------------
Eval num_timesteps=532500, episode_reward=853.84 +/- 680.42
Episode length: 36.16 +/- 6.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.2      |
|    mean_reward          | 854       |
| time/                   |           |
|    total_timesteps      | 532500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.21e-12 |
|    explained_variance   | 0.028     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.61e+04  |
|    n_updates            | 2630      |
|    policy_gradient_loss | -7.09e-10 |
|    value_loss           | 3.46e+04  |
---------------------------------------
Eval num_timesteps=533000, episode_reward=961.15 +/- 727.39
Episode length: 36.22 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 961      |
| time/              |          |
|    total_timesteps | 533000   |
---------------------------------
Eval num_timesteps=533500, episode_reward=846.36 +/- 663.50
Episode length: 36.12 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 846      |
| time/              |          |
|    total_timesteps | 533500   |
---------------------------------
Eval num_timesteps=534000, episode_reward=845.19 +/- 721.41
Episode length: 34.80 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
Eval num_timesteps=534500, episode_reward=769.13 +/- 641.48
Episode length: 34.44 +/- 7.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 534500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 916      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 261      |
|    time_elapsed    | 1931     |
|    total_timesteps | 534528   |
---------------------------------
Eval num_timesteps=535000, episode_reward=779.56 +/- 672.19
Episode length: 34.30 +/- 6.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 780       |
| time/                   |           |
|    total_timesteps      | 535000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.2e-15  |
|    explained_variance   | 0.0307    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.01e+04  |
|    n_updates            | 2640      |
|    policy_gradient_loss | -6.69e-11 |
|    value_loss           | 4.13e+04  |
---------------------------------------
Eval num_timesteps=535500, episode_reward=919.50 +/- 681.59
Episode length: 36.80 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 535500   |
---------------------------------
Eval num_timesteps=536000, episode_reward=829.66 +/- 673.49
Episode length: 35.54 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
Eval num_timesteps=536500, episode_reward=768.07 +/- 609.92
Episode length: 35.12 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 536500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 827      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 262      |
|    time_elapsed    | 1938     |
|    total_timesteps | 536576   |
---------------------------------
Eval num_timesteps=537000, episode_reward=834.69 +/- 675.49
Episode length: 35.88 +/- 6.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 835       |
| time/                   |           |
|    total_timesteps      | 537000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.43e-12 |
|    explained_variance   | 0.0285    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.23e+04  |
|    n_updates            | 2650      |
|    policy_gradient_loss | 7e-10     |
|    value_loss           | 3.51e+04  |
---------------------------------------
Eval num_timesteps=537500, episode_reward=849.44 +/- 609.66
Episode length: 36.40 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 537500   |
---------------------------------
Eval num_timesteps=538000, episode_reward=965.21 +/- 708.73
Episode length: 36.86 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 965      |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
Eval num_timesteps=538500, episode_reward=719.84 +/- 649.39
Episode length: 34.18 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 720      |
| time/              |          |
|    total_timesteps | 538500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 836      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 263      |
|    time_elapsed    | 1945     |
|    total_timesteps | 538624   |
---------------------------------
Eval num_timesteps=539000, episode_reward=755.81 +/- 652.91
Episode length: 34.68 +/- 7.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 756       |
| time/                   |           |
|    total_timesteps      | 539000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.85e-15 |
|    explained_variance   | 0.0226    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.49e+04  |
|    n_updates            | 2660      |
|    policy_gradient_loss | 7.42e-10  |
|    value_loss           | 3.67e+04  |
---------------------------------------
Eval num_timesteps=539500, episode_reward=697.83 +/- 606.28
Episode length: 34.28 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 539500   |
---------------------------------
Eval num_timesteps=540000, episode_reward=770.17 +/- 693.50
Episode length: 33.78 +/- 8.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=540500, episode_reward=787.24 +/- 641.36
Episode length: 35.52 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 540500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 746      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 264      |
|    time_elapsed    | 1953     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=541000, episode_reward=917.04 +/- 737.03
Episode length: 35.72 +/- 6.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 917       |
| time/                   |           |
|    total_timesteps      | 541000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.53e-12 |
|    explained_variance   | 0.0153    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.2e+04   |
|    n_updates            | 2670      |
|    policy_gradient_loss | -3.93e-11 |
|    value_loss           | 3.47e+04  |
---------------------------------------
Eval num_timesteps=541500, episode_reward=820.64 +/- 696.79
Episode length: 34.92 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 541500   |
---------------------------------
Eval num_timesteps=542000, episode_reward=702.20 +/- 659.88
Episode length: 33.76 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 702      |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
Eval num_timesteps=542500, episode_reward=763.05 +/- 647.73
Episode length: 34.64 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 542500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 724      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 265      |
|    time_elapsed    | 1960     |
|    total_timesteps | 542720   |
---------------------------------
Eval num_timesteps=543000, episode_reward=926.16 +/- 713.91
Episode length: 35.58 +/- 6.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.6      |
|    mean_reward          | 926       |
| time/                   |           |
|    total_timesteps      | 543000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.77e-15 |
|    explained_variance   | 0.0223    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.17e+04  |
|    n_updates            | 2680      |
|    policy_gradient_loss | 1.46e-11  |
|    value_loss           | 3.6e+04   |
---------------------------------------
Eval num_timesteps=543500, episode_reward=806.27 +/- 662.41
Episode length: 35.42 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 806      |
| time/              |          |
|    total_timesteps | 543500   |
---------------------------------
Eval num_timesteps=544000, episode_reward=830.42 +/- 654.58
Episode length: 35.48 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
Eval num_timesteps=544500, episode_reward=1073.40 +/- 768.18
Episode length: 36.32 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 544500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 266      |
|    time_elapsed    | 1967     |
|    total_timesteps | 544768   |
---------------------------------
Eval num_timesteps=545000, episode_reward=716.79 +/- 625.75
Episode length: 34.22 +/- 7.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.2      |
|    mean_reward          | 717       |
| time/                   |           |
|    total_timesteps      | 545000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.77e-12 |
|    explained_variance   | 0.035     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.2e+04   |
|    n_updates            | 2690      |
|    policy_gradient_loss | 6.98e-10  |
|    value_loss           | 3.87e+04  |
---------------------------------------
Eval num_timesteps=545500, episode_reward=875.47 +/- 690.38
Episode length: 35.88 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 545500   |
---------------------------------
Eval num_timesteps=546000, episode_reward=811.27 +/- 703.10
Episode length: 34.60 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
Eval num_timesteps=546500, episode_reward=750.73 +/- 702.53
Episode length: 33.68 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 751      |
| time/              |          |
|    total_timesteps | 546500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 807      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 267      |
|    time_elapsed    | 1975     |
|    total_timesteps | 546816   |
---------------------------------
Eval num_timesteps=547000, episode_reward=870.87 +/- 715.77
Episode length: 34.80 +/- 7.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 871       |
| time/                   |           |
|    total_timesteps      | 547000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.07e-14 |
|    explained_variance   | 0.0293    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.62e+04  |
|    n_updates            | 2700      |
|    policy_gradient_loss | 3.55e-10  |
|    value_loss           | 3.88e+04  |
---------------------------------------
Eval num_timesteps=547500, episode_reward=841.07 +/- 692.39
Episode length: 35.70 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 547500   |
---------------------------------
Eval num_timesteps=548000, episode_reward=759.77 +/- 678.20
Episode length: 34.12 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
Eval num_timesteps=548500, episode_reward=849.80 +/- 708.71
Episode length: 34.82 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 548500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 921      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 268      |
|    time_elapsed    | 1982     |
|    total_timesteps | 548864   |
---------------------------------
Eval num_timesteps=549000, episode_reward=740.11 +/- 618.95
Episode length: 34.68 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.7      |
|    mean_reward          | 740       |
| time/                   |           |
|    total_timesteps      | 549000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.66e-19 |
|    explained_variance   | 0.0312    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.08e+04  |
|    n_updates            | 2710      |
|    policy_gradient_loss | -1.11e-09 |
|    value_loss           | 3.88e+04  |
---------------------------------------
Eval num_timesteps=549500, episode_reward=785.76 +/- 659.23
Episode length: 35.44 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 549500   |
---------------------------------
Eval num_timesteps=550000, episode_reward=798.99 +/- 663.88
Episode length: 34.98 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
Eval num_timesteps=550500, episode_reward=1058.36 +/- 749.76
Episode length: 36.98 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 550500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 852      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 269      |
|    time_elapsed    | 1989     |
|    total_timesteps | 550912   |
---------------------------------
Eval num_timesteps=551000, episode_reward=764.01 +/- 677.86
Episode length: 33.98 +/- 7.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34        |
|    mean_reward          | 764       |
| time/                   |           |
|    total_timesteps      | 551000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.68e-16 |
|    explained_variance   | 0.0165    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.35e+04  |
|    n_updates            | 2720      |
|    policy_gradient_loss | 3.86e-10  |
|    value_loss           | 4.3e+04   |
---------------------------------------
Eval num_timesteps=551500, episode_reward=862.85 +/- 704.04
Episode length: 34.88 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 551500   |
---------------------------------
Eval num_timesteps=552000, episode_reward=874.27 +/- 706.84
Episode length: 35.58 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
Eval num_timesteps=552500, episode_reward=754.14 +/- 619.20
Episode length: 35.12 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 552500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 775      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 270      |
|    time_elapsed    | 1997     |
|    total_timesteps | 552960   |
---------------------------------
Eval num_timesteps=553000, episode_reward=773.68 +/- 679.17
Episode length: 34.12 +/- 6.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.1      |
|    mean_reward          | 774       |
| time/                   |           |
|    total_timesteps      | 553000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.26e-12 |
|    explained_variance   | 0.0231    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.08e+04  |
|    n_updates            | 2730      |
|    policy_gradient_loss | -2.97e-10 |
|    value_loss           | 3.62e+04  |
---------------------------------------
Eval num_timesteps=553500, episode_reward=895.13 +/- 675.58
Episode length: 36.12 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 553500   |
---------------------------------
Eval num_timesteps=554000, episode_reward=943.15 +/- 727.23
Episode length: 35.96 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 554000   |
---------------------------------
Eval num_timesteps=554500, episode_reward=740.52 +/- 627.74
Episode length: 34.90 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 554500   |
---------------------------------
Eval num_timesteps=555000, episode_reward=876.25 +/- 718.76
Episode length: 35.46 +/- 7.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 876      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 271      |
|    time_elapsed    | 2005     |
|    total_timesteps | 555008   |
---------------------------------
Eval num_timesteps=555500, episode_reward=816.42 +/- 639.92
Episode length: 35.68 +/- 6.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.7      |
|    mean_reward          | 816       |
| time/                   |           |
|    total_timesteps      | 555500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.77e-14 |
|    explained_variance   | 0.0328    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.36e+04  |
|    n_updates            | 2740      |
|    policy_gradient_loss | 2.5e-10   |
|    value_loss           | 4.15e+04  |
---------------------------------------
Eval num_timesteps=556000, episode_reward=974.30 +/- 733.86
Episode length: 36.46 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 974      |
| time/              |          |
|    total_timesteps | 556000   |
---------------------------------
Eval num_timesteps=556500, episode_reward=824.79 +/- 651.62
Episode length: 35.98 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 556500   |
---------------------------------
Eval num_timesteps=557000, episode_reward=734.19 +/- 589.93
Episode length: 35.00 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 734      |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 829      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 272      |
|    time_elapsed    | 2013     |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=557500, episode_reward=929.00 +/- 750.73
Episode length: 35.42 +/- 6.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 929       |
| time/                   |           |
|    total_timesteps      | 557500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.56e-12 |
|    explained_variance   | 0.0271    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.45e+04  |
|    n_updates            | 2750      |
|    policy_gradient_loss | 2.71e-10  |
|    value_loss           | 3.76e+04  |
---------------------------------------
Eval num_timesteps=558000, episode_reward=789.06 +/- 657.62
Episode length: 34.80 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
Eval num_timesteps=558500, episode_reward=745.03 +/- 593.38
Episode length: 35.04 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 558500   |
---------------------------------
Eval num_timesteps=559000, episode_reward=745.11 +/- 594.68
Episode length: 34.96 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 767      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 273      |
|    time_elapsed    | 2020     |
|    total_timesteps | 559104   |
---------------------------------
Eval num_timesteps=559500, episode_reward=641.92 +/- 634.92
Episode length: 32.62 +/- 7.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 32.6      |
|    mean_reward          | 642       |
| time/                   |           |
|    total_timesteps      | 559500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.29e-14 |
|    explained_variance   | 0.0311    |
|    learning_rate        | 0.0001    |
|    loss                 | 3.78e+04  |
|    n_updates            | 2760      |
|    policy_gradient_loss | -5.68e-10 |
|    value_loss           | 3.91e+04  |
---------------------------------------
Eval num_timesteps=560000, episode_reward=811.69 +/- 710.04
Episode length: 34.64 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 812      |
| time/              |          |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=560500, episode_reward=856.33 +/- 642.85
Episode length: 36.42 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 560500   |
---------------------------------
Eval num_timesteps=561000, episode_reward=803.49 +/- 637.42
Episode length: 35.50 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 803      |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 839      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 274      |
|    time_elapsed    | 2027     |
|    total_timesteps | 561152   |
---------------------------------
Eval num_timesteps=561500, episode_reward=930.59 +/- 738.10
Episode length: 35.90 +/- 6.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 931       |
| time/                   |           |
|    total_timesteps      | 561500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -3.85e-12 |
|    explained_variance   | 0.0296    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.35e+04  |
|    n_updates            | 2770      |
|    policy_gradient_loss | -7.35e-10 |
|    value_loss           | 3.84e+04  |
---------------------------------------
Eval num_timesteps=562000, episode_reward=911.43 +/- 718.27
Episode length: 35.76 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 911      |
| time/              |          |
|    total_timesteps | 562000   |
---------------------------------
Eval num_timesteps=562500, episode_reward=937.85 +/- 712.30
Episode length: 36.00 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 938      |
| time/              |          |
|    total_timesteps | 562500   |
---------------------------------
Eval num_timesteps=563000, episode_reward=797.47 +/- 616.20
Episode length: 35.30 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 814      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 275      |
|    time_elapsed    | 2035     |
|    total_timesteps | 563200   |
---------------------------------
Eval num_timesteps=563500, episode_reward=806.45 +/- 677.87
Episode length: 34.92 +/- 6.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 806       |
| time/                   |           |
|    total_timesteps      | 563500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.46e-14 |
|    explained_variance   | 0.0249    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.64e+04  |
|    n_updates            | 2780      |
|    policy_gradient_loss | -9.53e-10 |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=564000, episode_reward=878.74 +/- 741.26
Episode length: 34.96 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 879      |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
Eval num_timesteps=564500, episode_reward=803.76 +/- 679.46
Episode length: 35.14 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 564500   |
---------------------------------
Eval num_timesteps=565000, episode_reward=698.07 +/- 584.00
Episode length: 34.50 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 698      |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 779      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 276      |
|    time_elapsed    | 2042     |
|    total_timesteps | 565248   |
---------------------------------
Eval num_timesteps=565500, episode_reward=1009.89 +/- 775.99
Episode length: 35.52 +/- 7.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 1.01e+03  |
| time/                   |           |
|    total_timesteps      | 565500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.16e-12 |
|    explained_variance   | 0.028     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.27e+04  |
|    n_updates            | 2790      |
|    policy_gradient_loss | 9.9e-10   |
|    value_loss           | 3.53e+04  |
---------------------------------------
Eval num_timesteps=566000, episode_reward=698.80 +/- 563.91
Episode length: 34.80 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 566000   |
---------------------------------
Eval num_timesteps=566500, episode_reward=778.39 +/- 627.19
Episode length: 35.10 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 566500   |
---------------------------------
Eval num_timesteps=567000, episode_reward=893.87 +/- 665.60
Episode length: 35.92 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 894      |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 790      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 277      |
|    time_elapsed    | 2050     |
|    total_timesteps | 567296   |
---------------------------------
Eval num_timesteps=567500, episode_reward=727.59 +/- 625.93
Episode length: 34.58 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 728       |
| time/                   |           |
|    total_timesteps      | 567500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.33e-14 |
|    explained_variance   | 0.0249    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.44e+04  |
|    n_updates            | 2800      |
|    policy_gradient_loss | -1.18e-09 |
|    value_loss           | 3.77e+04  |
---------------------------------------
Eval num_timesteps=568000, episode_reward=772.84 +/- 669.90
Episode length: 34.72 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 568000   |
---------------------------------
Eval num_timesteps=568500, episode_reward=932.45 +/- 668.85
Episode length: 36.86 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 568500   |
---------------------------------
Eval num_timesteps=569000, episode_reward=999.47 +/- 779.17
Episode length: 35.90 +/- 7.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 999      |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 901      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 278      |
|    time_elapsed    | 2057     |
|    total_timesteps | 569344   |
---------------------------------
Eval num_timesteps=569500, episode_reward=723.44 +/- 616.77
Episode length: 34.60 +/- 6.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 723       |
| time/                   |           |
|    total_timesteps      | 569500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -9.58e-19 |
|    explained_variance   | 0.0382    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.48e+04  |
|    n_updates            | 2810      |
|    policy_gradient_loss | -3.11e-10 |
|    value_loss           | 3.98e+04  |
---------------------------------------
Eval num_timesteps=570000, episode_reward=888.88 +/- 693.69
Episode length: 35.92 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
Eval num_timesteps=570500, episode_reward=1061.47 +/- 733.18
Episode length: 37.62 +/- 5.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.6     |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 570500   |
---------------------------------
Eval num_timesteps=571000, episode_reward=651.97 +/- 599.34
Episode length: 33.20 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 822      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 279      |
|    time_elapsed    | 2064     |
|    total_timesteps | 571392   |
---------------------------------
Eval num_timesteps=571500, episode_reward=760.12 +/- 605.52
Episode length: 35.42 +/- 5.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.4      |
|    mean_reward          | 760       |
| time/                   |           |
|    total_timesteps      | 571500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.78e-16 |
|    explained_variance   | 0.000273  |
|    learning_rate        | 0.0001    |
|    loss                 | 2.4e+04   |
|    n_updates            | 2820      |
|    policy_gradient_loss | -1.34e-09 |
|    value_loss           | 4.71e+04  |
---------------------------------------
Eval num_timesteps=572000, episode_reward=654.35 +/- 625.10
Episode length: 32.94 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 654      |
| time/              |          |
|    total_timesteps | 572000   |
---------------------------------
Eval num_timesteps=572500, episode_reward=881.27 +/- 746.21
Episode length: 34.64 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 572500   |
---------------------------------
Eval num_timesteps=573000, episode_reward=700.91 +/- 640.24
Episode length: 34.30 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 764      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 280      |
|    time_elapsed    | 2071     |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=573500, episode_reward=744.90 +/- 626.11
Episode length: 34.94 +/- 5.70
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 34.9     |
|    mean_reward          | 745      |
| time/                   |          |
|    total_timesteps      | 573500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -4e-12   |
|    explained_variance   | 0.0344   |
|    learning_rate        | 0.0001   |
|    loss                 | 1.06e+04 |
|    n_updates            | 2830     |
|    policy_gradient_loss | 1.11e-09 |
|    value_loss           | 3.59e+04 |
--------------------------------------
Eval num_timesteps=574000, episode_reward=776.52 +/- 683.19
Episode length: 33.84 +/- 7.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 574000   |
---------------------------------
Eval num_timesteps=574500, episode_reward=647.39 +/- 599.90
Episode length: 33.66 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 574500   |
---------------------------------
Eval num_timesteps=575000, episode_reward=838.18 +/- 756.75
Episode length: 34.82 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 880      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 281      |
|    time_elapsed    | 2079     |
|    total_timesteps | 575488   |
---------------------------------
Eval num_timesteps=575500, episode_reward=907.01 +/- 716.32
Episode length: 35.50 +/- 6.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.5      |
|    mean_reward          | 907       |
| time/                   |           |
|    total_timesteps      | 575500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.36e-14 |
|    explained_variance   | 0.029     |
|    learning_rate        | 0.0001    |
|    loss                 | 1.24e+04  |
|    n_updates            | 2840      |
|    policy_gradient_loss | -4.26e-10 |
|    value_loss           | 3.93e+04  |
---------------------------------------
Eval num_timesteps=576000, episode_reward=743.51 +/- 574.38
Episode length: 35.46 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=576500, episode_reward=1038.58 +/- 747.42
Episode length: 36.96 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 576500   |
---------------------------------
Eval num_timesteps=577000, episode_reward=841.31 +/- 719.86
Episode length: 35.14 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 577000   |
---------------------------------
Eval num_timesteps=577500, episode_reward=848.48 +/- 649.07
Episode length: 36.28 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 848      |
| time/              |          |
|    total_timesteps | 577500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 818      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 282      |
|    time_elapsed    | 2087     |
|    total_timesteps | 577536   |
---------------------------------
Eval num_timesteps=578000, episode_reward=1045.12 +/- 718.85
Episode length: 36.90 +/- 6.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.9      |
|    mean_reward          | 1.05e+03  |
| time/                   |           |
|    total_timesteps      | 578000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -4.35e-12 |
|    explained_variance   | 0.0273    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.38e+04  |
|    n_updates            | 2850      |
|    policy_gradient_loss | -6.26e-11 |
|    value_loss           | 3.85e+04  |
---------------------------------------
Eval num_timesteps=578500, episode_reward=812.93 +/- 687.88
Episode length: 35.04 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 578500   |
---------------------------------
Eval num_timesteps=579000, episode_reward=870.18 +/- 736.96
Episode length: 35.12 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 870      |
| time/              |          |
|    total_timesteps | 579000   |
---------------------------------
Eval num_timesteps=579500, episode_reward=919.20 +/- 730.52
Episode length: 35.52 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 579500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 667      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 283      |
|    time_elapsed    | 2095     |
|    total_timesteps | 579584   |
---------------------------------
Eval num_timesteps=580000, episode_reward=856.85 +/- 650.50
Episode length: 36.36 +/- 5.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.4      |
|    mean_reward          | 857       |
| time/                   |           |
|    total_timesteps      | 580000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.87e-14 |
|    explained_variance   | 0.0173    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.32e+04  |
|    n_updates            | 2860      |
|    policy_gradient_loss | 8.59e-10  |
|    value_loss           | 2.89e+04  |
---------------------------------------
Eval num_timesteps=580500, episode_reward=797.37 +/- 607.28
Episode length: 36.48 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 580500   |
---------------------------------
Eval num_timesteps=581000, episode_reward=903.81 +/- 691.36
Episode length: 36.48 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 581000   |
---------------------------------
Eval num_timesteps=581500, episode_reward=916.32 +/- 721.99
Episode length: 36.12 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 581500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 855      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 284      |
|    time_elapsed    | 2102     |
|    total_timesteps | 581632   |
---------------------------------
Eval num_timesteps=582000, episode_reward=1040.42 +/- 736.72
Episode length: 36.58 +/- 6.32
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 1.04e+03  |
| time/                   |           |
|    total_timesteps      | 582000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -2.95e-18 |
|    explained_variance   | 0.037     |
|    learning_rate        | 0.0001    |
|    loss                 | 2.67e+04  |
|    n_updates            | 2870      |
|    policy_gradient_loss | 1.32e-09  |
|    value_loss           | 3.94e+04  |
---------------------------------------
Eval num_timesteps=582500, episode_reward=862.50 +/- 661.27
Episode length: 36.02 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 582500   |
---------------------------------
Eval num_timesteps=583000, episode_reward=710.84 +/- 667.45
Episode length: 33.64 +/- 7.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 583000   |
---------------------------------
Eval num_timesteps=583500, episode_reward=723.72 +/- 610.84
Episode length: 34.48 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 724      |
| time/              |          |
|    total_timesteps | 583500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 859      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 285      |
|    time_elapsed    | 2110     |
|    total_timesteps | 583680   |
---------------------------------
Eval num_timesteps=584000, episode_reward=908.54 +/- 698.94
Episode length: 36.02 +/- 6.37
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 909       |
| time/                   |           |
|    total_timesteps      | 584000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -6.19e-16 |
|    explained_variance   | 0.0223    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.58e+04  |
|    n_updates            | 2880      |
|    policy_gradient_loss | -1.06e-09 |
|    value_loss           | 4.64e+04  |
---------------------------------------
Eval num_timesteps=584500, episode_reward=761.29 +/- 651.78
Episode length: 34.96 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 761      |
| time/              |          |
|    total_timesteps | 584500   |
---------------------------------
Eval num_timesteps=585000, episode_reward=762.56 +/- 673.22
Episode length: 34.46 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 585000   |
---------------------------------
Eval num_timesteps=585500, episode_reward=864.07 +/- 705.69
Episode length: 35.66 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 864      |
| time/              |          |
|    total_timesteps | 585500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 735      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 286      |
|    time_elapsed    | 2117     |
|    total_timesteps | 585728   |
---------------------------------
Eval num_timesteps=586000, episode_reward=886.46 +/- 655.50
Episode length: 36.00 +/- 6.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36        |
|    mean_reward          | 886       |
| time/                   |           |
|    total_timesteps      | 586000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-11 |
|    explained_variance   | 0.0176    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.3e+04   |
|    n_updates            | 2890      |
|    policy_gradient_loss | -1.33e-09 |
|    value_loss           | 3.59e+04  |
---------------------------------------
Eval num_timesteps=586500, episode_reward=906.23 +/- 702.14
Episode length: 35.94 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 586500   |
---------------------------------
Eval num_timesteps=587000, episode_reward=802.19 +/- 723.20
Episode length: 33.66 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 587000   |
---------------------------------
Eval num_timesteps=587500, episode_reward=649.05 +/- 580.43
Episode length: 33.62 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 587500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 765      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 287      |
|    time_elapsed    | 2124     |
|    total_timesteps | 587776   |
---------------------------------
Eval num_timesteps=588000, episode_reward=790.47 +/- 710.64
Episode length: 34.38 +/- 6.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.4      |
|    mean_reward          | 790       |
| time/                   |           |
|    total_timesteps      | 588000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.91e-14 |
|    explained_variance   | 0.0268    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.82e+04  |
|    n_updates            | 2900      |
|    policy_gradient_loss | 1.69e-10  |
|    value_loss           | 3.68e+04  |
---------------------------------------
Eval num_timesteps=588500, episode_reward=675.01 +/- 620.80
Episode length: 33.36 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 588500   |
---------------------------------
Eval num_timesteps=589000, episode_reward=678.00 +/- 630.12
Episode length: 33.96 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 589000   |
---------------------------------
Eval num_timesteps=589500, episode_reward=956.38 +/- 744.18
Episode length: 36.00 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 956      |
| time/              |          |
|    total_timesteps | 589500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 714      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 288      |
|    time_elapsed    | 2131     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=590000, episode_reward=748.07 +/- 636.82
Episode length: 34.34 +/- 6.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 748       |
| time/                   |           |
|    total_timesteps      | 590000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.19e-11 |
|    explained_variance   | 0.0212    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.76e+04  |
|    n_updates            | 2910      |
|    policy_gradient_loss | 1.28e-09  |
|    value_loss           | 3.73e+04  |
---------------------------------------
Eval num_timesteps=590500, episode_reward=756.73 +/- 642.36
Episode length: 34.96 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 590500   |
---------------------------------
Eval num_timesteps=591000, episode_reward=916.36 +/- 710.81
Episode length: 35.72 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 591000   |
---------------------------------
Eval num_timesteps=591500, episode_reward=914.80 +/- 708.47
Episode length: 35.44 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 591500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 848      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 289      |
|    time_elapsed    | 2139     |
|    total_timesteps | 591872   |
---------------------------------
Eval num_timesteps=592000, episode_reward=926.06 +/- 749.02
Episode length: 35.88 +/- 7.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.9      |
|    mean_reward          | 926       |
| time/                   |           |
|    total_timesteps      | 592000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -7.33e-14 |
|    explained_variance   | 0.0299    |
|    learning_rate        | 0.0001    |
|    loss                 | 2.02e+04  |
|    n_updates            | 2920      |
|    policy_gradient_loss | -1.88e-09 |
|    value_loss           | 4.18e+04  |
---------------------------------------
Eval num_timesteps=592500, episode_reward=970.49 +/- 723.03
Episode length: 36.00 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 592500   |
---------------------------------
Eval num_timesteps=593000, episode_reward=862.68 +/- 700.22
Episode length: 35.38 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 863      |
| time/              |          |
|    total_timesteps | 593000   |
---------------------------------
Eval num_timesteps=593500, episode_reward=918.78 +/- 704.01
Episode length: 35.68 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 593500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 889      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 290      |
|    time_elapsed    | 2146     |
|    total_timesteps | 593920   |
---------------------------------
Eval num_timesteps=594000, episode_reward=762.94 +/- 631.43
Episode length: 34.80 +/- 6.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.8      |
|    mean_reward          | 763       |
| time/                   |           |
|    total_timesteps      | 594000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -1.27e-11 |
|    explained_variance   | 0.0307    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.38e+04  |
|    n_updates            | 2930      |
|    policy_gradient_loss | 2.71e-10  |
|    value_loss           | 4.06e+04  |
---------------------------------------
Eval num_timesteps=594500, episode_reward=696.06 +/- 601.64
Episode length: 33.98 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 696      |
| time/              |          |
|    total_timesteps | 594500   |
---------------------------------
Eval num_timesteps=595000, episode_reward=949.55 +/- 728.25
Episode length: 35.90 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 950      |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
Eval num_timesteps=595500, episode_reward=777.63 +/- 600.42
Episode length: 35.90 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 778      |
| time/              |          |
|    total_timesteps | 595500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 780      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 291      |
|    time_elapsed    | 2153     |
|    total_timesteps | 595968   |
---------------------------------
Eval num_timesteps=596000, episode_reward=769.10 +/- 624.70
Episode length: 35.12 +/- 6.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.1      |
|    mean_reward          | 769       |
| time/                   |           |
|    total_timesteps      | 596000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -8.34e-14 |
|    explained_variance   | 0.0249    |
|    learning_rate        | 0.0001    |
|    loss                 | 1.64e+04  |
|    n_updates            | 2940      |
|    policy_gradient_loss | 1.07e-09  |
|    value_loss           | 3.55e+04  |
---------------------------------------
Eval num_timesteps=596500, episode_reward=703.17 +/- 609.43
Episode length: 34.14 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 596500   |
---------------------------------
Eval num_timesteps=597000, episode_reward=706.70 +/- 632.12
Episode length: 33.90 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 597000   |
---------------------------------
Eval num_timesteps=597500, episode_reward=889.23 +/- 687.98
Episode length: 35.78 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 889      |
| time/              |          |
|    total_timesteps | 597500   |
---------------------------------
Eval num_timesteps=598000, episode_reward=802.43 +/- 751.32
Episode length: 33.62 +/- 7.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 802      |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 730      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 292      |
|    time_elapsed    | 2162     |
|    total_timesteps | 598016   |
---------------------------------
Eval num_timesteps=598500, episode_reward=854.18 +/- 657.54
Episode length: 35.92 +/- 5.47
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 35.9     |
|    mean_reward          | 854      |
| time/                   |          |
|    total_timesteps      | 598500   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.2      |
|    clip_range_vf        | 0.2      |
|    entropy_loss         | -3.8e-18 |
|    explained_variance   | 0.0202   |
|    learning_rate        | 0.0001   |
|    loss                 | 2.28e+04 |
|    n_updates            | 2950     |
|    policy_gradient_loss | 8.15e-11 |
|    value_loss           | 3.39e+04 |
--------------------------------------
Eval num_timesteps=599000, episode_reward=898.04 +/- 690.18
Episode length: 35.80 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 599000   |
---------------------------------
Eval num_timesteps=599500, episode_reward=1005.13 +/- 720.78
Episode length: 36.98 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 599500   |
---------------------------------
Eval num_timesteps=600000, episode_reward=705.94 +/- 584.28
Episode length: 34.70 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 706      |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 824      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 293      |
|    time_elapsed    | 2169     |
|    total_timesteps | 600064   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-ppo-2-last-2/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
