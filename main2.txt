/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 432      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 4        |
|    fps              | 3716     |
|    time_elapsed     | 0        |
|    total_timesteps  | 434      |
----------------------------------
Eval num_timesteps=500, episode_reward=162.06 +/- 36.67
Episode length: 40.86 +/- 9.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.9     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 500      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 8        |
|    fps              | 271      |
|    time_elapsed     | 2        |
|    total_timesteps  | 813      |
----------------------------------
Eval num_timesteps=1000, episode_reward=159.60 +/- 39.26
Episode length: 40.20 +/- 9.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.2     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 382      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 12       |
|    fps              | 263      |
|    time_elapsed     | 4        |
|    total_timesteps  | 1150     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.6     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 16       |
|    fps              | 309      |
|    time_elapsed     | 4        |
|    total_timesteps  | 1369     |
----------------------------------
Eval num_timesteps=1500, episode_reward=168.66 +/- 41.87
Episode length: 42.52 +/- 10.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 1500     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.7     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 20       |
|    fps              | 279      |
|    time_elapsed     | 5        |
|    total_timesteps  | 1673     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 24       |
|    fps              | 318      |
|    time_elapsed     | 6        |
|    total_timesteps  | 1924     |
----------------------------------
Eval num_timesteps=2000, episode_reward=162.84 +/- 42.19
Episode length: 41.14 +/- 10.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.1     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 28       |
|    fps              | 296      |
|    time_elapsed     | 7        |
|    total_timesteps  | 2231     |
----------------------------------
Eval num_timesteps=2500, episode_reward=162.36 +/- 42.77
Episode length: 40.96 +/- 10.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41       |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 2500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.8     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 32       |
|    fps              | 295      |
|    time_elapsed     | 8        |
|    total_timesteps  | 2649     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.5     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 36       |
|    fps              | 316      |
|    time_elapsed     | 9        |
|    total_timesteps  | 2861     |
----------------------------------
Eval num_timesteps=3000, episode_reward=170.30 +/- 43.48
Episode length: 42.90 +/- 10.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.6     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 40       |
|    fps              | 289      |
|    time_elapsed     | 10       |
|    total_timesteps  | 3025     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 44       |
|    fps              | 324      |
|    time_elapsed     | 10       |
|    total_timesteps  | 3421     |
----------------------------------
Eval num_timesteps=3500, episode_reward=181.62 +/- 45.10
Episode length: 45.84 +/- 11.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 3500     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 48       |
|    fps              | 310      |
|    time_elapsed     | 12       |
|    total_timesteps  | 3759     |
----------------------------------
Eval num_timesteps=4000, episode_reward=180.20 +/- 38.51
Episode length: 45.46 +/- 9.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 52       |
|    fps              | 295      |
|    time_elapsed     | 13       |
|    total_timesteps  | 4048     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 56       |
|    fps              | 320      |
|    time_elapsed     | 13       |
|    total_timesteps  | 4420     |
----------------------------------
Eval num_timesteps=4500, episode_reward=181.62 +/- 55.29
Episode length: 45.74 +/- 13.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 4500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 60       |
|    fps              | 300      |
|    time_elapsed     | 15       |
|    total_timesteps  | 4605     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 64       |
|    fps              | 313      |
|    time_elapsed     | 15       |
|    total_timesteps  | 4812     |
----------------------------------
Eval num_timesteps=5000, episode_reward=171.20 +/- 47.40
Episode length: 43.20 +/- 11.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 68       |
|    fps              | 306      |
|    time_elapsed     | 16       |
|    total_timesteps  | 5172     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 72       |
|    fps              | 319      |
|    time_elapsed     | 16       |
|    total_timesteps  | 5417     |
----------------------------------
Eval num_timesteps=5500, episode_reward=176.48 +/- 49.34
Episode length: 44.56 +/- 12.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 5500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 298      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 76       |
|    fps              | 309      |
|    time_elapsed     | 18       |
|    total_timesteps  | 5699     |
----------------------------------
Eval num_timesteps=6000, episode_reward=170.24 +/- 47.38
Episode length: 42.92 +/- 11.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.4     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 80       |
|    fps              | 303      |
|    time_elapsed     | 19       |
|    total_timesteps  | 6034     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 84       |
|    fps              | 318      |
|    time_elapsed     | 19       |
|    total_timesteps  | 6359     |
----------------------------------
Eval num_timesteps=6500, episode_reward=164.04 +/- 42.14
Episode length: 41.36 +/- 10.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 6500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 88       |
|    fps              | 313      |
|    time_elapsed     | 21       |
|    total_timesteps  | 6722     |
----------------------------------
Eval num_timesteps=7000, episode_reward=180.66 +/- 50.93
Episode length: 45.52 +/- 12.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 92       |
|    fps              | 307      |
|    time_elapsed     | 23       |
|    total_timesteps  | 7064     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 96       |
|    fps              | 316      |
|    time_elapsed     | 23       |
|    total_timesteps  | 7301     |
----------------------------------
Eval num_timesteps=7500, episode_reward=181.38 +/- 60.62
Episode length: 45.64 +/- 15.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 7500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 100      |
|    fps              | 311      |
|    time_elapsed     | 24       |
|    total_timesteps  | 7684     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 104      |
|    fps              | 321      |
|    time_elapsed     | 24       |
|    total_timesteps  | 7965     |
----------------------------------
Eval num_timesteps=8000, episode_reward=170.20 +/- 44.15
Episode length: 42.94 +/- 11.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 108      |
|    fps              | 318      |
|    time_elapsed     | 26       |
|    total_timesteps  | 8348     |
----------------------------------
Eval num_timesteps=8500, episode_reward=171.06 +/- 43.37
Episode length: 43.16 +/- 10.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 8500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 299      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 112      |
|    fps              | 312      |
|    time_elapsed     | 27       |
|    total_timesteps  | 8654     |
----------------------------------
Eval num_timesteps=9000, episode_reward=183.88 +/- 51.27
Episode length: 46.40 +/- 12.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 116      |
|    fps              | 311      |
|    time_elapsed     | 29       |
|    total_timesteps  | 9135     |
----------------------------------
Eval num_timesteps=9500, episode_reward=172.18 +/- 50.02
Episode length: 43.46 +/- 12.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 9500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.4     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 120      |
|    fps              | 308      |
|    time_elapsed     | 30       |
|    total_timesteps  | 9513     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | 310      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 124      |
|    fps              | 314      |
|    time_elapsed     | 30       |
|    total_timesteps  | 9722     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.6     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 128      |
|    fps              | 322      |
|    time_elapsed     | 30       |
|    total_timesteps  | 9987     |
----------------------------------
Eval num_timesteps=10000, episode_reward=181.36 +/- 55.63
Episode length: 45.74 +/- 13.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 132      |
|    fps              | 315      |
|    time_elapsed     | 32       |
|    total_timesteps  | 10307    |
----------------------------------
Eval num_timesteps=10500, episode_reward=168.62 +/- 41.85
Episode length: 42.56 +/- 10.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 10500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 136      |
|    fps              | 311      |
|    time_elapsed     | 34       |
|    total_timesteps  | 10628    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.6     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 140      |
|    fps              | 317      |
|    time_elapsed     | 34       |
|    total_timesteps  | 10884    |
----------------------------------
Eval num_timesteps=11000, episode_reward=170.72 +/- 45.09
Episode length: 43.06 +/- 11.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 315      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 144      |
|    fps              | 316      |
|    time_elapsed     | 35       |
|    total_timesteps  | 11323    |
----------------------------------
Eval num_timesteps=11500, episode_reward=175.28 +/- 51.28
Episode length: 44.18 +/- 12.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 11500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 315      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 148      |
|    fps              | 313      |
|    time_elapsed     | 37       |
|    total_timesteps  | 11664    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 152      |
|    fps              | 320      |
|    time_elapsed     | 37       |
|    total_timesteps  | 11982    |
----------------------------------
Eval num_timesteps=12000, episode_reward=169.24 +/- 47.07
Episode length: 42.60 +/- 11.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 156      |
|    fps              | 316      |
|    time_elapsed     | 38       |
|    total_timesteps  | 12267    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.4     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 160      |
|    fps              | 320      |
|    time_elapsed     | 38       |
|    total_timesteps  | 12447    |
----------------------------------
Eval num_timesteps=12500, episode_reward=178.04 +/- 49.80
Episode length: 44.88 +/- 12.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 12500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.6     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 164      |
|    fps              | 313      |
|    time_elapsed     | 40       |
|    total_timesteps  | 12674    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.2     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 168      |
|    fps              | 318      |
|    time_elapsed     | 40       |
|    total_timesteps  | 12894    |
----------------------------------
Eval num_timesteps=13000, episode_reward=176.90 +/- 49.94
Episode length: 44.60 +/- 12.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 172      |
|    fps              | 313      |
|    time_elapsed     | 41       |
|    total_timesteps  | 13128    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 176      |
|    fps              | 317      |
|    time_elapsed     | 41       |
|    total_timesteps  | 13316    |
----------------------------------
Eval num_timesteps=13500, episode_reward=159.02 +/- 37.21
Episode length: 40.10 +/- 9.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.1     |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 13500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 180      |
|    fps              | 314      |
|    time_elapsed     | 43       |
|    total_timesteps  | 13611    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 184      |
|    fps              | 317      |
|    time_elapsed     | 43       |
|    total_timesteps  | 13786    |
----------------------------------
Eval num_timesteps=14000, episode_reward=173.46 +/- 46.69
Episode length: 43.74 +/- 11.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.4     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 188      |
|    fps              | 313      |
|    time_elapsed     | 44       |
|    total_timesteps  | 14064    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74       |
|    ep_rew_mean      | 295      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 192      |
|    fps              | 321      |
|    time_elapsed     | 45       |
|    total_timesteps  | 14462    |
----------------------------------
Eval num_timesteps=14500, episode_reward=178.76 +/- 54.29
Episode length: 45.06 +/- 13.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 14500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 196      |
|    fps              | 316      |
|    time_elapsed     | 46       |
|    total_timesteps  | 14746    |
----------------------------------
Eval num_timesteps=15000, episode_reward=174.80 +/- 42.99
Episode length: 44.02 +/- 10.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 200      |
|    fps              | 314      |
|    time_elapsed     | 48       |
|    total_timesteps  | 15113    |
----------------------------------
Eval num_timesteps=15500, episode_reward=163.60 +/- 43.76
Episode length: 41.26 +/- 10.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.3     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 15500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 204      |
|    fps              | 314      |
|    time_elapsed     | 49       |
|    total_timesteps  | 15579    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 208      |
|    fps              | 319      |
|    time_elapsed     | 49       |
|    total_timesteps  | 15858    |
----------------------------------
Eval num_timesteps=16000, episode_reward=169.60 +/- 40.74
Episode length: 42.80 +/- 10.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.6     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 212      |
|    fps              | 317      |
|    time_elapsed     | 51       |
|    total_timesteps  | 16210    |
----------------------------------
Eval num_timesteps=16500, episode_reward=167.74 +/- 43.99
Episode length: 42.30 +/- 10.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 16500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 216      |
|    fps              | 316      |
|    time_elapsed     | 52       |
|    total_timesteps  | 16645    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.7     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 220      |
|    fps              | 322      |
|    time_elapsed     | 52       |
|    total_timesteps  | 16985    |
----------------------------------
Eval num_timesteps=17000, episode_reward=167.34 +/- 43.20
Episode length: 42.22 +/- 10.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 224      |
|    fps              | 318      |
|    time_elapsed     | 54       |
|    total_timesteps  | 17250    |
----------------------------------
Eval num_timesteps=17500, episode_reward=173.70 +/- 45.72
Episode length: 43.82 +/- 11.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 17500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 302      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 228      |
|    fps              | 315      |
|    time_elapsed     | 55       |
|    total_timesteps  | 17582    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 232      |
|    fps              | 318      |
|    time_elapsed     | 55       |
|    total_timesteps  | 17761    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.2     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 236      |
|    fps              | 321      |
|    time_elapsed     | 55       |
|    total_timesteps  | 17949    |
----------------------------------
Eval num_timesteps=18000, episode_reward=179.62 +/- 53.45
Episode length: 45.24 +/- 13.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 291      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 240      |
|    fps              | 317      |
|    time_elapsed     | 57       |
|    total_timesteps  | 18188    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.8     |
|    ep_rew_mean      | 282      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 244      |
|    fps              | 320      |
|    time_elapsed     | 57       |
|    total_timesteps  | 18404    |
----------------------------------
Eval num_timesteps=18500, episode_reward=175.66 +/- 50.03
Episode length: 44.32 +/- 12.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 18500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.3     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 248      |
|    fps              | 315      |
|    time_elapsed     | 58       |
|    total_timesteps  | 18596    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.3     |
|    ep_rew_mean      | 272      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 252      |
|    fps              | 319      |
|    time_elapsed     | 58       |
|    total_timesteps  | 18814    |
----------------------------------
Eval num_timesteps=19000, episode_reward=168.70 +/- 35.47
Episode length: 42.56 +/- 8.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.4     |
|    ep_rew_mean      | 272      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 256      |
|    fps              | 316      |
|    time_elapsed     | 60       |
|    total_timesteps  | 19104    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70       |
|    ep_rew_mean      | 279      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 260      |
|    fps              | 321      |
|    time_elapsed     | 60       |
|    total_timesteps  | 19450    |
----------------------------------
Eval num_timesteps=19500, episode_reward=174.10 +/- 51.18
Episode length: 43.90 +/- 12.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 19500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.8     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 264      |
|    fps              | 320      |
|    time_elapsed     | 62       |
|    total_timesteps  | 19857    |
----------------------------------
Eval num_timesteps=20000, episode_reward=170.94 +/- 40.19
Episode length: 43.08 +/- 10.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 268      |
|    fps              | 317      |
|    time_elapsed     | 63       |
|    total_timesteps  | 20171    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.5     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 272      |
|    fps              | 320      |
|    time_elapsed     | 63       |
|    total_timesteps  | 20375    |
----------------------------------
Eval num_timesteps=20500, episode_reward=180.04 +/- 46.01
Episode length: 45.42 +/- 11.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 20500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.1     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 276      |
|    fps              | 316      |
|    time_elapsed     | 65       |
|    total_timesteps  | 20625    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.5     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 280      |
|    fps              | 320      |
|    time_elapsed     | 65       |
|    total_timesteps  | 20864    |
----------------------------------
Eval num_timesteps=21000, episode_reward=167.56 +/- 42.04
Episode length: 42.28 +/- 10.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.2     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 284      |
|    fps              | 316      |
|    time_elapsed     | 66       |
|    total_timesteps  | 21106    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 290      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 288      |
|    fps              | 320      |
|    time_elapsed     | 66       |
|    total_timesteps  | 21365    |
----------------------------------
Eval num_timesteps=21500, episode_reward=160.50 +/- 43.61
Episode length: 40.54 +/- 10.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.5     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 21500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.6     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 292      |
|    fps              | 319      |
|    time_elapsed     | 68       |
|    total_timesteps  | 21724    |
----------------------------------
Eval num_timesteps=22000, episode_reward=168.26 +/- 41.28
Episode length: 42.42 +/- 10.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.9     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 296      |
|    fps              | 316      |
|    time_elapsed     | 69       |
|    total_timesteps  | 22038    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.5     |
|    ep_rew_mean      | 284      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 300      |
|    fps              | 319      |
|    time_elapsed     | 69       |
|    total_timesteps  | 22265    |
----------------------------------
Eval num_timesteps=22500, episode_reward=170.18 +/- 42.76
Episode length: 42.86 +/- 10.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 22500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.7     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 304      |
|    fps              | 318      |
|    time_elapsed     | 71       |
|    total_timesteps  | 22652    |
----------------------------------
Eval num_timesteps=23000, episode_reward=167.10 +/- 35.22
Episode length: 42.16 +/- 8.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72       |
|    ep_rew_mean      | 286      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 308      |
|    fps              | 317      |
|    time_elapsed     | 72       |
|    total_timesteps  | 23060    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.8     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 312      |
|    fps              | 320      |
|    time_elapsed     | 72       |
|    total_timesteps  | 23286    |
----------------------------------
Eval num_timesteps=23500, episode_reward=178.78 +/- 52.01
Episode length: 44.98 +/- 13.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 23500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.9     |
|    ep_rew_mean      | 278      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 316      |
|    fps              | 318      |
|    time_elapsed     | 74       |
|    total_timesteps  | 23635    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.2     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 320      |
|    fps              | 321      |
|    time_elapsed     | 74       |
|    total_timesteps  | 23906    |
----------------------------------
Eval num_timesteps=24000, episode_reward=166.92 +/- 40.93
Episode length: 42.08 +/- 10.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.1     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.7     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 324      |
|    fps              | 318      |
|    time_elapsed     | 75       |
|    total_timesteps  | 24116    |
----------------------------------
Eval num_timesteps=24500, episode_reward=170.76 +/- 45.61
Episode length: 43.12 +/- 11.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 24500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.6     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 328      |
|    fps              | 319      |
|    time_elapsed     | 77       |
|    total_timesteps  | 24639    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 332      |
|    fps              | 323      |
|    time_elapsed     | 77       |
|    total_timesteps  | 24989    |
----------------------------------
Eval num_timesteps=25000, episode_reward=168.50 +/- 41.02
Episode length: 42.48 +/- 10.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 336      |
|    fps              | 319      |
|    time_elapsed     | 78       |
|    total_timesteps  | 25183    |
----------------------------------
Eval num_timesteps=25500, episode_reward=174.16 +/- 48.77
Episode length: 43.92 +/- 12.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 25500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.1     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 340      |
|    fps              | 317      |
|    time_elapsed     | 80       |
|    total_timesteps  | 25500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 344      |
|    fps              | 322      |
|    time_elapsed     | 80       |
|    total_timesteps  | 25913    |
----------------------------------
Eval num_timesteps=26000, episode_reward=170.30 +/- 46.85
Episode length: 42.90 +/- 11.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.6     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 348      |
|    fps              | 319      |
|    time_elapsed     | 81       |
|    total_timesteps  | 26156    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 352      |
|    fps              | 323      |
|    time_elapsed     | 81       |
|    total_timesteps  | 26473    |
----------------------------------
Eval num_timesteps=26500, episode_reward=165.78 +/- 45.43
Episode length: 41.76 +/- 11.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.8     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 26500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.9     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 356      |
|    fps              | 320      |
|    time_elapsed     | 83       |
|    total_timesteps  | 26695    |
----------------------------------
Eval num_timesteps=27000, episode_reward=168.00 +/- 43.50
Episode length: 42.38 +/- 10.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 360      |
|    fps              | 319      |
|    time_elapsed     | 84       |
|    total_timesteps  | 27082    |
----------------------------------
Eval num_timesteps=27500, episode_reward=172.96 +/- 49.26
Episode length: 43.58 +/- 12.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 27500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 364      |
|    fps              | 318      |
|    time_elapsed     | 86       |
|    total_timesteps  | 27525    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 368      |
|    fps              | 321      |
|    time_elapsed     | 86       |
|    total_timesteps  | 27756    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 372      |
|    fps              | 323      |
|    time_elapsed     | 86       |
|    total_timesteps  | 27988    |
----------------------------------
Eval num_timesteps=28000, episode_reward=168.72 +/- 50.67
Episode length: 42.54 +/- 12.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77       |
|    ep_rew_mean      | 307      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 376      |
|    fps              | 322      |
|    time_elapsed     | 87       |
|    total_timesteps  | 28330    |
----------------------------------
Eval num_timesteps=28500, episode_reward=178.00 +/- 57.45
Episode length: 44.82 +/- 14.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 28500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 380      |
|    fps              | 318      |
|    time_elapsed     | 89       |
|    total_timesteps  | 28518    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 384      |
|    fps              | 321      |
|    time_elapsed     | 89       |
|    total_timesteps  | 28770    |
----------------------------------
Eval num_timesteps=29000, episode_reward=173.78 +/- 43.79
Episode length: 43.80 +/- 10.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 388      |
|    fps              | 319      |
|    time_elapsed     | 91       |
|    total_timesteps  | 29055    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 392      |
|    fps              | 322      |
|    time_elapsed     | 91       |
|    total_timesteps  | 29370    |
----------------------------------
Eval num_timesteps=29500, episode_reward=159.14 +/- 35.24
Episode length: 40.14 +/- 8.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.1     |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 29500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 396      |
|    fps              | 320      |
|    time_elapsed     | 92       |
|    total_timesteps  | 29615    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 400      |
|    fps              | 323      |
|    time_elapsed     | 92       |
|    total_timesteps  | 29920    |
----------------------------------
Eval num_timesteps=30000, episode_reward=183.80 +/- 44.98
Episode length: 46.36 +/- 11.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 404      |
|    fps              | 321      |
|    time_elapsed     | 94       |
|    total_timesteps  | 30231    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.8     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 408      |
|    fps              | 323      |
|    time_elapsed     | 94       |
|    total_timesteps  | 30440    |
----------------------------------
Eval num_timesteps=30500, episode_reward=178.24 +/- 52.38
Episode length: 44.90 +/- 13.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 30500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 412      |
|    fps              | 321      |
|    time_elapsed     | 95       |
|    total_timesteps  | 30797    |
----------------------------------
Eval num_timesteps=31000, episode_reward=169.56 +/- 38.76
Episode length: 42.64 +/- 9.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 298      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 416      |
|    fps              | 320      |
|    time_elapsed     | 97       |
|    total_timesteps  | 31133    |
----------------------------------
Eval num_timesteps=31500, episode_reward=168.32 +/- 40.59
Episode length: 42.44 +/- 10.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 31500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.5     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 420      |
|    fps              | 319      |
|    time_elapsed     | 98       |
|    total_timesteps  | 31553    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 424      |
|    fps              | 322      |
|    time_elapsed     | 98       |
|    total_timesteps  | 31884    |
----------------------------------
Eval num_timesteps=32000, episode_reward=176.76 +/- 42.81
Episode length: 44.56 +/- 10.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 428      |
|    fps              | 320      |
|    time_elapsed     | 100      |
|    total_timesteps  | 32092    |
----------------------------------
Eval num_timesteps=32500, episode_reward=178.36 +/- 41.19
Episode length: 44.94 +/- 10.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 32500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 432      |
|    fps              | 319      |
|    time_elapsed     | 101      |
|    total_timesteps  | 32516    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 302      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 436      |
|    fps              | 321      |
|    time_elapsed     | 101      |
|    total_timesteps  | 32778    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 440      |
|    fps              | 323      |
|    time_elapsed     | 101      |
|    total_timesteps  | 32964    |
----------------------------------
Eval num_timesteps=33000, episode_reward=164.26 +/- 42.60
Episode length: 41.50 +/- 10.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 444      |
|    fps              | 320      |
|    time_elapsed     | 103      |
|    total_timesteps  | 33145    |
----------------------------------
Eval num_timesteps=33500, episode_reward=178.50 +/- 48.41
Episode length: 44.98 +/- 12.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 33500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 448      |
|    fps              | 319      |
|    time_elapsed     | 104      |
|    total_timesteps  | 33505    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.3     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 452      |
|    fps              | 322      |
|    time_elapsed     | 104      |
|    total_timesteps  | 33799    |
----------------------------------
Eval num_timesteps=34000, episode_reward=177.40 +/- 53.27
Episode length: 44.74 +/- 13.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 456      |
|    fps              | 320      |
|    time_elapsed     | 106      |
|    total_timesteps  | 34155    |
----------------------------------
Eval num_timesteps=34500, episode_reward=178.54 +/- 51.23
Episode length: 45.06 +/- 12.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 34500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 460      |
|    fps              | 321      |
|    time_elapsed     | 108      |
|    total_timesteps  | 34744    |
----------------------------------
Eval num_timesteps=35000, episode_reward=168.88 +/- 52.03
Episode length: 42.66 +/- 12.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 464      |
|    fps              | 321      |
|    time_elapsed     | 109      |
|    total_timesteps  | 35200    |
----------------------------------
Eval num_timesteps=35500, episode_reward=170.82 +/- 48.82
Episode length: 43.02 +/- 12.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 35500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | 311      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 468      |
|    fps              | 320      |
|    time_elapsed     | 111      |
|    total_timesteps  | 35556    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 472      |
|    fps              | 321      |
|    time_elapsed     | 111      |
|    total_timesteps  | 35762    |
----------------------------------
Eval num_timesteps=36000, episode_reward=169.78 +/- 45.03
Episode length: 42.80 +/- 11.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 476      |
|    fps              | 320      |
|    time_elapsed     | 112      |
|    total_timesteps  | 36108    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.8     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 480      |
|    fps              | 322      |
|    time_elapsed     | 112      |
|    total_timesteps  | 36393    |
----------------------------------
Eval num_timesteps=36500, episode_reward=177.52 +/- 59.89
Episode length: 44.72 +/- 14.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 36500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 484      |
|    fps              | 321      |
|    time_elapsed     | 114      |
|    total_timesteps  | 36753    |
----------------------------------
Eval num_timesteps=37000, episode_reward=162.98 +/- 41.27
Episode length: 41.20 +/- 10.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.2     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 488      |
|    fps              | 320      |
|    time_elapsed     | 115      |
|    total_timesteps  | 37012    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 492      |
|    fps              | 322      |
|    time_elapsed     | 115      |
|    total_timesteps  | 37344    |
----------------------------------
Eval num_timesteps=37500, episode_reward=170.24 +/- 39.80
Episode length: 42.92 +/- 9.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 37500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 496      |
|    fps              | 320      |
|    time_elapsed     | 117      |
|    total_timesteps  | 37556    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.7     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 500      |
|    fps              | 322      |
|    time_elapsed     | 117      |
|    total_timesteps  | 37787    |
----------------------------------
Eval num_timesteps=38000, episode_reward=164.26 +/- 36.71
Episode length: 41.44 +/- 9.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 504      |
|    fps              | 320      |
|    time_elapsed     | 118      |
|    total_timesteps  | 38066    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 508      |
|    fps              | 323      |
|    time_elapsed     | 118      |
|    total_timesteps  | 38459    |
----------------------------------
Eval num_timesteps=38500, episode_reward=165.86 +/- 39.15
Episode length: 41.84 +/- 9.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.8     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 38500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 315      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 512      |
|    fps              | 322      |
|    time_elapsed     | 120      |
|    total_timesteps  | 38703    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.1     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 516      |
|    fps              | 323      |
|    time_elapsed     | 120      |
|    total_timesteps  | 38944    |
----------------------------------
Eval num_timesteps=39000, episode_reward=171.82 +/- 38.61
Episode length: 43.34 +/- 9.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 520      |
|    fps              | 323      |
|    time_elapsed     | 121      |
|    total_timesteps  | 39342    |
----------------------------------
Eval num_timesteps=39500, episode_reward=174.70 +/- 44.61
Episode length: 43.96 +/- 11.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 39500    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77       |
|    ep_rew_mean      | 307      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 524      |
|    fps              | 321      |
|    time_elapsed     | 123      |
|    total_timesteps  | 39585    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 528      |
|    fps              | 322      |
|    time_elapsed     | 123      |
|    total_timesteps  | 39759    |
----------------------------------
Eval num_timesteps=40000, episode_reward=172.36 +/- 44.40
Episode length: 43.42 +/- 11.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 532      |
|    fps              | 312      |
|    time_elapsed     | 128      |
|    total_timesteps  | 40005    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.54     |
|    n_updates        | 1        |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.4     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 536      |
|    fps              | 313      |
|    time_elapsed     | 128      |
|    total_timesteps  | 40220    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.148    |
|    n_updates        | 54       |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 540      |
|    fps              | 314      |
|    time_elapsed     | 128      |
|    total_timesteps  | 40395    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.132    |
|    n_updates        | 98       |
----------------------------------
Eval num_timesteps=40500, episode_reward=173.30 +/- 42.75
Episode length: 43.76 +/- 10.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0331   |
|    n_updates        | 124      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.9     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 544      |
|    fps              | 313      |
|    time_elapsed     | 130      |
|    total_timesteps  | 40734    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.089    |
|    n_updates        | 183      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 548      |
|    fps              | 314      |
|    time_elapsed     | 130      |
|    total_timesteps  | 40967    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0542   |
|    n_updates        | 241      |
----------------------------------
Eval num_timesteps=41000, episode_reward=161.42 +/- 61.80
Episode length: 40.78 +/- 15.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.8     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 41000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0339   |
|    n_updates        | 249      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 552      |
|    fps              | 313      |
|    time_elapsed     | 131      |
|    total_timesteps  | 41292    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0147   |
|    n_updates        | 322      |
----------------------------------
Eval num_timesteps=41500, episode_reward=191.26 +/- 57.33
Episode length: 48.16 +/- 14.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.2     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0148   |
|    n_updates        | 374      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 556      |
|    fps              | 311      |
|    time_elapsed     | 133      |
|    total_timesteps  | 41582    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00398  |
|    n_updates        | 395      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.8     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 560      |
|    fps              | 313      |
|    time_elapsed     | 133      |
|    total_timesteps  | 41924    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0138   |
|    n_updates        | 480      |
----------------------------------
Eval num_timesteps=42000, episode_reward=175.08 +/- 39.59
Episode length: 44.14 +/- 9.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 42000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.057    |
|    n_updates        | 499      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.7     |
|    ep_rew_mean      | 277      |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 564      |
|    fps              | 311      |
|    time_elapsed     | 135      |
|    total_timesteps  | 42173    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0749   |
|    n_updates        | 543      |
----------------------------------
Eval num_timesteps=42500, episode_reward=175.68 +/- 49.58
Episode length: 44.32 +/- 12.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 42500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00118  |
|    n_updates        | 624      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.7     |
|    ep_rew_mean      | 277      |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 568      |
|    fps              | 310      |
|    time_elapsed     | 137      |
|    total_timesteps  | 42528    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000871 |
|    n_updates        | 631      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70       |
|    ep_rew_mean      | 278      |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 572      |
|    fps              | 311      |
|    time_elapsed     | 137      |
|    total_timesteps  | 42760    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0192   |
|    n_updates        | 689      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.5     |
|    ep_rew_mean      | 272      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 576      |
|    fps              | 312      |
|    time_elapsed     | 137      |
|    total_timesteps  | 42955    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0289   |
|    n_updates        | 738      |
----------------------------------
Eval num_timesteps=43000, episode_reward=193.76 +/- 50.77
Episode length: 48.86 +/- 12.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.9     |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 43000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000591 |
|    n_updates        | 749      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.5     |
|    ep_rew_mean      | 269      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 580      |
|    fps              | 310      |
|    time_elapsed     | 139      |
|    total_timesteps  | 43147    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.05     |
|    n_updates        | 786      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.6     |
|    ep_rew_mean      | 265      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 584      |
|    fps              | 311      |
|    time_elapsed     | 139      |
|    total_timesteps  | 43413    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0435   |
|    n_updates        | 853      |
----------------------------------
Eval num_timesteps=43500, episode_reward=179.52 +/- 60.01
Episode length: 45.30 +/- 15.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 43500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0037   |
|    n_updates        | 874      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.3     |
|    ep_rew_mean      | 268      |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 588      |
|    fps              | 310      |
|    time_elapsed     | 141      |
|    total_timesteps  | 43747    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0263   |
|    n_updates        | 936      |
----------------------------------
Eval num_timesteps=44000, episode_reward=182.90 +/- 61.82
Episode length: 46.10 +/- 15.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 44000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0404   |
|    n_updates        | 999      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.7     |
|    ep_rew_mean      | 265      |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 592      |
|    fps              | 308      |
|    time_elapsed     | 142      |
|    total_timesteps  | 44013    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0116   |
|    n_updates        | 1003     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.4     |
|    ep_rew_mean      | 268      |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 596      |
|    fps              | 309      |
|    time_elapsed     | 142      |
|    total_timesteps  | 44296    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.043    |
|    n_updates        | 1073     |
----------------------------------
Eval num_timesteps=44500, episode_reward=178.30 +/- 42.72
Episode length: 44.94 +/- 10.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 44500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0485   |
|    n_updates        | 1124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.4     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 600      |
|    fps              | 309      |
|    time_elapsed     | 144      |
|    total_timesteps  | 44730    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000554 |
|    n_updates        | 1182     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.4     |
|    ep_rew_mean      | 272      |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 604      |
|    fps              | 310      |
|    time_elapsed     | 144      |
|    total_timesteps  | 44908    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00137  |
|    n_updates        | 1226     |
----------------------------------
Eval num_timesteps=45000, episode_reward=171.76 +/- 45.72
Episode length: 43.22 +/- 11.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0362   |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.4     |
|    ep_rew_mean      | 264      |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 608      |
|    fps              | 308      |
|    time_elapsed     | 146      |
|    total_timesteps  | 45096    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0304   |
|    n_updates        | 1273     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 65.8     |
|    ep_rew_mean      | 262      |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 612      |
|    fps              | 309      |
|    time_elapsed     | 146      |
|    total_timesteps  | 45287    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000746 |
|    n_updates        | 1321     |
----------------------------------
Eval num_timesteps=45500, episode_reward=170.64 +/- 34.06
Episode length: 42.98 +/- 8.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 45500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000474 |
|    n_updates        | 1374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.3     |
|    ep_rew_mean      | 268      |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 616      |
|    fps              | 308      |
|    time_elapsed     | 148      |
|    total_timesteps  | 45678    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0458   |
|    n_updates        | 1419     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.4     |
|    ep_rew_mean      | 264      |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 620      |
|    fps              | 310      |
|    time_elapsed     | 148      |
|    total_timesteps  | 45981    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0498   |
|    n_updates        | 1495     |
----------------------------------
Eval num_timesteps=46000, episode_reward=172.60 +/- 66.89
Episode length: 43.54 +/- 16.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 46000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0571   |
|    n_updates        | 1499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 66.2     |
|    ep_rew_mean      | 263      |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 624      |
|    fps              | 308      |
|    time_elapsed     | 149      |
|    total_timesteps  | 46200    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00075  |
|    n_updates        | 1549     |
----------------------------------
Eval num_timesteps=46500, episode_reward=157.80 +/- 55.48
Episode length: 39.76 +/- 13.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.8     |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 46500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0559   |
|    n_updates        | 1624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.7     |
|    ep_rew_mean      | 274      |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 628      |
|    fps              | 307      |
|    time_elapsed     | 151      |
|    total_timesteps  | 46632    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0273   |
|    n_updates        | 1657     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.1     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 632      |
|    fps              | 309      |
|    time_elapsed     | 151      |
|    total_timesteps  | 46917    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00677  |
|    n_updates        | 1729     |
----------------------------------
Eval num_timesteps=47000, episode_reward=174.76 +/- 42.64
Episode length: 44.00 +/- 10.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.991    |
| time/               |          |
|    total_timesteps  | 47000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0304   |
|    n_updates        | 1749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69       |
|    ep_rew_mean      | 275      |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 636      |
|    fps              | 307      |
|    time_elapsed     | 153      |
|    total_timesteps  | 47119    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00227  |
|    n_updates        | 1779     |
----------------------------------
Eval num_timesteps=47500, episode_reward=198.32 +/- 50.58
Episode length: 49.96 +/- 12.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50       |
|    mean_reward      | 198      |
| rollout/            |          |
|    exploration_rate | 0.99     |
| time/               |          |
|    total_timesteps  | 47500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00988  |
|    n_updates        | 1874     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.1     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 640      |
|    fps              | 306      |
|    time_elapsed     | 155      |
|    total_timesteps  | 47508    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00764  |
|    n_updates        | 1876     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.1     |
|    ep_rew_mean      | 279      |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 644      |
|    fps              | 307      |
|    time_elapsed     | 155      |
|    total_timesteps  | 47741    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0427   |
|    n_updates        | 1935     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.9     |
|    ep_rew_mean      | 278      |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 648      |
|    fps              | 308      |
|    time_elapsed     | 155      |
|    total_timesteps  | 47958    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00505  |
|    n_updates        | 1989     |
----------------------------------
Eval num_timesteps=48000, episode_reward=178.18 +/- 46.77
Episode length: 44.88 +/- 11.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.989    |
| time/               |          |
|    total_timesteps  | 48000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0524   |
|    n_updates        | 1999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.2     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 652      |
|    fps              | 306      |
|    time_elapsed     | 157      |
|    total_timesteps  | 48207    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0433   |
|    n_updates        | 2051     |
----------------------------------
Eval num_timesteps=48500, episode_reward=185.80 +/- 53.70
Episode length: 46.76 +/- 13.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 48500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.006    |
|    n_updates        | 2124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.2     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 656      |
|    fps              | 305      |
|    time_elapsed     | 158      |
|    total_timesteps  | 48502    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00628  |
|    n_updates        | 2125     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.4     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 660      |
|    fps              | 307      |
|    time_elapsed     | 159      |
|    total_timesteps  | 48868    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000636 |
|    n_updates        | 2216     |
----------------------------------
Eval num_timesteps=49000, episode_reward=173.00 +/- 52.23
Episode length: 43.60 +/- 13.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.987    |
| time/               |          |
|    total_timesteps  | 49000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00984  |
|    n_updates        | 2249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.7     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 664      |
|    fps              | 305      |
|    time_elapsed     | 160      |
|    total_timesteps  | 49042    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00981  |
|    n_updates        | 2260     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.2     |
|    ep_rew_mean      | 272      |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 668      |
|    fps              | 306      |
|    time_elapsed     | 160      |
|    total_timesteps  | 49353    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0164   |
|    n_updates        | 2338     |
----------------------------------
Eval num_timesteps=49500, episode_reward=173.04 +/- 45.79
Episode length: 43.56 +/- 11.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.986    |
| time/               |          |
|    total_timesteps  | 49500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00158  |
|    n_updates        | 2374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.4     |
|    ep_rew_mean      | 276      |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 672      |
|    fps              | 305      |
|    time_elapsed     | 162      |
|    total_timesteps  | 49702    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00558  |
|    n_updates        | 2425     |
----------------------------------
Eval num_timesteps=50000, episode_reward=178.80 +/- 53.66
Episode length: 45.08 +/- 13.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.984    |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000791 |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.8     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 676      |
|    fps              | 305      |
|    time_elapsed     | 164      |
|    total_timesteps  | 50140    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0677   |
|    n_updates        | 2534     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 680      |
|    fps              | 306      |
|    time_elapsed     | 164      |
|    total_timesteps  | 50373    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.097    |
|    n_updates        | 2593     |
----------------------------------
Eval num_timesteps=50500, episode_reward=147.62 +/- 44.39
Episode length: 37.36 +/- 11.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.4     |
|    mean_reward      | 148      |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 50500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00785  |
|    n_updates        | 2624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.5     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 684      |
|    fps              | 305      |
|    time_elapsed     | 165      |
|    total_timesteps  | 50663    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0138   |
|    n_updates        | 2665     |
----------------------------------
Eval num_timesteps=51000, episode_reward=153.94 +/- 45.47
Episode length: 38.80 +/- 11.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.8     |
|    mean_reward      | 154      |
| rollout/            |          |
|    exploration_rate | 0.982    |
| time/               |          |
|    total_timesteps  | 51000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0685   |
|    n_updates        | 2749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 688      |
|    fps              | 305      |
|    time_elapsed     | 167      |
|    total_timesteps  | 51100    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.253    |
|    n_updates        | 2774     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 692      |
|    fps              | 306      |
|    time_elapsed     | 167      |
|    total_timesteps  | 51366    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.167    |
|    n_updates        | 2841     |
----------------------------------
Eval num_timesteps=51500, episode_reward=164.44 +/- 44.65
Episode length: 41.50 +/- 11.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.981    |
| time/               |          |
|    total_timesteps  | 51500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00174  |
|    n_updates        | 2874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 696      |
|    fps              | 306      |
|    time_elapsed     | 169      |
|    total_timesteps  | 51774    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00942  |
|    n_updates        | 2943     |
----------------------------------
Eval num_timesteps=52000, episode_reward=194.68 +/- 49.48
Episode length: 49.08 +/- 12.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.1     |
|    mean_reward      | 195      |
| rollout/            |          |
|    exploration_rate | 0.979    |
| time/               |          |
|    total_timesteps  | 52000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.164    |
|    n_updates        | 2999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.1     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 700      |
|    fps              | 304      |
|    time_elapsed     | 171      |
|    total_timesteps  | 52137    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0824   |
|    n_updates        | 3034     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 704      |
|    fps              | 305      |
|    time_elapsed     | 171      |
|    total_timesteps  | 52360    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00637  |
|    n_updates        | 3089     |
----------------------------------
Eval num_timesteps=52500, episode_reward=179.84 +/- 43.08
Episode length: 45.34 +/- 10.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.978    |
| time/               |          |
|    total_timesteps  | 52500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0103   |
|    n_updates        | 3124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 708      |
|    fps              | 305      |
|    time_elapsed     | 172      |
|    total_timesteps  | 52788    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00332  |
|    n_updates        | 3196     |
----------------------------------
Eval num_timesteps=53000, episode_reward=184.36 +/- 51.58
Episode length: 46.42 +/- 13.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.977    |
| time/               |          |
|    total_timesteps  | 53000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.132    |
|    n_updates        | 3249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 712      |
|    fps              | 303      |
|    time_elapsed     | 174      |
|    total_timesteps  | 53053    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00676  |
|    n_updates        | 3263     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 716      |
|    fps              | 304      |
|    time_elapsed     | 174      |
|    total_timesteps  | 53207    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00077  |
|    n_updates        | 3301     |
----------------------------------
Eval num_timesteps=53500, episode_reward=169.94 +/- 45.71
Episode length: 42.78 +/- 11.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.976    |
| time/               |          |
|    total_timesteps  | 53500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.267    |
|    n_updates        | 3374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.9     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 720      |
|    fps              | 303      |
|    time_elapsed     | 176      |
|    total_timesteps  | 53573    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0911   |
|    n_updates        | 3393     |
----------------------------------
Eval num_timesteps=54000, episode_reward=167.96 +/- 45.09
Episode length: 42.38 +/- 11.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.974    |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0394   |
|    n_updates        | 3499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.4     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 724      |
|    fps              | 303      |
|    time_elapsed     | 178      |
|    total_timesteps  | 54041    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00268  |
|    n_updates        | 3510     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 728      |
|    fps              | 304      |
|    time_elapsed     | 178      |
|    total_timesteps  | 54323    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.149    |
|    n_updates        | 3580     |
----------------------------------
Eval num_timesteps=54500, episode_reward=178.08 +/- 43.30
Episode length: 44.84 +/- 10.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.973    |
| time/               |          |
|    total_timesteps  | 54500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.19     |
|    n_updates        | 3624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 732      |
|    fps              | 303      |
|    time_elapsed     | 179      |
|    total_timesteps  | 54556    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0132   |
|    n_updates        | 3638     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.4     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 736      |
|    fps              | 304      |
|    time_elapsed     | 180      |
|    total_timesteps  | 54857    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0216   |
|    n_updates        | 3714     |
----------------------------------
Eval num_timesteps=55000, episode_reward=189.38 +/- 60.40
Episode length: 47.68 +/- 15.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.7     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.971    |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0096   |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 740      |
|    fps              | 303      |
|    time_elapsed     | 181      |
|    total_timesteps  | 55150    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00618  |
|    n_updates        | 3787     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.6     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 744      |
|    fps              | 304      |
|    time_elapsed     | 182      |
|    total_timesteps  | 55400    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.102    |
|    n_updates        | 3849     |
----------------------------------
Eval num_timesteps=55500, episode_reward=172.54 +/- 37.84
Episode length: 43.52 +/- 9.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.97     |
| time/               |          |
|    total_timesteps  | 55500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00349  |
|    n_updates        | 3874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 748      |
|    fps              | 302      |
|    time_elapsed     | 183      |
|    total_timesteps  | 55581    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.202    |
|    n_updates        | 3895     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 752      |
|    fps              | 304      |
|    time_elapsed     | 183      |
|    total_timesteps  | 55913    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.119    |
|    n_updates        | 3978     |
----------------------------------
Eval num_timesteps=56000, episode_reward=181.36 +/- 44.52
Episode length: 45.68 +/- 11.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.968    |
| time/               |          |
|    total_timesteps  | 56000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0219   |
|    n_updates        | 3999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.3     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 756      |
|    fps              | 302      |
|    time_elapsed     | 185      |
|    total_timesteps  | 56229    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.057    |
|    n_updates        | 4057     |
----------------------------------
Eval num_timesteps=56500, episode_reward=171.86 +/- 46.03
Episode length: 43.36 +/- 11.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.967    |
| time/               |          |
|    total_timesteps  | 56500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00515  |
|    n_updates        | 4124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 760      |
|    fps              | 301      |
|    time_elapsed     | 187      |
|    total_timesteps  | 56507    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.12     |
|    n_updates        | 4126     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.7     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 764      |
|    fps              | 303      |
|    time_elapsed     | 187      |
|    total_timesteps  | 56814    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.16     |
|    n_updates        | 4203     |
----------------------------------
Eval num_timesteps=57000, episode_reward=169.38 +/- 41.85
Episode length: 42.68 +/- 10.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.965    |
| time/               |          |
|    total_timesteps  | 57000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0235   |
|    n_updates        | 4249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 768      |
|    fps              | 302      |
|    time_elapsed     | 189      |
|    total_timesteps  | 57182    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00202  |
|    n_updates        | 4295     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 772      |
|    fps              | 303      |
|    time_elapsed     | 189      |
|    total_timesteps  | 57386    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.22     |
|    n_updates        | 4346     |
----------------------------------
Eval num_timesteps=57500, episode_reward=165.20 +/- 48.08
Episode length: 41.66 +/- 12.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.7     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.964    |
| time/               |          |
|    total_timesteps  | 57500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00418  |
|    n_updates        | 4374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.963    |
| time/               |          |
|    episodes         | 776      |
|    fps              | 302      |
|    time_elapsed     | 190      |
|    total_timesteps  | 57707    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.015    |
|    n_updates        | 4426     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 780      |
|    fps              | 303      |
|    time_elapsed     | 190      |
|    total_timesteps  | 57945    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0155   |
|    n_updates        | 4486     |
----------------------------------
Eval num_timesteps=58000, episode_reward=184.86 +/- 59.24
Episode length: 46.58 +/- 14.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.6     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.962    |
| time/               |          |
|    total_timesteps  | 58000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00259  |
|    n_updates        | 4499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.961    |
| time/               |          |
|    episodes         | 784      |
|    fps              | 302      |
|    time_elapsed     | 192      |
|    total_timesteps  | 58267    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0878   |
|    n_updates        | 4566     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.6     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.961    |
| time/               |          |
|    episodes         | 788      |
|    fps              | 303      |
|    time_elapsed     | 192      |
|    total_timesteps  | 58461    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0742   |
|    n_updates        | 4615     |
----------------------------------
Eval num_timesteps=58500, episode_reward=172.88 +/- 60.91
Episode length: 43.56 +/- 15.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.961    |
| time/               |          |
|    total_timesteps  | 58500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0105   |
|    n_updates        | 4624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 792      |
|    fps              | 302      |
|    time_elapsed     | 194      |
|    total_timesteps  | 58721    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00338  |
|    n_updates        | 4680     |
----------------------------------
Eval num_timesteps=59000, episode_reward=170.32 +/- 39.58
Episode length: 43.02 +/- 9.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.959    |
| time/               |          |
|    total_timesteps  | 59000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0357   |
|    n_updates        | 4749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.5     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 796      |
|    fps              | 301      |
|    time_elapsed     | 196      |
|    total_timesteps  | 59019    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.228    |
|    n_updates        | 4754     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.1     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.958    |
| time/               |          |
|    episodes         | 800      |
|    fps              | 302      |
|    time_elapsed     | 196      |
|    total_timesteps  | 59350    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0369   |
|    n_updates        | 4837     |
----------------------------------
Eval num_timesteps=59500, episode_reward=181.92 +/- 45.67
Episode length: 45.86 +/- 11.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.957    |
| time/               |          |
|    total_timesteps  | 59500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0676   |
|    n_updates        | 4874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.4     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 804      |
|    fps              | 301      |
|    time_elapsed     | 197      |
|    total_timesteps  | 59604    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0707   |
|    n_updates        | 4900     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72       |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.956    |
| time/               |          |
|    episodes         | 808      |
|    fps              | 302      |
|    time_elapsed     | 198      |
|    total_timesteps  | 59992    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.135    |
|    n_updates        | 4997     |
----------------------------------
Eval num_timesteps=60000, episode_reward=190.30 +/- 56.97
Episode length: 47.96 +/- 14.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48       |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.956    |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.087    |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.955    |
| time/               |          |
|    episodes         | 812      |
|    fps              | 301      |
|    time_elapsed     | 200      |
|    total_timesteps  | 60351    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.12     |
|    n_updates        | 5087     |
----------------------------------
Eval num_timesteps=60500, episode_reward=168.32 +/- 45.61
Episode length: 42.46 +/- 11.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.954    |
| time/               |          |
|    total_timesteps  | 60500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.129    |
|    n_updates        | 5124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 816      |
|    fps              | 301      |
|    time_elapsed     | 201      |
|    total_timesteps  | 60733    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.13     |
|    n_updates        | 5183     |
----------------------------------
Eval num_timesteps=61000, episode_reward=172.02 +/- 78.06
Episode length: 43.38 +/- 19.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.953    |
| time/               |          |
|    total_timesteps  | 61000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0691   |
|    n_updates        | 5249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.952    |
| time/               |          |
|    episodes         | 820      |
|    fps              | 300      |
|    time_elapsed     | 203      |
|    total_timesteps  | 61199    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.155    |
|    n_updates        | 5299     |
----------------------------------
Eval num_timesteps=61500, episode_reward=167.14 +/- 54.47
Episode length: 42.12 +/- 13.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.1     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.951    |
| time/               |          |
|    total_timesteps  | 61500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.112    |
|    n_updates        | 5374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.951    |
| time/               |          |
|    episodes         | 824      |
|    fps              | 300      |
|    time_elapsed     | 205      |
|    total_timesteps  | 61553    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0297   |
|    n_updates        | 5388     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.95     |
| time/               |          |
|    episodes         | 828      |
|    fps              | 301      |
|    time_elapsed     | 205      |
|    total_timesteps  | 61803    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0149   |
|    n_updates        | 5450     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.9     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 832      |
|    fps              | 301      |
|    time_elapsed     | 205      |
|    total_timesteps  | 61948    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0515   |
|    n_updates        | 5486     |
----------------------------------
Eval num_timesteps=62000, episode_reward=182.20 +/- 50.74
Episode length: 45.98 +/- 12.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.949    |
| time/               |          |
|    total_timesteps  | 62000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0801   |
|    n_updates        | 5499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.7     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.948    |
| time/               |          |
|    episodes         | 836      |
|    fps              | 300      |
|    time_elapsed     | 207      |
|    total_timesteps  | 62322    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0197   |
|    n_updates        | 5580     |
----------------------------------
Eval num_timesteps=62500, episode_reward=176.24 +/- 79.87
Episode length: 44.50 +/- 20.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.947    |
| time/               |          |
|    total_timesteps  | 62500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0354   |
|    n_updates        | 5624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.947    |
| time/               |          |
|    episodes         | 840      |
|    fps              | 299      |
|    time_elapsed     | 208      |
|    total_timesteps  | 62584    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0343   |
|    n_updates        | 5645     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.6     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 844      |
|    fps              | 300      |
|    time_elapsed     | 208      |
|    total_timesteps  | 62763    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.13     |
|    n_updates        | 5690     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.2     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 848      |
|    fps              | 301      |
|    time_elapsed     | 209      |
|    total_timesteps  | 62996    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0203   |
|    n_updates        | 5748     |
----------------------------------
Eval num_timesteps=63000, episode_reward=188.52 +/- 40.50
Episode length: 47.48 +/- 10.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.946    |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0443   |
|    n_updates        | 5749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.944    |
| time/               |          |
|    episodes         | 852      |
|    fps              | 300      |
|    time_elapsed     | 210      |
|    total_timesteps  | 63396    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.296    |
|    n_updates        | 5848     |
----------------------------------
Eval num_timesteps=63500, episode_reward=177.22 +/- 64.58
Episode length: 44.70 +/- 16.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.944    |
| time/               |          |
|    total_timesteps  | 63500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0144   |
|    n_updates        | 5874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.943    |
| time/               |          |
|    episodes         | 856      |
|    fps              | 299      |
|    time_elapsed     | 212      |
|    total_timesteps  | 63713    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.174    |
|    n_updates        | 5928     |
----------------------------------
Eval num_timesteps=64000, episode_reward=166.92 +/- 37.13
Episode length: 42.14 +/- 9.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.1     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.942    |
| time/               |          |
|    total_timesteps  | 64000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00958  |
|    n_updates        | 5999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.942    |
| time/               |          |
|    episodes         | 860      |
|    fps              | 298      |
|    time_elapsed     | 214      |
|    total_timesteps  | 64027    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.155    |
|    n_updates        | 6006     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.941    |
| time/               |          |
|    episodes         | 864      |
|    fps              | 300      |
|    time_elapsed     | 214      |
|    total_timesteps  | 64399    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.195    |
|    n_updates        | 6099     |
----------------------------------
Eval num_timesteps=64500, episode_reward=170.22 +/- 41.22
Episode length: 43.02 +/- 10.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.94     |
| time/               |          |
|    total_timesteps  | 64500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00249  |
|    n_updates        | 6124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.2     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 868      |
|    fps              | 298      |
|    time_elapsed     | 216      |
|    total_timesteps  | 64602    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0557   |
|    n_updates        | 6150     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.9     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 872      |
|    fps              | 299      |
|    time_elapsed     | 216      |
|    total_timesteps  | 64776    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0191   |
|    n_updates        | 6193     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 876      |
|    fps              | 300      |
|    time_elapsed     | 216      |
|    total_timesteps  | 64986    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.29     |
|    n_updates        | 6246     |
----------------------------------
Eval num_timesteps=65000, episode_reward=350.04 +/- 195.58
Episode length: 87.92 +/- 48.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.9     |
|    mean_reward      | 350      |
| rollout/            |          |
|    exploration_rate | 0.938    |
| time/               |          |
|    total_timesteps  | 65000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.059    |
|    n_updates        | 6249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.937    |
| time/               |          |
|    episodes         | 880      |
|    fps              | 297      |
|    time_elapsed     | 219      |
|    total_timesteps  | 65220    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.109    |
|    n_updates        | 6304     |
----------------------------------
Eval num_timesteps=65500, episode_reward=180.42 +/- 40.19
Episode length: 45.44 +/- 10.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.936    |
| time/               |          |
|    total_timesteps  | 65500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.156    |
|    n_updates        | 6374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.5     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 884      |
|    fps              | 296      |
|    time_elapsed     | 221      |
|    total_timesteps  | 65517    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.136    |
|    n_updates        | 6379     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 888      |
|    fps              | 297      |
|    time_elapsed     | 221      |
|    total_timesteps  | 65764    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0819   |
|    n_updates        | 6440     |
----------------------------------
Eval num_timesteps=66000, episode_reward=174.50 +/- 68.41
Episode length: 44.00 +/- 17.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.935    |
| time/               |          |
|    total_timesteps  | 66000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.127    |
|    n_updates        | 6499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74       |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 892      |
|    fps              | 296      |
|    time_elapsed     | 223      |
|    total_timesteps  | 66125    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.196    |
|    n_updates        | 6531     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74       |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 896      |
|    fps              | 297      |
|    time_elapsed     | 223      |
|    total_timesteps  | 66424    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.397    |
|    n_updates        | 6605     |
----------------------------------
Eval num_timesteps=66500, episode_reward=191.12 +/- 54.82
Episode length: 48.12 +/- 13.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.933    |
| time/               |          |
|    total_timesteps  | 66500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.129    |
|    n_updates        | 6624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.932    |
| time/               |          |
|    episodes         | 900      |
|    fps              | 296      |
|    time_elapsed     | 225      |
|    total_timesteps  | 66785    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.128    |
|    n_updates        | 6696     |
----------------------------------
Eval num_timesteps=67000, episode_reward=177.50 +/- 45.57
Episode length: 44.72 +/- 11.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.931    |
| time/               |          |
|    total_timesteps  | 67000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0263   |
|    n_updates        | 6749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 904      |
|    fps              | 295      |
|    time_elapsed     | 226      |
|    total_timesteps  | 67065    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0043   |
|    n_updates        | 6766     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.929    |
| time/               |          |
|    episodes         | 908      |
|    fps              | 296      |
|    time_elapsed     | 227      |
|    total_timesteps  | 67419    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.134    |
|    n_updates        | 6854     |
----------------------------------
Eval num_timesteps=67500, episode_reward=179.38 +/- 62.96
Episode length: 45.20 +/- 15.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.929    |
| time/               |          |
|    total_timesteps  | 67500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.145    |
|    n_updates        | 6874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.4     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 912      |
|    fps              | 296      |
|    time_elapsed     | 228      |
|    total_timesteps  | 67793    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.214    |
|    n_updates        | 6948     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.927    |
| time/               |          |
|    episodes         | 916      |
|    fps              | 296      |
|    time_elapsed     | 229      |
|    total_timesteps  | 67966    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0164   |
|    n_updates        | 6991     |
----------------------------------
Eval num_timesteps=68000, episode_reward=178.36 +/- 53.42
Episode length: 45.02 +/- 13.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.927    |
| time/               |          |
|    total_timesteps  | 68000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.204    |
|    n_updates        | 6999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.9     |
|    ep_rew_mean      | 282      |
|    exploration_rate | 0.926    |
| time/               |          |
|    episodes         | 920      |
|    fps              | 295      |
|    time_elapsed     | 230      |
|    total_timesteps  | 68289    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0318   |
|    n_updates        | 7072     |
----------------------------------
Eval num_timesteps=68500, episode_reward=176.58 +/- 52.51
Episode length: 44.52 +/- 13.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.925    |
| time/               |          |
|    total_timesteps  | 68500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.263    |
|    n_updates        | 7124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.8     |
|    ep_rew_mean      | 278      |
|    exploration_rate | 0.925    |
| time/               |          |
|    episodes         | 924      |
|    fps              | 294      |
|    time_elapsed     | 232      |
|    total_timesteps  | 68534    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00296  |
|    n_updates        | 7133     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71       |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 928      |
|    fps              | 296      |
|    time_elapsed     | 232      |
|    total_timesteps  | 68902    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.407    |
|    n_updates        | 7225     |
----------------------------------
Eval num_timesteps=69000, episode_reward=186.08 +/- 55.66
Episode length: 46.90 +/- 13.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.923    |
| time/               |          |
|    total_timesteps  | 69000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0278   |
|    n_updates        | 7249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.6     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.922    |
| time/               |          |
|    episodes         | 932      |
|    fps              | 294      |
|    time_elapsed     | 234      |
|    total_timesteps  | 69104    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00409  |
|    n_updates        | 7275     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.1     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.921    |
| time/               |          |
|    episodes         | 936      |
|    fps              | 296      |
|    time_elapsed     | 234      |
|    total_timesteps  | 69432    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0162   |
|    n_updates        | 7357     |
----------------------------------
Eval num_timesteps=69500, episode_reward=179.24 +/- 44.46
Episode length: 45.22 +/- 11.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.921    |
| time/               |          |
|    total_timesteps  | 69500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.209    |
|    n_updates        | 7374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.6     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.92     |
| time/               |          |
|    episodes         | 940      |
|    fps              | 295      |
|    time_elapsed     | 236      |
|    total_timesteps  | 69748    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00514  |
|    n_updates        | 7436     |
----------------------------------
Eval num_timesteps=70000, episode_reward=169.88 +/- 78.39
Episode length: 42.86 +/- 19.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.919    |
| time/               |          |
|    total_timesteps  | 70000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.126    |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.919    |
| time/               |          |
|    episodes         | 944      |
|    fps              | 294      |
|    time_elapsed     | 237      |
|    total_timesteps  | 70065    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.738    |
|    n_updates        | 7516     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.2     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 948      |
|    fps              | 295      |
|    time_elapsed     | 238      |
|    total_timesteps  | 70321    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.124    |
|    n_updates        | 7580     |
----------------------------------
Eval num_timesteps=70500, episode_reward=189.70 +/- 56.20
Episode length: 47.76 +/- 14.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.8     |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.917    |
| time/               |          |
|    total_timesteps  | 70500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.255    |
|    n_updates        | 7624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.917    |
| time/               |          |
|    episodes         | 952      |
|    fps              | 294      |
|    time_elapsed     | 239      |
|    total_timesteps  | 70582    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0669   |
|    n_updates        | 7645     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.8     |
|    ep_rew_mean      | 282      |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 956      |
|    fps              | 295      |
|    time_elapsed     | 239      |
|    total_timesteps  | 70794    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.196    |
|    n_updates        | 7698     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.1     |
|    ep_rew_mean      | 275      |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 960      |
|    fps              | 295      |
|    time_elapsed     | 240      |
|    total_timesteps  | 70936    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.171    |
|    n_updates        | 7733     |
----------------------------------
Eval num_timesteps=71000, episode_reward=189.74 +/- 53.41
Episode length: 47.86 +/- 13.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.9     |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.915    |
| time/               |          |
|    total_timesteps  | 71000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0177   |
|    n_updates        | 7749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.6     |
|    ep_rew_mean      | 269      |
|    exploration_rate | 0.914    |
| time/               |          |
|    episodes         | 964      |
|    fps              | 294      |
|    time_elapsed     | 241      |
|    total_timesteps  | 71162    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0264   |
|    n_updates        | 7790     |
----------------------------------
Eval num_timesteps=71500, episode_reward=178.36 +/- 47.09
Episode length: 44.98 +/- 11.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.913    |
| time/               |          |
|    total_timesteps  | 71500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.194    |
|    n_updates        | 7874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.6     |
|    ep_rew_mean      | 277      |
|    exploration_rate | 0.912    |
| time/               |          |
|    episodes         | 968      |
|    fps              | 293      |
|    time_elapsed     | 243      |
|    total_timesteps  | 71563    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0135   |
|    n_updates        | 7890     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71       |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 972      |
|    fps              | 294      |
|    time_elapsed     | 243      |
|    total_timesteps  | 71879    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.196    |
|    n_updates        | 7969     |
----------------------------------
Eval num_timesteps=72000, episode_reward=181.62 +/- 47.65
Episode length: 45.80 +/- 11.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.911    |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.275    |
|    n_updates        | 7999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.91     |
| time/               |          |
|    episodes         | 976      |
|    fps              | 293      |
|    time_elapsed     | 245      |
|    total_timesteps  | 72177    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.184    |
|    n_updates        | 8044     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72       |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 980      |
|    fps              | 294      |
|    time_elapsed     | 245      |
|    total_timesteps  | 72420    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.38     |
|    n_updates        | 8104     |
----------------------------------
Eval num_timesteps=72500, episode_reward=170.60 +/- 49.57
Episode length: 42.92 +/- 12.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.909    |
| time/               |          |
|    total_timesteps  | 72500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.207    |
|    n_updates        | 8124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72       |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 984      |
|    fps              | 294      |
|    time_elapsed     | 247      |
|    total_timesteps  | 72718    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.242    |
|    n_updates        | 8179     |
----------------------------------
Eval num_timesteps=73000, episode_reward=180.18 +/- 55.46
Episode length: 45.46 +/- 13.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.906    |
| time/               |          |
|    total_timesteps  | 73000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.819    |
|    n_updates        | 8249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.8     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.906    |
| time/               |          |
|    episodes         | 988      |
|    fps              | 293      |
|    time_elapsed     | 249      |
|    total_timesteps  | 73143    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.262    |
|    n_updates        | 8285     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.9     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.905    |
| time/               |          |
|    episodes         | 992      |
|    fps              | 294      |
|    time_elapsed     | 249      |
|    total_timesteps  | 73418    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.441    |
|    n_updates        | 8354     |
----------------------------------
Eval num_timesteps=73500, episode_reward=178.56 +/- 54.46
Episode length: 45.08 +/- 13.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.904    |
| time/               |          |
|    total_timesteps  | 73500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0133   |
|    n_updates        | 8374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.6     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 996      |
|    fps              | 293      |
|    time_elapsed     | 250      |
|    total_timesteps  | 73683    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.35     |
|    n_updates        | 8420     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.902    |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 294      |
|    time_elapsed     | 251      |
|    total_timesteps  | 73979    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0199   |
|    n_updates        | 8494     |
----------------------------------
Eval num_timesteps=74000, episode_reward=175.12 +/- 37.53
Episode length: 44.14 +/- 9.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.902    |
| time/               |          |
|    total_timesteps  | 74000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.2      |
|    n_updates        | 8499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.2     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.901    |
| time/               |          |
|    episodes         | 1004     |
|    fps              | 293      |
|    time_elapsed     | 252      |
|    total_timesteps  | 74289    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0336   |
|    n_updates        | 8572     |
----------------------------------
Eval num_timesteps=74500, episode_reward=174.58 +/- 51.05
Episode length: 43.98 +/- 12.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.9      |
| time/               |          |
|    total_timesteps  | 74500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.152    |
|    n_updates        | 8624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.6     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.899    |
| time/               |          |
|    episodes         | 1008     |
|    fps              | 293      |
|    time_elapsed     | 254      |
|    total_timesteps  | 74683    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0728   |
|    n_updates        | 8670     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.898    |
| time/               |          |
|    episodes         | 1012     |
|    fps              | 294      |
|    time_elapsed     | 254      |
|    total_timesteps  | 74985    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.415    |
|    n_updates        | 8746     |
----------------------------------
Eval num_timesteps=75000, episode_reward=186.48 +/- 59.70
Episode length: 46.94 +/- 14.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.898    |
| time/               |          |
|    total_timesteps  | 75000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.228    |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.9     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.896    |
| time/               |          |
|    episodes         | 1016     |
|    fps              | 293      |
|    time_elapsed     | 256      |
|    total_timesteps  | 75352    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.211    |
|    n_updates        | 8837     |
----------------------------------
Eval num_timesteps=75500, episode_reward=177.44 +/- 44.04
Episode length: 44.74 +/- 11.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.896    |
| time/               |          |
|    total_timesteps  | 75500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0115   |
|    n_updates        | 8874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.895    |
| time/               |          |
|    episodes         | 1020     |
|    fps              | 292      |
|    time_elapsed     | 258      |
|    total_timesteps  | 75565    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00499  |
|    n_updates        | 8891     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.894    |
| time/               |          |
|    episodes         | 1024     |
|    fps              | 293      |
|    time_elapsed     | 258      |
|    total_timesteps  | 75817    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.12     |
|    n_updates        | 8954     |
----------------------------------
Eval num_timesteps=76000, episode_reward=195.66 +/- 53.08
Episode length: 49.30 +/- 13.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.3     |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.893    |
| time/               |          |
|    total_timesteps  | 76000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.386    |
|    n_updates        | 8999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.1     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.893    |
| time/               |          |
|    episodes         | 1028     |
|    fps              | 292      |
|    time_elapsed     | 260      |
|    total_timesteps  | 76013    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0248   |
|    n_updates        | 9003     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.8     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 0.893    |
| time/               |          |
|    episodes         | 1032     |
|    fps              | 292      |
|    time_elapsed     | 260      |
|    total_timesteps  | 76181    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0864   |
|    n_updates        | 9045     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.5     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 0.891    |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 293      |
|    time_elapsed     | 260      |
|    total_timesteps  | 76486    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.224    |
|    n_updates        | 9121     |
----------------------------------
Eval num_timesteps=76500, episode_reward=263.56 +/- 50.06
Episode length: 66.30 +/- 12.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 66.3     |
|    mean_reward      | 264      |
| rollout/            |          |
|    exploration_rate | 0.891    |
| time/               |          |
|    total_timesteps  | 76500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0221   |
|    n_updates        | 9124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.1     |
|    ep_rew_mean      | 279      |
|    exploration_rate | 0.89     |
| time/               |          |
|    episodes         | 1040     |
|    fps              | 292      |
|    time_elapsed     | 262      |
|    total_timesteps  | 76758    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.237    |
|    n_updates        | 9189     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.7     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 0.889    |
| time/               |          |
|    episodes         | 1044     |
|    fps              | 292      |
|    time_elapsed     | 262      |
|    total_timesteps  | 76938    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00662  |
|    n_updates        | 9234     |
----------------------------------
Eval num_timesteps=77000, episode_reward=171.32 +/- 49.31
Episode length: 43.18 +/- 12.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.889    |
| time/               |          |
|    total_timesteps  | 77000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0968   |
|    n_updates        | 9249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.8     |
|    ep_rew_mean      | 270      |
|    exploration_rate | 0.888    |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 291      |
|    time_elapsed     | 264      |
|    total_timesteps  | 77098    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.201    |
|    n_updates        | 9274     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 67.5     |
|    ep_rew_mean      | 268      |
|    exploration_rate | 0.887    |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 292      |
|    time_elapsed     | 264      |
|    total_timesteps  | 77328    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.221    |
|    n_updates        | 9331     |
----------------------------------
Eval num_timesteps=77500, episode_reward=201.74 +/- 96.09
Episode length: 50.90 +/- 23.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.9     |
|    mean_reward      | 202      |
| rollout/            |          |
|    exploration_rate | 0.887    |
| time/               |          |
|    total_timesteps  | 77500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0177   |
|    n_updates        | 9374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 68.7     |
|    ep_rew_mean      | 273      |
|    exploration_rate | 0.886    |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 291      |
|    time_elapsed     | 266      |
|    total_timesteps  | 77661    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.231    |
|    n_updates        | 9415     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 69.8     |
|    ep_rew_mean      | 278      |
|    exploration_rate | 0.885    |
| time/               |          |
|    episodes         | 1060     |
|    fps              | 292      |
|    time_elapsed     | 266      |
|    total_timesteps  | 77918    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.14     |
|    n_updates        | 9479     |
----------------------------------
Eval num_timesteps=78000, episode_reward=162.82 +/- 42.37
Episode length: 41.06 +/- 10.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.1     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.884    |
| time/               |          |
|    total_timesteps  | 78000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.243    |
|    n_updates        | 9499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71       |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.883    |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 291      |
|    time_elapsed     | 268      |
|    total_timesteps  | 78266    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.221    |
|    n_updates        | 9566     |
----------------------------------
Eval num_timesteps=78500, episode_reward=174.66 +/- 46.93
Episode length: 44.04 +/- 11.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.882    |
| time/               |          |
|    total_timesteps  | 78500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0627   |
|    n_updates        | 9624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.5     |
|    ep_rew_mean      | 285      |
|    exploration_rate | 0.881    |
| time/               |          |
|    episodes         | 1068     |
|    fps              | 291      |
|    time_elapsed     | 270      |
|    total_timesteps  | 78718    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0103   |
|    n_updates        | 9679     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.2     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.88     |
| time/               |          |
|    episodes         | 1072     |
|    fps              | 292      |
|    time_elapsed     | 270      |
|    total_timesteps  | 78996    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0332   |
|    n_updates        | 9748     |
----------------------------------
Eval num_timesteps=79000, episode_reward=185.04 +/- 61.34
Episode length: 46.58 +/- 15.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.6     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.88     |
| time/               |          |
|    total_timesteps  | 79000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0395   |
|    n_updates        | 9749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.878    |
| time/               |          |
|    episodes         | 1076     |
|    fps              | 291      |
|    time_elapsed     | 272      |
|    total_timesteps  | 79367    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0237   |
|    n_updates        | 9841     |
----------------------------------
Eval num_timesteps=79500, episode_reward=176.12 +/- 51.34
Episode length: 44.34 +/- 12.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.877    |
| time/               |          |
|    total_timesteps  | 79500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0193   |
|    n_updates        | 9874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.877    |
| time/               |          |
|    episodes         | 1080     |
|    fps              | 291      |
|    time_elapsed     | 273      |
|    total_timesteps  | 79702    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.233    |
|    n_updates        | 9925     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.2     |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 1084     |
|    fps              | 291      |
|    time_elapsed     | 273      |
|    total_timesteps  | 79939    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.45     |
|    n_updates        | 9984     |
----------------------------------
Eval num_timesteps=80000, episode_reward=171.44 +/- 66.81
Episode length: 43.22 +/- 16.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.875    |
| time/               |          |
|    total_timesteps  | 80000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.169    |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.5     |
|    ep_rew_mean      | 280      |
|    exploration_rate | 0.874    |
| time/               |          |
|    episodes         | 1088     |
|    fps              | 290      |
|    time_elapsed     | 275      |
|    total_timesteps  | 80193    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.256    |
|    n_updates        | 10048    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.3     |
|    ep_rew_mean      | 280      |
|    exploration_rate | 0.873    |
| time/               |          |
|    episodes         | 1092     |
|    fps              | 291      |
|    time_elapsed     | 275      |
|    total_timesteps  | 80452    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.833    |
|    n_updates        | 10112    |
----------------------------------
Eval num_timesteps=80500, episode_reward=157.92 +/- 39.37
Episode length: 39.88 +/- 9.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.9     |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.873    |
| time/               |          |
|    total_timesteps  | 80500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0174   |
|    n_updates        | 10124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.5     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 0.872    |
| time/               |          |
|    episodes         | 1096     |
|    fps              | 291      |
|    time_elapsed     | 277      |
|    total_timesteps  | 80738    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00963  |
|    n_updates        | 10184    |
----------------------------------
Eval num_timesteps=81000, episode_reward=177.40 +/- 46.55
Episode length: 44.78 +/- 11.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.87     |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00542  |
|    n_updates        | 10249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.7     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 0.87     |
| time/               |          |
|    episodes         | 1100     |
|    fps              | 290      |
|    time_elapsed     | 278      |
|    total_timesteps  | 81046    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0707   |
|    n_updates        | 10261    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.1     |
|    ep_rew_mean      | 279      |
|    exploration_rate | 0.869    |
| time/               |          |
|    episodes         | 1104     |
|    fps              | 291      |
|    time_elapsed     | 279      |
|    total_timesteps  | 81297    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0939   |
|    n_updates        | 10324    |
----------------------------------
Eval num_timesteps=81500, episode_reward=179.68 +/- 48.36
Episode length: 45.34 +/- 12.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.868    |
| time/               |          |
|    total_timesteps  | 81500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0296   |
|    n_updates        | 10374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.1     |
|    ep_rew_mean      | 279      |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 1108     |
|    fps              | 290      |
|    time_elapsed     | 280      |
|    total_timesteps  | 81690    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0161   |
|    n_updates        | 10422    |
----------------------------------
Eval num_timesteps=82000, episode_reward=179.12 +/- 51.63
Episode length: 45.18 +/- 12.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.866    |
| time/               |          |
|    total_timesteps  | 82000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0106   |
|    n_updates        | 10499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.7     |
|    ep_rew_mean      | 281      |
|    exploration_rate | 0.865    |
| time/               |          |
|    episodes         | 1112     |
|    fps              | 290      |
|    time_elapsed     | 282      |
|    total_timesteps  | 82050    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.229    |
|    n_updates        | 10512    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 70.3     |
|    ep_rew_mean      | 280      |
|    exploration_rate | 0.864    |
| time/               |          |
|    episodes         | 1116     |
|    fps              | 291      |
|    time_elapsed     | 282      |
|    total_timesteps  | 82380    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.011    |
|    n_updates        | 10594    |
----------------------------------
Eval num_timesteps=82500, episode_reward=180.02 +/- 52.24
Episode length: 45.44 +/- 13.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.863    |
| time/               |          |
|    total_timesteps  | 82500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0109   |
|    n_updates        | 10624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.8     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.862    |
| time/               |          |
|    episodes         | 1120     |
|    fps              | 290      |
|    time_elapsed     | 284      |
|    total_timesteps  | 82746    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.199    |
|    n_updates        | 10686    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.2     |
|    ep_rew_mean      | 283      |
|    exploration_rate | 0.861    |
| time/               |          |
|    episodes         | 1124     |
|    fps              | 291      |
|    time_elapsed     | 284      |
|    total_timesteps  | 82932    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0117   |
|    n_updates        | 10732    |
----------------------------------
Eval num_timesteps=83000, episode_reward=156.44 +/- 49.52
Episode length: 39.44 +/- 12.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.4     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.861    |
| time/               |          |
|    total_timesteps  | 83000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.479    |
|    n_updates        | 10749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.3     |
|    ep_rew_mean      | 284      |
|    exploration_rate | 0.86     |
| time/               |          |
|    episodes         | 1128     |
|    fps              | 290      |
|    time_elapsed     | 286      |
|    total_timesteps  | 83147    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0188   |
|    n_updates        | 10786    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.6     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.859    |
| time/               |          |
|    episodes         | 1132     |
|    fps              | 291      |
|    time_elapsed     | 286      |
|    total_timesteps  | 83445    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.355    |
|    n_updates        | 10861    |
----------------------------------
Eval num_timesteps=83500, episode_reward=164.06 +/- 38.16
Episode length: 41.38 +/- 9.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.858    |
| time/               |          |
|    total_timesteps  | 83500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.167    |
|    n_updates        | 10874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72       |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.857    |
| time/               |          |
|    episodes         | 1136     |
|    fps              | 290      |
|    time_elapsed     | 288      |
|    total_timesteps  | 83681    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.445    |
|    n_updates        | 10920    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72       |
|    ep_rew_mean      | 287      |
|    exploration_rate | 0.856    |
| time/               |          |
|    episodes         | 1140     |
|    fps              | 291      |
|    time_elapsed     | 288      |
|    total_timesteps  | 83959    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0245   |
|    n_updates        | 10989    |
----------------------------------
Eval num_timesteps=84000, episode_reward=189.38 +/- 54.95
Episode length: 47.78 +/- 13.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.8     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.856    |
| time/               |          |
|    total_timesteps  | 84000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.51     |
|    n_updates        | 10999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.5     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.855    |
| time/               |          |
|    episodes         | 1144     |
|    fps              | 290      |
|    time_elapsed     | 290      |
|    total_timesteps  | 84289    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.274    |
|    n_updates        | 11072    |
----------------------------------
Eval num_timesteps=84500, episode_reward=161.72 +/- 34.87
Episode length: 40.76 +/- 8.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.8     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.853    |
| time/               |          |
|    total_timesteps  | 84500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.386    |
|    n_updates        | 11124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.853    |
| time/               |          |
|    episodes         | 1148     |
|    fps              | 289      |
|    time_elapsed     | 291      |
|    total_timesteps  | 84580    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.172    |
|    n_updates        | 11144    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.6     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.852    |
| time/               |          |
|    episodes         | 1152     |
|    fps              | 290      |
|    time_elapsed     | 292      |
|    total_timesteps  | 84886    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.229    |
|    n_updates        | 11221    |
----------------------------------
Eval num_timesteps=85000, episode_reward=181.24 +/- 54.84
Episode length: 45.78 +/- 13.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.851    |
| time/               |          |
|    total_timesteps  | 85000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.29     |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.85     |
| time/               |          |
|    episodes         | 1156     |
|    fps              | 289      |
|    time_elapsed     | 293      |
|    total_timesteps  | 85171    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0412   |
|    n_updates        | 11292    |
----------------------------------
Eval num_timesteps=85500, episode_reward=175.78 +/- 44.46
Episode length: 44.32 +/- 11.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.849    |
| time/               |          |
|    total_timesteps  | 85500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0714   |
|    n_updates        | 11374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.848    |
| time/               |          |
|    episodes         | 1160     |
|    fps              | 289      |
|    time_elapsed     | 295      |
|    total_timesteps  | 85527    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.24     |
|    n_updates        | 11381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.9     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.847    |
| time/               |          |
|    episodes         | 1164     |
|    fps              | 290      |
|    time_elapsed     | 295      |
|    total_timesteps  | 85852    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00505  |
|    n_updates        | 11462    |
----------------------------------
Eval num_timesteps=86000, episode_reward=176.56 +/- 79.74
Episode length: 44.54 +/- 19.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.846    |
| time/               |          |
|    total_timesteps  | 86000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0199   |
|    n_updates        | 11499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.845    |
| time/               |          |
|    episodes         | 1168     |
|    fps              | 289      |
|    time_elapsed     | 297      |
|    total_timesteps  | 86151    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.521    |
|    n_updates        | 11537    |
----------------------------------
Eval num_timesteps=86500, episode_reward=174.34 +/- 54.86
Episode length: 43.98 +/- 13.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.843    |
| time/               |          |
|    total_timesteps  | 86500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.31     |
|    n_updates        | 11624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.843    |
| time/               |          |
|    episodes         | 1172     |
|    fps              | 289      |
|    time_elapsed     | 299      |
|    total_timesteps  | 86531    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0347   |
|    n_updates        | 11632    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.7     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.842    |
| time/               |          |
|    episodes         | 1176     |
|    fps              | 289      |
|    time_elapsed     | 299      |
|    total_timesteps  | 86736    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.734    |
|    n_updates        | 11683    |
----------------------------------
Eval num_timesteps=87000, episode_reward=173.86 +/- 66.28
Episode length: 43.80 +/- 16.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.841    |
| time/               |          |
|    total_timesteps  | 87000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0218   |
|    n_updates        | 11749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.841    |
| time/               |          |
|    episodes         | 1180     |
|    fps              | 288      |
|    time_elapsed     | 301      |
|    total_timesteps  | 87000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.4     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.84     |
| time/               |          |
|    episodes         | 1184     |
|    fps              | 289      |
|    time_elapsed     | 301      |
|    total_timesteps  | 87279    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.237    |
|    n_updates        | 11819    |
----------------------------------
Eval num_timesteps=87500, episode_reward=178.96 +/- 42.10
Episode length: 45.12 +/- 10.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.838    |
| time/               |          |
|    total_timesteps  | 87500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0245   |
|    n_updates        | 11874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.4     |
|    ep_rew_mean      | 292      |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 1188     |
|    fps              | 288      |
|    time_elapsed     | 302      |
|    total_timesteps  | 87535    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.278    |
|    n_updates        | 11883    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.1     |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.837    |
| time/               |          |
|    episodes         | 1192     |
|    fps              | 289      |
|    time_elapsed     | 303      |
|    total_timesteps  | 87764    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00384  |
|    n_updates        | 11940    |
----------------------------------
Eval num_timesteps=88000, episode_reward=164.22 +/- 54.07
Episode length: 41.40 +/- 13.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.836    |
| time/               |          |
|    total_timesteps  | 88000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.291    |
|    n_updates        | 11999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.1     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.835    |
| time/               |          |
|    episodes         | 1196     |
|    fps              | 289      |
|    time_elapsed     | 304      |
|    total_timesteps  | 88145    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.141    |
|    n_updates        | 12036    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.4     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.833    |
| time/               |          |
|    episodes         | 1200     |
|    fps              | 290      |
|    time_elapsed     | 305      |
|    total_timesteps  | 88484    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00797  |
|    n_updates        | 12120    |
----------------------------------
Eval num_timesteps=88500, episode_reward=152.28 +/- 44.12
Episode length: 38.50 +/- 11.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.5     |
|    mean_reward      | 152      |
| rollout/            |          |
|    exploration_rate | 0.833    |
| time/               |          |
|    total_timesteps  | 88500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0187   |
|    n_updates        | 12124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.1     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.832    |
| time/               |          |
|    episodes         | 1204     |
|    fps              | 289      |
|    time_elapsed     | 306      |
|    total_timesteps  | 88709    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.428    |
|    n_updates        | 12177    |
----------------------------------
Eval num_timesteps=89000, episode_reward=181.56 +/- 44.36
Episode length: 45.76 +/- 11.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.831    |
| time/               |          |
|    total_timesteps  | 89000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0098   |
|    n_updates        | 12249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74       |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.83     |
| time/               |          |
|    episodes         | 1208     |
|    fps              | 288      |
|    time_elapsed     | 308      |
|    total_timesteps  | 89093    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.609    |
|    n_updates        | 12273    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.2     |
|    ep_rew_mean      | 295      |
|    exploration_rate | 0.828    |
| time/               |          |
|    episodes         | 1212     |
|    fps              | 289      |
|    time_elapsed     | 308      |
|    total_timesteps  | 89466    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.294    |
|    n_updates        | 12366    |
----------------------------------
Eval num_timesteps=89500, episode_reward=167.02 +/- 41.62
Episode length: 42.16 +/- 10.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.828    |
| time/               |          |
|    total_timesteps  | 89500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.345    |
|    n_updates        | 12374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.8     |
|    ep_rew_mean      | 290      |
|    exploration_rate | 0.827    |
| time/               |          |
|    episodes         | 1216     |
|    fps              | 289      |
|    time_elapsed     | 310      |
|    total_timesteps  | 89664    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.124    |
|    n_updates        | 12415    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.8     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.826    |
| time/               |          |
|    episodes         | 1220     |
|    fps              | 289      |
|    time_elapsed     | 310      |
|    total_timesteps  | 89925    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00916  |
|    n_updates        | 12481    |
----------------------------------
Eval num_timesteps=90000, episode_reward=178.42 +/- 42.79
Episode length: 44.98 +/- 10.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.825    |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.645    |
|    n_updates        | 12499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.3     |
|    ep_rew_mean      | 288      |
|    exploration_rate | 0.825    |
| time/               |          |
|    episodes         | 1224     |
|    fps              | 289      |
|    time_elapsed     | 311      |
|    total_timesteps  | 90162    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.262    |
|    n_updates        | 12540    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.7     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.823    |
| time/               |          |
|    episodes         | 1228     |
|    fps              | 289      |
|    time_elapsed     | 312      |
|    total_timesteps  | 90417    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.393    |
|    n_updates        | 12604    |
----------------------------------
Eval num_timesteps=90500, episode_reward=173.48 +/- 52.21
Episode length: 43.74 +/- 13.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.823    |
| time/               |          |
|    total_timesteps  | 90500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.029    |
|    n_updates        | 12624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.821    |
| time/               |          |
|    episodes         | 1232     |
|    fps              | 289      |
|    time_elapsed     | 313      |
|    total_timesteps  | 90898    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.223    |
|    n_updates        | 12724    |
----------------------------------
Eval num_timesteps=91000, episode_reward=166.72 +/- 45.16
Episode length: 42.12 +/- 11.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.1     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.82     |
| time/               |          |
|    total_timesteps  | 91000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0135   |
|    n_updates        | 12749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.82     |
| time/               |          |
|    episodes         | 1236     |
|    fps              | 288      |
|    time_elapsed     | 315      |
|    total_timesteps  | 91135    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.268    |
|    n_updates        | 12783    |
----------------------------------
Eval num_timesteps=91500, episode_reward=166.12 +/- 37.99
Episode length: 41.94 +/- 9.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.9     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.818    |
| time/               |          |
|    total_timesteps  | 91500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0206   |
|    n_updates        | 12874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.817    |
| time/               |          |
|    episodes         | 1240     |
|    fps              | 288      |
|    time_elapsed     | 317      |
|    total_timesteps  | 91536    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.679    |
|    n_updates        | 12883    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.816    |
| time/               |          |
|    episodes         | 1244     |
|    fps              | 289      |
|    time_elapsed     | 317      |
|    total_timesteps  | 91842    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00539  |
|    n_updates        | 12960    |
----------------------------------
Eval num_timesteps=92000, episode_reward=182.28 +/- 50.76
Episode length: 45.94 +/- 12.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.815    |
| time/               |          |
|    total_timesteps  | 92000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.301    |
|    n_updates        | 12999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 1248     |
|    fps              | 288      |
|    time_elapsed     | 319      |
|    total_timesteps  | 92072    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.326    |
|    n_updates        | 13017    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.1     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.813    |
| time/               |          |
|    episodes         | 1252     |
|    fps              | 289      |
|    time_elapsed     | 319      |
|    total_timesteps  | 92394    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.73     |
|    n_updates        | 13098    |
----------------------------------
Eval num_timesteps=92500, episode_reward=172.08 +/- 48.45
Episode length: 43.40 +/- 12.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.812    |
| time/               |          |
|    total_timesteps  | 92500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.581    |
|    n_updates        | 13124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.811    |
| time/               |          |
|    episodes         | 1256     |
|    fps              | 289      |
|    time_elapsed     | 321      |
|    total_timesteps  | 92785    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.636    |
|    n_updates        | 13196    |
----------------------------------
Eval num_timesteps=93000, episode_reward=179.36 +/- 50.63
Episode length: 45.14 +/- 12.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.81     |
| time/               |          |
|    total_timesteps  | 93000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0259   |
|    n_updates        | 13249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.808    |
| time/               |          |
|    episodes         | 1260     |
|    fps              | 288      |
|    time_elapsed     | 322      |
|    total_timesteps  | 93198    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.297    |
|    n_updates        | 13299    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.807    |
| time/               |          |
|    episodes         | 1264     |
|    fps              | 289      |
|    time_elapsed     | 323      |
|    total_timesteps  | 93461    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0189   |
|    n_updates        | 13365    |
----------------------------------
Eval num_timesteps=93500, episode_reward=188.92 +/- 49.07
Episode length: 47.66 +/- 12.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.7     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.807    |
| time/               |          |
|    total_timesteps  | 93500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0336   |
|    n_updates        | 13374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.806    |
| time/               |          |
|    episodes         | 1268     |
|    fps              | 288      |
|    time_elapsed     | 324      |
|    total_timesteps  | 93734    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.656    |
|    n_updates        | 13433    |
----------------------------------
Eval num_timesteps=94000, episode_reward=166.28 +/- 56.62
Episode length: 41.94 +/- 14.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.9     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.804    |
| time/               |          |
|    total_timesteps  | 94000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.014    |
|    n_updates        | 13499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.7     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.804    |
| time/               |          |
|    episodes         | 1272     |
|    fps              | 288      |
|    time_elapsed     | 326      |
|    total_timesteps  | 94005    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0167   |
|    n_updates        | 13501    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.803    |
| time/               |          |
|    episodes         | 1276     |
|    fps              | 288      |
|    time_elapsed     | 326      |
|    total_timesteps  | 94241    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.274    |
|    n_updates        | 13560    |
----------------------------------
Eval num_timesteps=94500, episode_reward=178.16 +/- 47.33
Episode length: 44.94 +/- 11.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.801    |
| time/               |          |
|    total_timesteps  | 94500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.322    |
|    n_updates        | 13624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.8      |
| time/               |          |
|    episodes         | 1280     |
|    fps              | 288      |
|    time_elapsed     | 328      |
|    total_timesteps  | 94682    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0199   |
|    n_updates        | 13670    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.799    |
| time/               |          |
|    episodes         | 1284     |
|    fps              | 288      |
|    time_elapsed     | 328      |
|    total_timesteps  | 94897    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.544    |
|    n_updates        | 13724    |
----------------------------------
Eval num_timesteps=95000, episode_reward=174.38 +/- 47.12
Episode length: 44.02 +/- 11.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.799    |
| time/               |          |
|    total_timesteps  | 95000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.288    |
|    n_updates        | 13749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.798    |
| time/               |          |
|    episodes         | 1288     |
|    fps              | 288      |
|    time_elapsed     | 330      |
|    total_timesteps  | 95151    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.582    |
|    n_updates        | 13787    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.796    |
| time/               |          |
|    episodes         | 1292     |
|    fps              | 288      |
|    time_elapsed     | 330      |
|    total_timesteps  | 95403    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.276    |
|    n_updates        | 13850    |
----------------------------------
Eval num_timesteps=95500, episode_reward=175.46 +/- 62.24
Episode length: 44.24 +/- 15.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.796    |
| time/               |          |
|    total_timesteps  | 95500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.424    |
|    n_updates        | 13874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76       |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.795    |
| time/               |          |
|    episodes         | 1296     |
|    fps              | 288      |
|    time_elapsed     | 332      |
|    total_timesteps  | 95747    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.38     |
|    n_updates        | 13936    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.6     |
|    ep_rew_mean      | 297      |
|    exploration_rate | 0.793    |
| time/               |          |
|    episodes         | 1300     |
|    fps              | 288      |
|    time_elapsed     | 332      |
|    total_timesteps  | 95943    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0426   |
|    n_updates        | 13985    |
----------------------------------
Eval num_timesteps=96000, episode_reward=180.36 +/- 48.17
Episode length: 45.42 +/- 12.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.793    |
| time/               |          |
|    total_timesteps  | 96000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.244    |
|    n_updates        | 13999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.2     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.791    |
| time/               |          |
|    episodes         | 1304     |
|    fps              | 288      |
|    time_elapsed     | 334      |
|    total_timesteps  | 96324    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.632    |
|    n_updates        | 14080    |
----------------------------------
Eval num_timesteps=96500, episode_reward=173.82 +/- 47.61
Episode length: 43.78 +/- 11.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.79     |
| time/               |          |
|    total_timesteps  | 96500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.891    |
|    n_updates        | 14124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.7     |
|    ep_rew_mean      | 301      |
|    exploration_rate | 0.789    |
| time/               |          |
|    episodes         | 1308     |
|    fps              | 287      |
|    time_elapsed     | 335      |
|    total_timesteps  | 96667    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.581    |
|    n_updates        | 14166    |
----------------------------------
Eval num_timesteps=97000, episode_reward=164.56 +/- 53.06
Episode length: 41.54 +/- 13.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.788    |
| time/               |          |
|    total_timesteps  | 97000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.757    |
|    n_updates        | 14249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.787    |
| time/               |          |
|    episodes         | 1312     |
|    fps              | 287      |
|    time_elapsed     | 337      |
|    total_timesteps  | 97096    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.343    |
|    n_updates        | 14273    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.785    |
| time/               |          |
|    episodes         | 1316     |
|    fps              | 288      |
|    time_elapsed     | 337      |
|    total_timesteps  | 97444    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0287   |
|    n_updates        | 14360    |
----------------------------------
Eval num_timesteps=97500, episode_reward=141.58 +/- 42.39
Episode length: 35.78 +/- 10.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.8     |
|    mean_reward      | 142      |
| rollout/            |          |
|    exploration_rate | 0.785    |
| time/               |          |
|    total_timesteps  | 97500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00816  |
|    n_updates        | 14374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.8     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.783    |
| time/               |          |
|    episodes         | 1320     |
|    fps              | 288      |
|    time_elapsed     | 339      |
|    total_timesteps  | 97803    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.348    |
|    n_updates        | 14450    |
----------------------------------
Eval num_timesteps=98000, episode_reward=152.00 +/- 39.57
Episode length: 38.34 +/- 9.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.3     |
|    mean_reward      | 152      |
| rollout/            |          |
|    exploration_rate | 0.782    |
| time/               |          |
|    total_timesteps  | 98000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.758    |
|    n_updates        | 14499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.8     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.782    |
| time/               |          |
|    episodes         | 1324     |
|    fps              | 287      |
|    time_elapsed     | 340      |
|    total_timesteps  | 98047    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0245   |
|    n_updates        | 14511    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.78     |
| time/               |          |
|    episodes         | 1328     |
|    fps              | 288      |
|    time_elapsed     | 340      |
|    total_timesteps  | 98400    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0091   |
|    n_updates        | 14599    |
----------------------------------
Eval num_timesteps=98500, episode_reward=158.78 +/- 53.26
Episode length: 40.08 +/- 13.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.1     |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.779    |
| time/               |          |
|    total_timesteps  | 98500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.328    |
|    n_updates        | 14624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.779    |
| time/               |          |
|    episodes         | 1332     |
|    fps              | 287      |
|    time_elapsed     | 342      |
|    total_timesteps  | 98564    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0357   |
|    n_updates        | 14640    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.4     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.778    |
| time/               |          |
|    episodes         | 1336     |
|    fps              | 288      |
|    time_elapsed     | 342      |
|    total_timesteps  | 98772    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.313    |
|    n_updates        | 14692    |
----------------------------------
Eval num_timesteps=99000, episode_reward=172.50 +/- 49.35
Episode length: 43.50 +/- 12.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.776    |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0357   |
|    n_updates        | 14749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.7     |
|    ep_rew_mean      | 305      |
|    exploration_rate | 0.775    |
| time/               |          |
|    episodes         | 1340     |
|    fps              | 288      |
|    time_elapsed     | 344      |
|    total_timesteps  | 99208    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0182   |
|    n_updates        | 14801    |
----------------------------------
Eval num_timesteps=99500, episode_reward=156.44 +/- 42.93
Episode length: 39.38 +/- 10.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.4     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.773    |
| time/               |          |
|    total_timesteps  | 99500    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.399    |
|    n_updates        | 14874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.4     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.773    |
| time/               |          |
|    episodes         | 1344     |
|    fps              | 287      |
|    time_elapsed     | 345      |
|    total_timesteps  | 99580    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.02     |
|    n_updates        | 14894    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.771    |
| time/               |          |
|    episodes         | 1348     |
|    fps              | 288      |
|    time_elapsed     | 346      |
|    total_timesteps  | 99853    |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.599    |
|    n_updates        | 14963    |
----------------------------------
Eval num_timesteps=100000, episode_reward=177.74 +/- 62.11
Episode length: 44.76 +/- 15.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.771    |
| time/               |          |
|    total_timesteps  | 100000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.034    |
|    n_updates        | 14999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.77     |
| time/               |          |
|    episodes         | 1352     |
|    fps              | 288      |
|    time_elapsed     | 347      |
|    total_timesteps  | 100187   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.076    |
|    n_updates        | 15046    |
----------------------------------
Eval num_timesteps=100500, episode_reward=182.86 +/- 45.34
Episode length: 46.08 +/- 11.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.768    |
| time/               |          |
|    total_timesteps  | 100500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0231   |
|    n_updates        | 15124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.6     |
|    ep_rew_mean      | 317      |
|    exploration_rate | 0.766    |
| time/               |          |
|    episodes         | 1356     |
|    fps              | 288      |
|    time_elapsed     | 349      |
|    total_timesteps  | 100744   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.394    |
|    n_updates        | 15185    |
----------------------------------
Eval num_timesteps=101000, episode_reward=160.88 +/- 54.65
Episode length: 40.64 +/- 13.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.6     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.765    |
| time/               |          |
|    total_timesteps  | 101000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.806    |
|    n_updates        | 15249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.765    |
| time/               |          |
|    episodes         | 1360     |
|    fps              | 287      |
|    time_elapsed     | 351      |
|    total_timesteps  | 101047   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.478    |
|    n_updates        | 15261    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.764    |
| time/               |          |
|    episodes         | 1364     |
|    fps              | 287      |
|    time_elapsed     | 351      |
|    total_timesteps  | 101214   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.959    |
|    n_updates        | 15303    |
----------------------------------
Eval num_timesteps=101500, episode_reward=183.38 +/- 51.59
Episode length: 46.16 +/- 12.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.762    |
| time/               |          |
|    total_timesteps  | 101500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0248   |
|    n_updates        | 15374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80       |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.761    |
| time/               |          |
|    episodes         | 1368     |
|    fps              | 287      |
|    time_elapsed     | 353      |
|    total_timesteps  | 101731   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.014    |
|    n_updates        | 15432    |
----------------------------------
Eval num_timesteps=102000, episode_reward=161.44 +/- 55.01
Episode length: 40.70 +/- 13.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.7     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.759    |
| time/               |          |
|    total_timesteps  | 102000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.626    |
|    n_updates        | 15499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.9     |
|    ep_rew_mean      | 322      |
|    exploration_rate | 0.758    |
| time/               |          |
|    episodes         | 1372     |
|    fps              | 287      |
|    time_elapsed     | 355      |
|    total_timesteps  | 102095   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.728    |
|    n_updates        | 15523    |
----------------------------------
Eval num_timesteps=102500, episode_reward=158.14 +/- 51.83
Episode length: 39.86 +/- 12.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.9     |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.756    |
| time/               |          |
|    total_timesteps  | 102500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.668    |
|    n_updates        | 15624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.8     |
|    ep_rew_mean      | 330      |
|    exploration_rate | 0.756    |
| time/               |          |
|    episodes         | 1376     |
|    fps              | 287      |
|    time_elapsed     | 356      |
|    total_timesteps  | 102523   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.138    |
|    n_updates        | 15630    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.754    |
| time/               |          |
|    episodes         | 1380     |
|    fps              | 288      |
|    time_elapsed     | 356      |
|    total_timesteps  | 102800   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.464    |
|    n_updates        | 15699    |
----------------------------------
Eval num_timesteps=103000, episode_reward=164.54 +/- 44.17
Episode length: 41.56 +/- 11.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.753    |
| time/               |          |
|    total_timesteps  | 103000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.709    |
|    n_updates        | 15749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.753    |
| time/               |          |
|    episodes         | 1384     |
|    fps              | 287      |
|    time_elapsed     | 358      |
|    total_timesteps  | 103096   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.38     |
|    n_updates        | 15773    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.1     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.751    |
| time/               |          |
|    episodes         | 1388     |
|    fps              | 288      |
|    time_elapsed     | 358      |
|    total_timesteps  | 103363   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.377    |
|    n_updates        | 15840    |
----------------------------------
Eval num_timesteps=103500, episode_reward=160.52 +/- 52.50
Episode length: 40.46 +/- 13.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.5     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.75     |
| time/               |          |
|    total_timesteps  | 103500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0438   |
|    n_updates        | 15874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.75     |
| time/               |          |
|    episodes         | 1392     |
|    fps              | 287      |
|    time_elapsed     | 360      |
|    total_timesteps  | 103601   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.676    |
|    n_updates        | 15900    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81       |
|    ep_rew_mean      | 323      |
|    exploration_rate | 0.748    |
| time/               |          |
|    episodes         | 1396     |
|    fps              | 288      |
|    time_elapsed     | 360      |
|    total_timesteps  | 103847   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15     |
|    n_updates        | 15961    |
----------------------------------
Eval num_timesteps=104000, episode_reward=176.74 +/- 53.35
Episode length: 44.60 +/- 13.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.747    |
| time/               |          |
|    total_timesteps  | 104000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0207   |
|    n_updates        | 15999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.747    |
| time/               |          |
|    episodes         | 1400     |
|    fps              | 287      |
|    time_elapsed     | 362      |
|    total_timesteps  | 104071   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.133    |
|    n_updates        | 16017    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.745    |
| time/               |          |
|    episodes         | 1404     |
|    fps              | 288      |
|    time_elapsed     | 362      |
|    total_timesteps  | 104342   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0273   |
|    n_updates        | 16085    |
----------------------------------
Eval num_timesteps=104500, episode_reward=172.14 +/- 57.47
Episode length: 43.36 +/- 14.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.744    |
| time/               |          |
|    total_timesteps  | 104500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.237    |
|    n_updates        | 16124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.744    |
| time/               |          |
|    episodes         | 1408     |
|    fps              | 287      |
|    time_elapsed     | 363      |
|    total_timesteps  | 104560   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0195   |
|    n_updates        | 16139    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.742    |
| time/               |          |
|    episodes         | 1412     |
|    fps              | 287      |
|    time_elapsed     | 364      |
|    total_timesteps  | 104806   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00746  |
|    n_updates        | 16201    |
----------------------------------
Eval num_timesteps=105000, episode_reward=177.68 +/- 52.43
Episode length: 44.78 +/- 13.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.741    |
| time/               |          |
|    total_timesteps  | 105000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0407   |
|    n_updates        | 16249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.74     |
| time/               |          |
|    episodes         | 1416     |
|    fps              | 287      |
|    time_elapsed     | 365      |
|    total_timesteps  | 105196   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0504   |
|    n_updates        | 16298    |
----------------------------------
Eval num_timesteps=105500, episode_reward=176.20 +/- 51.05
Episode length: 44.44 +/- 12.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.738    |
| time/               |          |
|    total_timesteps  | 105500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.843    |
|    n_updates        | 16374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.738    |
| time/               |          |
|    episodes         | 1420     |
|    fps              | 287      |
|    time_elapsed     | 367      |
|    total_timesteps  | 105625   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0235   |
|    n_updates        | 16406    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.736    |
| time/               |          |
|    episodes         | 1424     |
|    fps              | 287      |
|    time_elapsed     | 367      |
|    total_timesteps  | 105836   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.375    |
|    n_updates        | 16458    |
----------------------------------
Eval num_timesteps=106000, episode_reward=183.34 +/- 56.25
Episode length: 46.24 +/- 14.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.735    |
| time/               |          |
|    total_timesteps  | 106000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.395    |
|    n_updates        | 16499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.734    |
| time/               |          |
|    episodes         | 1428     |
|    fps              | 287      |
|    time_elapsed     | 369      |
|    total_timesteps  | 106200   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.417    |
|    n_updates        | 16549    |
----------------------------------
Eval num_timesteps=106500, episode_reward=160.20 +/- 36.03
Episode length: 40.34 +/- 8.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.3     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.732    |
| time/               |          |
|    total_timesteps  | 106500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.601    |
|    n_updates        | 16624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.732    |
| time/               |          |
|    episodes         | 1432     |
|    fps              | 287      |
|    time_elapsed     | 371      |
|    total_timesteps  | 106537   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.26     |
|    n_updates        | 16634    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.73     |
| time/               |          |
|    episodes         | 1436     |
|    fps              | 287      |
|    time_elapsed     | 371      |
|    total_timesteps  | 106836   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0327   |
|    n_updates        | 16708    |
----------------------------------
Eval num_timesteps=107000, episode_reward=171.74 +/- 35.91
Episode length: 43.36 +/- 9.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.729    |
| time/               |          |
|    total_timesteps  | 107000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.429    |
|    n_updates        | 16749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.8     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.729    |
| time/               |          |
|    episodes         | 1440     |
|    fps              | 287      |
|    time_elapsed     | 373      |
|    total_timesteps  | 107084   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0205   |
|    n_updates        | 16770    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.6     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.727    |
| time/               |          |
|    episodes         | 1444     |
|    fps              | 287      |
|    time_elapsed     | 373      |
|    total_timesteps  | 107340   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.405    |
|    n_updates        | 16834    |
----------------------------------
Eval num_timesteps=107500, episode_reward=169.92 +/- 56.12
Episode length: 42.76 +/- 14.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.726    |
| time/               |          |
|    total_timesteps  | 107500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00762  |
|    n_updates        | 16874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.8     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.725    |
| time/               |          |
|    episodes         | 1448     |
|    fps              | 287      |
|    time_elapsed     | 374      |
|    total_timesteps  | 107631   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.385    |
|    n_updates        | 16907    |
----------------------------------
Eval num_timesteps=108000, episode_reward=173.52 +/- 47.85
Episode length: 43.76 +/- 11.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.723    |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.801    |
|    n_updates        | 16999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.4     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.723    |
| time/               |          |
|    episodes         | 1452     |
|    fps              | 286      |
|    time_elapsed     | 376      |
|    total_timesteps  | 108025   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.762    |
|    n_updates        | 17006    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.5     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.722    |
| time/               |          |
|    episodes         | 1456     |
|    fps              | 287      |
|    time_elapsed     | 376      |
|    total_timesteps  | 108192   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0377   |
|    n_updates        | 17047    |
----------------------------------
Eval num_timesteps=108500, episode_reward=166.52 +/- 61.15
Episode length: 42.02 +/- 15.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.72     |
| time/               |          |
|    total_timesteps  | 108500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0994   |
|    n_updates        | 17124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.9     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.72     |
| time/               |          |
|    episodes         | 1460     |
|    fps              | 286      |
|    time_elapsed     | 378      |
|    total_timesteps  | 108535   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.958    |
|    n_updates        | 17133    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.719    |
| time/               |          |
|    episodes         | 1464     |
|    fps              | 287      |
|    time_elapsed     | 378      |
|    total_timesteps  | 108713   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.73     |
|    n_updates        | 17178    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 72.7     |
|    ep_rew_mean      | 289      |
|    exploration_rate | 0.717    |
| time/               |          |
|    episodes         | 1468     |
|    fps              | 287      |
|    time_elapsed     | 378      |
|    total_timesteps  | 108999   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0102   |
|    n_updates        | 17249    |
----------------------------------
Eval num_timesteps=109000, episode_reward=189.18 +/- 56.10
Episode length: 47.66 +/- 14.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 109000   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.8     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.715    |
| time/               |          |
|    episodes         | 1472     |
|    fps              | 287      |
|    time_elapsed     | 380      |
|    total_timesteps  | 109278   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.01     |
|    n_updates        | 17319    |
----------------------------------
Eval num_timesteps=109500, episode_reward=217.88 +/- 60.42
Episode length: 54.80 +/- 15.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 54.8     |
|    mean_reward      | 218      |
| rollout/            |          |
|    exploration_rate | 0.714    |
| time/               |          |
|    total_timesteps  | 109500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07     |
|    n_updates        | 17374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 71.9     |
|    ep_rew_mean      | 286      |
|    exploration_rate | 0.713    |
| time/               |          |
|    episodes         | 1476     |
|    fps              | 286      |
|    time_elapsed     | 382      |
|    total_timesteps  | 109715   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.712    |
|    n_updates        | 17428    |
----------------------------------
Eval num_timesteps=110000, episode_reward=162.02 +/- 44.55
Episode length: 40.88 +/- 11.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.9     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.711    |
| time/               |          |
|    total_timesteps  | 110000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.425    |
|    n_updates        | 17499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73       |
|    ep_rew_mean      | 291      |
|    exploration_rate | 0.71     |
| time/               |          |
|    episodes         | 1480     |
|    fps              | 286      |
|    time_elapsed     | 384      |
|    total_timesteps  | 110103   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.169    |
|    n_updates        | 17525    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.7     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.708    |
| time/               |          |
|    episodes         | 1484     |
|    fps              | 287      |
|    time_elapsed     | 384      |
|    total_timesteps  | 110462   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.891    |
|    n_updates        | 17615    |
----------------------------------
Eval num_timesteps=110500, episode_reward=174.76 +/- 47.39
Episode length: 44.08 +/- 11.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.708    |
| time/               |          |
|    total_timesteps  | 110500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.142    |
|    n_updates        | 17624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.9     |
|    ep_rew_mean      | 294      |
|    exploration_rate | 0.706    |
| time/               |          |
|    episodes         | 1488     |
|    fps              | 286      |
|    time_elapsed     | 386      |
|    total_timesteps  | 110750   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0226   |
|    n_updates        | 17687    |
----------------------------------
Eval num_timesteps=111000, episode_reward=152.98 +/- 41.92
Episode length: 38.60 +/- 10.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.6     |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.705    |
| time/               |          |
|    total_timesteps  | 111000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0183   |
|    n_updates        | 17749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.3     |
|    ep_rew_mean      | 296      |
|    exploration_rate | 0.705    |
| time/               |          |
|    episodes         | 1492     |
|    fps              | 286      |
|    time_elapsed     | 387      |
|    total_timesteps  | 111027   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.439    |
|    n_updates        | 17756    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 73.7     |
|    ep_rew_mean      | 293      |
|    exploration_rate | 0.703    |
| time/               |          |
|    episodes         | 1496     |
|    fps              | 286      |
|    time_elapsed     | 387      |
|    total_timesteps  | 111216   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.102    |
|    n_updates        | 17803    |
----------------------------------
Eval num_timesteps=111500, episode_reward=223.80 +/- 87.20
Episode length: 56.34 +/- 21.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 56.3     |
|    mean_reward      | 224      |
| rollout/            |          |
|    exploration_rate | 0.702    |
| time/               |          |
|    total_timesteps  | 111500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.738    |
|    n_updates        | 17874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75       |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.701    |
| time/               |          |
|    episodes         | 1500     |
|    fps              | 286      |
|    time_elapsed     | 390      |
|    total_timesteps  | 111567   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.849    |
|    n_updates        | 17891    |
----------------------------------
Eval num_timesteps=112000, episode_reward=172.96 +/- 53.74
Episode length: 43.56 +/- 13.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.698    |
| time/               |          |
|    total_timesteps  | 112000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0281   |
|    n_updates        | 17999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 309      |
|    exploration_rate | 0.698    |
| time/               |          |
|    episodes         | 1504     |
|    fps              | 285      |
|    time_elapsed     | 391      |
|    total_timesteps  | 112093   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.431    |
|    n_updates        | 18023    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.697    |
| time/               |          |
|    episodes         | 1508     |
|    fps              | 286      |
|    time_elapsed     | 392      |
|    total_timesteps  | 112254   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0421   |
|    n_updates        | 18063    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.8     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.695    |
| time/               |          |
|    episodes         | 1512     |
|    fps              | 286      |
|    time_elapsed     | 392      |
|    total_timesteps  | 112489   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0667   |
|    n_updates        | 18122    |
----------------------------------
Eval num_timesteps=112500, episode_reward=172.62 +/- 44.30
Episode length: 43.58 +/- 11.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.695    |
| time/               |          |
|    total_timesteps  | 112500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.433    |
|    n_updates        | 18124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.3     |
|    ep_rew_mean      | 304      |
|    exploration_rate | 0.693    |
| time/               |          |
|    episodes         | 1516     |
|    fps              | 286      |
|    time_elapsed     | 393      |
|    total_timesteps  | 112830   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.419    |
|    n_updates        | 18207    |
----------------------------------
Eval num_timesteps=113000, episode_reward=167.24 +/- 41.41
Episode length: 42.24 +/- 10.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.692    |
| time/               |          |
|    total_timesteps  | 113000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.444    |
|    n_updates        | 18249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.3     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.691    |
| time/               |          |
|    episodes         | 1520     |
|    fps              | 285      |
|    time_elapsed     | 395      |
|    total_timesteps  | 113151   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.883    |
|    n_updates        | 18287    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.9     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.689    |
| time/               |          |
|    episodes         | 1524     |
|    fps              | 286      |
|    time_elapsed     | 395      |
|    total_timesteps  | 113425   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0222   |
|    n_updates        | 18356    |
----------------------------------
Eval num_timesteps=113500, episode_reward=176.64 +/- 42.85
Episode length: 44.56 +/- 10.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.689    |
| time/               |          |
|    total_timesteps  | 113500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0122   |
|    n_updates        | 18374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.687    |
| time/               |          |
|    episodes         | 1528     |
|    fps              | 286      |
|    time_elapsed     | 397      |
|    total_timesteps  | 113808   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.218    |
|    n_updates        | 18451    |
----------------------------------
Eval num_timesteps=114000, episode_reward=157.86 +/- 53.85
Episode length: 39.80 +/- 13.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.8     |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.686    |
| time/               |          |
|    total_timesteps  | 114000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.47     |
|    n_updates        | 18499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.7     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.686    |
| time/               |          |
|    episodes         | 1532     |
|    fps              | 285      |
|    time_elapsed     | 399      |
|    total_timesteps  | 114011   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00776  |
|    n_updates        | 18502    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 74.8     |
|    ep_rew_mean      | 298      |
|    exploration_rate | 0.684    |
| time/               |          |
|    episodes         | 1536     |
|    fps              | 286      |
|    time_elapsed     | 399      |
|    total_timesteps  | 114319   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.136    |
|    n_updates        | 18579    |
----------------------------------
Eval num_timesteps=114500, episode_reward=180.68 +/- 53.23
Episode length: 45.56 +/- 13.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.683    |
| time/               |          |
|    total_timesteps  | 114500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0134   |
|    n_updates        | 18624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.2     |
|    ep_rew_mean      | 299      |
|    exploration_rate | 0.682    |
| time/               |          |
|    episodes         | 1540     |
|    fps              | 285      |
|    time_elapsed     | 401      |
|    total_timesteps  | 114604   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.442    |
|    n_updates        | 18650    |
----------------------------------
Eval num_timesteps=115000, episode_reward=170.44 +/- 38.73
Episode length: 42.94 +/- 9.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.679    |
| time/               |          |
|    total_timesteps  | 115000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0883   |
|    n_updates        | 18749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.678    |
| time/               |          |
|    episodes         | 1544     |
|    fps              | 285      |
|    time_elapsed     | 402      |
|    total_timesteps  | 115140   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.096    |
|    n_updates        | 18784    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.677    |
| time/               |          |
|    episodes         | 1548     |
|    fps              | 286      |
|    time_elapsed     | 403      |
|    total_timesteps  | 115425   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.823    |
|    n_updates        | 18856    |
----------------------------------
Eval num_timesteps=115500, episode_reward=166.16 +/- 57.32
Episode length: 41.92 +/- 14.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.9     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.676    |
| time/               |          |
|    total_timesteps  | 115500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.634    |
|    n_updates        | 18874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.674    |
| time/               |          |
|    episodes         | 1552     |
|    fps              | 286      |
|    time_elapsed     | 404      |
|    total_timesteps  | 115871   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0162   |
|    n_updates        | 18967    |
----------------------------------
Eval num_timesteps=116000, episode_reward=164.08 +/- 42.08
Episode length: 41.38 +/- 10.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.673    |
| time/               |          |
|    total_timesteps  | 116000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.695    |
|    n_updates        | 18999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.672    |
| time/               |          |
|    episodes         | 1556     |
|    fps              | 285      |
|    time_elapsed     | 406      |
|    total_timesteps  | 116205   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.469    |
|    n_updates        | 19051    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.671    |
| time/               |          |
|    episodes         | 1560     |
|    fps              | 286      |
|    time_elapsed     | 406      |
|    total_timesteps  | 116370   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0235   |
|    n_updates        | 19092    |
----------------------------------
Eval num_timesteps=116500, episode_reward=221.74 +/- 70.95
Episode length: 55.86 +/- 17.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 55.9     |
|    mean_reward      | 222      |
| rollout/            |          |
|    exploration_rate | 0.67     |
| time/               |          |
|    total_timesteps  | 116500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.441    |
|    n_updates        | 19124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.4     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.669    |
| time/               |          |
|    episodes         | 1564     |
|    fps              | 285      |
|    time_elapsed     | 408      |
|    total_timesteps  | 116556   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.44     |
|    n_updates        | 19138    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.5     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.667    |
| time/               |          |
|    episodes         | 1568     |
|    fps              | 285      |
|    time_elapsed     | 408      |
|    total_timesteps  | 116848   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0567   |
|    n_updates        | 19211    |
----------------------------------
Eval num_timesteps=117000, episode_reward=169.72 +/- 47.01
Episode length: 42.76 +/- 11.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.666    |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.459    |
|    n_updates        | 19249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.3     |
|    ep_rew_mean      | 316      |
|    exploration_rate | 0.665    |
| time/               |          |
|    episodes         | 1572     |
|    fps              | 285      |
|    time_elapsed     | 410      |
|    total_timesteps  | 117210   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.014    |
|    n_updates        | 19302    |
----------------------------------
Eval num_timesteps=117500, episode_reward=175.38 +/- 50.15
Episode length: 44.20 +/- 12.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.663    |
| time/               |          |
|    total_timesteps  | 117500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.424    |
|    n_updates        | 19374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.663    |
| time/               |          |
|    episodes         | 1576     |
|    fps              | 285      |
|    time_elapsed     | 412      |
|    total_timesteps  | 117605   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22     |
|    n_updates        | 19401    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.66     |
| time/               |          |
|    episodes         | 1580     |
|    fps              | 285      |
|    time_elapsed     | 412      |
|    total_timesteps  | 117993   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.876    |
|    n_updates        | 19498    |
----------------------------------
Eval num_timesteps=118000, episode_reward=175.46 +/- 44.69
Episode length: 44.26 +/- 11.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.66     |
| time/               |          |
|    total_timesteps  | 118000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.454    |
|    n_updates        | 19499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.2     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.658    |
| time/               |          |
|    episodes         | 1584     |
|    fps              | 285      |
|    time_elapsed     | 414      |
|    total_timesteps  | 118283   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.17     |
|    n_updates        | 19570    |
----------------------------------
Eval num_timesteps=118500, episode_reward=169.96 +/- 38.79
Episode length: 42.80 +/- 9.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.657    |
| time/               |          |
|    total_timesteps  | 118500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00863  |
|    n_updates        | 19624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.9     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.656    |
| time/               |          |
|    episodes         | 1588     |
|    fps              | 285      |
|    time_elapsed     | 416      |
|    total_timesteps  | 118636   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.42     |
|    n_updates        | 19658    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 315      |
|    exploration_rate | 0.654    |
| time/               |          |
|    episodes         | 1592     |
|    fps              | 285      |
|    time_elapsed     | 416      |
|    total_timesteps  | 118932   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0159   |
|    n_updates        | 19732    |
----------------------------------
Eval num_timesteps=119000, episode_reward=162.30 +/- 54.21
Episode length: 40.96 +/- 13.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41       |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.653    |
| time/               |          |
|    total_timesteps  | 119000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0104   |
|    n_updates        | 19749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.9     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.652    |
| time/               |          |
|    episodes         | 1596     |
|    fps              | 285      |
|    time_elapsed     | 417      |
|    total_timesteps  | 119210   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.105    |
|    n_updates        | 19802    |
----------------------------------
Eval num_timesteps=119500, episode_reward=164.16 +/- 51.47
Episode length: 41.40 +/- 12.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.65     |
| time/               |          |
|    total_timesteps  | 119500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0462   |
|    n_updates        | 19874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.9     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.65     |
| time/               |          |
|    episodes         | 1600     |
|    fps              | 285      |
|    time_elapsed     | 419      |
|    total_timesteps  | 119559   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.428    |
|    n_updates        | 19889    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.7     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.647    |
| time/               |          |
|    episodes         | 1604     |
|    fps              | 285      |
|    time_elapsed     | 419      |
|    total_timesteps  | 119963   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00525  |
|    n_updates        | 19990    |
----------------------------------
Eval num_timesteps=120000, episode_reward=171.16 +/- 46.60
Episode length: 43.16 +/- 11.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.647    |
| time/               |          |
|    total_timesteps  | 120000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00589  |
|    n_updates        | 19999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 324      |
|    exploration_rate | 0.644    |
| time/               |          |
|    episodes         | 1608     |
|    fps              | 285      |
|    time_elapsed     | 421      |
|    total_timesteps  | 120386   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.517    |
|    n_updates        | 20096    |
----------------------------------
Eval num_timesteps=120500, episode_reward=173.52 +/- 46.85
Episode length: 43.78 +/- 11.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.643    |
| time/               |          |
|    total_timesteps  | 120500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0316   |
|    n_updates        | 20124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.7     |
|    ep_rew_mean      | 333      |
|    exploration_rate | 0.641    |
| time/               |          |
|    episodes         | 1612     |
|    fps              | 285      |
|    time_elapsed     | 423      |
|    total_timesteps  | 120862   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0203   |
|    n_updates        | 20215    |
----------------------------------
Eval num_timesteps=121000, episode_reward=198.64 +/- 58.17
Episode length: 50.06 +/- 14.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.1     |
|    mean_reward      | 199      |
| rollout/            |          |
|    exploration_rate | 0.64     |
| time/               |          |
|    total_timesteps  | 121000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.51     |
|    n_updates        | 20249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.64     |
| time/               |          |
|    episodes         | 1616     |
|    fps              | 284      |
|    time_elapsed     | 425      |
|    total_timesteps  | 121075   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.478    |
|    n_updates        | 20268    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.638    |
| time/               |          |
|    episodes         | 1620     |
|    fps              | 285      |
|    time_elapsed     | 425      |
|    total_timesteps  | 121346   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.496    |
|    n_updates        | 20336    |
----------------------------------
Eval num_timesteps=121500, episode_reward=187.50 +/- 63.99
Episode length: 47.32 +/- 15.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.637    |
| time/               |          |
|    total_timesteps  | 121500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0357   |
|    n_updates        | 20374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.635    |
| time/               |          |
|    episodes         | 1624     |
|    fps              | 284      |
|    time_elapsed     | 427      |
|    total_timesteps  | 121752   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0528   |
|    n_updates        | 20437    |
----------------------------------
Eval num_timesteps=122000, episode_reward=171.82 +/- 43.79
Episode length: 43.30 +/- 10.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.633    |
| time/               |          |
|    total_timesteps  | 122000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.423    |
|    n_updates        | 20499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.9     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.631    |
| time/               |          |
|    episodes         | 1628     |
|    fps              | 284      |
|    time_elapsed     | 429      |
|    total_timesteps  | 122298   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.122    |
|    n_updates        | 20574    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.63     |
| time/               |          |
|    episodes         | 1632     |
|    fps              | 285      |
|    time_elapsed     | 429      |
|    total_timesteps  | 122485   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.543    |
|    n_updates        | 20621    |
----------------------------------
Eval num_timesteps=122500, episode_reward=137.12 +/- 38.21
Episode length: 34.62 +/- 9.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 34.6     |
|    mean_reward      | 137      |
| rollout/            |          |
|    exploration_rate | 0.63     |
| time/               |          |
|    total_timesteps  | 122500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.396    |
|    n_updates        | 20624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.1     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.628    |
| time/               |          |
|    episodes         | 1636     |
|    fps              | 285      |
|    time_elapsed     | 430      |
|    total_timesteps  | 122825   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.586    |
|    n_updates        | 20706    |
----------------------------------
Eval num_timesteps=123000, episode_reward=164.70 +/- 40.31
Episode length: 41.54 +/- 10.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.627    |
| time/               |          |
|    total_timesteps  | 123000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0131   |
|    n_updates        | 20749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.4     |
|    ep_rew_mean      | 340      |
|    exploration_rate | 0.626    |
| time/               |          |
|    episodes         | 1640     |
|    fps              | 284      |
|    time_elapsed     | 432      |
|    total_timesteps  | 123141   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.979    |
|    n_updates        | 20785    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.624    |
| time/               |          |
|    episodes         | 1644     |
|    fps              | 285      |
|    time_elapsed     | 432      |
|    total_timesteps  | 123443   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.377    |
|    n_updates        | 20860    |
----------------------------------
Eval num_timesteps=123500, episode_reward=182.26 +/- 47.05
Episode length: 45.98 +/- 11.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.623    |
| time/               |          |
|    total_timesteps  | 123500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.25     |
|    n_updates        | 20874    |
----------------------------------
Eval num_timesteps=124000, episode_reward=183.32 +/- 57.59
Episode length: 46.20 +/- 14.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.62     |
| time/               |          |
|    total_timesteps  | 124000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.905    |
|    n_updates        | 20999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.8     |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.62     |
| time/               |          |
|    episodes         | 1648     |
|    fps              | 284      |
|    time_elapsed     | 436      |
|    total_timesteps  | 124004   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.524    |
|    n_updates        | 21000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.9     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.618    |
| time/               |          |
|    episodes         | 1652     |
|    fps              | 284      |
|    time_elapsed     | 436      |
|    total_timesteps  | 124359   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.241    |
|    n_updates        | 21089    |
----------------------------------
Eval num_timesteps=124500, episode_reward=176.34 +/- 43.76
Episode length: 44.46 +/- 10.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.617    |
| time/               |          |
|    total_timesteps  | 124500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0133   |
|    n_updates        | 21124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.616    |
| time/               |          |
|    episodes         | 1656     |
|    fps              | 284      |
|    time_elapsed     | 438      |
|    total_timesteps  | 124640   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0104   |
|    n_updates        | 21159    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.8     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.614    |
| time/               |          |
|    episodes         | 1660     |
|    fps              | 285      |
|    time_elapsed     | 438      |
|    total_timesteps  | 124948   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.487    |
|    n_updates        | 21236    |
----------------------------------
Eval num_timesteps=125000, episode_reward=181.44 +/- 56.25
Episode length: 45.78 +/- 14.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.613    |
| time/               |          |
|    total_timesteps  | 125000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.548    |
|    n_updates        | 21249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.3     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.611    |
| time/               |          |
|    episodes         | 1664     |
|    fps              | 284      |
|    time_elapsed     | 440      |
|    total_timesteps  | 125282   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0255   |
|    n_updates        | 21320    |
----------------------------------
Eval num_timesteps=125500, episode_reward=157.16 +/- 46.30
Episode length: 39.68 +/- 11.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.7     |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.61     |
| time/               |          |
|    total_timesteps  | 125500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.47     |
|    n_updates        | 21374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.5     |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.608    |
| time/               |          |
|    episodes         | 1668     |
|    fps              | 284      |
|    time_elapsed     | 441      |
|    total_timesteps  | 125699   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.537    |
|    n_updates        | 21424    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.5     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.607    |
| time/               |          |
|    episodes         | 1672     |
|    fps              | 284      |
|    time_elapsed     | 442      |
|    total_timesteps  | 125964   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.269    |
|    n_updates        | 21490    |
----------------------------------
Eval num_timesteps=126000, episode_reward=173.98 +/- 42.89
Episode length: 43.88 +/- 10.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.606    |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0565   |
|    n_updates        | 21499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.7     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.604    |
| time/               |          |
|    episodes         | 1676     |
|    fps              | 284      |
|    time_elapsed     | 443      |
|    total_timesteps  | 126270   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.792    |
|    n_updates        | 21567    |
----------------------------------
Eval num_timesteps=126500, episode_reward=178.20 +/- 46.50
Episode length: 44.90 +/- 11.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.603    |
| time/               |          |
|    total_timesteps  | 126500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0337   |
|    n_updates        | 21624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.8     |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.602    |
| time/               |          |
|    episodes         | 1680     |
|    fps              | 284      |
|    time_elapsed     | 445      |
|    total_timesteps  | 126570   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.47     |
|    n_updates        | 21642    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.4     |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.6      |
| time/               |          |
|    episodes         | 1684     |
|    fps              | 284      |
|    time_elapsed     | 445      |
|    total_timesteps  | 126925   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.249    |
|    n_updates        | 21731    |
----------------------------------
Eval num_timesteps=127000, episode_reward=165.48 +/- 57.17
Episode length: 41.72 +/- 14.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.7     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.599    |
| time/               |          |
|    total_timesteps  | 127000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0548   |
|    n_updates        | 21749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.7     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.598    |
| time/               |          |
|    episodes         | 1688     |
|    fps              | 284      |
|    time_elapsed     | 447      |
|    total_timesteps  | 127208   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.496    |
|    n_updates        | 21801    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.596    |
| time/               |          |
|    episodes         | 1692     |
|    fps              | 284      |
|    time_elapsed     | 447      |
|    total_timesteps  | 127449   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.023    |
|    n_updates        | 21862    |
----------------------------------
Eval num_timesteps=127500, episode_reward=170.30 +/- 44.51
Episode length: 42.98 +/- 11.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.596    |
| time/               |          |
|    total_timesteps  | 127500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0301   |
|    n_updates        | 21874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.6     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.594    |
| time/               |          |
|    episodes         | 1696     |
|    fps              | 284      |
|    time_elapsed     | 449      |
|    total_timesteps  | 127770   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.528    |
|    n_updates        | 21942    |
----------------------------------
Eval num_timesteps=128000, episode_reward=193.24 +/- 62.15
Episode length: 48.66 +/- 15.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.593    |
| time/               |          |
|    total_timesteps  | 128000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.203    |
|    n_updates        | 21999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.5     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.592    |
| time/               |          |
|    episodes         | 1700     |
|    fps              | 283      |
|    time_elapsed     | 450      |
|    total_timesteps  | 128008   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.971    |
|    n_updates        | 22001    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 327      |
|    exploration_rate | 0.591    |
| time/               |          |
|    episodes         | 1704     |
|    fps              | 284      |
|    time_elapsed     | 451      |
|    total_timesteps  | 128188   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0628   |
|    n_updates        | 22046    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.589    |
| time/               |          |
|    episodes         | 1708     |
|    fps              | 284      |
|    time_elapsed     | 451      |
|    total_timesteps  | 128435   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.119    |
|    n_updates        | 22108    |
----------------------------------
Eval num_timesteps=128500, episode_reward=174.80 +/- 48.01
Episode length: 44.06 +/- 11.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.589    |
| time/               |          |
|    total_timesteps  | 128500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.115    |
|    n_updates        | 22124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.1     |
|    ep_rew_mean      | 311      |
|    exploration_rate | 0.588    |
| time/               |          |
|    episodes         | 1712     |
|    fps              | 284      |
|    time_elapsed     | 452      |
|    total_timesteps  | 128673   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.344    |
|    n_updates        | 22168    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.8     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.586    |
| time/               |          |
|    episodes         | 1716     |
|    fps              | 284      |
|    time_elapsed     | 453      |
|    total_timesteps  | 128954   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.391    |
|    n_updates        | 22238    |
----------------------------------
Eval num_timesteps=129000, episode_reward=181.78 +/- 39.97
Episode length: 45.82 +/- 10.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.586    |
| time/               |          |
|    total_timesteps  | 129000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.14     |
|    n_updates        | 22249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 319      |
|    exploration_rate | 0.583    |
| time/               |          |
|    episodes         | 1720     |
|    fps              | 284      |
|    time_elapsed     | 455      |
|    total_timesteps  | 129364   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.509    |
|    n_updates        | 22340    |
----------------------------------
Eval num_timesteps=129500, episode_reward=260.78 +/- 142.28
Episode length: 65.62 +/- 35.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 65.6     |
|    mean_reward      | 261      |
| rollout/            |          |
|    exploration_rate | 0.582    |
| time/               |          |
|    total_timesteps  | 129500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.108    |
|    n_updates        | 22374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.9     |
|    ep_rew_mean      | 318      |
|    exploration_rate | 0.58     |
| time/               |          |
|    episodes         | 1724     |
|    fps              | 283      |
|    time_elapsed     | 457      |
|    total_timesteps  | 129739   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00334  |
|    n_updates        | 22434    |
----------------------------------
Eval num_timesteps=130000, episode_reward=170.68 +/- 44.29
Episode length: 43.00 +/- 11.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.579    |
| time/               |          |
|    total_timesteps  | 130000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00823  |
|    n_updates        | 22499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.5     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.578    |
| time/               |          |
|    episodes         | 1728     |
|    fps              | 283      |
|    time_elapsed     | 459      |
|    total_timesteps  | 130045   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0769   |
|    n_updates        | 22511    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.7     |
|    ep_rew_mean      | 313      |
|    exploration_rate | 0.576    |
| time/               |          |
|    episodes         | 1732     |
|    fps              | 283      |
|    time_elapsed     | 459      |
|    total_timesteps  | 130354   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0475   |
|    n_updates        | 22588    |
----------------------------------
Eval num_timesteps=130500, episode_reward=185.64 +/- 49.02
Episode length: 46.78 +/- 12.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.575    |
| time/               |          |
|    total_timesteps  | 130500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.66     |
|    n_updates        | 22624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.574    |
| time/               |          |
|    episodes         | 1736     |
|    fps              | 283      |
|    time_elapsed     | 461      |
|    total_timesteps  | 130628   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86     |
|    n_updates        | 22656    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78       |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.572    |
| time/               |          |
|    episodes         | 1740     |
|    fps              | 283      |
|    time_elapsed     | 461      |
|    total_timesteps  | 130937   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00623  |
|    n_updates        | 22734    |
----------------------------------
Eval num_timesteps=131000, episode_reward=172.62 +/- 50.50
Episode length: 43.52 +/- 12.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.572    |
| time/               |          |
|    total_timesteps  | 131000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0415   |
|    n_updates        | 22749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.8     |
|    ep_rew_mean      | 314      |
|    exploration_rate | 0.569    |
| time/               |          |
|    episodes         | 1744     |
|    fps              | 283      |
|    time_elapsed     | 463      |
|    total_timesteps  | 131328   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0131   |
|    n_updates        | 22831    |
----------------------------------
Eval num_timesteps=131500, episode_reward=179.02 +/- 45.42
Episode length: 45.16 +/- 11.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.568    |
| time/               |          |
|    total_timesteps  | 131500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.21     |
|    n_updates        | 22874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.9     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.567    |
| time/               |          |
|    episodes         | 1748     |
|    fps              | 283      |
|    time_elapsed     | 464      |
|    total_timesteps  | 131590   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.18     |
|    n_updates        | 22897    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.5     |
|    ep_rew_mean      | 300      |
|    exploration_rate | 0.565    |
| time/               |          |
|    episodes         | 1752     |
|    fps              | 283      |
|    time_elapsed     | 465      |
|    total_timesteps  | 131906   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13     |
|    n_updates        | 22976    |
----------------------------------
Eval num_timesteps=132000, episode_reward=169.98 +/- 32.99
Episode length: 42.84 +/- 8.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.564    |
| time/               |          |
|    total_timesteps  | 132000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.927    |
|    n_updates        | 22999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 75.8     |
|    ep_rew_mean      | 302      |
|    exploration_rate | 0.563    |
| time/               |          |
|    episodes         | 1756     |
|    fps              | 283      |
|    time_elapsed     | 466      |
|    total_timesteps  | 132215   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.9      |
|    n_updates        | 23053    |
----------------------------------
Eval num_timesteps=132500, episode_reward=171.68 +/- 32.55
Episode length: 43.28 +/- 8.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.561    |
| time/               |          |
|    total_timesteps  | 132500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.1      |
|    n_updates        | 23124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.3     |
|    ep_rew_mean      | 308      |
|    exploration_rate | 0.56     |
| time/               |          |
|    episodes         | 1760     |
|    fps              | 283      |
|    time_elapsed     | 468      |
|    total_timesteps  | 132676   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.716    |
|    n_updates        | 23168    |
----------------------------------
Eval num_timesteps=133000, episode_reward=171.32 +/- 41.79
Episode length: 43.16 +/- 10.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.557    |
| time/               |          |
|    total_timesteps  | 133000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.185    |
|    n_updates        | 23249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 78.3     |
|    ep_rew_mean      | 312      |
|    exploration_rate | 0.556    |
| time/               |          |
|    episodes         | 1764     |
|    fps              | 282      |
|    time_elapsed     | 470      |
|    total_timesteps  | 133115   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.308    |
|    n_updates        | 23278    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.555    |
| time/               |          |
|    episodes         | 1768     |
|    fps              | 283      |
|    time_elapsed     | 470      |
|    total_timesteps  | 133385   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.986    |
|    n_updates        | 23346    |
----------------------------------
Eval num_timesteps=133500, episode_reward=171.62 +/- 54.65
Episode length: 43.32 +/- 13.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.554    |
| time/               |          |
|    total_timesteps  | 133500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56     |
|    n_updates        | 23374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.2     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.552    |
| time/               |          |
|    episodes         | 1772     |
|    fps              | 283      |
|    time_elapsed     | 472      |
|    total_timesteps  | 133683   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.966    |
|    n_updates        | 23420    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.1     |
|    ep_rew_mean      | 303      |
|    exploration_rate | 0.551    |
| time/               |          |
|    episodes         | 1776     |
|    fps              | 283      |
|    time_elapsed     | 472      |
|    total_timesteps  | 133880   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.49     |
|    n_updates        | 23469    |
----------------------------------
Eval num_timesteps=134000, episode_reward=170.24 +/- 40.93
Episode length: 42.96 +/- 10.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.55     |
| time/               |          |
|    total_timesteps  | 134000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.578    |
|    n_updates        | 23499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.1     |
|    ep_rew_mean      | 307      |
|    exploration_rate | 0.548    |
| time/               |          |
|    episodes         | 1780     |
|    fps              | 283      |
|    time_elapsed     | 474      |
|    total_timesteps  | 134276   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0104   |
|    n_updates        | 23568    |
----------------------------------
Eval num_timesteps=134500, episode_reward=183.24 +/- 55.67
Episode length: 46.16 +/- 14.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.547    |
| time/               |          |
|    total_timesteps  | 134500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0954   |
|    n_updates        | 23624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 76.9     |
|    ep_rew_mean      | 306      |
|    exploration_rate | 0.546    |
| time/               |          |
|    episodes         | 1784     |
|    fps              | 282      |
|    time_elapsed     | 476      |
|    total_timesteps  | 134619   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.576    |
|    n_updates        | 23654    |
----------------------------------
Eval num_timesteps=135000, episode_reward=167.36 +/- 61.28
Episode length: 42.22 +/- 15.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.543    |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.556    |
|    n_updates        | 23749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 77.9     |
|    ep_rew_mean      | 310      |
|    exploration_rate | 0.543    |
| time/               |          |
|    episodes         | 1788     |
|    fps              | 282      |
|    time_elapsed     | 477      |
|    total_timesteps  | 135001   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.55     |
|    n_updates        | 23750    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.2     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.54     |
| time/               |          |
|    episodes         | 1792     |
|    fps              | 283      |
|    time_elapsed     | 478      |
|    total_timesteps  | 135472   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0239   |
|    n_updates        | 23867    |
----------------------------------
Eval num_timesteps=135500, episode_reward=173.18 +/- 64.59
Episode length: 43.70 +/- 16.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.539    |
| time/               |          |
|    total_timesteps  | 135500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0821   |
|    n_updates        | 23874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.536    |
| time/               |          |
|    episodes         | 1796     |
|    fps              | 283      |
|    time_elapsed     | 480      |
|    total_timesteps  | 135940   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.212    |
|    n_updates        | 23984    |
----------------------------------
Eval num_timesteps=136000, episode_reward=172.26 +/- 44.94
Episode length: 43.46 +/- 11.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.536    |
| time/               |          |
|    total_timesteps  | 136000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00995  |
|    n_updates        | 23999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 329      |
|    exploration_rate | 0.534    |
| time/               |          |
|    episodes         | 1800     |
|    fps              | 282      |
|    time_elapsed     | 481      |
|    total_timesteps  | 136258   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.51     |
|    n_updates        | 24064    |
----------------------------------
Eval num_timesteps=136500, episode_reward=178.04 +/- 46.35
Episode length: 44.92 +/- 11.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.532    |
| time/               |          |
|    total_timesteps  | 136500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0607   |
|    n_updates        | 24124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.531    |
| time/               |          |
|    episodes         | 1804     |
|    fps              | 282      |
|    time_elapsed     | 483      |
|    total_timesteps  | 136611   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.8      |
|    n_updates        | 24152    |
----------------------------------
Eval num_timesteps=137000, episode_reward=192.34 +/- 58.93
Episode length: 48.46 +/- 14.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.5     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.528    |
| time/               |          |
|    total_timesteps  | 137000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.047    |
|    n_updates        | 24249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.8     |
|    ep_rew_mean      | 342      |
|    exploration_rate | 0.528    |
| time/               |          |
|    episodes         | 1808     |
|    fps              | 282      |
|    time_elapsed     | 485      |
|    total_timesteps  | 137020   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.57     |
|    n_updates        | 24254    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.1     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.526    |
| time/               |          |
|    episodes         | 1812     |
|    fps              | 282      |
|    time_elapsed     | 485      |
|    total_timesteps  | 137380   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.547    |
|    n_updates        | 24344    |
----------------------------------
Eval num_timesteps=137500, episode_reward=169.28 +/- 62.36
Episode length: 42.66 +/- 15.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.525    |
| time/               |          |
|    total_timesteps  | 137500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.203    |
|    n_updates        | 24374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.8     |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.523    |
| time/               |          |
|    episodes         | 1816     |
|    fps              | 282      |
|    time_elapsed     | 487      |
|    total_timesteps  | 137737   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.029    |
|    n_updates        | 24434    |
----------------------------------
Eval num_timesteps=138000, episode_reward=188.70 +/- 43.00
Episode length: 47.50 +/- 10.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.521    |
| time/               |          |
|    total_timesteps  | 138000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.549    |
|    n_updates        | 24499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.9     |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.52     |
| time/               |          |
|    episodes         | 1820     |
|    fps              | 282      |
|    time_elapsed     | 489      |
|    total_timesteps  | 138154   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0258   |
|    n_updates        | 24538    |
----------------------------------
Eval num_timesteps=138500, episode_reward=188.20 +/- 53.31
Episode length: 47.46 +/- 13.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.517    |
| time/               |          |
|    total_timesteps  | 138500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03     |
|    n_updates        | 24624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.9     |
|    ep_rew_mean      | 358      |
|    exploration_rate | 0.516    |
| time/               |          |
|    episodes         | 1824     |
|    fps              | 282      |
|    time_elapsed     | 491      |
|    total_timesteps  | 138727   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.214    |
|    n_updates        | 24681    |
----------------------------------
Eval num_timesteps=139000, episode_reward=170.18 +/- 46.07
Episode length: 42.92 +/- 11.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.514    |
| time/               |          |
|    total_timesteps  | 139000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.016    |
|    n_updates        | 24749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.5     |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.512    |
| time/               |          |
|    episodes         | 1828     |
|    fps              | 282      |
|    time_elapsed     | 493      |
|    total_timesteps  | 139300   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0318   |
|    n_updates        | 24824    |
----------------------------------
Eval num_timesteps=139500, episode_reward=174.16 +/- 67.44
Episode length: 43.90 +/- 16.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.51     |
| time/               |          |
|    total_timesteps  | 139500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.807    |
|    n_updates        | 24874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.2     |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.509    |
| time/               |          |
|    episodes         | 1832     |
|    fps              | 282      |
|    time_elapsed     | 495      |
|    total_timesteps  | 139671   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.037    |
|    n_updates        | 24917    |
----------------------------------
Eval num_timesteps=140000, episode_reward=193.06 +/- 58.11
Episode length: 48.66 +/- 14.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.506    |
| time/               |          |
|    total_timesteps  | 140000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.541    |
|    n_updates        | 24999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.7     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.506    |
| time/               |          |
|    episodes         | 1836     |
|    fps              | 281      |
|    time_elapsed     | 497      |
|    total_timesteps  | 140002   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.77     |
|    n_updates        | 25000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.504    |
| time/               |          |
|    episodes         | 1840     |
|    fps              | 282      |
|    time_elapsed     | 497      |
|    total_timesteps  | 140332   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0471   |
|    n_updates        | 25082    |
----------------------------------
Eval num_timesteps=140500, episode_reward=177.22 +/- 69.59
Episode length: 44.68 +/- 17.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.503    |
| time/               |          |
|    total_timesteps  | 140500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0782   |
|    n_updates        | 25124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.8     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.5      |
| time/               |          |
|    episodes         | 1844     |
|    fps              | 282      |
|    time_elapsed     | 499      |
|    total_timesteps  | 140812   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.62     |
|    n_updates        | 25202    |
----------------------------------
Eval num_timesteps=141000, episode_reward=199.16 +/- 106.95
Episode length: 50.16 +/- 26.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.2     |
|    mean_reward      | 199      |
| rollout/            |          |
|    exploration_rate | 0.499    |
| time/               |          |
|    total_timesteps  | 141000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.466    |
|    n_updates        | 25249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.497    |
| time/               |          |
|    episodes         | 1848     |
|    fps              | 281      |
|    time_elapsed     | 501      |
|    total_timesteps  | 141272   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00659  |
|    n_updates        | 25317    |
----------------------------------
Eval num_timesteps=141500, episode_reward=175.64 +/- 45.49
Episode length: 44.28 +/- 11.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.495    |
| time/               |          |
|    total_timesteps  | 141500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.721    |
|    n_updates        | 25374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.493    |
| time/               |          |
|    episodes         | 1852     |
|    fps              | 281      |
|    time_elapsed     | 503      |
|    total_timesteps  | 141735   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.146    |
|    n_updates        | 25433    |
----------------------------------
Eval num_timesteps=142000, episode_reward=250.04 +/- 123.67
Episode length: 62.90 +/- 30.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 62.9     |
|    mean_reward      | 250      |
| rollout/            |          |
|    exploration_rate | 0.492    |
| time/               |          |
|    total_timesteps  | 142000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.55     |
|    n_updates        | 25499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.49     |
| time/               |          |
|    episodes         | 1856     |
|    fps              | 281      |
|    time_elapsed     | 505      |
|    total_timesteps  | 142249   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0424   |
|    n_updates        | 25562    |
----------------------------------
Eval num_timesteps=142500, episode_reward=177.06 +/- 47.25
Episode length: 44.64 +/- 11.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.488    |
| time/               |          |
|    total_timesteps  | 142500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0386   |
|    n_updates        | 25624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.2     |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.488    |
| time/               |          |
|    episodes         | 1860     |
|    fps              | 280      |
|    time_elapsed     | 507      |
|    total_timesteps  | 142500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.4     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.486    |
| time/               |          |
|    episodes         | 1864     |
|    fps              | 281      |
|    time_elapsed     | 507      |
|    total_timesteps  | 142755   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.529    |
|    n_updates        | 25688    |
----------------------------------
Eval num_timesteps=143000, episode_reward=171.00 +/- 62.85
Episode length: 43.12 +/- 15.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.484    |
| time/               |          |
|    total_timesteps  | 143000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.639    |
|    n_updates        | 25749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.482    |
| time/               |          |
|    episodes         | 1868     |
|    fps              | 281      |
|    time_elapsed     | 509      |
|    total_timesteps  | 143236   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.573    |
|    n_updates        | 25808    |
----------------------------------
Eval num_timesteps=143500, episode_reward=170.14 +/- 41.98
Episode length: 42.96 +/- 10.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.48     |
| time/               |          |
|    total_timesteps  | 143500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.012    |
|    n_updates        | 25874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.478    |
| time/               |          |
|    episodes         | 1872     |
|    fps              | 281      |
|    time_elapsed     | 511      |
|    total_timesteps  | 143753   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.695    |
|    n_updates        | 25938    |
----------------------------------
Eval num_timesteps=144000, episode_reward=175.86 +/- 44.57
Episode length: 44.40 +/- 11.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.476    |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.411    |
|    n_updates        | 25999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.476    |
| time/               |          |
|    episodes         | 1876     |
|    fps              | 280      |
|    time_elapsed     | 513      |
|    total_timesteps  | 144080   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0152   |
|    n_updates        | 26019    |
----------------------------------
Eval num_timesteps=144500, episode_reward=176.92 +/- 61.31
Episode length: 44.58 +/- 15.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.473    |
| time/               |          |
|    total_timesteps  | 144500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01     |
|    n_updates        | 26124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.473    |
| time/               |          |
|    episodes         | 1880     |
|    fps              | 280      |
|    time_elapsed     | 514      |
|    total_timesteps  | 144513   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.406    |
|    n_updates        | 26128    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.47     |
| time/               |          |
|    episodes         | 1884     |
|    fps              | 281      |
|    time_elapsed     | 515      |
|    total_timesteps  | 144922   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.24     |
|    n_updates        | 26230    |
----------------------------------
Eval num_timesteps=145000, episode_reward=180.26 +/- 54.72
Episode length: 45.42 +/- 13.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.469    |
| time/               |          |
|    total_timesteps  | 145000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0129   |
|    n_updates        | 26249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 417      |
|    exploration_rate | 0.466    |
| time/               |          |
|    episodes         | 1888     |
|    fps              | 281      |
|    time_elapsed     | 517      |
|    total_timesteps  | 145447   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.616    |
|    n_updates        | 26361    |
----------------------------------
Eval num_timesteps=145500, episode_reward=160.80 +/- 59.04
Episode length: 40.54 +/- 14.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.5     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.465    |
| time/               |          |
|    total_timesteps  | 145500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.997    |
|    n_updates        | 26374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.464    |
| time/               |          |
|    episodes         | 1892     |
|    fps              | 280      |
|    time_elapsed     | 518      |
|    total_timesteps  | 145678   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0165   |
|    n_updates        | 26419    |
----------------------------------
Eval num_timesteps=146000, episode_reward=172.72 +/- 52.00
Episode length: 43.50 +/- 13.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.461    |
| time/               |          |
|    total_timesteps  | 146000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00886  |
|    n_updates        | 26499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.461    |
| time/               |          |
|    episodes         | 1896     |
|    fps              | 280      |
|    time_elapsed     | 520      |
|    total_timesteps  | 146051   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.69     |
|    n_updates        | 26512    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.458    |
| time/               |          |
|    episodes         | 1900     |
|    fps              | 281      |
|    time_elapsed     | 520      |
|    total_timesteps  | 146426   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15     |
|    n_updates        | 26606    |
----------------------------------
Eval num_timesteps=146500, episode_reward=161.00 +/- 46.05
Episode length: 40.60 +/- 11.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.6     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.457    |
| time/               |          |
|    total_timesteps  | 146500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.512    |
|    n_updates        | 26624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.456    |
| time/               |          |
|    episodes         | 1904     |
|    fps              | 280      |
|    time_elapsed     | 522      |
|    total_timesteps  | 146754   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.897    |
|    n_updates        | 26688    |
----------------------------------
Eval num_timesteps=147000, episode_reward=186.80 +/- 52.89
Episode length: 47.14 +/- 13.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.1     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.454    |
| time/               |          |
|    total_timesteps  | 147000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.011    |
|    n_updates        | 26749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.453    |
| time/               |          |
|    episodes         | 1908     |
|    fps              | 280      |
|    time_elapsed     | 524      |
|    total_timesteps  | 147060   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.65     |
|    n_updates        | 26764    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.45     |
| time/               |          |
|    episodes         | 1912     |
|    fps              | 281      |
|    time_elapsed     | 524      |
|    total_timesteps  | 147425   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.255    |
|    n_updates        | 26856    |
----------------------------------
Eval num_timesteps=147500, episode_reward=146.72 +/- 47.46
Episode length: 37.02 +/- 11.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37       |
|    mean_reward      | 147      |
| rollout/            |          |
|    exploration_rate | 0.45     |
| time/               |          |
|    total_timesteps  | 147500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.544    |
|    n_updates        | 26874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.447    |
| time/               |          |
|    episodes         | 1916     |
|    fps              | 281      |
|    time_elapsed     | 526      |
|    total_timesteps  | 147908   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.402    |
|    n_updates        | 26976    |
----------------------------------
Eval num_timesteps=148000, episode_reward=155.48 +/- 45.96
Episode length: 39.24 +/- 11.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.2     |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.446    |
| time/               |          |
|    total_timesteps  | 148000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0348   |
|    n_updates        | 26999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.443    |
| time/               |          |
|    episodes         | 1920     |
|    fps              | 280      |
|    time_elapsed     | 527      |
|    total_timesteps  | 148332   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.8      |
|    n_updates        | 27082    |
----------------------------------
Eval num_timesteps=148500, episode_reward=183.50 +/- 52.27
Episode length: 46.24 +/- 13.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.442    |
| time/               |          |
|    total_timesteps  | 148500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.526    |
|    n_updates        | 27124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.44     |
| time/               |          |
|    episodes         | 1924     |
|    fps              | 280      |
|    time_elapsed     | 529      |
|    total_timesteps  | 148798   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0068   |
|    n_updates        | 27199    |
----------------------------------
Eval num_timesteps=149000, episode_reward=184.60 +/- 49.58
Episode length: 46.52 +/- 12.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.438    |
| time/               |          |
|    total_timesteps  | 149000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.698    |
|    n_updates        | 27249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.3     |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.436    |
| time/               |          |
|    episodes         | 1928     |
|    fps              | 280      |
|    time_elapsed     | 531      |
|    total_timesteps  | 149233   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22     |
|    n_updates        | 27308    |
----------------------------------
Eval num_timesteps=149500, episode_reward=163.04 +/- 58.83
Episode length: 41.18 +/- 14.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.2     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.434    |
| time/               |          |
|    total_timesteps  | 149500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.674    |
|    n_updates        | 27374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.6     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.434    |
| time/               |          |
|    episodes         | 1932     |
|    fps              | 280      |
|    time_elapsed     | 533      |
|    total_timesteps  | 149527   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0138   |
|    n_updates        | 27381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.431    |
| time/               |          |
|    episodes         | 1936     |
|    fps              | 280      |
|    time_elapsed     | 533      |
|    total_timesteps  | 149877   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.216    |
|    n_updates        | 27469    |
----------------------------------
Eval num_timesteps=150000, episode_reward=191.14 +/- 78.71
Episode length: 48.14 +/- 19.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.431    |
| time/               |          |
|    total_timesteps  | 150000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00874  |
|    n_updates        | 27499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.429    |
| time/               |          |
|    episodes         | 1940     |
|    fps              | 280      |
|    time_elapsed     | 535      |
|    total_timesteps  | 150216   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0516   |
|    n_updates        | 27553    |
----------------------------------
Eval num_timesteps=150500, episode_reward=204.84 +/- 90.42
Episode length: 51.58 +/- 22.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 51.6     |
|    mean_reward      | 205      |
| rollout/            |          |
|    exploration_rate | 0.427    |
| time/               |          |
|    total_timesteps  | 150500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.689    |
|    n_updates        | 27624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.427    |
| time/               |          |
|    episodes         | 1944     |
|    fps              | 279      |
|    time_elapsed     | 537      |
|    total_timesteps  | 150510   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00437  |
|    n_updates        | 27627    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.424    |
| time/               |          |
|    episodes         | 1948     |
|    fps              | 280      |
|    time_elapsed     | 537      |
|    total_timesteps  | 150892   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0516   |
|    n_updates        | 27722    |
----------------------------------
Eval num_timesteps=151000, episode_reward=177.40 +/- 46.58
Episode length: 44.68 +/- 11.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.423    |
| time/               |          |
|    total_timesteps  | 151000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00574  |
|    n_updates        | 27749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.8     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.421    |
| time/               |          |
|    episodes         | 1952     |
|    fps              | 280      |
|    time_elapsed     | 539      |
|    total_timesteps  | 151211   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.656    |
|    n_updates        | 27802    |
----------------------------------
Eval num_timesteps=151500, episode_reward=182.08 +/- 53.85
Episode length: 45.88 +/- 13.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.419    |
| time/               |          |
|    total_timesteps  | 151500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0112   |
|    n_updates        | 27874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.417    |
| time/               |          |
|    episodes         | 1956     |
|    fps              | 280      |
|    time_elapsed     | 541      |
|    total_timesteps  | 151684   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.326    |
|    n_updates        | 27920    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.4     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.415    |
| time/               |          |
|    episodes         | 1960     |
|    fps              | 280      |
|    time_elapsed     | 541      |
|    total_timesteps  | 151939   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0396   |
|    n_updates        | 27984    |
----------------------------------
Eval num_timesteps=152000, episode_reward=171.36 +/- 51.03
Episode length: 43.24 +/- 12.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.415    |
| time/               |          |
|    total_timesteps  | 152000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.703    |
|    n_updates        | 27999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.8     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.413    |
| time/               |          |
|    episodes         | 1964     |
|    fps              | 280      |
|    time_elapsed     | 543      |
|    total_timesteps  | 152234   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.694    |
|    n_updates        | 28058    |
----------------------------------
Eval num_timesteps=152500, episode_reward=172.84 +/- 40.61
Episode length: 43.64 +/- 10.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.411    |
| time/               |          |
|    total_timesteps  | 152500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.682    |
|    n_updates        | 28124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.4     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.41     |
| time/               |          |
|    episodes         | 1968     |
|    fps              | 279      |
|    time_elapsed     | 545      |
|    total_timesteps  | 152676   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.162    |
|    n_updates        | 28168    |
----------------------------------
Eval num_timesteps=153000, episode_reward=173.36 +/- 49.25
Episode length: 43.72 +/- 12.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.407    |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0168   |
|    n_updates        | 28249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.4     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.406    |
| time/               |          |
|    episodes         | 1972     |
|    fps              | 279      |
|    time_elapsed     | 547      |
|    total_timesteps  | 153190   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.02     |
|    n_updates        | 28297    |
----------------------------------
Eval num_timesteps=153500, episode_reward=165.70 +/- 56.76
Episode length: 41.78 +/- 14.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.8     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.403    |
| time/               |          |
|    total_timesteps  | 153500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.478    |
|    n_updates        | 28374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.3     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.402    |
| time/               |          |
|    episodes         | 1976     |
|    fps              | 279      |
|    time_elapsed     | 548      |
|    total_timesteps  | 153609   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.123    |
|    n_updates        | 28402    |
----------------------------------
Eval num_timesteps=154000, episode_reward=175.68 +/- 46.21
Episode length: 44.32 +/- 11.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.399    |
| time/               |          |
|    total_timesteps  | 154000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39     |
|    n_updates        | 28499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.4     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.398    |
| time/               |          |
|    episodes         | 1980     |
|    fps              | 279      |
|    time_elapsed     | 550      |
|    total_timesteps  | 154152   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0795   |
|    n_updates        | 28537    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.395    |
| time/               |          |
|    episodes         | 1984     |
|    fps              | 280      |
|    time_elapsed     | 551      |
|    total_timesteps  | 154497   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.337    |
|    n_updates        | 28624    |
----------------------------------
Eval num_timesteps=154500, episode_reward=173.80 +/- 47.13
Episode length: 43.82 +/- 11.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.395    |
| time/               |          |
|    total_timesteps  | 154500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.9     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.393    |
| time/               |          |
|    episodes         | 1988     |
|    fps              | 279      |
|    time_elapsed     | 553      |
|    total_timesteps  | 154835   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.726    |
|    n_updates        | 28708    |
----------------------------------
Eval num_timesteps=155000, episode_reward=184.90 +/- 50.66
Episode length: 46.48 +/- 12.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.391    |
| time/               |          |
|    total_timesteps  | 155000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0117   |
|    n_updates        | 28749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.388    |
| time/               |          |
|    episodes         | 1992     |
|    fps              | 279      |
|    time_elapsed     | 555      |
|    total_timesteps  | 155395   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.766    |
|    n_updates        | 28848    |
----------------------------------
Eval num_timesteps=155500, episode_reward=173.40 +/- 42.88
Episode length: 43.68 +/- 10.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.387    |
| time/               |          |
|    total_timesteps  | 155500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00849  |
|    n_updates        | 28874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.385    |
| time/               |          |
|    episodes         | 1996     |
|    fps              | 279      |
|    time_elapsed     | 556      |
|    total_timesteps  | 155802   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00589  |
|    n_updates        | 28950    |
----------------------------------
Eval num_timesteps=156000, episode_reward=158.42 +/- 51.11
Episode length: 40.00 +/- 12.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40       |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.383    |
| time/               |          |
|    total_timesteps  | 156000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00335  |
|    n_updates        | 28999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.3     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.383    |
| time/               |          |
|    episodes         | 2000     |
|    fps              | 279      |
|    time_elapsed     | 558      |
|    total_timesteps  | 156058   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.714    |
|    n_updates        | 29014    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.38     |
| time/               |          |
|    episodes         | 2004     |
|    fps              | 279      |
|    time_elapsed     | 558      |
|    total_timesteps  | 156439   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.191    |
|    n_updates        | 29109    |
----------------------------------
Eval num_timesteps=156500, episode_reward=179.08 +/- 44.20
Episode length: 45.12 +/- 10.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.379    |
| time/               |          |
|    total_timesteps  | 156500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.31     |
|    n_updates        | 29124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.2     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.376    |
| time/               |          |
|    episodes         | 2008     |
|    fps              | 279      |
|    time_elapsed     | 560      |
|    total_timesteps  | 156880   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0438   |
|    n_updates        | 29219    |
----------------------------------
Eval num_timesteps=157000, episode_reward=164.42 +/- 43.95
Episode length: 41.48 +/- 11.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.375    |
| time/               |          |
|    total_timesteps  | 157000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.769    |
|    n_updates        | 29249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.373    |
| time/               |          |
|    episodes         | 2012     |
|    fps              | 279      |
|    time_elapsed     | 562      |
|    total_timesteps  | 157290   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.645    |
|    n_updates        | 29322    |
----------------------------------
Eval num_timesteps=157500, episode_reward=177.04 +/- 48.35
Episode length: 44.64 +/- 12.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.371    |
| time/               |          |
|    total_timesteps  | 157500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00333  |
|    n_updates        | 29374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.371    |
| time/               |          |
|    episodes         | 2016     |
|    fps              | 279      |
|    time_elapsed     | 564      |
|    total_timesteps  | 157529   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28     |
|    n_updates        | 29382    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.4     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.368    |
| time/               |          |
|    episodes         | 2020     |
|    fps              | 279      |
|    time_elapsed     | 564      |
|    total_timesteps  | 157973   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00132  |
|    n_updates        | 29493    |
----------------------------------
Eval num_timesteps=158000, episode_reward=175.50 +/- 43.25
Episode length: 44.18 +/- 10.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.367    |
| time/               |          |
|    total_timesteps  | 158000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0693   |
|    n_updates        | 29499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.9     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.366    |
| time/               |          |
|    episodes         | 2024     |
|    fps              | 279      |
|    time_elapsed     | 566      |
|    total_timesteps  | 158192   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.68     |
|    n_updates        | 29547    |
----------------------------------
Eval num_timesteps=158500, episode_reward=175.42 +/- 44.03
Episode length: 44.18 +/- 11.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.363    |
| time/               |          |
|    total_timesteps  | 158500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.125    |
|    n_updates        | 29624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.9     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.361    |
| time/               |          |
|    episodes         | 2028     |
|    fps              | 279      |
|    time_elapsed     | 568      |
|    total_timesteps  | 158723   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0152   |
|    n_updates        | 29680    |
----------------------------------
Eval num_timesteps=159000, episode_reward=152.84 +/- 31.75
Episode length: 38.56 +/- 7.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.6     |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.359    |
| time/               |          |
|    total_timesteps  | 159000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00394  |
|    n_updates        | 29749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.357    |
| time/               |          |
|    episodes         | 2032     |
|    fps              | 279      |
|    time_elapsed     | 569      |
|    total_timesteps  | 159271   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56     |
|    n_updates        | 29817    |
----------------------------------
Eval num_timesteps=159500, episode_reward=183.56 +/- 51.83
Episode length: 46.30 +/- 12.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.355    |
| time/               |          |
|    total_timesteps  | 159500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00811  |
|    n_updates        | 29874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.6     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.353    |
| time/               |          |
|    episodes         | 2036     |
|    fps              | 279      |
|    time_elapsed     | 571      |
|    total_timesteps  | 159738   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00814  |
|    n_updates        | 29934    |
----------------------------------
Eval num_timesteps=160000, episode_reward=181.22 +/- 50.43
Episode length: 45.74 +/- 12.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.351    |
| time/               |          |
|    total_timesteps  | 160000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0115   |
|    n_updates        | 29999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.35     |
| time/               |          |
|    episodes         | 2040     |
|    fps              | 279      |
|    time_elapsed     | 573      |
|    total_timesteps  | 160164   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.804    |
|    n_updates        | 30040    |
----------------------------------
Eval num_timesteps=160500, episode_reward=178.06 +/- 43.01
Episode length: 44.96 +/- 10.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.347    |
| time/               |          |
|    total_timesteps  | 160500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07     |
|    n_updates        | 30124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.346    |
| time/               |          |
|    episodes         | 2044     |
|    fps              | 279      |
|    time_elapsed     | 575      |
|    total_timesteps  | 160628   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0414   |
|    n_updates        | 30156    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.9     |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.344    |
| time/               |          |
|    episodes         | 2048     |
|    fps              | 279      |
|    time_elapsed     | 575      |
|    total_timesteps  | 160885   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00423  |
|    n_updates        | 30221    |
----------------------------------
Eval num_timesteps=161000, episode_reward=175.26 +/- 39.97
Episode length: 44.20 +/- 9.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.343    |
| time/               |          |
|    total_timesteps  | 161000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.763    |
|    n_updates        | 30249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.341    |
| time/               |          |
|    episodes         | 2052     |
|    fps              | 279      |
|    time_elapsed     | 577      |
|    total_timesteps  | 161273   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.477    |
|    n_updates        | 30318    |
----------------------------------
Eval num_timesteps=161500, episode_reward=184.10 +/- 40.99
Episode length: 46.36 +/- 10.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.339    |
| time/               |          |
|    total_timesteps  | 161500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.71     |
|    n_updates        | 30374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.336    |
| time/               |          |
|    episodes         | 2056     |
|    fps              | 279      |
|    time_elapsed     | 579      |
|    total_timesteps  | 161829   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0371   |
|    n_updates        | 30457    |
----------------------------------
Eval num_timesteps=162000, episode_reward=167.28 +/- 37.96
Episode length: 42.12 +/- 9.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.1     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.335    |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.853    |
|    n_updates        | 30499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.334    |
| time/               |          |
|    episodes         | 2060     |
|    fps              | 278      |
|    time_elapsed     | 581      |
|    total_timesteps  | 162100   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.805    |
|    n_updates        | 30524    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.332    |
| time/               |          |
|    episodes         | 2064     |
|    fps              | 279      |
|    time_elapsed     | 581      |
|    total_timesteps  | 162302   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0107   |
|    n_updates        | 30575    |
----------------------------------
Eval num_timesteps=162500, episode_reward=193.24 +/- 75.99
Episode length: 48.68 +/- 18.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.331    |
| time/               |          |
|    total_timesteps  | 162500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.488    |
|    n_updates        | 30624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.33     |
| time/               |          |
|    episodes         | 2068     |
|    fps              | 278      |
|    time_elapsed     | 583      |
|    total_timesteps  | 162557   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.368    |
|    n_updates        | 30639    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.327    |
| time/               |          |
|    episodes         | 2072     |
|    fps              | 279      |
|    time_elapsed     | 583      |
|    total_timesteps  | 162913   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00282  |
|    n_updates        | 30728    |
----------------------------------
Eval num_timesteps=163000, episode_reward=176.12 +/- 53.56
Episode length: 44.32 +/- 13.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.327    |
| time/               |          |
|    total_timesteps  | 163000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00465  |
|    n_updates        | 30749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.324    |
| time/               |          |
|    episodes         | 2076     |
|    fps              | 278      |
|    time_elapsed     | 585      |
|    total_timesteps  | 163276   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00868  |
|    n_updates        | 30818    |
----------------------------------
Eval num_timesteps=163500, episode_reward=188.38 +/- 72.45
Episode length: 47.50 +/- 18.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.323    |
| time/               |          |
|    total_timesteps  | 163500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00413  |
|    n_updates        | 30874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.6     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.321    |
| time/               |          |
|    episodes         | 2080     |
|    fps              | 278      |
|    time_elapsed     | 587      |
|    total_timesteps  | 163708   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00477  |
|    n_updates        | 30926    |
----------------------------------
Eval num_timesteps=164000, episode_reward=187.44 +/- 46.85
Episode length: 47.22 +/- 11.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.2     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.318    |
| time/               |          |
|    total_timesteps  | 164000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.1      |
|    n_updates        | 30999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.318    |
| time/               |          |
|    episodes         | 2084     |
|    fps              | 278      |
|    time_elapsed     | 589      |
|    total_timesteps  | 164065   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08     |
|    n_updates        | 31016    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.315    |
| time/               |          |
|    episodes         | 2088     |
|    fps              | 278      |
|    time_elapsed     | 589      |
|    total_timesteps  | 164418   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.36     |
|    n_updates        | 31104    |
----------------------------------
Eval num_timesteps=164500, episode_reward=189.02 +/- 54.45
Episode length: 47.56 +/- 13.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.314    |
| time/               |          |
|    total_timesteps  | 164500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.739    |
|    n_updates        | 31124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.312    |
| time/               |          |
|    episodes         | 2092     |
|    fps              | 278      |
|    time_elapsed     | 591      |
|    total_timesteps  | 164830   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00142  |
|    n_updates        | 31207    |
----------------------------------
Eval num_timesteps=165000, episode_reward=177.86 +/- 79.11
Episode length: 44.88 +/- 19.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.31     |
| time/               |          |
|    total_timesteps  | 165000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00948  |
|    n_updates        | 31249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.307    |
| time/               |          |
|    episodes         | 2096     |
|    fps              | 278      |
|    time_elapsed     | 593      |
|    total_timesteps  | 165350   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00261  |
|    n_updates        | 31337    |
----------------------------------
Eval num_timesteps=165500, episode_reward=175.94 +/- 69.50
Episode length: 44.42 +/- 17.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.306    |
| time/               |          |
|    total_timesteps  | 165500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.721    |
|    n_updates        | 31374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.6     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.303    |
| time/               |          |
|    episodes         | 2100     |
|    fps              | 278      |
|    time_elapsed     | 595      |
|    total_timesteps  | 165816   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0221   |
|    n_updates        | 31453    |
----------------------------------
Eval num_timesteps=166000, episode_reward=161.78 +/- 34.00
Episode length: 40.84 +/- 8.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.8     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.302    |
| time/               |          |
|    total_timesteps  | 166000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.709    |
|    n_updates        | 31499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.9     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.299    |
| time/               |          |
|    episodes         | 2104     |
|    fps              | 278      |
|    time_elapsed     | 597      |
|    total_timesteps  | 166330   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.189    |
|    n_updates        | 31582    |
----------------------------------
Eval num_timesteps=166500, episode_reward=180.02 +/- 45.96
Episode length: 45.40 +/- 11.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.298    |
| time/               |          |
|    total_timesteps  | 166500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00497  |
|    n_updates        | 31624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.2     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.296    |
| time/               |          |
|    episodes         | 2108     |
|    fps              | 278      |
|    time_elapsed     | 599      |
|    total_timesteps  | 166698   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.117    |
|    n_updates        | 31674    |
----------------------------------
Eval num_timesteps=167000, episode_reward=178.62 +/- 48.36
Episode length: 45.06 +/- 12.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.294    |
| time/               |          |
|    total_timesteps  | 167000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00426  |
|    n_updates        | 31749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.291    |
| time/               |          |
|    episodes         | 2112     |
|    fps              | 278      |
|    time_elapsed     | 601      |
|    total_timesteps  | 167324   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0154   |
|    n_updates        | 31830    |
----------------------------------
Eval num_timesteps=167500, episode_reward=174.82 +/- 40.45
Episode length: 44.12 +/- 10.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.289    |
| time/               |          |
|    total_timesteps  | 167500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.707    |
|    n_updates        | 31874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.288    |
| time/               |          |
|    episodes         | 2116     |
|    fps              | 278      |
|    time_elapsed     | 602      |
|    total_timesteps  | 167668   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.791    |
|    n_updates        | 31916    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.285    |
| time/               |          |
|    episodes         | 2120     |
|    fps              | 278      |
|    time_elapsed     | 603      |
|    total_timesteps  | 167972   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0154   |
|    n_updates        | 31992    |
----------------------------------
Eval num_timesteps=168000, episode_reward=199.12 +/- 97.97
Episode length: 50.12 +/- 24.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.1     |
|    mean_reward      | 199      |
| rollout/            |          |
|    exploration_rate | 0.285    |
| time/               |          |
|    total_timesteps  | 168000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.027    |
|    n_updates        | 31999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.282    |
| time/               |          |
|    episodes         | 2124     |
|    fps              | 278      |
|    time_elapsed     | 605      |
|    total_timesteps  | 168415   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0139   |
|    n_updates        | 32103    |
----------------------------------
Eval num_timesteps=168500, episode_reward=177.90 +/- 44.82
Episode length: 44.86 +/- 11.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.281    |
| time/               |          |
|    total_timesteps  | 168500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0298   |
|    n_updates        | 32124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.278    |
| time/               |          |
|    episodes         | 2128     |
|    fps              | 278      |
|    time_elapsed     | 607      |
|    total_timesteps  | 168840   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.383    |
|    n_updates        | 32209    |
----------------------------------
Eval num_timesteps=169000, episode_reward=186.10 +/- 69.85
Episode length: 46.88 +/- 17.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.277    |
| time/               |          |
|    total_timesteps  | 169000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0106   |
|    n_updates        | 32249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.274    |
| time/               |          |
|    episodes         | 2132     |
|    fps              | 277      |
|    time_elapsed     | 608      |
|    total_timesteps  | 169295   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0122   |
|    n_updates        | 32323    |
----------------------------------
Eval num_timesteps=169500, episode_reward=178.28 +/- 47.92
Episode length: 44.98 +/- 12.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.273    |
| time/               |          |
|    total_timesteps  | 169500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.31     |
|    n_updates        | 32374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.271    |
| time/               |          |
|    episodes         | 2136     |
|    fps              | 277      |
|    time_elapsed     | 610      |
|    total_timesteps  | 169690   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41     |
|    n_updates        | 32422    |
----------------------------------
Eval num_timesteps=170000, episode_reward=162.56 +/- 53.59
Episode length: 41.04 +/- 13.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41       |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.268    |
| time/               |          |
|    total_timesteps  | 170000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.118    |
|    n_updates        | 32499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.6     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.268    |
| time/               |          |
|    episodes         | 2140     |
|    fps              | 277      |
|    time_elapsed     | 612      |
|    total_timesteps  | 170024   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.73     |
|    n_updates        | 32505    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.266    |
| time/               |          |
|    episodes         | 2144     |
|    fps              | 277      |
|    time_elapsed     | 612      |
|    total_timesteps  | 170299   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000509 |
|    n_updates        | 32574    |
----------------------------------
Eval num_timesteps=170500, episode_reward=169.36 +/- 66.08
Episode length: 42.68 +/- 16.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.264    |
| time/               |          |
|    total_timesteps  | 170500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.741    |
|    n_updates        | 32624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.263    |
| time/               |          |
|    episodes         | 2148     |
|    fps              | 277      |
|    time_elapsed     | 614      |
|    total_timesteps  | 170589   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0111   |
|    n_updates        | 32647    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.1     |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.26     |
| time/               |          |
|    episodes         | 2152     |
|    fps              | 278      |
|    time_elapsed     | 614      |
|    total_timesteps  | 170981   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000191 |
|    n_updates        | 32745    |
----------------------------------
Eval num_timesteps=171000, episode_reward=199.08 +/- 57.68
Episode length: 50.12 +/- 14.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.1     |
|    mean_reward      | 199      |
| rollout/            |          |
|    exploration_rate | 0.26     |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00211  |
|    n_updates        | 32749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.8     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.257    |
| time/               |          |
|    episodes         | 2156     |
|    fps              | 277      |
|    time_elapsed     | 616      |
|    total_timesteps  | 171308   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.712    |
|    n_updates        | 32826    |
----------------------------------
Eval num_timesteps=171500, episode_reward=169.88 +/- 38.18
Episode length: 42.86 +/- 9.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.256    |
| time/               |          |
|    total_timesteps  | 171500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0225   |
|    n_updates        | 32874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.254    |
| time/               |          |
|    episodes         | 2160     |
|    fps              | 277      |
|    time_elapsed     | 618      |
|    total_timesteps  | 171691   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00342  |
|    n_updates        | 32922    |
----------------------------------
Eval num_timesteps=172000, episode_reward=174.18 +/- 53.83
Episode length: 43.92 +/- 13.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.251    |
| time/               |          |
|    total_timesteps  | 172000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29     |
|    n_updates        | 32999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.1     |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.25     |
| time/               |          |
|    episodes         | 2164     |
|    fps              | 277      |
|    time_elapsed     | 620      |
|    total_timesteps  | 172208   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0336   |
|    n_updates        | 33051    |
----------------------------------
Eval num_timesteps=172500, episode_reward=184.86 +/- 45.89
Episode length: 46.62 +/- 11.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.6     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.247    |
| time/               |          |
|    total_timesteps  | 172500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.818    |
|    n_updates        | 33124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.246    |
| time/               |          |
|    episodes         | 2168     |
|    fps              | 277      |
|    time_elapsed     | 622      |
|    total_timesteps  | 172622   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00516  |
|    n_updates        | 33155    |
----------------------------------
Eval num_timesteps=173000, episode_reward=173.72 +/- 41.27
Episode length: 43.82 +/- 10.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.243    |
| time/               |          |
|    total_timesteps  | 173000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.65     |
|    n_updates        | 33249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.243    |
| time/               |          |
|    episodes         | 2172     |
|    fps              | 277      |
|    time_elapsed     | 624      |
|    total_timesteps  | 173008   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.34     |
|    n_updates        | 33251    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.239    |
| time/               |          |
|    episodes         | 2176     |
|    fps              | 277      |
|    time_elapsed     | 624      |
|    total_timesteps  | 173489   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.31     |
|    n_updates        | 33372    |
----------------------------------
Eval num_timesteps=173500, episode_reward=183.76 +/- 52.31
Episode length: 46.36 +/- 13.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.239    |
| time/               |          |
|    total_timesteps  | 173500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00524  |
|    n_updates        | 33374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.236    |
| time/               |          |
|    episodes         | 2180     |
|    fps              | 277      |
|    time_elapsed     | 626      |
|    total_timesteps  | 173825   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.3      |
|    n_updates        | 33456    |
----------------------------------
Eval num_timesteps=174000, episode_reward=161.96 +/- 42.90
Episode length: 40.86 +/- 10.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.9     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.234    |
| time/               |          |
|    total_timesteps  | 174000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000785 |
|    n_updates        | 33499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 413      |
|    exploration_rate | 0.231    |
| time/               |          |
|    episodes         | 2184     |
|    fps              | 277      |
|    time_elapsed     | 628      |
|    total_timesteps  | 174415   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00928  |
|    n_updates        | 33603    |
----------------------------------
Eval num_timesteps=174500, episode_reward=169.44 +/- 62.38
Episode length: 42.72 +/- 15.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.23     |
| time/               |          |
|    total_timesteps  | 174500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00767  |
|    n_updates        | 33624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.228    |
| time/               |          |
|    episodes         | 2188     |
|    fps              | 277      |
|    time_elapsed     | 630      |
|    total_timesteps  | 174714   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00114  |
|    n_updates        | 33678    |
----------------------------------
Eval num_timesteps=175000, episode_reward=171.58 +/- 68.63
Episode length: 43.30 +/- 17.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.226    |
| time/               |          |
|    total_timesteps  | 175000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.492    |
|    n_updates        | 33749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 417      |
|    exploration_rate | 0.223    |
| time/               |          |
|    episodes         | 2192     |
|    fps              | 277      |
|    time_elapsed     | 632      |
|    total_timesteps  | 175288   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0083   |
|    n_updates        | 33821    |
----------------------------------
Eval num_timesteps=175500, episode_reward=171.48 +/- 61.53
Episode length: 43.30 +/- 15.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.221    |
| time/               |          |
|    total_timesteps  | 175500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.71     |
|    n_updates        | 33874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.22     |
| time/               |          |
|    episodes         | 2196     |
|    fps              | 277      |
|    time_elapsed     | 633      |
|    total_timesteps  | 175645   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.764    |
|    n_updates        | 33911    |
----------------------------------
Eval num_timesteps=176000, episode_reward=190.70 +/- 84.25
Episode length: 48.08 +/- 21.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.217    |
| time/               |          |
|    total_timesteps  | 176000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000212 |
|    n_updates        | 33999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.216    |
| time/               |          |
|    episodes         | 2200     |
|    fps              | 276      |
|    time_elapsed     | 635      |
|    total_timesteps  | 176130   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.287    |
|    n_updates        | 34032    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.213    |
| time/               |          |
|    episodes         | 2204     |
|    fps              | 277      |
|    time_elapsed     | 636      |
|    total_timesteps  | 176485   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.75     |
|    n_updates        | 34121    |
----------------------------------
Eval num_timesteps=176500, episode_reward=192.92 +/- 80.77
Episode length: 48.62 +/- 20.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.6     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.213    |
| time/               |          |
|    total_timesteps  | 176500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00624  |
|    n_updates        | 34124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.21     |
| time/               |          |
|    episodes         | 2208     |
|    fps              | 276      |
|    time_elapsed     | 638      |
|    total_timesteps  | 176766   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.27     |
|    n_updates        | 34191    |
----------------------------------
Eval num_timesteps=177000, episode_reward=178.02 +/- 46.20
Episode length: 44.86 +/- 11.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.208    |
| time/               |          |
|    total_timesteps  | 177000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00328  |
|    n_updates        | 34249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.206    |
| time/               |          |
|    episodes         | 2212     |
|    fps              | 276      |
|    time_elapsed     | 640      |
|    total_timesteps  | 177276   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.46e-05 |
|    n_updates        | 34318    |
----------------------------------
Eval num_timesteps=177500, episode_reward=193.12 +/- 51.52
Episode length: 48.68 +/- 12.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.204    |
| time/               |          |
|    total_timesteps  | 177500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00399  |
|    n_updates        | 34374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.203    |
| time/               |          |
|    episodes         | 2216     |
|    fps              | 276      |
|    time_elapsed     | 642      |
|    total_timesteps  | 177666   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.809    |
|    n_updates        | 34416    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.2      |
| time/               |          |
|    episodes         | 2220     |
|    fps              | 277      |
|    time_elapsed     | 642      |
|    total_timesteps  | 177999   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.57     |
|    n_updates        | 34499    |
----------------------------------
Eval num_timesteps=178000, episode_reward=164.56 +/- 59.36
Episode length: 41.52 +/- 14.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
Eval num_timesteps=178500, episode_reward=177.02 +/- 52.15
Episode length: 44.60 +/- 13.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.195    |
| time/               |          |
|    total_timesteps  | 178500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0935   |
|    n_updates        | 34624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.194    |
| time/               |          |
|    episodes         | 2224     |
|    fps              | 276      |
|    time_elapsed     | 645      |
|    total_timesteps  | 178678   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00221  |
|    n_updates        | 34669    |
----------------------------------
Eval num_timesteps=179000, episode_reward=168.58 +/- 49.93
Episode length: 42.48 +/- 12.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.191    |
| time/               |          |
|    total_timesteps  | 179000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.57     |
|    n_updates        | 34749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.191    |
| time/               |          |
|    episodes         | 2228     |
|    fps              | 276      |
|    time_elapsed     | 647      |
|    total_timesteps  | 179009   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0234   |
|    n_updates        | 34752    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.187    |
| time/               |          |
|    episodes         | 2232     |
|    fps              | 276      |
|    time_elapsed     | 648      |
|    total_timesteps  | 179441   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0233   |
|    n_updates        | 34860    |
----------------------------------
Eval num_timesteps=179500, episode_reward=183.52 +/- 66.54
Episode length: 46.34 +/- 16.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.187    |
| time/               |          |
|    total_timesteps  | 179500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00191  |
|    n_updates        | 34874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 410      |
|    exploration_rate | 0.182    |
| time/               |          |
|    episodes         | 2236     |
|    fps              | 276      |
|    time_elapsed     | 650      |
|    total_timesteps  | 179982   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000197 |
|    n_updates        | 34995    |
----------------------------------
Eval num_timesteps=180000, episode_reward=188.04 +/- 70.00
Episode length: 47.42 +/- 17.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.4     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.182    |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.779    |
|    n_updates        | 34999    |
----------------------------------
Eval num_timesteps=180500, episode_reward=177.42 +/- 44.74
Episode length: 44.70 +/- 11.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.178    |
| time/               |          |
|    total_timesteps  | 180500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.828    |
|    n_updates        | 35124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 420      |
|    exploration_rate | 0.177    |
| time/               |          |
|    episodes         | 2240     |
|    fps              | 276      |
|    time_elapsed     | 653      |
|    total_timesteps  | 180564   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.023    |
|    n_updates        | 35140    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 422      |
|    exploration_rate | 0.175    |
| time/               |          |
|    episodes         | 2244     |
|    fps              | 276      |
|    time_elapsed     | 654      |
|    total_timesteps  | 180878   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.2      |
|    n_updates        | 35219    |
----------------------------------
Eval num_timesteps=181000, episode_reward=164.18 +/- 50.57
Episode length: 41.46 +/- 12.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.174    |
| time/               |          |
|    total_timesteps  | 181000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.101    |
|    n_updates        | 35249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 423      |
|    exploration_rate | 0.172    |
| time/               |          |
|    episodes         | 2248     |
|    fps              | 276      |
|    time_elapsed     | 655      |
|    total_timesteps  | 181190   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.889    |
|    n_updates        | 35297    |
----------------------------------
Eval num_timesteps=181500, episode_reward=165.58 +/- 54.22
Episode length: 41.72 +/- 13.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.7     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.169    |
| time/               |          |
|    total_timesteps  | 181500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.859    |
|    n_updates        | 35374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 429      |
|    exploration_rate | 0.167    |
| time/               |          |
|    episodes         | 2252     |
|    fps              | 276      |
|    time_elapsed     | 657      |
|    total_timesteps  | 181740   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.819    |
|    n_updates        | 35434    |
----------------------------------
Eval num_timesteps=182000, episode_reward=182.54 +/- 58.03
Episode length: 46.04 +/- 14.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.165    |
| time/               |          |
|    total_timesteps  | 182000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0202   |
|    n_updates        | 35499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 432      |
|    exploration_rate | 0.163    |
| time/               |          |
|    episodes         | 2256     |
|    fps              | 276      |
|    time_elapsed     | 659      |
|    total_timesteps  | 182146   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.2      |
|    n_updates        | 35536    |
----------------------------------
Eval num_timesteps=182500, episode_reward=168.24 +/- 49.18
Episode length: 42.34 +/- 12.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.16     |
| time/               |          |
|    total_timesteps  | 182500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.68     |
|    n_updates        | 35624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 432      |
|    exploration_rate | 0.16     |
| time/               |          |
|    episodes         | 2260     |
|    fps              | 275      |
|    time_elapsed     | 661      |
|    total_timesteps  | 182517   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00132  |
|    n_updates        | 35629    |
----------------------------------
Eval num_timesteps=183000, episode_reward=174.88 +/- 44.41
Episode length: 44.12 +/- 11.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.156    |
| time/               |          |
|    total_timesteps  | 183000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.603    |
|    n_updates        | 35749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 431      |
|    exploration_rate | 0.156    |
| time/               |          |
|    episodes         | 2264     |
|    fps              | 275      |
|    time_elapsed     | 663      |
|    total_timesteps  | 183013   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.776    |
|    n_updates        | 35753    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 426      |
|    exploration_rate | 0.153    |
| time/               |          |
|    episodes         | 2268     |
|    fps              | 276      |
|    time_elapsed     | 663      |
|    total_timesteps  | 183317   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0185   |
|    n_updates        | 35829    |
----------------------------------
Eval num_timesteps=183500, episode_reward=180.70 +/- 51.75
Episode length: 45.56 +/- 12.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.151    |
| time/               |          |
|    total_timesteps  | 183500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0167   |
|    n_updates        | 35874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 426      |
|    exploration_rate | 0.15     |
| time/               |          |
|    episodes         | 2272     |
|    fps              | 276      |
|    time_elapsed     | 665      |
|    total_timesteps  | 183685   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00219  |
|    n_updates        | 35921    |
----------------------------------
Eval num_timesteps=184000, episode_reward=146.06 +/- 37.62
Episode length: 36.90 +/- 9.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.9     |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.147    |
| time/               |          |
|    total_timesteps  | 184000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.01     |
|    n_updates        | 35999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 421      |
|    exploration_rate | 0.147    |
| time/               |          |
|    episodes         | 2276     |
|    fps              | 275      |
|    time_elapsed     | 667      |
|    total_timesteps  | 184050   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.855    |
|    n_updates        | 36012    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 425      |
|    exploration_rate | 0.143    |
| time/               |          |
|    episodes         | 2280     |
|    fps              | 276      |
|    time_elapsed     | 667      |
|    total_timesteps  | 184487   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00146  |
|    n_updates        | 36121    |
----------------------------------
Eval num_timesteps=184500, episode_reward=190.72 +/- 55.32
Episode length: 48.06 +/- 13.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.143    |
| time/               |          |
|    total_timesteps  | 184500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.817    |
|    n_updates        | 36124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 414      |
|    exploration_rate | 0.14     |
| time/               |          |
|    episodes         | 2284     |
|    fps              | 276      |
|    time_elapsed     | 669      |
|    total_timesteps  | 184794   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.863    |
|    n_updates        | 36198    |
----------------------------------
Eval num_timesteps=185000, episode_reward=166.42 +/- 42.19
Episode length: 41.98 +/- 10.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.138    |
| time/               |          |
|    total_timesteps  | 185000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.824    |
|    n_updates        | 36249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 421      |
|    exploration_rate | 0.136    |
| time/               |          |
|    episodes         | 2288     |
|    fps              | 276      |
|    time_elapsed     | 671      |
|    total_timesteps  | 185272   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00355  |
|    n_updates        | 36317    |
----------------------------------
Eval num_timesteps=185500, episode_reward=176.64 +/- 42.57
Episode length: 44.52 +/- 10.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.134    |
| time/               |          |
|    total_timesteps  | 185500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.456    |
|    n_updates        | 36374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 417      |
|    exploration_rate | 0.131    |
| time/               |          |
|    episodes         | 2292     |
|    fps              | 275      |
|    time_elapsed     | 673      |
|    total_timesteps  | 185753   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000244 |
|    n_updates        | 36438    |
----------------------------------
Eval num_timesteps=186000, episode_reward=165.52 +/- 39.65
Episode length: 41.84 +/- 9.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.8     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.129    |
| time/               |          |
|    total_timesteps  | 186000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00621  |
|    n_updates        | 36499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 425      |
|    exploration_rate | 0.126    |
| time/               |          |
|    episodes         | 2296     |
|    fps              | 275      |
|    time_elapsed     | 675      |
|    total_timesteps  | 186312   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0385   |
|    n_updates        | 36577    |
----------------------------------
Eval num_timesteps=186500, episode_reward=147.92 +/- 35.27
Episode length: 37.38 +/- 8.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.4     |
|    mean_reward      | 148      |
| rollout/            |          |
|    exploration_rate | 0.125    |
| time/               |          |
|    total_timesteps  | 186500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.778    |
|    n_updates        | 36624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 425      |
|    exploration_rate | 0.122    |
| time/               |          |
|    episodes         | 2300     |
|    fps              | 276      |
|    time_elapsed     | 676      |
|    total_timesteps  | 186799   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7      |
|    n_updates        | 36699    |
----------------------------------
Eval num_timesteps=187000, episode_reward=175.56 +/- 46.35
Episode length: 44.30 +/- 11.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.12     |
| time/               |          |
|    total_timesteps  | 187000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0079   |
|    n_updates        | 36749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 436      |
|    exploration_rate | 0.117    |
| time/               |          |
|    episodes         | 2304     |
|    fps              | 276      |
|    time_elapsed     | 678      |
|    total_timesteps  | 187414   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0149   |
|    n_updates        | 36853    |
----------------------------------
Eval num_timesteps=187500, episode_reward=168.50 +/- 39.38
Episode length: 42.44 +/- 9.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.116    |
| time/               |          |
|    total_timesteps  | 187500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.51     |
|    n_updates        | 36874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 436      |
|    exploration_rate | 0.114    |
| time/               |          |
|    episodes         | 2308     |
|    fps              | 275      |
|    time_elapsed     | 680      |
|    total_timesteps  | 187716   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00637  |
|    n_updates        | 36928    |
----------------------------------
Eval num_timesteps=188000, episode_reward=151.16 +/- 47.93
Episode length: 38.22 +/- 11.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.2     |
|    mean_reward      | 151      |
| rollout/            |          |
|    exploration_rate | 0.111    |
| time/               |          |
|    total_timesteps  | 188000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.847    |
|    n_updates        | 36999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 430      |
|    exploration_rate | 0.111    |
| time/               |          |
|    episodes         | 2312     |
|    fps              | 275      |
|    time_elapsed     | 682      |
|    total_timesteps  | 188059   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.98     |
|    n_updates        | 37014    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 425      |
|    exploration_rate | 0.108    |
| time/               |          |
|    episodes         | 2316     |
|    fps              | 275      |
|    time_elapsed     | 682      |
|    total_timesteps  | 188326   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00987  |
|    n_updates        | 37081    |
----------------------------------
Eval num_timesteps=188500, episode_reward=180.24 +/- 43.17
Episode length: 45.48 +/- 10.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.107    |
| time/               |          |
|    total_timesteps  | 188500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.824    |
|    n_updates        | 37124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 423      |
|    exploration_rate | 0.106    |
| time/               |          |
|    episodes         | 2320     |
|    fps              | 275      |
|    time_elapsed     | 684      |
|    total_timesteps  | 188616   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00174  |
|    n_updates        | 37153    |
----------------------------------
Eval num_timesteps=189000, episode_reward=163.42 +/- 56.92
Episode length: 41.22 +/- 14.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.2     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.102    |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0162   |
|    n_updates        | 37249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.102    |
| time/               |          |
|    episodes         | 2324     |
|    fps              | 275      |
|    time_elapsed     | 686      |
|    total_timesteps  | 189003   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0117   |
|    n_updates        | 37250    |
----------------------------------
Eval num_timesteps=189500, episode_reward=183.00 +/- 51.03
Episode length: 46.14 +/- 12.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.0977   |
| time/               |          |
|    total_timesteps  | 189500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.868    |
|    n_updates        | 37374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 423      |
|    exploration_rate | 0.0966   |
| time/               |          |
|    episodes         | 2328     |
|    fps              | 275      |
|    time_elapsed     | 688      |
|    total_timesteps  | 189620   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.863    |
|    n_updates        | 37404    |
----------------------------------
Eval num_timesteps=190000, episode_reward=181.04 +/- 41.08
Episode length: 45.68 +/- 10.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.0932   |
| time/               |          |
|    total_timesteps  | 190000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0502   |
|    n_updates        | 37499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 422      |
|    exploration_rate | 0.0929   |
| time/               |          |
|    episodes         | 2332     |
|    fps              | 275      |
|    time_elapsed     | 690      |
|    total_timesteps  | 190028   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.82     |
|    n_updates        | 37506    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.0902   |
| time/               |          |
|    episodes         | 2336     |
|    fps              | 275      |
|    time_elapsed     | 690      |
|    total_timesteps  | 190327   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.915    |
|    n_updates        | 37581    |
----------------------------------
Eval num_timesteps=190500, episode_reward=152.64 +/- 42.36
Episode length: 38.56 +/- 10.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.6     |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.0886   |
| time/               |          |
|    total_timesteps  | 190500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.77     |
|    n_updates        | 37624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.0875   |
| time/               |          |
|    episodes         | 2340     |
|    fps              | 275      |
|    time_elapsed     | 691      |
|    total_timesteps  | 190623   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.21     |
|    n_updates        | 37655    |
----------------------------------
Eval num_timesteps=191000, episode_reward=174.00 +/- 38.48
Episode length: 43.92 +/- 9.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.0841   |
| time/               |          |
|    total_timesteps  | 191000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00355  |
|    n_updates        | 37749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.0839   |
| time/               |          |
|    episodes         | 2344     |
|    fps              | 275      |
|    time_elapsed     | 693      |
|    total_timesteps  | 191024   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.89     |
|    n_updates        | 37755    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.0811   |
| time/               |          |
|    episodes         | 2348     |
|    fps              | 275      |
|    time_elapsed     | 694      |
|    total_timesteps  | 191325   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.887    |
|    n_updates        | 37831    |
----------------------------------
Eval num_timesteps=191500, episode_reward=168.96 +/- 34.00
Episode length: 42.60 +/- 8.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.0796   |
| time/               |          |
|    total_timesteps  | 191500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.888    |
|    n_updates        | 37874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.1     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.0791   |
| time/               |          |
|    episodes         | 2352     |
|    fps              | 275      |
|    time_elapsed     | 695      |
|    total_timesteps  | 191553   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0196   |
|    n_updates        | 37888    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.0764   |
| time/               |          |
|    episodes         | 2356     |
|    fps              | 275      |
|    time_elapsed     | 696      |
|    total_timesteps  | 191849   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.122    |
|    n_updates        | 37962    |
----------------------------------
Eval num_timesteps=192000, episode_reward=168.90 +/- 47.07
Episode length: 42.56 +/- 11.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.075    |
| time/               |          |
|    total_timesteps  | 192000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00259  |
|    n_updates        | 37999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.0729   |
| time/               |          |
|    episodes         | 2360     |
|    fps              | 275      |
|    time_elapsed     | 697      |
|    total_timesteps  | 192232   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000307 |
|    n_updates        | 38057    |
----------------------------------
Eval num_timesteps=192500, episode_reward=174.46 +/- 47.97
Episode length: 43.90 +/- 12.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.0704   |
| time/               |          |
|    total_timesteps  | 192500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00125  |
|    n_updates        | 38124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.1     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.0693   |
| time/               |          |
|    episodes         | 2364     |
|    fps              | 275      |
|    time_elapsed     | 699      |
|    total_timesteps  | 192621   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.78     |
|    n_updates        | 38155    |
----------------------------------
Eval num_timesteps=193000, episode_reward=174.20 +/- 45.35
Episode length: 43.90 +/- 11.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.0658   |
| time/               |          |
|    total_timesteps  | 193000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00208  |
|    n_updates        | 38249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.1     |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.0656   |
| time/               |          |
|    episodes         | 2368     |
|    fps              | 275      |
|    time_elapsed     | 701      |
|    total_timesteps  | 193023   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00348  |
|    n_updates        | 38255    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.0622   |
| time/               |          |
|    episodes         | 2372     |
|    fps              | 275      |
|    time_elapsed     | 701      |
|    total_timesteps  | 193402   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000466 |
|    n_updates        | 38350    |
----------------------------------
Eval num_timesteps=193500, episode_reward=155.16 +/- 42.24
Episode length: 39.24 +/- 10.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.2     |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.0613   |
| time/               |          |
|    total_timesteps  | 193500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.017    |
|    n_updates        | 38374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.059    |
| time/               |          |
|    episodes         | 2376     |
|    fps              | 275      |
|    time_elapsed     | 703      |
|    total_timesteps  | 193747   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.902    |
|    n_updates        | 38436    |
----------------------------------
Eval num_timesteps=194000, episode_reward=157.42 +/- 36.30
Episode length: 39.74 +/- 9.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.7     |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.0567   |
| time/               |          |
|    total_timesteps  | 194000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0115   |
|    n_updates        | 38499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.9     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.0551   |
| time/               |          |
|    episodes         | 2380     |
|    fps              | 275      |
|    time_elapsed     | 705      |
|    total_timesteps  | 194174   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00297  |
|    n_updates        | 38543    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.0526   |
| time/               |          |
|    episodes         | 2384     |
|    fps              | 275      |
|    time_elapsed     | 705      |
|    total_timesteps  | 194439   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.36     |
|    n_updates        | 38609    |
----------------------------------
Eval num_timesteps=194500, episode_reward=187.76 +/- 54.84
Episode length: 47.36 +/- 13.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.4     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.0521   |
| time/               |          |
|    total_timesteps  | 194500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0011   |
|    n_updates        | 38624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.0502   |
| time/               |          |
|    episodes         | 2388     |
|    fps              | 275      |
|    time_elapsed     | 707      |
|    total_timesteps  | 194706   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.77     |
|    n_updates        | 38676    |
----------------------------------
Eval num_timesteps=195000, episode_reward=181.64 +/- 53.96
Episode length: 45.82 +/- 13.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.0475   |
| time/               |          |
|    total_timesteps  | 195000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00264  |
|    n_updates        | 38749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.9     |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.047    |
| time/               |          |
|    episodes         | 2392     |
|    fps              | 274      |
|    time_elapsed     | 709      |
|    total_timesteps  | 195045   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.55     |
|    n_updates        | 38761    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.4     |
|    ep_rew_mean      | 360      |
|    exploration_rate | 0.0442   |
| time/               |          |
|    episodes         | 2396     |
|    fps              | 275      |
|    time_elapsed     | 709      |
|    total_timesteps  | 195349   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.884    |
|    n_updates        | 38837    |
----------------------------------
Eval num_timesteps=195500, episode_reward=155.48 +/- 48.49
Episode length: 39.24 +/- 12.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.2     |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.0429   |
| time/               |          |
|    total_timesteps  | 195500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15     |
|    n_updates        | 38874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.2     |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.0409   |
| time/               |          |
|    episodes         | 2400     |
|    fps              | 275      |
|    time_elapsed     | 711      |
|    total_timesteps  | 195714   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00509  |
|    n_updates        | 38928    |
----------------------------------
Eval num_timesteps=196000, episode_reward=176.22 +/- 51.28
Episode length: 44.50 +/- 12.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.0382   |
| time/               |          |
|    total_timesteps  | 196000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000179 |
|    n_updates        | 38999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 345      |
|    exploration_rate | 0.0374   |
| time/               |          |
|    episodes         | 2404     |
|    fps              | 274      |
|    time_elapsed     | 713      |
|    total_timesteps  | 196090   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.811    |
|    n_updates        | 39022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.6     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.0338   |
| time/               |          |
|    episodes         | 2408     |
|    fps              | 275      |
|    time_elapsed     | 713      |
|    total_timesteps  | 196476   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.902    |
|    n_updates        | 39118    |
----------------------------------
Eval num_timesteps=196500, episode_reward=156.12 +/- 46.40
Episode length: 39.38 +/- 11.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.4     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.0336   |
| time/               |          |
|    total_timesteps  | 196500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00148  |
|    n_updates        | 39124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.1     |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.0302   |
| time/               |          |
|    episodes         | 2412     |
|    fps              | 275      |
|    time_elapsed     | 715      |
|    total_timesteps  | 196867   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0026   |
|    n_updates        | 39216    |
----------------------------------
Eval num_timesteps=197000, episode_reward=156.16 +/- 42.91
Episode length: 39.42 +/- 10.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.4     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.029    |
| time/               |          |
|    total_timesteps  | 197000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.153    |
|    n_updates        | 39249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.1     |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.0277   |
| time/               |          |
|    episodes         | 2416     |
|    fps              | 275      |
|    time_elapsed     | 716      |
|    total_timesteps  | 197135   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00284  |
|    n_updates        | 39283    |
----------------------------------
Eval num_timesteps=197500, episode_reward=180.34 +/- 55.26
Episode length: 45.52 +/- 13.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.0243   |
| time/               |          |
|    total_timesteps  | 197500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00401  |
|    n_updates        | 39374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.1     |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.0241   |
| time/               |          |
|    episodes         | 2420     |
|    fps              | 274      |
|    time_elapsed     | 718      |
|    total_timesteps  | 197522   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.695    |
|    n_updates        | 39380    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.1     |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.0214   |
| time/               |          |
|    episodes         | 2424     |
|    fps              | 275      |
|    time_elapsed     | 719      |
|    total_timesteps  | 197817   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00023  |
|    n_updates        | 39454    |
----------------------------------
Eval num_timesteps=198000, episode_reward=170.50 +/- 38.81
Episode length: 43.02 +/- 9.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.0197   |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00136  |
|    n_updates        | 39499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.4     |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.0154   |
| time/               |          |
|    episodes         | 2428     |
|    fps              | 275      |
|    time_elapsed     | 721      |
|    total_timesteps  | 198463   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.65     |
|    n_updates        | 39615    |
----------------------------------
Eval num_timesteps=198500, episode_reward=173.06 +/- 44.22
Episode length: 43.64 +/- 11.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.015    |
| time/               |          |
|    total_timesteps  | 198500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.66     |
|    n_updates        | 39624    |
----------------------------------
Eval num_timesteps=199000, episode_reward=174.58 +/- 45.50
Episode length: 43.98 +/- 11.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.0104   |
| time/               |          |
|    total_timesteps  | 199000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.977    |
|    n_updates        | 39749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.6     |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.00956  |
| time/               |          |
|    episodes         | 2432     |
|    fps              | 274      |
|    time_elapsed     | 724      |
|    total_timesteps  | 199085   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00471  |
|    n_updates        | 39771    |
----------------------------------
Eval num_timesteps=199500, episode_reward=186.68 +/- 63.31
Episode length: 47.04 +/- 15.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.00569  |
| time/               |          |
|    total_timesteps  | 199500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000187 |
|    n_updates        | 39874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.7     |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.00477  |
| time/               |          |
|    episodes         | 2436     |
|    fps              | 274      |
|    time_elapsed     | 726      |
|    total_timesteps  | 199597   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00338  |
|    n_updates        | 39899    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.3     |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.00237  |
| time/               |          |
|    episodes         | 2440     |
|    fps              | 274      |
|    time_elapsed     | 727      |
|    total_timesteps  | 199854   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.875    |
|    n_updates        | 39963    |
----------------------------------
Eval num_timesteps=200000, episode_reward=177.30 +/- 46.50
Episode length: 44.72 +/- 11.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.00101  |
| time/               |          |
|    total_timesteps  | 200000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.89     |
|    n_updates        | 39999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2444     |
|    fps              | 274      |
|    time_elapsed     | 728      |
|    total_timesteps  | 200177   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00929  |
|    n_updates        | 40044    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.4     |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2448     |
|    fps              | 274      |
|    time_elapsed     | 729      |
|    total_timesteps  | 200466   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0424   |
|    n_updates        | 40116    |
----------------------------------
Eval num_timesteps=200500, episode_reward=158.10 +/- 28.29
Episode length: 39.92 +/- 7.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.9     |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 200500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00188  |
|    n_updates        | 40124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.8     |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2452     |
|    fps              | 274      |
|    time_elapsed     | 730      |
|    total_timesteps  | 200837   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0105   |
|    n_updates        | 40209    |
----------------------------------
Eval num_timesteps=201000, episode_reward=177.70 +/- 48.17
Episode length: 44.80 +/- 11.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 201000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00589  |
|    n_updates        | 40249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2456     |
|    fps              | 274      |
|    time_elapsed     | 732      |
|    total_timesteps  | 201231   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00687  |
|    n_updates        | 40307    |
----------------------------------
Eval num_timesteps=201500, episode_reward=172.26 +/- 45.60
Episode length: 43.44 +/- 11.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 201500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.968    |
|    n_updates        | 40374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.1     |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2460     |
|    fps              | 274      |
|    time_elapsed     | 734      |
|    total_timesteps  | 201539   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.982    |
|    n_updates        | 40384    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.5     |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2464     |
|    fps              | 274      |
|    time_elapsed     | 734      |
|    total_timesteps  | 201974   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.72     |
|    n_updates        | 40493    |
----------------------------------
Eval num_timesteps=202000, episode_reward=156.64 +/- 37.05
Episode length: 39.56 +/- 9.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.6     |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 202000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00447  |
|    n_updates        | 40499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.1     |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2468     |
|    fps              | 274      |
|    time_elapsed     | 736      |
|    total_timesteps  | 202329   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9      |
|    n_updates        | 40582    |
----------------------------------
Eval num_timesteps=202500, episode_reward=174.24 +/- 54.06
Episode length: 43.92 +/- 13.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 202500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54     |
|    n_updates        | 40624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.9     |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2472     |
|    fps              | 274      |
|    time_elapsed     | 738      |
|    total_timesteps  | 202688   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.27     |
|    n_updates        | 40671    |
----------------------------------
Eval num_timesteps=203000, episode_reward=175.90 +/- 45.25
Episode length: 44.36 +/- 11.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 203000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0107   |
|    n_updates        | 40749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.3     |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2476     |
|    fps              | 274      |
|    time_elapsed     | 740      |
|    total_timesteps  | 203078   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02     |
|    n_updates        | 40769    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.9     |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2480     |
|    fps              | 274      |
|    time_elapsed     | 740      |
|    total_timesteps  | 203465   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00108  |
|    n_updates        | 40866    |
----------------------------------
Eval num_timesteps=203500, episode_reward=187.22 +/- 55.17
Episode length: 47.18 +/- 13.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.2     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 203500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9      |
|    n_updates        | 40874    |
----------------------------------
Eval num_timesteps=204000, episode_reward=167.94 +/- 37.52
Episode length: 42.34 +/- 9.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 204000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.775    |
|    n_updates        | 40999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.1     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2484     |
|    fps              | 274      |
|    time_elapsed     | 744      |
|    total_timesteps  | 204053   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0297   |
|    n_updates        | 41013    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2488     |
|    fps              | 274      |
|    time_elapsed     | 744      |
|    total_timesteps  | 204377   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.921    |
|    n_updates        | 41094    |
----------------------------------
Eval num_timesteps=204500, episode_reward=173.36 +/- 41.49
Episode length: 43.72 +/- 10.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 204500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00631  |
|    n_updates        | 41124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2492     |
|    fps              | 274      |
|    time_elapsed     | 746      |
|    total_timesteps  | 204900   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.578    |
|    n_updates        | 41224    |
----------------------------------
Eval num_timesteps=205000, episode_reward=178.90 +/- 51.26
Episode length: 45.16 +/- 12.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 205000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.66     |
|    n_updates        | 41249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2496     |
|    fps              | 274      |
|    time_elapsed     | 748      |
|    total_timesteps  | 205234   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03     |
|    n_updates        | 41308    |
----------------------------------
Eval num_timesteps=205500, episode_reward=160.60 +/- 56.50
Episode length: 40.48 +/- 14.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.5     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 205500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0413   |
|    n_updates        | 41374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.1     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2500     |
|    fps              | 274      |
|    time_elapsed     | 750      |
|    total_timesteps  | 205528   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.961    |
|    n_updates        | 41381    |
----------------------------------
Eval num_timesteps=206000, episode_reward=163.98 +/- 39.91
Episode length: 41.38 +/- 10.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 206000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.54     |
|    n_updates        | 41499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2504     |
|    fps              | 274      |
|    time_elapsed     | 751      |
|    total_timesteps  | 206047   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.452    |
|    n_updates        | 41511    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.2     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2508     |
|    fps              | 274      |
|    time_elapsed     | 752      |
|    total_timesteps  | 206293   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.04     |
|    n_updates        | 41573    |
----------------------------------
Eval num_timesteps=206500, episode_reward=159.72 +/- 64.44
Episode length: 40.30 +/- 16.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.3     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 206500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0173   |
|    n_updates        | 41624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2512     |
|    fps              | 274      |
|    time_elapsed     | 754      |
|    total_timesteps  | 206844   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.937    |
|    n_updates        | 41710    |
----------------------------------
Eval num_timesteps=207000, episode_reward=183.74 +/- 52.69
Episode length: 46.38 +/- 13.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.23     |
|    n_updates        | 41749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2516     |
|    fps              | 274      |
|    time_elapsed     | 756      |
|    total_timesteps  | 207242   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0057   |
|    n_updates        | 41810    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2520     |
|    fps              | 274      |
|    time_elapsed     | 756      |
|    total_timesteps  | 207493   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00433  |
|    n_updates        | 41873    |
----------------------------------
Eval num_timesteps=207500, episode_reward=172.90 +/- 51.35
Episode length: 43.58 +/- 12.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 207500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.79     |
|    n_updates        | 41874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2524     |
|    fps              | 274      |
|    time_elapsed     | 758      |
|    total_timesteps  | 207959   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0162   |
|    n_updates        | 41989    |
----------------------------------
Eval num_timesteps=208000, episode_reward=193.60 +/- 50.44
Episode length: 48.72 +/- 12.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.7     |
|    mean_reward      | 194      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 208000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.87     |
|    n_updates        | 41999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2528     |
|    fps              | 273      |
|    time_elapsed     | 759      |
|    total_timesteps  | 208145   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000548 |
|    n_updates        | 42036    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.2     |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2532     |
|    fps              | 274      |
|    time_elapsed     | 760      |
|    total_timesteps  | 208408   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.5      |
|    n_updates        | 42101    |
----------------------------------
Eval num_timesteps=208500, episode_reward=174.58 +/- 44.93
Episode length: 44.00 +/- 11.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 208500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.968    |
|    n_updates        | 42124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.9     |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2536     |
|    fps              | 273      |
|    time_elapsed     | 762      |
|    total_timesteps  | 208788   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.89     |
|    n_updates        | 42196    |
----------------------------------
Eval num_timesteps=209000, episode_reward=163.04 +/- 41.03
Episode length: 41.14 +/- 10.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.1     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 209000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.977    |
|    n_updates        | 42249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.7     |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2540     |
|    fps              | 273      |
|    time_elapsed     | 763      |
|    total_timesteps  | 209123   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.036    |
|    n_updates        | 42280    |
----------------------------------
Eval num_timesteps=209500, episode_reward=173.44 +/- 42.26
Episode length: 43.74 +/- 10.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 209500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0291   |
|    n_updates        | 42374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2544     |
|    fps              | 273      |
|    time_elapsed     | 765      |
|    total_timesteps  | 209690   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.413    |
|    n_updates        | 42422    |
----------------------------------
Eval num_timesteps=210000, episode_reward=175.48 +/- 52.71
Episode length: 44.22 +/- 13.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 210000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00971  |
|    n_updates        | 42499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2548     |
|    fps              | 273      |
|    time_elapsed     | 767      |
|    total_timesteps  | 210011   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.84     |
|    n_updates        | 42502    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2552     |
|    fps              | 273      |
|    time_elapsed     | 768      |
|    total_timesteps  | 210454   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03     |
|    n_updates        | 42613    |
----------------------------------
Eval num_timesteps=210500, episode_reward=175.50 +/- 43.72
Episode length: 44.32 +/- 10.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 210500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26     |
|    n_updates        | 42624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.1     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2556     |
|    fps              | 273      |
|    time_elapsed     | 769      |
|    total_timesteps  | 210838   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000309 |
|    n_updates        | 42709    |
----------------------------------
Eval num_timesteps=211000, episode_reward=170.52 +/- 46.10
Episode length: 42.98 +/- 11.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 211000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.976    |
|    n_updates        | 42749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2560     |
|    fps              | 273      |
|    time_elapsed     | 771      |
|    total_timesteps  | 211290   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.424    |
|    n_updates        | 42822    |
----------------------------------
Eval num_timesteps=211500, episode_reward=167.50 +/- 73.08
Episode length: 42.30 +/- 18.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 211500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00176  |
|    n_updates        | 42874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2564     |
|    fps              | 273      |
|    time_elapsed     | 773      |
|    total_timesteps  | 211653   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.945    |
|    n_updates        | 42913    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2568     |
|    fps              | 273      |
|    time_elapsed     | 773      |
|    total_timesteps  | 211840   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2        |
|    n_updates        | 42959    |
----------------------------------
Eval num_timesteps=212000, episode_reward=166.30 +/- 51.72
Episode length: 41.98 +/- 12.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 212000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00426  |
|    n_updates        | 42999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.4     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2572     |
|    fps              | 273      |
|    time_elapsed     | 775      |
|    total_timesteps  | 212125   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0848   |
|    n_updates        | 43031    |
----------------------------------
Eval num_timesteps=212500, episode_reward=180.80 +/- 74.09
Episode length: 45.56 +/- 18.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 212500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.21     |
|    n_updates        | 43124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2576     |
|    fps              | 273      |
|    time_elapsed     | 777      |
|    total_timesteps  | 212627   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00548  |
|    n_updates        | 43156    |
----------------------------------
Eval num_timesteps=213000, episode_reward=187.60 +/- 57.68
Episode length: 47.24 +/- 14.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.2     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 213000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01     |
|    n_updates        | 43249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2580     |
|    fps              | 273      |
|    time_elapsed     | 779      |
|    total_timesteps  | 213055   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01     |
|    n_updates        | 43263    |
----------------------------------
Eval num_timesteps=213500, episode_reward=168.34 +/- 66.54
Episode length: 42.52 +/- 16.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 213500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.96     |
|    n_updates        | 43374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2584     |
|    fps              | 273      |
|    time_elapsed     | 781      |
|    total_timesteps  | 213548   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00165  |
|    n_updates        | 43386    |
----------------------------------
Eval num_timesteps=214000, episode_reward=174.88 +/- 43.32
Episode length: 43.94 +/- 10.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 214000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00544  |
|    n_updates        | 43499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2588     |
|    fps              | 273      |
|    time_elapsed     | 783      |
|    total_timesteps  | 214114   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0027   |
|    n_updates        | 43528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2592     |
|    fps              | 273      |
|    time_elapsed     | 783      |
|    total_timesteps  | 214329   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0575   |
|    n_updates        | 43582    |
----------------------------------
Eval num_timesteps=214500, episode_reward=180.98 +/- 49.43
Episode length: 45.62 +/- 12.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 214500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.29     |
|    n_updates        | 43624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.8     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2596     |
|    fps              | 273      |
|    time_elapsed     | 785      |
|    total_timesteps  | 214709   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00322  |
|    n_updates        | 43677    |
----------------------------------
Eval num_timesteps=215000, episode_reward=173.56 +/- 44.12
Episode length: 43.82 +/- 11.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 215000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00118  |
|    n_updates        | 43749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.9     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2600     |
|    fps              | 273      |
|    time_elapsed     | 787      |
|    total_timesteps  | 215019   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.81     |
|    n_updates        | 43754    |
----------------------------------
Eval num_timesteps=215500, episode_reward=158.52 +/- 36.04
Episode length: 40.00 +/- 9.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40       |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 215500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000443 |
|    n_updates        | 43874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.7     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2604     |
|    fps              | 273      |
|    time_elapsed     | 789      |
|    total_timesteps  | 215519   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.82     |
|    n_updates        | 43879    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2608     |
|    fps              | 273      |
|    time_elapsed     | 789      |
|    total_timesteps  | 215938   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.991    |
|    n_updates        | 43984    |
----------------------------------
Eval num_timesteps=216000, episode_reward=163.02 +/- 53.95
Episode length: 41.12 +/- 13.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.1     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00115  |
|    n_updates        | 43999    |
----------------------------------
Eval num_timesteps=216500, episode_reward=167.48 +/- 40.04
Episode length: 42.24 +/- 10.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 216500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1        |
|    n_updates        | 44124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2612     |
|    fps              | 273      |
|    time_elapsed     | 793      |
|    total_timesteps  | 216612   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0766   |
|    n_updates        | 44152    |
----------------------------------
Eval num_timesteps=217000, episode_reward=170.66 +/- 40.54
Episode length: 43.06 +/- 10.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 217000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.973    |
|    n_updates        | 44249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.1     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2616     |
|    fps              | 272      |
|    time_elapsed     | 795      |
|    total_timesteps  | 217049   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.993    |
|    n_updates        | 44262    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2620     |
|    fps              | 273      |
|    time_elapsed     | 795      |
|    total_timesteps  | 217395   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.534    |
|    n_updates        | 44348    |
----------------------------------
Eval num_timesteps=217500, episode_reward=163.36 +/- 59.31
Episode length: 41.22 +/- 14.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.2     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 217500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0397   |
|    n_updates        | 44374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2624     |
|    fps              | 273      |
|    time_elapsed     | 797      |
|    total_timesteps  | 217698   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.88     |
|    n_updates        | 44424    |
----------------------------------
Eval num_timesteps=218000, episode_reward=180.14 +/- 40.05
Episode length: 45.46 +/- 10.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 218000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.949    |
|    n_updates        | 44499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2628     |
|    fps              | 272      |
|    time_elapsed     | 798      |
|    total_timesteps  | 218022   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01     |
|    n_updates        | 44505    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2632     |
|    fps              | 273      |
|    time_elapsed     | 799      |
|    total_timesteps  | 218393   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00935  |
|    n_updates        | 44598    |
----------------------------------
Eval num_timesteps=218500, episode_reward=171.10 +/- 49.72
Episode length: 43.14 +/- 12.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 218500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00786  |
|    n_updates        | 44624    |
----------------------------------
Eval num_timesteps=219000, episode_reward=174.82 +/- 38.32
Episode length: 44.06 +/- 9.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 219000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000611 |
|    n_updates        | 44749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 410      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2636     |
|    fps              | 272      |
|    time_elapsed     | 802      |
|    total_timesteps  | 219064   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.03     |
|    n_updates        | 44765    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2640     |
|    fps              | 273      |
|    time_elapsed     | 803      |
|    total_timesteps  | 219431   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.25     |
|    n_updates        | 44857    |
----------------------------------
Eval num_timesteps=219500, episode_reward=190.72 +/- 53.35
Episode length: 48.08 +/- 13.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 219500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00849  |
|    n_updates        | 44874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2644     |
|    fps              | 272      |
|    time_elapsed     | 805      |
|    total_timesteps  | 219777   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.141    |
|    n_updates        | 44944    |
----------------------------------
Eval num_timesteps=220000, episode_reward=180.58 +/- 54.94
Episode length: 45.50 +/- 13.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 220000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0541   |
|    n_updates        | 44999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2648     |
|    fps              | 272      |
|    time_elapsed     | 807      |
|    total_timesteps  | 220219   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0681   |
|    n_updates        | 45054    |
----------------------------------
Eval num_timesteps=220500, episode_reward=153.58 +/- 51.97
Episode length: 38.82 +/- 12.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.8     |
|    mean_reward      | 154      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 220500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.49     |
|    n_updates        | 45124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2652     |
|    fps              | 272      |
|    time_elapsed     | 808      |
|    total_timesteps  | 220701   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00934  |
|    n_updates        | 45175    |
----------------------------------
Eval num_timesteps=221000, episode_reward=158.16 +/- 49.01
Episode length: 39.90 +/- 12.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.9     |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 221000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.14     |
|    n_updates        | 45249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2656     |
|    fps              | 272      |
|    time_elapsed     | 810      |
|    total_timesteps  | 221027   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00184  |
|    n_updates        | 45256    |
----------------------------------
Eval num_timesteps=221500, episode_reward=145.68 +/- 46.38
Episode length: 36.78 +/- 11.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.8     |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 221500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00534  |
|    n_updates        | 45374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 413      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2660     |
|    fps              | 272      |
|    time_elapsed     | 812      |
|    total_timesteps  | 221643   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08     |
|    n_updates        | 45410    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2664     |
|    fps              | 273      |
|    time_elapsed     | 812      |
|    total_timesteps  | 221919   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00179  |
|    n_updates        | 45479    |
----------------------------------
Eval num_timesteps=222000, episode_reward=170.58 +/- 45.04
Episode length: 43.18 +/- 11.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 222000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0071   |
|    n_updates        | 45499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 417      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2668     |
|    fps              | 272      |
|    time_elapsed     | 814      |
|    total_timesteps  | 222295   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.226    |
|    n_updates        | 45573    |
----------------------------------
Eval num_timesteps=222500, episode_reward=178.92 +/- 36.83
Episode length: 45.08 +/- 9.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 222500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2        |
|    n_updates        | 45624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 429      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2672     |
|    fps              | 272      |
|    time_elapsed     | 816      |
|    total_timesteps  | 222877   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35     |
|    n_updates        | 45719    |
----------------------------------
Eval num_timesteps=223000, episode_reward=163.00 +/- 38.26
Episode length: 41.10 +/- 9.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.1     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 223000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06     |
|    n_updates        | 45749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 420      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2676     |
|    fps              | 272      |
|    time_elapsed     | 818      |
|    total_timesteps  | 223159   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.96     |
|    n_updates        | 45789    |
----------------------------------
Eval num_timesteps=223500, episode_reward=169.00 +/- 40.40
Episode length: 42.62 +/- 10.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 223500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05     |
|    n_updates        | 45874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 421      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2680     |
|    fps              | 272      |
|    time_elapsed     | 820      |
|    total_timesteps  | 223622   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.11     |
|    n_updates        | 45905    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 416      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2684     |
|    fps              | 272      |
|    time_elapsed     | 820      |
|    total_timesteps  | 223976   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.09     |
|    n_updates        | 45993    |
----------------------------------
Eval num_timesteps=224000, episode_reward=176.18 +/- 51.55
Episode length: 44.42 +/- 12.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 224000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00297  |
|    n_updates        | 45999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2688     |
|    fps              | 272      |
|    time_elapsed     | 822      |
|    total_timesteps  | 224276   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.23     |
|    n_updates        | 46068    |
----------------------------------
Eval num_timesteps=224500, episode_reward=172.32 +/- 41.34
Episode length: 43.42 +/- 10.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 224500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.14     |
|    n_updates        | 46124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2692     |
|    fps              | 272      |
|    time_elapsed     | 824      |
|    total_timesteps  | 224596   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00528  |
|    n_updates        | 46148    |
----------------------------------
Eval num_timesteps=225000, episode_reward=164.66 +/- 64.58
Episode length: 41.52 +/- 16.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 225000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.376    |
|    n_updates        | 46249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2696     |
|    fps              | 272      |
|    time_elapsed     | 826      |
|    total_timesteps  | 225039   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.273    |
|    n_updates        | 46259    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2700     |
|    fps              | 272      |
|    time_elapsed     | 826      |
|    total_timesteps  | 225338   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07     |
|    n_updates        | 46334    |
----------------------------------
Eval num_timesteps=225500, episode_reward=155.94 +/- 52.35
Episode length: 39.40 +/- 13.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.4     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 225500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00571  |
|    n_updates        | 46374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2704     |
|    fps              | 272      |
|    time_elapsed     | 828      |
|    total_timesteps  | 225757   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.05     |
|    n_updates        | 46439    |
----------------------------------
Eval num_timesteps=226000, episode_reward=160.14 +/- 55.34
Episode length: 40.44 +/- 13.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.4     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 226000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0743   |
|    n_updates        | 46499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2708     |
|    fps              | 272      |
|    time_elapsed     | 829      |
|    total_timesteps  | 226068   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.943    |
|    n_updates        | 46516    |
----------------------------------
Eval num_timesteps=226500, episode_reward=163.08 +/- 55.97
Episode length: 41.12 +/- 14.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.1     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 226500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.123    |
|    n_updates        | 46624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2712     |
|    fps              | 272      |
|    time_elapsed     | 831      |
|    total_timesteps  | 226694   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00106  |
|    n_updates        | 46673    |
----------------------------------
Eval num_timesteps=227000, episode_reward=174.86 +/- 52.46
Episode length: 44.12 +/- 13.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 227000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.04     |
|    n_updates        | 46749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2716     |
|    fps              | 272      |
|    time_elapsed     | 833      |
|    total_timesteps  | 227247   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.131    |
|    n_updates        | 46811    |
----------------------------------
Eval num_timesteps=227500, episode_reward=166.02 +/- 59.63
Episode length: 41.86 +/- 14.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.9     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 227500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.04     |
|    n_updates        | 46874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2720     |
|    fps              | 272      |
|    time_elapsed     | 835      |
|    total_timesteps  | 227586   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06     |
|    n_updates        | 46896    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2724     |
|    fps              | 272      |
|    time_elapsed     | 835      |
|    total_timesteps  | 227904   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.04     |
|    n_updates        | 46975    |
----------------------------------
Eval num_timesteps=228000, episode_reward=184.56 +/- 50.36
Episode length: 46.44 +/- 12.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 228000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2        |
|    n_updates        | 46999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2728     |
|    fps              | 272      |
|    time_elapsed     | 837      |
|    total_timesteps  | 228259   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08     |
|    n_updates        | 47064    |
----------------------------------
Eval num_timesteps=228500, episode_reward=162.56 +/- 56.22
Episode length: 41.02 +/- 13.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41       |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 228500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.91     |
|    n_updates        | 47124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2732     |
|    fps              | 272      |
|    time_elapsed     | 839      |
|    total_timesteps  | 228733   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.04     |
|    n_updates        | 47183    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.3     |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2736     |
|    fps              | 272      |
|    time_elapsed     | 839      |
|    total_timesteps  | 228996   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.04     |
|    n_updates        | 47248    |
----------------------------------
Eval num_timesteps=229000, episode_reward=175.96 +/- 53.88
Episode length: 44.44 +/- 13.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 229000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00344  |
|    n_updates        | 47249    |
----------------------------------
Eval num_timesteps=229500, episode_reward=180.44 +/- 50.64
Episode length: 45.44 +/- 12.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 229500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.72     |
|    n_updates        | 47374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2740     |
|    fps              | 272      |
|    time_elapsed     | 843      |
|    total_timesteps  | 229569   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.23     |
|    n_updates        | 47392    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2744     |
|    fps              | 272      |
|    time_elapsed     | 843      |
|    total_timesteps  | 229903   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.06     |
|    n_updates        | 47475    |
----------------------------------
Eval num_timesteps=230000, episode_reward=155.78 +/- 51.90
Episode length: 39.30 +/- 12.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.3     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 230000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00388  |
|    n_updates        | 47499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2748     |
|    fps              | 272      |
|    time_elapsed     | 845      |
|    total_timesteps  | 230214   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.09     |
|    n_updates        | 47553    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.6     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2752     |
|    fps              | 272      |
|    time_elapsed     | 845      |
|    total_timesteps  | 230462   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.14     |
|    n_updates        | 47615    |
----------------------------------
Eval num_timesteps=230500, episode_reward=165.00 +/- 39.36
Episode length: 41.56 +/- 9.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 230500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0259   |
|    n_updates        | 47624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.6     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2756     |
|    fps              | 272      |
|    time_elapsed     | 847      |
|    total_timesteps  | 230890   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.52     |
|    n_updates        | 47722    |
----------------------------------
Eval num_timesteps=231000, episode_reward=168.70 +/- 40.29
Episode length: 42.54 +/- 10.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 231000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05     |
|    n_updates        | 47749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.2     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2760     |
|    fps              | 272      |
|    time_elapsed     | 849      |
|    total_timesteps  | 231461   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0216   |
|    n_updates        | 47865    |
----------------------------------
Eval num_timesteps=231500, episode_reward=174.56 +/- 57.73
Episode length: 44.00 +/- 14.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 231500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02     |
|    n_updates        | 47874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2764     |
|    fps              | 272      |
|    time_elapsed     | 851      |
|    total_timesteps  | 231793   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03     |
|    n_updates        | 47948    |
----------------------------------
Eval num_timesteps=232000, episode_reward=162.48 +/- 36.55
Episode length: 40.96 +/- 9.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41       |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 232000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0496   |
|    n_updates        | 47999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2768     |
|    fps              | 272      |
|    time_elapsed     | 853      |
|    total_timesteps  | 232298   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.832    |
|    n_updates        | 48074    |
----------------------------------
Eval num_timesteps=232500, episode_reward=167.60 +/- 39.27
Episode length: 42.28 +/- 9.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 232500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9      |
|    n_updates        | 48124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.6     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2772     |
|    fps              | 272      |
|    time_elapsed     | 854      |
|    total_timesteps  | 232635   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00192  |
|    n_updates        | 48158    |
----------------------------------
Eval num_timesteps=233000, episode_reward=178.04 +/- 49.11
Episode length: 44.90 +/- 12.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 233000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.824    |
|    n_updates        | 48249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.9     |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2776     |
|    fps              | 272      |
|    time_elapsed     | 857      |
|    total_timesteps  | 233145   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00471  |
|    n_updates        | 48286    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2780     |
|    fps              | 272      |
|    time_elapsed     | 857      |
|    total_timesteps  | 233419   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00949  |
|    n_updates        | 48354    |
----------------------------------
Eval num_timesteps=233500, episode_reward=168.04 +/- 52.77
Episode length: 42.38 +/- 13.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 233500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00936  |
|    n_updates        | 48374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.6     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2784     |
|    fps              | 272      |
|    time_elapsed     | 859      |
|    total_timesteps  | 233734   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.303    |
|    n_updates        | 48433    |
----------------------------------
Eval num_timesteps=234000, episode_reward=180.24 +/- 44.50
Episode length: 45.40 +/- 11.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 234000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0064   |
|    n_updates        | 48499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2788     |
|    fps              | 272      |
|    time_elapsed     | 861      |
|    total_timesteps  | 234300   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.375    |
|    n_updates        | 48574    |
----------------------------------
Eval num_timesteps=234500, episode_reward=175.58 +/- 51.07
Episode length: 44.34 +/- 12.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 234500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.18     |
|    n_updates        | 48624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2792     |
|    fps              | 271      |
|    time_elapsed     | 862      |
|    total_timesteps  | 234575   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.16     |
|    n_updates        | 48643    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2796     |
|    fps              | 272      |
|    time_elapsed     | 863      |
|    total_timesteps  | 234890   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0105   |
|    n_updates        | 48722    |
----------------------------------
Eval num_timesteps=235000, episode_reward=172.18 +/- 45.40
Episode length: 43.40 +/- 11.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 235000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00378  |
|    n_updates        | 48749    |
----------------------------------
Eval num_timesteps=235500, episode_reward=188.34 +/- 46.03
Episode length: 47.44 +/- 11.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.4     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 235500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01     |
|    n_updates        | 48874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2800     |
|    fps              | 271      |
|    time_elapsed     | 866      |
|    total_timesteps  | 235562   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22     |
|    n_updates        | 48890    |
----------------------------------
Eval num_timesteps=236000, episode_reward=151.50 +/- 43.34
Episode length: 38.20 +/- 10.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.2     |
|    mean_reward      | 152      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 236000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43     |
|    n_updates        | 48999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 415      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2804     |
|    fps              | 271      |
|    time_elapsed     | 868      |
|    total_timesteps  | 236177   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11     |
|    n_updates        | 49044    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2808     |
|    fps              | 271      |
|    time_elapsed     | 869      |
|    total_timesteps  | 236340   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.356    |
|    n_updates        | 49084    |
----------------------------------
Eval num_timesteps=236500, episode_reward=179.10 +/- 89.31
Episode length: 45.12 +/- 22.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 236500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.03     |
|    n_updates        | 49124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2812     |
|    fps              | 271      |
|    time_elapsed     | 870      |
|    total_timesteps  | 236616   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13     |
|    n_updates        | 49153    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2816     |
|    fps              | 271      |
|    time_elapsed     | 871      |
|    total_timesteps  | 236928   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29     |
|    n_updates        | 49231    |
----------------------------------
Eval num_timesteps=237000, episode_reward=161.22 +/- 42.56
Episode length: 40.64 +/- 10.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.6     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 237000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00259  |
|    n_updates        | 49249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.6     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2820     |
|    fps              | 271      |
|    time_elapsed     | 872      |
|    total_timesteps  | 237244   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.679    |
|    n_updates        | 49310    |
----------------------------------
Eval num_timesteps=237500, episode_reward=173.74 +/- 39.84
Episode length: 43.80 +/- 10.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 237500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.723    |
|    n_updates        | 49374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.1     |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2824     |
|    fps              | 271      |
|    time_elapsed     | 874      |
|    total_timesteps  | 237814   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.961    |
|    n_updates        | 49453    |
----------------------------------
Eval num_timesteps=238000, episode_reward=186.48 +/- 52.08
Episode length: 47.02 +/- 13.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 238000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.887    |
|    n_updates        | 49499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2828     |
|    fps              | 271      |
|    time_elapsed     | 876      |
|    total_timesteps  | 238268   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23     |
|    n_updates        | 49566    |
----------------------------------
Eval num_timesteps=238500, episode_reward=172.28 +/- 46.92
Episode length: 43.44 +/- 11.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 238500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.12     |
|    n_updates        | 49624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2832     |
|    fps              | 271      |
|    time_elapsed     | 878      |
|    total_timesteps  | 238781   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08     |
|    n_updates        | 49695    |
----------------------------------
Eval num_timesteps=239000, episode_reward=168.72 +/- 40.71
Episode length: 42.58 +/- 10.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 239000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000704 |
|    n_updates        | 49749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2836     |
|    fps              | 271      |
|    time_elapsed     | 880      |
|    total_timesteps  | 239183   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.63     |
|    n_updates        | 49795    |
----------------------------------
Eval num_timesteps=239500, episode_reward=171.26 +/- 48.93
Episode length: 43.18 +/- 12.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 239500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01     |
|    n_updates        | 49874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2840     |
|    fps              | 271      |
|    time_elapsed     | 882      |
|    total_timesteps  | 239586   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08     |
|    n_updates        | 49896    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.9     |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2844     |
|    fps              | 271      |
|    time_elapsed     | 882      |
|    total_timesteps  | 239896   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00189  |
|    n_updates        | 49973    |
----------------------------------
Eval num_timesteps=240000, episode_reward=185.54 +/- 43.31
Episode length: 46.80 +/- 10.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 240000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0158   |
|    n_updates        | 49999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.3     |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2848     |
|    fps              | 271      |
|    time_elapsed     | 884      |
|    total_timesteps  | 240149   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16     |
|    n_updates        | 50037    |
----------------------------------
Eval num_timesteps=240500, episode_reward=180.30 +/- 43.85
Episode length: 45.44 +/- 10.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 240500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16     |
|    n_updates        | 50124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2852     |
|    fps              | 271      |
|    time_elapsed     | 886      |
|    total_timesteps  | 240650   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0048   |
|    n_updates        | 50162    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2856     |
|    fps              | 271      |
|    time_elapsed     | 887      |
|    total_timesteps  | 240982   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00307  |
|    n_updates        | 50245    |
----------------------------------
Eval num_timesteps=241000, episode_reward=167.14 +/- 44.24
Episode length: 42.22 +/- 11.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 241000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15     |
|    n_updates        | 50249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2860     |
|    fps              | 271      |
|    time_elapsed     | 888      |
|    total_timesteps  | 241468   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23     |
|    n_updates        | 50366    |
----------------------------------
Eval num_timesteps=241500, episode_reward=176.84 +/- 48.44
Episode length: 44.62 +/- 12.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 241500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0475   |
|    n_updates        | 50374    |
----------------------------------
Eval num_timesteps=242000, episode_reward=183.92 +/- 54.43
Episode length: 46.36 +/- 13.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 242000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00158  |
|    n_updates        | 50499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2864     |
|    fps              | 271      |
|    time_elapsed     | 892      |
|    total_timesteps  | 242017   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.649    |
|    n_updates        | 50504    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2868     |
|    fps              | 271      |
|    time_elapsed     | 892      |
|    total_timesteps  | 242478   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0803   |
|    n_updates        | 50619    |
----------------------------------
Eval num_timesteps=242500, episode_reward=150.52 +/- 46.16
Episode length: 38.02 +/- 11.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38       |
|    mean_reward      | 151      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 242500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16     |
|    n_updates        | 50624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2872     |
|    fps              | 271      |
|    time_elapsed     | 894      |
|    total_timesteps  | 242783   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000353 |
|    n_updates        | 50695    |
----------------------------------
Eval num_timesteps=243000, episode_reward=168.12 +/- 57.53
Episode length: 42.42 +/- 14.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 243000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00439  |
|    n_updates        | 50749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2876     |
|    fps              | 271      |
|    time_elapsed     | 896      |
|    total_timesteps  | 243262   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.183    |
|    n_updates        | 50815    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2880     |
|    fps              | 271      |
|    time_elapsed     | 896      |
|    total_timesteps  | 243441   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03     |
|    n_updates        | 50860    |
----------------------------------
Eval num_timesteps=243500, episode_reward=166.50 +/- 41.46
Episode length: 41.98 +/- 10.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 243500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.077    |
|    n_updates        | 50874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2884     |
|    fps              | 271      |
|    time_elapsed     | 898      |
|    total_timesteps  | 243974   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.18     |
|    n_updates        | 50993    |
----------------------------------
Eval num_timesteps=244000, episode_reward=178.38 +/- 54.42
Episode length: 44.98 +/- 13.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 244000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00448  |
|    n_updates        | 50999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2888     |
|    fps              | 271      |
|    time_elapsed     | 900      |
|    total_timesteps  | 244381   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.08     |
|    n_updates        | 51095    |
----------------------------------
Eval num_timesteps=244500, episode_reward=172.56 +/- 36.25
Episode length: 43.62 +/- 9.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 244500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00883  |
|    n_updates        | 51124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2892     |
|    fps              | 271      |
|    time_elapsed     | 902      |
|    total_timesteps  | 244722   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.19     |
|    n_updates        | 51180    |
----------------------------------
Eval num_timesteps=245000, episode_reward=183.46 +/- 43.62
Episode length: 46.24 +/- 10.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 245000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000762 |
|    n_updates        | 51249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 410      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2896     |
|    fps              | 271      |
|    time_elapsed     | 904      |
|    total_timesteps  | 245182   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00812  |
|    n_updates        | 51295    |
----------------------------------
Eval num_timesteps=245500, episode_reward=179.54 +/- 43.54
Episode length: 45.20 +/- 10.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 245500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.18     |
|    n_updates        | 51374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2900     |
|    fps              | 270      |
|    time_elapsed     | 906      |
|    total_timesteps  | 245558   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0264   |
|    n_updates        | 51389    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.3     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2904     |
|    fps              | 271      |
|    time_elapsed     | 906      |
|    total_timesteps  | 245908   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.27     |
|    n_updates        | 51476    |
----------------------------------
Eval num_timesteps=246000, episode_reward=158.52 +/- 54.42
Episode length: 39.98 +/- 13.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40       |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 246000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.804    |
|    n_updates        | 51499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2908     |
|    fps              | 271      |
|    time_elapsed     | 908      |
|    total_timesteps  | 246140   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.237    |
|    n_updates        | 51534    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.1     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2912     |
|    fps              | 271      |
|    time_elapsed     | 908      |
|    total_timesteps  | 246426   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.27     |
|    n_updates        | 51606    |
----------------------------------
Eval num_timesteps=246500, episode_reward=167.32 +/- 43.66
Episode length: 42.18 +/- 10.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 246500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.12     |
|    n_updates        | 51624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2916     |
|    fps              | 271      |
|    time_elapsed     | 910      |
|    total_timesteps  | 246696   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00481  |
|    n_updates        | 51673    |
----------------------------------
Eval num_timesteps=247000, episode_reward=184.04 +/- 66.27
Episode length: 46.40 +/- 16.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.4     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 247000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2        |
|    n_updates        | 51749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2920     |
|    fps              | 270      |
|    time_elapsed     | 912      |
|    total_timesteps  | 247011   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54     |
|    n_updates        | 51752    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2924     |
|    fps              | 271      |
|    time_elapsed     | 912      |
|    total_timesteps  | 247465   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.291    |
|    n_updates        | 51866    |
----------------------------------
Eval num_timesteps=247500, episode_reward=185.76 +/- 47.82
Episode length: 46.82 +/- 11.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 247500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00512  |
|    n_updates        | 51874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2928     |
|    fps              | 270      |
|    time_elapsed     | 914      |
|    total_timesteps  | 247776   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05     |
|    n_updates        | 51943    |
----------------------------------
Eval num_timesteps=248000, episode_reward=172.24 +/- 48.23
Episode length: 43.38 +/- 12.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 248000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.12     |
|    n_updates        | 51999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2932     |
|    fps              | 270      |
|    time_elapsed     | 916      |
|    total_timesteps  | 248279   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.1      |
|    n_updates        | 52069    |
----------------------------------
Eval num_timesteps=248500, episode_reward=186.46 +/- 54.40
Episode length: 47.02 +/- 13.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 248500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.878    |
|    n_updates        | 52124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.9     |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2936     |
|    fps              | 271      |
|    time_elapsed     | 918      |
|    total_timesteps  | 248975   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.684    |
|    n_updates        | 52243    |
----------------------------------
Eval num_timesteps=249000, episode_reward=178.36 +/- 36.88
Episode length: 45.02 +/- 9.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 249000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.08     |
|    n_updates        | 52249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.9     |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2940     |
|    fps              | 270      |
|    time_elapsed     | 920      |
|    total_timesteps  | 249377   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.93     |
|    n_updates        | 52344    |
----------------------------------
Eval num_timesteps=249500, episode_reward=170.14 +/- 37.96
Episode length: 42.92 +/- 9.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 249500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00457  |
|    n_updates        | 52374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2944     |
|    fps              | 270      |
|    time_elapsed     | 922      |
|    total_timesteps  | 249700   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00946  |
|    n_updates        | 52424    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2948     |
|    fps              | 270      |
|    time_elapsed     | 922      |
|    total_timesteps  | 249991   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.195    |
|    n_updates        | 52497    |
----------------------------------
Eval num_timesteps=250000, episode_reward=187.22 +/- 47.51
Episode length: 47.18 +/- 11.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.2     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 250000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00102  |
|    n_updates        | 52499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2952     |
|    fps              | 270      |
|    time_elapsed     | 924      |
|    total_timesteps  | 250370   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.68     |
|    n_updates        | 52592    |
----------------------------------
Eval num_timesteps=250500, episode_reward=171.24 +/- 48.22
Episode length: 43.18 +/- 12.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 250500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.923    |
|    n_updates        | 52624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2956     |
|    fps              | 270      |
|    time_elapsed     | 926      |
|    total_timesteps  | 250898   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0174   |
|    n_updates        | 52724    |
----------------------------------
Eval num_timesteps=251000, episode_reward=163.76 +/- 34.77
Episode length: 41.32 +/- 8.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.3     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 251000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.739    |
|    n_updates        | 52749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2960     |
|    fps              | 270      |
|    time_elapsed     | 928      |
|    total_timesteps  | 251172   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00311  |
|    n_updates        | 52792    |
----------------------------------
Eval num_timesteps=251500, episode_reward=183.42 +/- 49.66
Episode length: 46.28 +/- 12.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 251500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.57     |
|    n_updates        | 52874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2964     |
|    fps              | 270      |
|    time_elapsed     | 930      |
|    total_timesteps  | 251642   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.21     |
|    n_updates        | 52910    |
----------------------------------
Eval num_timesteps=252000, episode_reward=169.58 +/- 47.57
Episode length: 42.78 +/- 11.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 252000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0343   |
|    n_updates        | 52999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.9     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2968     |
|    fps              | 270      |
|    time_elapsed     | 932      |
|    total_timesteps  | 252172   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.67     |
|    n_updates        | 53042    |
----------------------------------
Eval num_timesteps=252500, episode_reward=177.54 +/- 47.77
Episode length: 44.80 +/- 11.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 252500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47     |
|    n_updates        | 53124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2972     |
|    fps              | 270      |
|    time_elapsed     | 934      |
|    total_timesteps  | 252665   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.2      |
|    n_updates        | 53166    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2976     |
|    fps              | 270      |
|    time_elapsed     | 934      |
|    total_timesteps  | 252882   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0185   |
|    n_updates        | 53220    |
----------------------------------
Eval num_timesteps=253000, episode_reward=156.06 +/- 33.70
Episode length: 39.34 +/- 8.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.3     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 253000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00213  |
|    n_updates        | 53249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2980     |
|    fps              | 270      |
|    time_elapsed     | 936      |
|    total_timesteps  | 253295   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.23     |
|    n_updates        | 53323    |
----------------------------------
Eval num_timesteps=253500, episode_reward=179.06 +/- 59.03
Episode length: 45.18 +/- 14.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 253500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.758    |
|    n_updates        | 53374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2984     |
|    fps              | 270      |
|    time_elapsed     | 938      |
|    total_timesteps  | 253839   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.32     |
|    n_updates        | 53459    |
----------------------------------
Eval num_timesteps=254000, episode_reward=173.70 +/- 49.99
Episode length: 43.86 +/- 12.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 254000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.55     |
|    n_updates        | 53499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2988     |
|    fps              | 270      |
|    time_elapsed     | 940      |
|    total_timesteps  | 254403   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22     |
|    n_updates        | 53600    |
----------------------------------
Eval num_timesteps=254500, episode_reward=168.74 +/- 66.32
Episode length: 42.60 +/- 16.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 254500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.21     |
|    n_updates        | 53624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2992     |
|    fps              | 270      |
|    time_elapsed     | 942      |
|    total_timesteps  | 254880   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.69     |
|    n_updates        | 53719    |
----------------------------------
Eval num_timesteps=255000, episode_reward=173.94 +/- 43.67
Episode length: 43.82 +/- 10.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 255000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.447    |
|    n_updates        | 53749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 2996     |
|    fps              | 270      |
|    time_elapsed     | 944      |
|    total_timesteps  | 255439   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.76     |
|    n_updates        | 53859    |
----------------------------------
Eval num_timesteps=255500, episode_reward=172.20 +/- 45.53
Episode length: 43.42 +/- 11.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 255500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.826    |
|    n_updates        | 53874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 413      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3000     |
|    fps              | 270      |
|    time_elapsed     | 946      |
|    total_timesteps  | 255927   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.374    |
|    n_updates        | 53981    |
----------------------------------
Eval num_timesteps=256000, episode_reward=180.10 +/- 41.48
Episode length: 45.42 +/- 10.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 256000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.09     |
|    n_updates        | 53999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 413      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3004     |
|    fps              | 270      |
|    time_elapsed     | 948      |
|    total_timesteps  | 256279   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.18     |
|    n_updates        | 54069    |
----------------------------------
Eval num_timesteps=256500, episode_reward=171.62 +/- 45.41
Episode length: 43.28 +/- 11.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 256500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00834  |
|    n_updates        | 54124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 424      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3008     |
|    fps              | 270      |
|    time_elapsed     | 950      |
|    total_timesteps  | 256777   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0161   |
|    n_updates        | 54194    |
----------------------------------
Eval num_timesteps=257000, episode_reward=174.34 +/- 41.64
Episode length: 43.92 +/- 10.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 257000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0985   |
|    n_updates        | 54249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 439      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3012     |
|    fps              | 270      |
|    time_elapsed     | 952      |
|    total_timesteps  | 257449   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26     |
|    n_updates        | 54362    |
----------------------------------
Eval num_timesteps=257500, episode_reward=186.38 +/- 58.48
Episode length: 47.00 +/- 14.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 257500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00126  |
|    n_updates        | 54374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 441      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3016     |
|    fps              | 270      |
|    time_elapsed     | 954      |
|    total_timesteps  | 257745   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.43     |
|    n_updates        | 54436    |
----------------------------------
Eval num_timesteps=258000, episode_reward=186.90 +/- 40.10
Episode length: 47.14 +/- 10.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.1     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 258000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.07     |
|    n_updates        | 54499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 447      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3020     |
|    fps              | 270      |
|    time_elapsed     | 956      |
|    total_timesteps  | 258227   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0586   |
|    n_updates        | 54556    |
----------------------------------
Eval num_timesteps=258500, episode_reward=173.04 +/- 45.79
Episode length: 43.54 +/- 11.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 258500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08     |
|    n_updates        | 54624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 444      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3024     |
|    fps              | 269      |
|    time_elapsed     | 958      |
|    total_timesteps  | 258589   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.68     |
|    n_updates        | 54647    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 441      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3028     |
|    fps              | 270      |
|    time_elapsed     | 958      |
|    total_timesteps  | 258830   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000471 |
|    n_updates        | 54707    |
----------------------------------
Eval num_timesteps=259000, episode_reward=149.82 +/- 48.88
Episode length: 37.82 +/- 12.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.8     |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 259000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.606    |
|    n_updates        | 54749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 441      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3032     |
|    fps              | 270      |
|    time_elapsed     | 960      |
|    total_timesteps  | 259351   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.01     |
|    n_updates        | 54837    |
----------------------------------
Eval num_timesteps=259500, episode_reward=167.72 +/- 39.53
Episode length: 42.32 +/- 9.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 259500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.2      |
|    n_updates        | 54874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 430      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3036     |
|    fps              | 270      |
|    time_elapsed     | 961      |
|    total_timesteps  | 259758   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.38     |
|    n_updates        | 54939    |
----------------------------------
Eval num_timesteps=260000, episode_reward=181.70 +/- 48.19
Episode length: 45.84 +/- 12.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 260000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.23     |
|    n_updates        | 54999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 429      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3040     |
|    fps              | 269      |
|    time_elapsed     | 963      |
|    total_timesteps  | 260130   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.134    |
|    n_updates        | 55032    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 429      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3044     |
|    fps              | 270      |
|    time_elapsed     | 964      |
|    total_timesteps  | 260470   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00442  |
|    n_updates        | 55117    |
----------------------------------
Eval num_timesteps=260500, episode_reward=168.24 +/- 60.36
Episode length: 42.38 +/- 15.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 260500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.25     |
|    n_updates        | 55124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 425      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3048     |
|    fps              | 269      |
|    time_elapsed     | 965      |
|    total_timesteps  | 260651   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.31     |
|    n_updates        | 55162    |
----------------------------------
Eval num_timesteps=261000, episode_reward=173.72 +/- 46.40
Episode length: 43.78 +/- 11.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 261000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0031   |
|    n_updates        | 55249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 438      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3052     |
|    fps              | 270      |
|    time_elapsed     | 967      |
|    total_timesteps  | 261364   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.339    |
|    n_updates        | 55340    |
----------------------------------
Eval num_timesteps=261500, episode_reward=187.84 +/- 53.38
Episode length: 47.30 +/- 13.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 261500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00839  |
|    n_updates        | 55374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 441      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3056     |
|    fps              | 270      |
|    time_elapsed     | 970      |
|    total_timesteps  | 261955   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00236  |
|    n_updates        | 55488    |
----------------------------------
Eval num_timesteps=262000, episode_reward=162.38 +/- 52.90
Episode length: 40.98 +/- 13.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41       |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 262000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.92     |
|    n_updates        | 55499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 448      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3060     |
|    fps              | 269      |
|    time_elapsed     | 971      |
|    total_timesteps  | 262412   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.99     |
|    n_updates        | 55602    |
----------------------------------
Eval num_timesteps=262500, episode_reward=171.62 +/- 51.83
Episode length: 43.22 +/- 13.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 262500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0422   |
|    n_updates        | 55624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 441      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3064     |
|    fps              | 269      |
|    time_elapsed     | 973      |
|    total_timesteps  | 262702   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.886    |
|    n_updates        | 55675    |
----------------------------------
Eval num_timesteps=263000, episode_reward=154.90 +/- 53.30
Episode length: 39.14 +/- 13.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.1     |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 263000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.002    |
|    n_updates        | 55749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 443      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3068     |
|    fps              | 269      |
|    time_elapsed     | 975      |
|    total_timesteps  | 263274   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.003    |
|    n_updates        | 55818    |
----------------------------------
Eval num_timesteps=263500, episode_reward=171.72 +/- 36.44
Episode length: 43.30 +/- 9.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 263500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00934  |
|    n_updates        | 55874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 434      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3072     |
|    fps              | 269      |
|    time_elapsed     | 977      |
|    total_timesteps  | 263557   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28     |
|    n_updates        | 55889    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 442      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3076     |
|    fps              | 269      |
|    time_elapsed     | 977      |
|    total_timesteps  | 263966   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.109    |
|    n_updates        | 55991    |
----------------------------------
Eval num_timesteps=264000, episode_reward=188.38 +/- 50.79
Episode length: 47.48 +/- 12.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 264000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00668  |
|    n_updates        | 55999    |
----------------------------------
Eval num_timesteps=264500, episode_reward=179.68 +/- 52.47
Episode length: 45.26 +/- 13.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 264500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.23     |
|    n_updates        | 56124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 450      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3080     |
|    fps              | 269      |
|    time_elapsed     | 981      |
|    total_timesteps  | 264579   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0409   |
|    n_updates        | 56144    |
----------------------------------
Eval num_timesteps=265000, episode_reward=161.36 +/- 64.75
Episode length: 40.72 +/- 16.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.7     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 265000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.17     |
|    n_updates        | 56249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 446      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3084     |
|    fps              | 269      |
|    time_elapsed     | 983      |
|    total_timesteps  | 265036   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.982    |
|    n_updates        | 56258    |
----------------------------------
Eval num_timesteps=265500, episode_reward=164.02 +/- 37.97
Episode length: 41.42 +/- 9.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 265500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00267  |
|    n_updates        | 56374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 448      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3088     |
|    fps              | 269      |
|    time_elapsed     | 985      |
|    total_timesteps  | 265638   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41     |
|    n_updates        | 56409    |
----------------------------------
Eval num_timesteps=266000, episode_reward=179.80 +/- 56.59
Episode length: 45.28 +/- 14.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 266000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.5      |
|    n_updates        | 56499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 444      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3092     |
|    fps              | 269      |
|    time_elapsed     | 987      |
|    total_timesteps  | 266013   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00342  |
|    n_updates        | 56503    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 435      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3096     |
|    fps              | 269      |
|    time_elapsed     | 987      |
|    total_timesteps  | 266354   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.535    |
|    n_updates        | 56588    |
----------------------------------
Eval num_timesteps=266500, episode_reward=186.56 +/- 55.95
Episode length: 46.98 +/- 13.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 266500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0181   |
|    n_updates        | 56624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 432      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3100     |
|    fps              | 269      |
|    time_elapsed     | 989      |
|    total_timesteps  | 266762   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23     |
|    n_updates        | 56690    |
----------------------------------
Eval num_timesteps=267000, episode_reward=167.26 +/- 45.14
Episode length: 42.18 +/- 11.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 267000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.25     |
|    n_updates        | 56749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 439      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3104     |
|    fps              | 269      |
|    time_elapsed     | 991      |
|    total_timesteps  | 267279   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02     |
|    n_updates        | 56819    |
----------------------------------
Eval num_timesteps=267500, episode_reward=179.78 +/- 45.23
Episode length: 45.32 +/- 11.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 267500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.23     |
|    n_updates        | 56874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 432      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3108     |
|    fps              | 269      |
|    time_elapsed     | 993      |
|    total_timesteps  | 267602   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26     |
|    n_updates        | 56900    |
----------------------------------
Eval num_timesteps=268000, episode_reward=164.76 +/- 60.89
Episode length: 41.60 +/- 15.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 268000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26     |
|    n_updates        | 56999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 427      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3112     |
|    fps              | 269      |
|    time_elapsed     | 995      |
|    total_timesteps  | 268157   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.994    |
|    n_updates        | 57039    |
----------------------------------
Eval num_timesteps=268500, episode_reward=166.78 +/- 54.16
Episode length: 42.10 +/- 13.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.1     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 268500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00283  |
|    n_updates        | 57124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 434      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3116     |
|    fps              | 269      |
|    time_elapsed     | 996      |
|    total_timesteps  | 268632   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00895  |
|    n_updates        | 57157    |
----------------------------------
Eval num_timesteps=269000, episode_reward=189.68 +/- 49.44
Episode length: 47.80 +/- 12.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.8     |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 269000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.82     |
|    n_updates        | 57249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 438      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3120     |
|    fps              | 269      |
|    time_elapsed     | 999      |
|    total_timesteps  | 269224   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26     |
|    n_updates        | 57305    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 435      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3124     |
|    fps              | 269      |
|    time_elapsed     | 999      |
|    total_timesteps  | 269493   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.564    |
|    n_updates        | 57373    |
----------------------------------
Eval num_timesteps=269500, episode_reward=175.48 +/- 42.61
Episode length: 44.20 +/- 10.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 269500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26     |
|    n_updates        | 57374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 440      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3128     |
|    fps              | 269      |
|    time_elapsed     | 1001     |
|    total_timesteps  | 269876   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.151    |
|    n_updates        | 57468    |
----------------------------------
Eval num_timesteps=270000, episode_reward=174.12 +/- 48.63
Episode length: 43.92 +/- 12.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 270000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.18     |
|    n_updates        | 57499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 433      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3132     |
|    fps              | 269      |
|    time_elapsed     | 1003     |
|    total_timesteps  | 270201   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33     |
|    n_updates        | 57550    |
----------------------------------
Eval num_timesteps=270500, episode_reward=166.22 +/- 54.30
Episode length: 41.90 +/- 13.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.9     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 270500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.228    |
|    n_updates        | 57624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 430      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3136     |
|    fps              | 269      |
|    time_elapsed     | 1004     |
|    total_timesteps  | 270553   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.207    |
|    n_updates        | 57638    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 426      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3140     |
|    fps              | 269      |
|    time_elapsed     | 1005     |
|    total_timesteps  | 270828   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35     |
|    n_updates        | 57706    |
----------------------------------
Eval num_timesteps=271000, episode_reward=162.12 +/- 42.63
Episode length: 40.92 +/- 10.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.9     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 271000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28     |
|    n_updates        | 57749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 432      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3144     |
|    fps              | 269      |
|    time_elapsed     | 1007     |
|    total_timesteps  | 271304   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.27     |
|    n_updates        | 57825    |
----------------------------------
Eval num_timesteps=271500, episode_reward=185.44 +/- 46.83
Episode length: 46.72 +/- 11.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 271500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.67     |
|    n_updates        | 57874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 449      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3148     |
|    fps              | 269      |
|    time_elapsed     | 1009     |
|    total_timesteps  | 271924   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.55     |
|    n_updates        | 57980    |
----------------------------------
Eval num_timesteps=272000, episode_reward=180.38 +/- 74.18
Episode length: 45.48 +/- 18.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 272000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0788   |
|    n_updates        | 57999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 435      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3152     |
|    fps              | 269      |
|    time_elapsed     | 1011     |
|    total_timesteps  | 272274   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00665  |
|    n_updates        | 58068    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 419      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3156     |
|    fps              | 269      |
|    time_elapsed     | 1011     |
|    total_timesteps  | 272462   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00272  |
|    n_updates        | 58115    |
----------------------------------
Eval num_timesteps=272500, episode_reward=147.68 +/- 44.21
Episode length: 37.32 +/- 11.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.3     |
|    mean_reward      | 148      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 272500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47     |
|    n_updates        | 58124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 419      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3160     |
|    fps              | 269      |
|    time_elapsed     | 1013     |
|    total_timesteps  | 272920   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.925    |
|    n_updates        | 58229    |
----------------------------------
Eval num_timesteps=273000, episode_reward=153.32 +/- 38.76
Episode length: 38.70 +/- 9.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.7     |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 273000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000639 |
|    n_updates        | 58249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 422      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3164     |
|    fps              | 269      |
|    time_elapsed     | 1014     |
|    total_timesteps  | 273304   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00899  |
|    n_updates        | 58325    |
----------------------------------
Eval num_timesteps=273500, episode_reward=162.60 +/- 55.46
Episode length: 41.06 +/- 13.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.1     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 273500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000193 |
|    n_updates        | 58374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 414      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3168     |
|    fps              | 269      |
|    time_elapsed     | 1016     |
|    total_timesteps  | 273668   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00115  |
|    n_updates        | 58416    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 413      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3172     |
|    fps              | 269      |
|    time_elapsed     | 1016     |
|    total_timesteps  | 273930   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.203    |
|    n_updates        | 58482    |
----------------------------------
Eval num_timesteps=274000, episode_reward=182.10 +/- 43.26
Episode length: 45.92 +/- 10.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 274000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0129   |
|    n_updates        | 58499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 413      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3176     |
|    fps              | 269      |
|    time_elapsed     | 1018     |
|    total_timesteps  | 274343   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.3      |
|    n_updates        | 58585    |
----------------------------------
Eval num_timesteps=274500, episode_reward=172.54 +/- 40.87
Episode length: 43.46 +/- 10.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 274500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.14     |
|    n_updates        | 58624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3180     |
|    fps              | 269      |
|    time_elapsed     | 1020     |
|    total_timesteps  | 274725   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.34     |
|    n_updates        | 58681    |
----------------------------------
Eval num_timesteps=275000, episode_reward=173.18 +/- 41.52
Episode length: 43.70 +/- 10.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 275000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.31     |
|    n_updates        | 58749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3184     |
|    fps              | 269      |
|    time_elapsed     | 1022     |
|    total_timesteps  | 275087   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.881    |
|    n_updates        | 58771    |
----------------------------------
Eval num_timesteps=275500, episode_reward=191.66 +/- 54.03
Episode length: 48.32 +/- 13.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.3     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 275500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00382  |
|    n_updates        | 58874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3188     |
|    fps              | 268      |
|    time_elapsed     | 1024     |
|    total_timesteps  | 275511   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00254  |
|    n_updates        | 58877    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3192     |
|    fps              | 269      |
|    time_elapsed     | 1024     |
|    total_timesteps  | 275868   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00297  |
|    n_updates        | 58966    |
----------------------------------
Eval num_timesteps=276000, episode_reward=147.78 +/- 41.40
Episode length: 37.30 +/- 10.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.3     |
|    mean_reward      | 148      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 276000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00333  |
|    n_updates        | 58999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3196     |
|    fps              | 269      |
|    time_elapsed     | 1026     |
|    total_timesteps  | 276198   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.62     |
|    n_updates        | 59049    |
----------------------------------
Eval num_timesteps=276500, episode_reward=167.78 +/- 42.07
Episode length: 42.34 +/- 10.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 276500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0267   |
|    n_updates        | 59124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3200     |
|    fps              | 268      |
|    time_elapsed     | 1028     |
|    total_timesteps  | 276561   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0145   |
|    n_updates        | 59140    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.3     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3204     |
|    fps              | 269      |
|    time_elapsed     | 1028     |
|    total_timesteps  | 276907   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00117  |
|    n_updates        | 59226    |
----------------------------------
Eval num_timesteps=277000, episode_reward=180.18 +/- 52.29
Episode length: 45.42 +/- 13.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 277000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.62     |
|    n_updates        | 59249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3208     |
|    fps              | 269      |
|    time_elapsed     | 1030     |
|    total_timesteps  | 277373   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.18     |
|    n_updates        | 59343    |
----------------------------------
Eval num_timesteps=277500, episode_reward=161.32 +/- 57.95
Episode length: 40.68 +/- 14.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.7     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 277500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.27     |
|    n_updates        | 59374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3212     |
|    fps              | 269      |
|    time_elapsed     | 1032     |
|    total_timesteps  | 277760   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.82     |
|    n_updates        | 59439    |
----------------------------------
Eval num_timesteps=278000, episode_reward=180.98 +/- 47.71
Episode length: 45.66 +/- 11.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 278000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0032   |
|    n_updates        | 59499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3216     |
|    fps              | 269      |
|    time_elapsed     | 1034     |
|    total_timesteps  | 278404   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29     |
|    n_updates        | 59600    |
----------------------------------
Eval num_timesteps=278500, episode_reward=164.92 +/- 53.41
Episode length: 41.60 +/- 13.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 278500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.58     |
|    n_updates        | 59624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.5     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3220     |
|    fps              | 268      |
|    time_elapsed     | 1036     |
|    total_timesteps  | 278674   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16     |
|    n_updates        | 59668    |
----------------------------------
Eval num_timesteps=279000, episode_reward=179.16 +/- 48.50
Episode length: 45.24 +/- 12.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 279000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.5      |
|    n_updates        | 59749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3224     |
|    fps              | 268      |
|    time_elapsed     | 1038     |
|    total_timesteps  | 279109   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000564 |
|    n_updates        | 59777    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.6     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3228     |
|    fps              | 269      |
|    time_elapsed     | 1038     |
|    total_timesteps  | 279337   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.2      |
|    n_updates        | 59834    |
----------------------------------
Eval num_timesteps=279500, episode_reward=175.40 +/- 47.92
Episode length: 44.18 +/- 11.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 279500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.791    |
|    n_updates        | 59874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.7     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3232     |
|    fps              | 268      |
|    time_elapsed     | 1040     |
|    total_timesteps  | 279672   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.124    |
|    n_updates        | 59917    |
----------------------------------
Eval num_timesteps=280000, episode_reward=163.60 +/- 48.82
Episode length: 41.30 +/- 12.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.3     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 280000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33     |
|    n_updates        | 59999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3236     |
|    fps              | 268      |
|    time_elapsed     | 1041     |
|    total_timesteps  | 280201   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.34     |
|    n_updates        | 60050    |
----------------------------------
Eval num_timesteps=280500, episode_reward=165.34 +/- 52.80
Episode length: 41.74 +/- 13.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.7     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 280500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0119   |
|    n_updates        | 60124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3240     |
|    fps              | 268      |
|    time_elapsed     | 1043     |
|    total_timesteps  | 280568   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.5      |
|    n_updates        | 60141    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.4     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3244     |
|    fps              | 268      |
|    time_elapsed     | 1044     |
|    total_timesteps  | 280846   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4      |
|    n_updates        | 60211    |
----------------------------------
Eval num_timesteps=281000, episode_reward=181.14 +/- 50.51
Episode length: 45.56 +/- 12.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 281000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.36     |
|    n_updates        | 60249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3248     |
|    fps              | 268      |
|    time_elapsed     | 1046     |
|    total_timesteps  | 281358   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.19     |
|    n_updates        | 60339    |
----------------------------------
Eval num_timesteps=281500, episode_reward=172.24 +/- 43.11
Episode length: 43.36 +/- 10.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 281500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.05     |
|    n_updates        | 60374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.7     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3252     |
|    fps              | 268      |
|    time_elapsed     | 1047     |
|    total_timesteps  | 281646   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000572 |
|    n_updates        | 60411    |
----------------------------------
Eval num_timesteps=282000, episode_reward=169.66 +/- 37.02
Episode length: 42.86 +/- 9.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 282000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0102   |
|    n_updates        | 60499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3256     |
|    fps              | 268      |
|    time_elapsed     | 1049     |
|    total_timesteps  | 282114   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00119  |
|    n_updates        | 60528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.6     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3260     |
|    fps              | 268      |
|    time_elapsed     | 1050     |
|    total_timesteps  | 282478   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00424  |
|    n_updates        | 60619    |
----------------------------------
Eval num_timesteps=282500, episode_reward=168.12 +/- 51.73
Episode length: 42.36 +/- 12.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 282500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33     |
|    n_updates        | 60624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3264     |
|    fps              | 268      |
|    time_elapsed     | 1051     |
|    total_timesteps  | 282735   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00693  |
|    n_updates        | 60683    |
----------------------------------
Eval num_timesteps=283000, episode_reward=169.82 +/- 41.43
Episode length: 42.86 +/- 10.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 283000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00129  |
|    n_updates        | 60749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.3     |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3268     |
|    fps              | 268      |
|    time_elapsed     | 1053     |
|    total_timesteps  | 283000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3272     |
|    fps              | 268      |
|    time_elapsed     | 1053     |
|    total_timesteps  | 283308   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33     |
|    n_updates        | 60826    |
----------------------------------
Eval num_timesteps=283500, episode_reward=162.68 +/- 50.21
Episode length: 41.08 +/- 12.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.1     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 283500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.575    |
|    n_updates        | 60874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3276     |
|    fps              | 268      |
|    time_elapsed     | 1055     |
|    total_timesteps  | 283991   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0186   |
|    n_updates        | 60997    |
----------------------------------
Eval num_timesteps=284000, episode_reward=167.54 +/- 41.58
Episode length: 42.22 +/- 10.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 284000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0148   |
|    n_updates        | 60999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.3     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3280     |
|    fps              | 268      |
|    time_elapsed     | 1057     |
|    total_timesteps  | 284255   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.69     |
|    n_updates        | 61063    |
----------------------------------
Eval num_timesteps=284500, episode_reward=151.40 +/- 42.38
Episode length: 38.20 +/- 10.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.2     |
|    mean_reward      | 151      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 284500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0129   |
|    n_updates        | 61124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3284     |
|    fps              | 268      |
|    time_elapsed     | 1059     |
|    total_timesteps  | 284689   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00459  |
|    n_updates        | 61172    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.1     |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3288     |
|    fps              | 268      |
|    time_elapsed     | 1059     |
|    total_timesteps  | 284919   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.88     |
|    n_updates        | 61229    |
----------------------------------
Eval num_timesteps=285000, episode_reward=165.30 +/- 46.79
Episode length: 41.62 +/- 11.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 285000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0221   |
|    n_updates        | 61249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.5     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3292     |
|    fps              | 268      |
|    time_elapsed     | 1061     |
|    total_timesteps  | 285318   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.59     |
|    n_updates        | 61329    |
----------------------------------
Eval num_timesteps=285500, episode_reward=150.28 +/- 43.85
Episode length: 38.02 +/- 10.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38       |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 285500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0454   |
|    n_updates        | 61374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3296     |
|    fps              | 268      |
|    time_elapsed     | 1063     |
|    total_timesteps  | 285817   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00403  |
|    n_updates        | 61454    |
----------------------------------
Eval num_timesteps=286000, episode_reward=171.92 +/- 50.53
Episode length: 43.38 +/- 12.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 286000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000904 |
|    n_updates        | 61499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3300     |
|    fps              | 268      |
|    time_elapsed     | 1065     |
|    total_timesteps  | 286285   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41     |
|    n_updates        | 61571    |
----------------------------------
Eval num_timesteps=286500, episode_reward=169.38 +/- 43.45
Episode length: 42.70 +/- 10.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 286500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33     |
|    n_updates        | 61624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.9     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3304     |
|    fps              | 268      |
|    time_elapsed     | 1066     |
|    total_timesteps  | 286799   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.36     |
|    n_updates        | 61699    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3308     |
|    fps              | 268      |
|    time_elapsed     | 1067     |
|    total_timesteps  | 286996   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00167  |
|    n_updates        | 61748    |
----------------------------------
Eval num_timesteps=287000, episode_reward=176.16 +/- 40.24
Episode length: 44.46 +/- 10.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 287000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.11     |
|    n_updates        | 61749    |
----------------------------------
Eval num_timesteps=287500, episode_reward=177.38 +/- 43.82
Episode length: 44.76 +/- 10.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 287500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.843    |
|    n_updates        | 61874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3312     |
|    fps              | 268      |
|    time_elapsed     | 1070     |
|    total_timesteps  | 287510   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00716  |
|    n_updates        | 61877    |
----------------------------------
Eval num_timesteps=288000, episode_reward=175.94 +/- 51.44
Episode length: 44.30 +/- 12.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 288000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.57     |
|    n_updates        | 61999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3316     |
|    fps              | 268      |
|    time_elapsed     | 1072     |
|    total_timesteps  | 288069   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0404   |
|    n_updates        | 62017    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3320     |
|    fps              | 268      |
|    time_elapsed     | 1073     |
|    total_timesteps  | 288418   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43     |
|    n_updates        | 62104    |
----------------------------------
Eval num_timesteps=288500, episode_reward=157.54 +/- 37.63
Episode length: 39.80 +/- 9.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.8     |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 288500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.03     |
|    n_updates        | 62124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3324     |
|    fps              | 268      |
|    time_elapsed     | 1074     |
|    total_timesteps  | 288656   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.33     |
|    n_updates        | 62163    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3328     |
|    fps              | 268      |
|    time_elapsed     | 1074     |
|    total_timesteps  | 288958   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000548 |
|    n_updates        | 62239    |
----------------------------------
Eval num_timesteps=289000, episode_reward=167.68 +/- 37.49
Episode length: 42.28 +/- 9.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 289000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.363    |
|    n_updates        | 62249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3332     |
|    fps              | 268      |
|    time_elapsed     | 1076     |
|    total_timesteps  | 289243   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0205   |
|    n_updates        | 62310    |
----------------------------------
Eval num_timesteps=289500, episode_reward=142.60 +/- 39.65
Episode length: 36.04 +/- 9.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36       |
|    mean_reward      | 143      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 289500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.38     |
|    n_updates        | 62374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.9     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3336     |
|    fps              | 268      |
|    time_elapsed     | 1078     |
|    total_timesteps  | 289595   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0807   |
|    n_updates        | 62398    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.7     |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3340     |
|    fps              | 268      |
|    time_elapsed     | 1078     |
|    total_timesteps  | 289839   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0265   |
|    n_updates        | 62459    |
----------------------------------
Eval num_timesteps=290000, episode_reward=175.12 +/- 41.97
Episode length: 44.12 +/- 10.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 290000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00463  |
|    n_updates        | 62499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3344     |
|    fps              | 268      |
|    time_elapsed     | 1080     |
|    total_timesteps  | 290360   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00337  |
|    n_updates        | 62589    |
----------------------------------
Eval num_timesteps=290500, episode_reward=188.96 +/- 58.91
Episode length: 47.64 +/- 14.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 290500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00148  |
|    n_updates        | 62624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3348     |
|    fps              | 268      |
|    time_elapsed     | 1082     |
|    total_timesteps  | 290868   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00104  |
|    n_updates        | 62716    |
----------------------------------
Eval num_timesteps=291000, episode_reward=170.48 +/- 43.56
Episode length: 43.04 +/- 10.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 291000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00114  |
|    n_updates        | 62749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3352     |
|    fps              | 268      |
|    time_elapsed     | 1084     |
|    total_timesteps  | 291263   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.82     |
|    n_updates        | 62815    |
----------------------------------
Eval num_timesteps=291500, episode_reward=174.44 +/- 46.85
Episode length: 43.94 +/- 11.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 291500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0146   |
|    n_updates        | 62874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3356     |
|    fps              | 268      |
|    time_elapsed     | 1086     |
|    total_timesteps  | 291859   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0142   |
|    n_updates        | 62964    |
----------------------------------
Eval num_timesteps=292000, episode_reward=188.92 +/- 41.58
Episode length: 47.64 +/- 10.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 292000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00045  |
|    n_updates        | 62999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.6     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3360     |
|    fps              | 268      |
|    time_elapsed     | 1088     |
|    total_timesteps  | 292142   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4      |
|    n_updates        | 63035    |
----------------------------------
Eval num_timesteps=292500, episode_reward=157.26 +/- 41.07
Episode length: 39.72 +/- 10.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.7     |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 292500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41     |
|    n_updates        | 63124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3364     |
|    fps              | 268      |
|    time_elapsed     | 1090     |
|    total_timesteps  | 292600   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.38     |
|    n_updates        | 63149    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.1     |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3368     |
|    fps              | 268      |
|    time_elapsed     | 1090     |
|    total_timesteps  | 292907   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.64     |
|    n_updates        | 63226    |
----------------------------------
Eval num_timesteps=293000, episode_reward=169.22 +/- 42.63
Episode length: 42.72 +/- 10.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 293000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.031    |
|    n_updates        | 63249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3372     |
|    fps              | 268      |
|    time_elapsed     | 1092     |
|    total_timesteps  | 293229   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.51     |
|    n_updates        | 63307    |
----------------------------------
Eval num_timesteps=293500, episode_reward=170.54 +/- 39.83
Episode length: 42.88 +/- 9.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 293500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00866  |
|    n_updates        | 63374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.1     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3376     |
|    fps              | 268      |
|    time_elapsed     | 1093     |
|    total_timesteps  | 293599   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.84     |
|    n_updates        | 63399    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3380     |
|    fps              | 268      |
|    time_elapsed     | 1094     |
|    total_timesteps  | 293876   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.65     |
|    n_updates        | 63468    |
----------------------------------
Eval num_timesteps=294000, episode_reward=172.80 +/- 48.49
Episode length: 43.66 +/- 12.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 294000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41     |
|    n_updates        | 63499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.4     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3384     |
|    fps              | 268      |
|    time_elapsed     | 1096     |
|    total_timesteps  | 294225   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00145  |
|    n_updates        | 63556    |
----------------------------------
Eval num_timesteps=294500, episode_reward=170.16 +/- 41.73
Episode length: 42.92 +/- 10.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 294500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00373  |
|    n_updates        | 63624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3388     |
|    fps              | 268      |
|    time_elapsed     | 1097     |
|    total_timesteps  | 294599   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.39     |
|    n_updates        | 63649    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3392     |
|    fps              | 268      |
|    time_elapsed     | 1098     |
|    total_timesteps  | 294915   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.8      |
|    n_updates        | 63728    |
----------------------------------
Eval num_timesteps=295000, episode_reward=166.36 +/- 37.57
Episode length: 41.96 +/- 9.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 295000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0073   |
|    n_updates        | 63749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3396     |
|    fps              | 268      |
|    time_elapsed     | 1100     |
|    total_timesteps  | 295330   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.45     |
|    n_updates        | 63832    |
----------------------------------
Eval num_timesteps=295500, episode_reward=167.84 +/- 46.67
Episode length: 42.32 +/- 11.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 295500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00557  |
|    n_updates        | 63874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.7     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3400     |
|    fps              | 268      |
|    time_elapsed     | 1101     |
|    total_timesteps  | 295755   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35     |
|    n_updates        | 63938    |
----------------------------------
Eval num_timesteps=296000, episode_reward=176.34 +/- 51.63
Episode length: 44.52 +/- 12.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 296000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.13     |
|    n_updates        | 63999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.5     |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3404     |
|    fps              | 268      |
|    time_elapsed     | 1103     |
|    total_timesteps  | 296148   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.81     |
|    n_updates        | 64036    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.7     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3408     |
|    fps              | 268      |
|    time_elapsed     | 1104     |
|    total_timesteps  | 296470   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4      |
|    n_updates        | 64117    |
----------------------------------
Eval num_timesteps=296500, episode_reward=164.34 +/- 50.89
Episode length: 41.50 +/- 12.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 296500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00208  |
|    n_updates        | 64124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.6     |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3412     |
|    fps              | 268      |
|    time_elapsed     | 1105     |
|    total_timesteps  | 296872   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.4      |
|    n_updates        | 64217    |
----------------------------------
Eval num_timesteps=297000, episode_reward=172.56 +/- 36.00
Episode length: 43.52 +/- 9.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 297000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41     |
|    n_updates        | 64249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.1     |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3416     |
|    fps              | 268      |
|    time_elapsed     | 1107     |
|    total_timesteps  | 297279   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0019   |
|    n_updates        | 64319    |
----------------------------------
Eval num_timesteps=297500, episode_reward=172.76 +/- 45.20
Episode length: 43.62 +/- 11.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 297500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.13     |
|    n_updates        | 64374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.7     |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3420     |
|    fps              | 268      |
|    time_elapsed     | 1109     |
|    total_timesteps  | 297683   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0051   |
|    n_updates        | 64420    |
----------------------------------
Eval num_timesteps=298000, episode_reward=182.64 +/- 59.16
Episode length: 45.98 +/- 14.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 298000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.31     |
|    n_updates        | 64499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.8     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3424     |
|    fps              | 268      |
|    time_elapsed     | 1111     |
|    total_timesteps  | 298140   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.1      |
|    n_updates        | 64534    |
----------------------------------
Eval num_timesteps=298500, episode_reward=151.06 +/- 52.91
Episode length: 38.14 +/- 13.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.1     |
|    mean_reward      | 151      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 298500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.41     |
|    n_updates        | 64624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3428     |
|    fps              | 268      |
|    time_elapsed     | 1113     |
|    total_timesteps  | 298557   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0482   |
|    n_updates        | 64639    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3432     |
|    fps              | 268      |
|    time_elapsed     | 1113     |
|    total_timesteps  | 298966   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.79     |
|    n_updates        | 64741    |
----------------------------------
Eval num_timesteps=299000, episode_reward=155.98 +/- 47.65
Episode length: 39.38 +/- 11.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.4     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 299000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43     |
|    n_updates        | 64749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3436     |
|    fps              | 268      |
|    time_elapsed     | 1115     |
|    total_timesteps  | 299430   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00471  |
|    n_updates        | 64857    |
----------------------------------
Eval num_timesteps=299500, episode_reward=181.42 +/- 40.49
Episode length: 45.66 +/- 10.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 299500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.284    |
|    n_updates        | 64874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.4     |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3440     |
|    fps              | 268      |
|    time_elapsed     | 1117     |
|    total_timesteps  | 299781   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000171 |
|    n_updates        | 64945    |
----------------------------------
Eval num_timesteps=300000, episode_reward=181.52 +/- 55.97
Episode length: 45.84 +/- 14.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 300000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00497  |
|    n_updates        | 64999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3444     |
|    fps              | 268      |
|    time_elapsed     | 1119     |
|    total_timesteps  | 300225   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43     |
|    n_updates        | 65056    |
----------------------------------
Eval num_timesteps=300500, episode_reward=170.20 +/- 40.57
Episode length: 42.94 +/- 10.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 300500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0104   |
|    n_updates        | 65124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3448     |
|    fps              | 268      |
|    time_elapsed     | 1121     |
|    total_timesteps  | 300667   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.45     |
|    n_updates        | 65166    |
----------------------------------
Eval num_timesteps=301000, episode_reward=162.98 +/- 56.44
Episode length: 41.04 +/- 14.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41       |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 301000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0373   |
|    n_updates        | 65249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3452     |
|    fps              | 268      |
|    time_elapsed     | 1122     |
|    total_timesteps  | 301012   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0218   |
|    n_updates        | 65252    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.2     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3456     |
|    fps              | 268      |
|    time_elapsed     | 1123     |
|    total_timesteps  | 301378   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.84     |
|    n_updates        | 65344    |
----------------------------------
Eval num_timesteps=301500, episode_reward=189.32 +/- 50.78
Episode length: 47.72 +/- 12.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.7     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 301500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.223    |
|    n_updates        | 65374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.6     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3460     |
|    fps              | 268      |
|    time_elapsed     | 1125     |
|    total_timesteps  | 301701   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00836  |
|    n_updates        | 65425    |
----------------------------------
Eval num_timesteps=302000, episode_reward=180.26 +/- 62.30
Episode length: 45.50 +/- 15.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 302000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.37     |
|    n_updates        | 65499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3464     |
|    fps              | 268      |
|    time_elapsed     | 1127     |
|    total_timesteps  | 302297   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.9      |
|    n_updates        | 65574    |
----------------------------------
Eval num_timesteps=302500, episode_reward=161.10 +/- 49.87
Episode length: 40.60 +/- 12.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.6     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 302500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.26     |
|    n_updates        | 65624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3468     |
|    fps              | 268      |
|    time_elapsed     | 1129     |
|    total_timesteps  | 302652   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.48     |
|    n_updates        | 65662    |
----------------------------------
Eval num_timesteps=303000, episode_reward=183.20 +/- 51.16
Episode length: 46.18 +/- 12.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 303000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35     |
|    n_updates        | 65749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.1     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3472     |
|    fps              | 267      |
|    time_elapsed     | 1131     |
|    total_timesteps  | 303039   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.02     |
|    n_updates        | 65759    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.1     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3476     |
|    fps              | 268      |
|    time_elapsed     | 1131     |
|    total_timesteps  | 303210   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.15     |
|    n_updates        | 65802    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3480     |
|    fps              | 268      |
|    time_elapsed     | 1131     |
|    total_timesteps  | 303466   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35     |
|    n_updates        | 65866    |
----------------------------------
Eval num_timesteps=303500, episode_reward=191.66 +/- 57.71
Episode length: 48.32 +/- 14.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.3     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 303500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00488  |
|    n_updates        | 65874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3484     |
|    fps              | 268      |
|    time_elapsed     | 1133     |
|    total_timesteps  | 303904   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.28     |
|    n_updates        | 65975    |
----------------------------------
Eval num_timesteps=304000, episode_reward=167.90 +/- 51.44
Episode length: 42.36 +/- 12.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 304000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0191   |
|    n_updates        | 65999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3488     |
|    fps              | 268      |
|    time_elapsed     | 1135     |
|    total_timesteps  | 304317   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.91     |
|    n_updates        | 66079    |
----------------------------------
Eval num_timesteps=304500, episode_reward=164.66 +/- 52.72
Episode length: 41.54 +/- 13.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 304500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.71     |
|    n_updates        | 66124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3492     |
|    fps              | 268      |
|    time_elapsed     | 1137     |
|    total_timesteps  | 304819   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.01     |
|    n_updates        | 66204    |
----------------------------------
Eval num_timesteps=305000, episode_reward=165.20 +/- 56.02
Episode length: 41.68 +/- 14.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.7     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 305000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.91     |
|    n_updates        | 66249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3496     |
|    fps              | 267      |
|    time_elapsed     | 1139     |
|    total_timesteps  | 305175   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0044   |
|    n_updates        | 66293    |
----------------------------------
Eval num_timesteps=305500, episode_reward=161.64 +/- 32.27
Episode length: 40.78 +/- 8.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.8     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 305500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.46     |
|    n_updates        | 66374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3500     |
|    fps              | 267      |
|    time_elapsed     | 1140     |
|    total_timesteps  | 305670   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02     |
|    n_updates        | 66417    |
----------------------------------
Eval num_timesteps=306000, episode_reward=175.70 +/- 51.09
Episode length: 44.30 +/- 12.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 306000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0144   |
|    n_updates        | 66499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3504     |
|    fps              | 267      |
|    time_elapsed     | 1142     |
|    total_timesteps  | 306147   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0277   |
|    n_updates        | 66536    |
----------------------------------
Eval num_timesteps=306500, episode_reward=166.62 +/- 34.34
Episode length: 42.08 +/- 8.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.1     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 306500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.03     |
|    n_updates        | 66624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3508     |
|    fps              | 267      |
|    time_elapsed     | 1144     |
|    total_timesteps  | 306535   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00177  |
|    n_updates        | 66633    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3512     |
|    fps              | 268      |
|    time_elapsed     | 1145     |
|    total_timesteps  | 306951   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.89     |
|    n_updates        | 66737    |
----------------------------------
Eval num_timesteps=307000, episode_reward=165.38 +/- 41.59
Episode length: 41.72 +/- 10.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.7     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 307000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0155   |
|    n_updates        | 66749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3516     |
|    fps              | 267      |
|    time_elapsed     | 1147     |
|    total_timesteps  | 307343   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.44     |
|    n_updates        | 66835    |
----------------------------------
Eval num_timesteps=307500, episode_reward=151.90 +/- 42.83
Episode length: 38.38 +/- 10.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.4     |
|    mean_reward      | 152      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 307500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0139   |
|    n_updates        | 66874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3520     |
|    fps              | 267      |
|    time_elapsed     | 1148     |
|    total_timesteps  | 307636   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.947    |
|    n_updates        | 66908    |
----------------------------------
Eval num_timesteps=308000, episode_reward=178.80 +/- 47.67
Episode length: 45.12 +/- 11.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 308000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.44     |
|    n_updates        | 66999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.3     |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3524     |
|    fps              | 267      |
|    time_elapsed     | 1150     |
|    total_timesteps  | 308071   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00386  |
|    n_updates        | 67017    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3528     |
|    fps              | 267      |
|    time_elapsed     | 1150     |
|    total_timesteps  | 308359   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000816 |
|    n_updates        | 67089    |
----------------------------------
Eval num_timesteps=308500, episode_reward=169.08 +/- 42.10
Episode length: 42.68 +/- 10.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 308500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0604   |
|    n_updates        | 67124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3532     |
|    fps              | 267      |
|    time_elapsed     | 1152     |
|    total_timesteps  | 308810   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00776  |
|    n_updates        | 67202    |
----------------------------------
Eval num_timesteps=309000, episode_reward=166.16 +/- 31.38
Episode length: 41.94 +/- 7.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.9     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 309000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00287  |
|    n_updates        | 67249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3536     |
|    fps              | 267      |
|    time_elapsed     | 1154     |
|    total_timesteps  | 309132   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00529  |
|    n_updates        | 67282    |
----------------------------------
Eval num_timesteps=309500, episode_reward=168.94 +/- 35.35
Episode length: 42.62 +/- 8.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 309500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.45     |
|    n_updates        | 67374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.9     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3540     |
|    fps              | 267      |
|    time_elapsed     | 1156     |
|    total_timesteps  | 309668   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.23     |
|    n_updates        | 67416    |
----------------------------------
Eval num_timesteps=310000, episode_reward=176.80 +/- 45.80
Episode length: 44.60 +/- 11.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 310000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.46     |
|    n_updates        | 67499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3544     |
|    fps              | 267      |
|    time_elapsed     | 1158     |
|    total_timesteps  | 310386   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00883  |
|    n_updates        | 67596    |
----------------------------------
Eval num_timesteps=310500, episode_reward=186.42 +/- 55.68
Episode length: 46.94 +/- 13.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 310500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47     |
|    n_updates        | 67624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.3     |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3548     |
|    fps              | 267      |
|    time_elapsed     | 1160     |
|    total_timesteps  | 310598   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00133  |
|    n_updates        | 67649    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3552     |
|    fps              | 267      |
|    time_elapsed     | 1160     |
|    total_timesteps  | 310988   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000402 |
|    n_updates        | 67746    |
----------------------------------
Eval num_timesteps=311000, episode_reward=148.48 +/- 39.12
Episode length: 37.50 +/- 9.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.5     |
|    mean_reward      | 148      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 311000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47     |
|    n_updates        | 67749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3556     |
|    fps              | 267      |
|    time_elapsed     | 1162     |
|    total_timesteps  | 311406   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0093   |
|    n_updates        | 67851    |
----------------------------------
Eval num_timesteps=311500, episode_reward=179.78 +/- 46.36
Episode length: 45.38 +/- 11.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 311500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00161  |
|    n_updates        | 67874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3560     |
|    fps              | 267      |
|    time_elapsed     | 1164     |
|    total_timesteps  | 311728   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.41     |
|    n_updates        | 67931    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3564     |
|    fps              | 267      |
|    time_elapsed     | 1164     |
|    total_timesteps  | 311969   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.48     |
|    n_updates        | 67992    |
----------------------------------
Eval num_timesteps=312000, episode_reward=176.40 +/- 43.27
Episode length: 44.52 +/- 10.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 312000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.04     |
|    n_updates        | 67999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.1     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3568     |
|    fps              | 267      |
|    time_elapsed     | 1166     |
|    total_timesteps  | 312460   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0127   |
|    n_updates        | 68114    |
----------------------------------
Eval num_timesteps=312500, episode_reward=176.58 +/- 52.73
Episode length: 44.58 +/- 13.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 312500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00265  |
|    n_updates        | 68124    |
----------------------------------
Eval num_timesteps=313000, episode_reward=169.10 +/- 31.15
Episode length: 42.68 +/- 7.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 313000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54     |
|    n_updates        | 68249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3572     |
|    fps              | 267      |
|    time_elapsed     | 1170     |
|    total_timesteps  | 313054   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000812 |
|    n_updates        | 68263    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3576     |
|    fps              | 267      |
|    time_elapsed     | 1170     |
|    total_timesteps  | 313460   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0561   |
|    n_updates        | 68364    |
----------------------------------
Eval num_timesteps=313500, episode_reward=166.96 +/- 44.52
Episode length: 42.14 +/- 11.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.1     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 313500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.04     |
|    n_updates        | 68374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 410      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3580     |
|    fps              | 267      |
|    time_elapsed     | 1172     |
|    total_timesteps  | 313749   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0607   |
|    n_updates        | 68437    |
----------------------------------
Eval num_timesteps=314000, episode_reward=167.98 +/- 52.33
Episode length: 42.34 +/- 13.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 314000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00229  |
|    n_updates        | 68499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3584     |
|    fps              | 267      |
|    time_elapsed     | 1173     |
|    total_timesteps  | 314115   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00201  |
|    n_updates        | 68528    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3588     |
|    fps              | 267      |
|    time_elapsed     | 1174     |
|    total_timesteps  | 314395   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.03     |
|    n_updates        | 68598    |
----------------------------------
Eval num_timesteps=314500, episode_reward=185.92 +/- 39.99
Episode length: 46.84 +/- 9.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 314500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.68     |
|    n_updates        | 68624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3592     |
|    fps              | 267      |
|    time_elapsed     | 1176     |
|    total_timesteps  | 314699   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00626  |
|    n_updates        | 68674    |
----------------------------------
Eval num_timesteps=315000, episode_reward=171.44 +/- 41.16
Episode length: 43.18 +/- 10.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 315000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.5      |
|    n_updates        | 68749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.6     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3596     |
|    fps              | 267      |
|    time_elapsed     | 1177     |
|    total_timesteps  | 315032   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.51     |
|    n_updates        | 68757    |
----------------------------------
Eval num_timesteps=315500, episode_reward=183.28 +/- 45.81
Episode length: 46.20 +/- 11.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.2     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 315500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.82     |
|    n_updates        | 68874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3600     |
|    fps              | 267      |
|    time_elapsed     | 1179     |
|    total_timesteps  | 315513   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00903  |
|    n_updates        | 68878    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3604     |
|    fps              | 267      |
|    time_elapsed     | 1180     |
|    total_timesteps  | 315726   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00636  |
|    n_updates        | 68931    |
----------------------------------
Eval num_timesteps=316000, episode_reward=173.04 +/- 45.39
Episode length: 43.64 +/- 11.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 316000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.55     |
|    n_updates        | 68999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.9     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3608     |
|    fps              | 267      |
|    time_elapsed     | 1181     |
|    total_timesteps  | 316028   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.01     |
|    n_updates        | 69006    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.6     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3612     |
|    fps              | 267      |
|    time_elapsed     | 1182     |
|    total_timesteps  | 316407   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3        |
|    n_updates        | 69101    |
----------------------------------
Eval num_timesteps=316500, episode_reward=171.04 +/- 49.89
Episode length: 43.12 +/- 12.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 316500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.51     |
|    n_updates        | 69124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.4     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3616     |
|    fps              | 267      |
|    time_elapsed     | 1184     |
|    total_timesteps  | 316886   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.55     |
|    n_updates        | 69221    |
----------------------------------
Eval num_timesteps=317000, episode_reward=161.18 +/- 51.58
Episode length: 40.66 +/- 12.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.7     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 317000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47     |
|    n_updates        | 69249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3620     |
|    fps              | 267      |
|    time_elapsed     | 1186     |
|    total_timesteps  | 317373   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.51     |
|    n_updates        | 69343    |
----------------------------------
Eval num_timesteps=317500, episode_reward=145.32 +/- 40.50
Episode length: 36.62 +/- 10.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.6     |
|    mean_reward      | 145      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 317500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3        |
|    n_updates        | 69374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3624     |
|    fps              | 267      |
|    time_elapsed     | 1187     |
|    total_timesteps  | 317754   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.48     |
|    n_updates        | 69438    |
----------------------------------
Eval num_timesteps=318000, episode_reward=153.48 +/- 49.76
Episode length: 38.78 +/- 12.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.8     |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 318000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.52     |
|    n_updates        | 69499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3628     |
|    fps              | 267      |
|    time_elapsed     | 1189     |
|    total_timesteps  | 318060   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.53     |
|    n_updates        | 69514    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3632     |
|    fps              | 267      |
|    time_elapsed     | 1189     |
|    total_timesteps  | 318413   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00278  |
|    n_updates        | 69603    |
----------------------------------
Eval num_timesteps=318500, episode_reward=152.30 +/- 40.56
Episode length: 38.48 +/- 10.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.5     |
|    mean_reward      | 152      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 318500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0004   |
|    n_updates        | 69624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3636     |
|    fps              | 267      |
|    time_elapsed     | 1191     |
|    total_timesteps  | 318686   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3        |
|    n_updates        | 69671    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93       |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3640     |
|    fps              | 267      |
|    time_elapsed     | 1191     |
|    total_timesteps  | 318967   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00788  |
|    n_updates        | 69741    |
----------------------------------
Eval num_timesteps=319000, episode_reward=179.48 +/- 45.83
Episode length: 45.20 +/- 11.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 319000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.48     |
|    n_updates        | 69749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.7     |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3644     |
|    fps              | 267      |
|    time_elapsed     | 1193     |
|    total_timesteps  | 319354   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.96     |
|    n_updates        | 69838    |
----------------------------------
Eval num_timesteps=319500, episode_reward=174.02 +/- 46.24
Episode length: 43.88 +/- 11.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 319500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00241  |
|    n_updates        | 69874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.6     |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3648     |
|    fps              | 267      |
|    time_elapsed     | 1195     |
|    total_timesteps  | 319656   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00374  |
|    n_updates        | 69913    |
----------------------------------
Eval num_timesteps=320000, episode_reward=176.90 +/- 48.72
Episode length: 44.54 +/- 12.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 320000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54     |
|    n_updates        | 69999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.1     |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3652     |
|    fps              | 267      |
|    time_elapsed     | 1197     |
|    total_timesteps  | 320000   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.1     |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3656     |
|    fps              | 267      |
|    time_elapsed     | 1197     |
|    total_timesteps  | 320419   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.34e-05 |
|    n_updates        | 70104    |
----------------------------------
Eval num_timesteps=320500, episode_reward=175.22 +/- 47.38
Episode length: 44.20 +/- 11.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 320500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.13     |
|    n_updates        | 70124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.4     |
|    ep_rew_mean      | 360      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3660     |
|    fps              | 267      |
|    time_elapsed     | 1199     |
|    total_timesteps  | 320766   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0036   |
|    n_updates        | 70191    |
----------------------------------
Eval num_timesteps=321000, episode_reward=155.36 +/- 44.05
Episode length: 39.20 +/- 10.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.2     |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 321000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00906  |
|    n_updates        | 70249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.3     |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3664     |
|    fps              | 267      |
|    time_elapsed     | 1201     |
|    total_timesteps  | 321202   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.63     |
|    n_updates        | 70300    |
----------------------------------
Eval num_timesteps=321500, episode_reward=167.44 +/- 44.73
Episode length: 42.20 +/- 11.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 321500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.09     |
|    n_updates        | 70374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.7     |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3668     |
|    fps              | 267      |
|    time_elapsed     | 1202     |
|    total_timesteps  | 321530   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00907  |
|    n_updates        | 70382    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89       |
|    ep_rew_mean      | 354      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3672     |
|    fps              | 267      |
|    time_elapsed     | 1203     |
|    total_timesteps  | 321953   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54     |
|    n_updates        | 70488    |
----------------------------------
Eval num_timesteps=322000, episode_reward=173.54 +/- 45.69
Episode length: 43.72 +/- 11.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 322000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00438  |
|    n_updates        | 70499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.6     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3676     |
|    fps              | 267      |
|    time_elapsed     | 1205     |
|    total_timesteps  | 322322   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00233  |
|    n_updates        | 70580    |
----------------------------------
Eval num_timesteps=322500, episode_reward=177.00 +/- 55.18
Episode length: 44.66 +/- 13.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 322500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.1      |
|    n_updates        | 70624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.5     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3680     |
|    fps              | 267      |
|    time_elapsed     | 1207     |
|    total_timesteps  | 322696   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.07     |
|    n_updates        | 70673    |
----------------------------------
Eval num_timesteps=323000, episode_reward=183.04 +/- 45.67
Episode length: 46.12 +/- 11.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 323000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.06     |
|    n_updates        | 70749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.5     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3684     |
|    fps              | 267      |
|    time_elapsed     | 1208     |
|    total_timesteps  | 323061   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54     |
|    n_updates        | 70765    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.1     |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3688     |
|    fps              | 267      |
|    time_elapsed     | 1209     |
|    total_timesteps  | 323303   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0381   |
|    n_updates        | 70825    |
----------------------------------
Eval num_timesteps=323500, episode_reward=172.52 +/- 47.28
Episode length: 43.60 +/- 11.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 323500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0181   |
|    n_updates        | 70874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92       |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3692     |
|    fps              | 267      |
|    time_elapsed     | 1211     |
|    total_timesteps  | 323899   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000469 |
|    n_updates        | 70974    |
----------------------------------
Eval num_timesteps=324000, episode_reward=173.82 +/- 43.86
Episode length: 43.80 +/- 10.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 324000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.62     |
|    n_updates        | 70999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.7     |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3696     |
|    fps              | 267      |
|    time_elapsed     | 1213     |
|    total_timesteps  | 324399   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0135   |
|    n_updates        | 71099    |
----------------------------------
Eval num_timesteps=324500, episode_reward=187.78 +/- 53.79
Episode length: 47.26 +/- 13.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 324500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.53     |
|    n_updates        | 71124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.1     |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3700     |
|    fps              | 267      |
|    time_elapsed     | 1215     |
|    total_timesteps  | 324823   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54     |
|    n_updates        | 71205    |
----------------------------------
Eval num_timesteps=325000, episode_reward=171.98 +/- 45.32
Episode length: 43.34 +/- 11.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 325000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0128   |
|    n_updates        | 71249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3704     |
|    fps              | 267      |
|    time_elapsed     | 1217     |
|    total_timesteps  | 325299   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.11     |
|    n_updates        | 71324    |
----------------------------------
Eval num_timesteps=325500, episode_reward=173.16 +/- 35.54
Episode length: 43.64 +/- 8.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 325500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00434  |
|    n_updates        | 71374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3708     |
|    fps              | 267      |
|    time_elapsed     | 1219     |
|    total_timesteps  | 325880   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00343  |
|    n_updates        | 71469    |
----------------------------------
Eval num_timesteps=326000, episode_reward=171.86 +/- 41.61
Episode length: 43.32 +/- 10.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 326000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000778 |
|    n_updates        | 71499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3712     |
|    fps              | 267      |
|    time_elapsed     | 1221     |
|    total_timesteps  | 326088   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.05     |
|    n_updates        | 71521    |
----------------------------------
Eval num_timesteps=326500, episode_reward=158.10 +/- 50.84
Episode length: 39.92 +/- 12.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.9     |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 326500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00176  |
|    n_updates        | 71624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.4     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3716     |
|    fps              | 267      |
|    time_elapsed     | 1222     |
|    total_timesteps  | 326525   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.51     |
|    n_updates        | 71631    |
----------------------------------
Eval num_timesteps=327000, episode_reward=155.84 +/- 45.24
Episode length: 39.32 +/- 11.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.3     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 327000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.537    |
|    n_updates        | 71749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.9     |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3720     |
|    fps              | 267      |
|    time_elapsed     | 1224     |
|    total_timesteps  | 327167   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.496    |
|    n_updates        | 71791    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.4     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3724     |
|    fps              | 267      |
|    time_elapsed     | 1225     |
|    total_timesteps  | 327394   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00343  |
|    n_updates        | 71848    |
----------------------------------
Eval num_timesteps=327500, episode_reward=163.48 +/- 57.81
Episode length: 41.26 +/- 14.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.3     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 327500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.04     |
|    n_updates        | 71874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97       |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3728     |
|    fps              | 267      |
|    time_elapsed     | 1226     |
|    total_timesteps  | 327757   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.06     |
|    n_updates        | 71939    |
----------------------------------
Eval num_timesteps=328000, episode_reward=181.24 +/- 44.59
Episode length: 45.68 +/- 11.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 328000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.045    |
|    n_updates        | 71999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.9     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3732     |
|    fps              | 267      |
|    time_elapsed     | 1228     |
|    total_timesteps  | 328105   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.55     |
|    n_updates        | 72026    |
----------------------------------
Eval num_timesteps=328500, episode_reward=164.44 +/- 51.83
Episode length: 41.52 +/- 13.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 328500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.424    |
|    n_updates        | 72124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.2     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3736     |
|    fps              | 266      |
|    time_elapsed     | 1230     |
|    total_timesteps  | 328507   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.61     |
|    n_updates        | 72126    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.1     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3740     |
|    fps              | 267      |
|    time_elapsed     | 1230     |
|    total_timesteps  | 328778   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.06     |
|    n_updates        | 72194    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.3     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3744     |
|    fps              | 267      |
|    time_elapsed     | 1230     |
|    total_timesteps  | 328983   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0239   |
|    n_updates        | 72245    |
----------------------------------
Eval num_timesteps=329000, episode_reward=164.20 +/- 55.82
Episode length: 41.40 +/- 14.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 329000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.93e-06 |
|    n_updates        | 72249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3748     |
|    fps              | 267      |
|    time_elapsed     | 1232     |
|    total_timesteps  | 329332   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0112   |
|    n_updates        | 72332    |
----------------------------------
Eval num_timesteps=329500, episode_reward=178.92 +/- 40.96
Episode length: 45.12 +/- 10.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 329500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0149   |
|    n_updates        | 72374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3752     |
|    fps              | 266      |
|    time_elapsed     | 1234     |
|    total_timesteps  | 329576   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00235  |
|    n_updates        | 72393    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3756     |
|    fps              | 267      |
|    time_elapsed     | 1234     |
|    total_timesteps  | 329984   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.09     |
|    n_updates        | 72495    |
----------------------------------
Eval num_timesteps=330000, episode_reward=170.96 +/- 43.84
Episode length: 43.10 +/- 10.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 330000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.69     |
|    n_updates        | 72499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.6     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3760     |
|    fps              | 267      |
|    time_elapsed     | 1236     |
|    total_timesteps  | 330226   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.62     |
|    n_updates        | 72556    |
----------------------------------
Eval num_timesteps=330500, episode_reward=170.94 +/- 38.96
Episode length: 43.14 +/- 9.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 330500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.53     |
|    n_updates        | 72624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.2     |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3764     |
|    fps              | 266      |
|    time_elapsed     | 1238     |
|    total_timesteps  | 330524   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.19     |
|    n_updates        | 72630    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.7     |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3768     |
|    fps              | 267      |
|    time_elapsed     | 1238     |
|    total_timesteps  | 330897   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56     |
|    n_updates        | 72724    |
----------------------------------
Eval num_timesteps=331000, episode_reward=164.70 +/- 40.79
Episode length: 41.48 +/- 10.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 331000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58     |
|    n_updates        | 72749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.1     |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3772     |
|    fps              | 267      |
|    time_elapsed     | 1240     |
|    total_timesteps  | 331262   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.14     |
|    n_updates        | 72815    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.7     |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3776     |
|    fps              | 267      |
|    time_elapsed     | 1240     |
|    total_timesteps  | 331488   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.11     |
|    n_updates        | 72871    |
----------------------------------
Eval num_timesteps=331500, episode_reward=165.04 +/- 46.47
Episode length: 41.58 +/- 11.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 331500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0062   |
|    n_updates        | 72874    |
----------------------------------
Eval num_timesteps=332000, episode_reward=152.30 +/- 48.35
Episode length: 38.46 +/- 12.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.5     |
|    mean_reward      | 152      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 332000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0365   |
|    n_updates        | 72999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.1     |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3780     |
|    fps              | 266      |
|    time_elapsed     | 1243     |
|    total_timesteps  | 332005   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0381   |
|    n_updates        | 73001    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.8     |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3784     |
|    fps              | 267      |
|    time_elapsed     | 1244     |
|    total_timesteps  | 332336   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9      |
|    n_updates        | 73083    |
----------------------------------
Eval num_timesteps=332500, episode_reward=166.86 +/- 44.35
Episode length: 42.12 +/- 11.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.1     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 332500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.17     |
|    n_updates        | 73124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.9     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3788     |
|    fps              | 267      |
|    time_elapsed     | 1246     |
|    total_timesteps  | 332690   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.17     |
|    n_updates        | 73172    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.9     |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3792     |
|    fps              | 267      |
|    time_elapsed     | 1246     |
|    total_timesteps  | 332993   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.21     |
|    n_updates        | 73248    |
----------------------------------
Eval num_timesteps=333000, episode_reward=182.14 +/- 53.33
Episode length: 45.90 +/- 13.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 333000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67     |
|    n_updates        | 73249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.4     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3796     |
|    fps              | 267      |
|    time_elapsed     | 1248     |
|    total_timesteps  | 333337   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.63     |
|    n_updates        | 73334    |
----------------------------------
Eval num_timesteps=333500, episode_reward=177.72 +/- 43.12
Episode length: 44.78 +/- 10.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 333500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00239  |
|    n_updates        | 73374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.2     |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3800     |
|    fps              | 266      |
|    time_elapsed     | 1250     |
|    total_timesteps  | 333746   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.62     |
|    n_updates        | 73436    |
----------------------------------
Eval num_timesteps=334000, episode_reward=177.20 +/- 43.90
Episode length: 44.74 +/- 10.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 334000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.34     |
|    n_updates        | 73499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.5     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3804     |
|    fps              | 266      |
|    time_elapsed     | 1251     |
|    total_timesteps  | 334050   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00604  |
|    n_updates        | 73512    |
----------------------------------
Eval num_timesteps=334500, episode_reward=188.10 +/- 50.42
Episode length: 47.50 +/- 12.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.5     |
|    mean_reward      | 188      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 334500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.53     |
|    n_updates        | 73624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.8     |
|    ep_rew_mean      | 350      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3808     |
|    fps              | 266      |
|    time_elapsed     | 1254     |
|    total_timesteps  | 334659   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58     |
|    n_updates        | 73664    |
----------------------------------
Eval num_timesteps=335000, episode_reward=142.28 +/- 51.10
Episode length: 35.92 +/- 12.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.9     |
|    mean_reward      | 142      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 335000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.22     |
|    n_updates        | 73749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.2     |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3812     |
|    fps              | 266      |
|    time_elapsed     | 1255     |
|    total_timesteps  | 335012   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.02     |
|    n_updates        | 73752    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.5     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3816     |
|    fps              | 267      |
|    time_elapsed     | 1256     |
|    total_timesteps  | 335471   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00707  |
|    n_updates        | 73867    |
----------------------------------
Eval num_timesteps=335500, episode_reward=168.84 +/- 43.87
Episode length: 42.54 +/- 10.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 335500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.291    |
|    n_updates        | 73874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.2     |
|    ep_rew_mean      | 343      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3820     |
|    fps              | 266      |
|    time_elapsed     | 1257     |
|    total_timesteps  | 335786   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.53     |
|    n_updates        | 73946    |
----------------------------------
Eval num_timesteps=336000, episode_reward=181.20 +/- 58.50
Episode length: 45.68 +/- 14.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 336000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.66     |
|    n_updates        | 73999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87       |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3824     |
|    fps              | 266      |
|    time_elapsed     | 1259     |
|    total_timesteps  | 336090   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0324   |
|    n_updates        | 74022    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.7     |
|    ep_rew_mean      | 341      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3828     |
|    fps              | 266      |
|    time_elapsed     | 1259     |
|    total_timesteps  | 336322   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.65     |
|    n_updates        | 74080    |
----------------------------------
Eval num_timesteps=336500, episode_reward=163.62 +/- 51.96
Episode length: 41.30 +/- 13.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.3     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 336500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00488  |
|    n_updates        | 74124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.9     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3832     |
|    fps              | 266      |
|    time_elapsed     | 1261     |
|    total_timesteps  | 336597   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.61     |
|    n_updates        | 74149    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 335      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3836     |
|    fps              | 266      |
|    time_elapsed     | 1261     |
|    total_timesteps  | 336912   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0021   |
|    n_updates        | 74227    |
----------------------------------
Eval num_timesteps=337000, episode_reward=181.50 +/- 38.72
Episode length: 45.66 +/- 9.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 337000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.23     |
|    n_updates        | 74249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.5     |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3840     |
|    fps              | 266      |
|    time_elapsed     | 1263     |
|    total_timesteps  | 337427   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00785  |
|    n_updates        | 74356    |
----------------------------------
Eval num_timesteps=337500, episode_reward=152.76 +/- 41.38
Episode length: 38.60 +/- 10.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.6     |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 337500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00108  |
|    n_updates        | 74374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.4     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3844     |
|    fps              | 266      |
|    time_elapsed     | 1265     |
|    total_timesteps  | 337720   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.59     |
|    n_updates        | 74429    |
----------------------------------
Eval num_timesteps=338000, episode_reward=144.30 +/- 35.19
Episode length: 36.40 +/- 8.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.4     |
|    mean_reward      | 144      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 338000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.21     |
|    n_updates        | 74499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.8     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3848     |
|    fps              | 266      |
|    time_elapsed     | 1267     |
|    total_timesteps  | 338107   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.61     |
|    n_updates        | 74526    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.5     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3852     |
|    fps              | 266      |
|    time_elapsed     | 1267     |
|    total_timesteps  | 338324   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58     |
|    n_updates        | 74580    |
----------------------------------
Eval num_timesteps=338500, episode_reward=164.02 +/- 59.57
Episode length: 41.32 +/- 14.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.3     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 338500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.58     |
|    n_updates        | 74624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.7     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3856     |
|    fps              | 266      |
|    time_elapsed     | 1269     |
|    total_timesteps  | 338755   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00286  |
|    n_updates        | 74688    |
----------------------------------
Eval num_timesteps=339000, episode_reward=173.52 +/- 41.73
Episode length: 43.78 +/- 10.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 339000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.45     |
|    n_updates        | 74749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.2     |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3860     |
|    fps              | 266      |
|    time_elapsed     | 1271     |
|    total_timesteps  | 339151   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.65     |
|    n_updates        | 74787    |
----------------------------------
Eval num_timesteps=339500, episode_reward=175.84 +/- 46.59
Episode length: 44.32 +/- 11.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 339500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.014    |
|    n_updates        | 74874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.2     |
|    ep_rew_mean      | 359      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3864     |
|    fps              | 266      |
|    time_elapsed     | 1272     |
|    total_timesteps  | 339540   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0843   |
|    n_updates        | 74884    |
----------------------------------
Eval num_timesteps=340000, episode_reward=177.54 +/- 37.29
Episode length: 44.84 +/- 9.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 340000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.61     |
|    n_updates        | 74999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.7     |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3868     |
|    fps              | 266      |
|    time_elapsed     | 1274     |
|    total_timesteps  | 340065   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.222    |
|    n_updates        | 75016    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3872     |
|    fps              | 266      |
|    time_elapsed     | 1275     |
|    total_timesteps  | 340415   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00316  |
|    n_updates        | 75103    |
----------------------------------
Eval num_timesteps=340500, episode_reward=170.54 +/- 36.31
Episode length: 43.08 +/- 9.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 340500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.62     |
|    n_updates        | 75124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.5     |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3876     |
|    fps              | 266      |
|    time_elapsed     | 1277     |
|    total_timesteps  | 340839   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00389  |
|    n_updates        | 75209    |
----------------------------------
Eval num_timesteps=341000, episode_reward=173.46 +/- 44.96
Episode length: 43.68 +/- 11.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 341000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.61     |
|    n_updates        | 75249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.2     |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3880     |
|    fps              | 266      |
|    time_elapsed     | 1278     |
|    total_timesteps  | 341123   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.65     |
|    n_updates        | 75280    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.9     |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3884     |
|    fps              | 266      |
|    time_elapsed     | 1279     |
|    total_timesteps  | 341423   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00597  |
|    n_updates        | 75355    |
----------------------------------
Eval num_timesteps=341500, episode_reward=177.62 +/- 45.75
Episode length: 44.76 +/- 11.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 341500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.6      |
|    n_updates        | 75374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91       |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3888     |
|    fps              | 266      |
|    time_elapsed     | 1281     |
|    total_timesteps  | 341792   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0486   |
|    n_updates        | 75447    |
----------------------------------
Eval num_timesteps=342000, episode_reward=171.48 +/- 62.25
Episode length: 43.20 +/- 15.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 342000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0224   |
|    n_updates        | 75499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.7     |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3892     |
|    fps              | 266      |
|    time_elapsed     | 1282     |
|    total_timesteps  | 342066   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.04     |
|    n_updates        | 75516    |
----------------------------------
Eval num_timesteps=342500, episode_reward=179.36 +/- 48.48
Episode length: 45.18 +/- 12.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 342500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.88     |
|    n_updates        | 75624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.1     |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3896     |
|    fps              | 266      |
|    time_elapsed     | 1284     |
|    total_timesteps  | 342548   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00629  |
|    n_updates        | 75636    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.4     |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3900     |
|    fps              | 266      |
|    time_elapsed     | 1285     |
|    total_timesteps  | 342987   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.63     |
|    n_updates        | 75746    |
----------------------------------
Eval num_timesteps=343000, episode_reward=171.74 +/- 47.28
Episode length: 43.28 +/- 11.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 343000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.005    |
|    n_updates        | 75749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.2     |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3904     |
|    fps              | 266      |
|    time_elapsed     | 1286     |
|    total_timesteps  | 343173   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00253  |
|    n_updates        | 75793    |
----------------------------------
Eval num_timesteps=343500, episode_reward=166.88 +/- 40.32
Episode length: 42.14 +/- 10.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.1     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 343500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.25     |
|    n_updates        | 75874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.9     |
|    ep_rew_mean      | 354      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3908     |
|    fps              | 266      |
|    time_elapsed     | 1288     |
|    total_timesteps  | 343552   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.4      |
|    n_updates        | 75887    |
----------------------------------
Eval num_timesteps=344000, episode_reward=177.02 +/- 35.87
Episode length: 44.68 +/- 9.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 344000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.84     |
|    n_updates        | 75999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.8     |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3912     |
|    fps              | 266      |
|    time_elapsed     | 1290     |
|    total_timesteps  | 344197   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.26     |
|    n_updates        | 76049    |
----------------------------------
Eval num_timesteps=344500, episode_reward=190.26 +/- 47.29
Episode length: 47.90 +/- 11.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.9     |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 344500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64     |
|    n_updates        | 76124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.4     |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3916     |
|    fps              | 266      |
|    time_elapsed     | 1292     |
|    total_timesteps  | 344608   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.65     |
|    n_updates        | 76151    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.1     |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3920     |
|    fps              | 266      |
|    time_elapsed     | 1293     |
|    total_timesteps  | 344894   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00154  |
|    n_updates        | 76223    |
----------------------------------
Eval num_timesteps=345000, episode_reward=176.24 +/- 45.75
Episode length: 44.40 +/- 11.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 345000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0036   |
|    n_updates        | 76249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.7     |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3924     |
|    fps              | 266      |
|    time_elapsed     | 1295     |
|    total_timesteps  | 345359   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0162   |
|    n_updates        | 76339    |
----------------------------------
Eval num_timesteps=345500, episode_reward=161.10 +/- 35.63
Episode length: 40.70 +/- 8.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.7     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 345500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.063    |
|    n_updates        | 76374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3928     |
|    fps              | 266      |
|    time_elapsed     | 1297     |
|    total_timesteps  | 345832   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0135   |
|    n_updates        | 76457    |
----------------------------------
Eval num_timesteps=346000, episode_reward=188.88 +/- 45.10
Episode length: 47.60 +/- 11.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 346000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67     |
|    n_updates        | 76499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.2     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3932     |
|    fps              | 266      |
|    time_elapsed     | 1298     |
|    total_timesteps  | 346121   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00073  |
|    n_updates        | 76530    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3936     |
|    fps              | 266      |
|    time_elapsed     | 1299     |
|    total_timesteps  | 346423   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.033    |
|    n_updates        | 76605    |
----------------------------------
Eval num_timesteps=346500, episode_reward=164.16 +/- 46.62
Episode length: 41.36 +/- 11.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 346500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.66     |
|    n_updates        | 76624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3940     |
|    fps              | 266      |
|    time_elapsed     | 1301     |
|    total_timesteps  | 346939   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67     |
|    n_updates        | 76734    |
----------------------------------
Eval num_timesteps=347000, episode_reward=172.00 +/- 35.02
Episode length: 43.36 +/- 8.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 347000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0458   |
|    n_updates        | 76749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.3     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3944     |
|    fps              | 266      |
|    time_elapsed     | 1303     |
|    total_timesteps  | 347351   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00522  |
|    n_updates        | 76837    |
----------------------------------
Eval num_timesteps=347500, episode_reward=154.86 +/- 46.05
Episode length: 39.16 +/- 11.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.2     |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 347500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00265  |
|    n_updates        | 76874    |
----------------------------------
Eval num_timesteps=348000, episode_reward=177.44 +/- 47.27
Episode length: 44.80 +/- 11.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 348000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00486  |
|    n_updates        | 76999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3948     |
|    fps              | 266      |
|    time_elapsed     | 1306     |
|    total_timesteps  | 348132   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.66     |
|    n_updates        | 77032    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3952     |
|    fps              | 266      |
|    time_elapsed     | 1306     |
|    total_timesteps  | 348452   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.35     |
|    n_updates        | 77112    |
----------------------------------
Eval num_timesteps=348500, episode_reward=170.08 +/- 40.89
Episode length: 42.84 +/- 10.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 348500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0336   |
|    n_updates        | 77124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3956     |
|    fps              | 266      |
|    time_elapsed     | 1309     |
|    total_timesteps  | 348965   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.44     |
|    n_updates        | 77241    |
----------------------------------
Eval num_timesteps=349000, episode_reward=168.36 +/- 57.72
Episode length: 42.42 +/- 14.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 349000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.2      |
|    n_updates        | 77249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3960     |
|    fps              | 266      |
|    time_elapsed     | 1310     |
|    total_timesteps  | 349357   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.071    |
|    n_updates        | 77339    |
----------------------------------
Eval num_timesteps=349500, episode_reward=166.28 +/- 45.19
Episode length: 41.96 +/- 11.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 349500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0275   |
|    n_updates        | 77374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3964     |
|    fps              | 266      |
|    time_elapsed     | 1312     |
|    total_timesteps  | 349880   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.26     |
|    n_updates        | 77469    |
----------------------------------
Eval num_timesteps=350000, episode_reward=179.48 +/- 50.68
Episode length: 45.24 +/- 12.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 350000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0347   |
|    n_updates        | 77499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3968     |
|    fps              | 266      |
|    time_elapsed     | 1314     |
|    total_timesteps  | 350369   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.064    |
|    n_updates        | 77592    |
----------------------------------
Eval num_timesteps=350500, episode_reward=163.14 +/- 55.23
Episode length: 41.16 +/- 13.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.2     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 350500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0104   |
|    n_updates        | 77624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3972     |
|    fps              | 266      |
|    time_elapsed     | 1316     |
|    total_timesteps  | 350562   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.83     |
|    n_updates        | 77640    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3976     |
|    fps              | 266      |
|    time_elapsed     | 1316     |
|    total_timesteps  | 350862   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.05     |
|    n_updates        | 77715    |
----------------------------------
Eval num_timesteps=351000, episode_reward=179.06 +/- 52.40
Episode length: 45.16 +/- 13.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 351000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.48     |
|    n_updates        | 77749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3980     |
|    fps              | 266      |
|    time_elapsed     | 1318     |
|    total_timesteps  | 351074   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.01     |
|    n_updates        | 77768    |
----------------------------------
Eval num_timesteps=351500, episode_reward=170.12 +/- 43.34
Episode length: 42.88 +/- 10.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 351500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.45     |
|    n_updates        | 77874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3984     |
|    fps              | 266      |
|    time_elapsed     | 1320     |
|    total_timesteps  | 351523   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.22     |
|    n_updates        | 77880    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.3     |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3988     |
|    fps              | 266      |
|    time_elapsed     | 1320     |
|    total_timesteps  | 351726   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00249  |
|    n_updates        | 77931    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.9     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3992     |
|    fps              | 266      |
|    time_elapsed     | 1320     |
|    total_timesteps  | 351960   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00222  |
|    n_updates        | 77989    |
----------------------------------
Eval num_timesteps=352000, episode_reward=167.10 +/- 43.59
Episode length: 42.18 +/- 10.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 352000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0202   |
|    n_updates        | 77999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 3996     |
|    fps              | 266      |
|    time_elapsed     | 1322     |
|    total_timesteps  | 352144   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00122  |
|    n_updates        | 78035    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4000     |
|    fps              | 266      |
|    time_elapsed     | 1322     |
|    total_timesteps  | 352483   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0453   |
|    n_updates        | 78120    |
----------------------------------
Eval num_timesteps=352500, episode_reward=174.00 +/- 43.51
Episode length: 43.88 +/- 10.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 352500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.34     |
|    n_updates        | 78124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4004     |
|    fps              | 266      |
|    time_elapsed     | 1324     |
|    total_timesteps  | 352857   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0151   |
|    n_updates        | 78214    |
----------------------------------
Eval num_timesteps=353000, episode_reward=164.58 +/- 51.80
Episode length: 41.44 +/- 12.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 353000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.62     |
|    n_updates        | 78249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.6     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4008     |
|    fps              | 266      |
|    time_elapsed     | 1326     |
|    total_timesteps  | 353013   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.107    |
|    n_updates        | 78253    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.9     |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4012     |
|    fps              | 266      |
|    time_elapsed     | 1326     |
|    total_timesteps  | 353389   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00717  |
|    n_updates        | 78347    |
----------------------------------
Eval num_timesteps=353500, episode_reward=148.22 +/- 43.15
Episode length: 37.34 +/- 10.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.3     |
|    mean_reward      | 148      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 353500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0305   |
|    n_updates        | 78374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.9     |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4016     |
|    fps              | 266      |
|    time_elapsed     | 1328     |
|    total_timesteps  | 353702   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.71     |
|    n_updates        | 78425    |
----------------------------------
Eval num_timesteps=354000, episode_reward=170.48 +/- 50.32
Episode length: 43.04 +/- 12.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 354000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.39     |
|    n_updates        | 78499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.4     |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4020     |
|    fps              | 266      |
|    time_elapsed     | 1330     |
|    total_timesteps  | 354234   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.43     |
|    n_updates        | 78558    |
----------------------------------
Eval num_timesteps=354500, episode_reward=169.76 +/- 43.73
Episode length: 42.84 +/- 10.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 354500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.22     |
|    n_updates        | 78624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4024     |
|    fps              | 266      |
|    time_elapsed     | 1332     |
|    total_timesteps  | 354737   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.61     |
|    n_updates        | 78684    |
----------------------------------
Eval num_timesteps=355000, episode_reward=171.38 +/- 41.93
Episode length: 43.22 +/- 10.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 355000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.4      |
|    n_updates        | 78749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.9     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4028     |
|    fps              | 266      |
|    time_elapsed     | 1334     |
|    total_timesteps  | 355221   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000797 |
|    n_updates        | 78805    |
----------------------------------
Eval num_timesteps=355500, episode_reward=191.02 +/- 54.01
Episode length: 48.04 +/- 13.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48       |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 355500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00261  |
|    n_updates        | 78874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.6     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4032     |
|    fps              | 266      |
|    time_elapsed     | 1336     |
|    total_timesteps  | 355585   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.48     |
|    n_updates        | 78896    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.4     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4036     |
|    fps              | 266      |
|    time_elapsed     | 1336     |
|    total_timesteps  | 355964   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.213    |
|    n_updates        | 78990    |
----------------------------------
Eval num_timesteps=356000, episode_reward=192.70 +/- 59.67
Episode length: 48.58 +/- 14.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.6     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 356000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0359   |
|    n_updates        | 78999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.5     |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4040     |
|    fps              | 266      |
|    time_elapsed     | 1338     |
|    total_timesteps  | 356285   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.127    |
|    n_updates        | 79071    |
----------------------------------
Eval num_timesteps=356500, episode_reward=164.78 +/- 39.78
Episode length: 41.60 +/- 9.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 356500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.37     |
|    n_updates        | 79124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.6     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4044     |
|    fps              | 266      |
|    time_elapsed     | 1340     |
|    total_timesteps  | 356811   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00333  |
|    n_updates        | 79202    |
----------------------------------
Eval num_timesteps=357000, episode_reward=170.72 +/- 56.88
Episode length: 43.06 +/- 14.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 357000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.71     |
|    n_updates        | 79249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.7     |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4048     |
|    fps              | 266      |
|    time_elapsed     | 1342     |
|    total_timesteps  | 357203   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.04     |
|    n_updates        | 79300    |
----------------------------------
Eval num_timesteps=357500, episode_reward=149.92 +/- 51.44
Episode length: 37.84 +/- 12.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.8     |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 357500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.69     |
|    n_updates        | 79374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.2     |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4052     |
|    fps              | 266      |
|    time_elapsed     | 1343     |
|    total_timesteps  | 357669   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.74     |
|    n_updates        | 79417    |
----------------------------------
Eval num_timesteps=358000, episode_reward=166.24 +/- 55.75
Episode length: 41.88 +/- 13.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.9     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 358000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.71     |
|    n_updates        | 79499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91       |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4056     |
|    fps              | 266      |
|    time_elapsed     | 1345     |
|    total_timesteps  | 358062   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.164    |
|    n_updates        | 79515    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.3     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4060     |
|    fps              | 266      |
|    time_elapsed     | 1345     |
|    total_timesteps  | 358288   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00948  |
|    n_updates        | 79571    |
----------------------------------
Eval num_timesteps=358500, episode_reward=186.10 +/- 53.91
Episode length: 46.94 +/- 13.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 358500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.82     |
|    n_updates        | 79624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.6     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4064     |
|    fps              | 266      |
|    time_elapsed     | 1347     |
|    total_timesteps  | 358741   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0615   |
|    n_updates        | 79685    |
----------------------------------
Eval num_timesteps=359000, episode_reward=174.72 +/- 45.51
Episode length: 44.04 +/- 11.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 359000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.13     |
|    n_updates        | 79749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.6     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4068     |
|    fps              | 266      |
|    time_elapsed     | 1349     |
|    total_timesteps  | 359233   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0582   |
|    n_updates        | 79808    |
----------------------------------
Eval num_timesteps=359500, episode_reward=197.44 +/- 58.59
Episode length: 49.72 +/- 14.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.7     |
|    mean_reward      | 197      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 359500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.254    |
|    n_updates        | 79874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.4     |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4072     |
|    fps              | 266      |
|    time_elapsed     | 1352     |
|    total_timesteps  | 359702   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00992  |
|    n_updates        | 79925    |
----------------------------------
Eval num_timesteps=360000, episode_reward=141.64 +/- 36.10
Episode length: 35.76 +/- 8.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 35.8     |
|    mean_reward      | 142      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 360000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00349  |
|    n_updates        | 79999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92       |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4076     |
|    fps              | 265      |
|    time_elapsed     | 1353     |
|    total_timesteps  | 360057   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.81     |
|    n_updates        | 80014    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.8     |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4080     |
|    fps              | 266      |
|    time_elapsed     | 1353     |
|    total_timesteps  | 360354   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00489  |
|    n_updates        | 80088    |
----------------------------------
Eval num_timesteps=360500, episode_reward=160.32 +/- 41.05
Episode length: 40.48 +/- 10.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.5     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 360500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.44     |
|    n_updates        | 80124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93       |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4084     |
|    fps              | 266      |
|    time_elapsed     | 1355     |
|    total_timesteps  | 360821   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.95     |
|    n_updates        | 80205    |
----------------------------------
Eval num_timesteps=361000, episode_reward=173.24 +/- 36.87
Episode length: 43.70 +/- 9.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 361000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.24     |
|    n_updates        | 80249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.4     |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4088     |
|    fps              | 265      |
|    time_elapsed     | 1357     |
|    total_timesteps  | 361063   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0159   |
|    n_updates        | 80265    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4092     |
|    fps              | 266      |
|    time_elapsed     | 1357     |
|    total_timesteps  | 361337   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.27     |
|    n_updates        | 80334    |
----------------------------------
Eval num_timesteps=361500, episode_reward=171.48 +/- 43.97
Episode length: 43.36 +/- 10.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 361500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.16     |
|    n_updates        | 80374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4096     |
|    fps              | 266      |
|    time_elapsed     | 1359     |
|    total_timesteps  | 361741   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3        |
|    n_updates        | 80435    |
----------------------------------
Eval num_timesteps=362000, episode_reward=172.58 +/- 39.99
Episode length: 43.52 +/- 10.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 362000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00398  |
|    n_updates        | 80499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4100     |
|    fps              | 266      |
|    time_elapsed     | 1361     |
|    total_timesteps  | 362440   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.88     |
|    n_updates        | 80609    |
----------------------------------
Eval num_timesteps=362500, episode_reward=185.22 +/- 46.37
Episode length: 46.72 +/- 11.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 362500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2        |
|    n_updates        | 80624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4104     |
|    fps              | 266      |
|    time_elapsed     | 1363     |
|    total_timesteps  | 362853   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.76     |
|    n_updates        | 80713    |
----------------------------------
Eval num_timesteps=363000, episode_reward=172.74 +/- 45.15
Episode length: 43.58 +/- 11.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 363000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0125   |
|    n_updates        | 80749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4108     |
|    fps              | 265      |
|    time_elapsed     | 1365     |
|    total_timesteps  | 363185   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00579  |
|    n_updates        | 80796    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4112     |
|    fps              | 266      |
|    time_elapsed     | 1365     |
|    total_timesteps  | 363487   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.88     |
|    n_updates        | 80871    |
----------------------------------
Eval num_timesteps=363500, episode_reward=169.14 +/- 38.50
Episode length: 42.62 +/- 9.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 363500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.43     |
|    n_updates        | 80874    |
----------------------------------
Eval num_timesteps=364000, episode_reward=170.34 +/- 39.00
Episode length: 43.02 +/- 9.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 364000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.595    |
|    n_updates        | 80999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4116     |
|    fps              | 265      |
|    time_elapsed     | 1369     |
|    total_timesteps  | 364009   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.76     |
|    n_updates        | 81002    |
----------------------------------
Eval num_timesteps=364500, episode_reward=180.04 +/- 46.13
Episode length: 45.40 +/- 11.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 364500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64     |
|    n_updates        | 81124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4120     |
|    fps              | 265      |
|    time_elapsed     | 1371     |
|    total_timesteps  | 364559   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00551  |
|    n_updates        | 81139    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4124     |
|    fps              | 266      |
|    time_elapsed     | 1371     |
|    total_timesteps  | 364938   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.72     |
|    n_updates        | 81234    |
----------------------------------
Eval num_timesteps=365000, episode_reward=156.48 +/- 46.85
Episode length: 39.52 +/- 11.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.5     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 365000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.73     |
|    n_updates        | 81249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 410      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4128     |
|    fps              | 266      |
|    time_elapsed     | 1373     |
|    total_timesteps  | 365497   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.01     |
|    n_updates        | 81374    |
----------------------------------
Eval num_timesteps=365500, episode_reward=192.24 +/- 52.78
Episode length: 48.46 +/- 13.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.5     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 365500   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4132     |
|    fps              | 265      |
|    time_elapsed     | 1375     |
|    total_timesteps  | 365855   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.68     |
|    n_updates        | 81463    |
----------------------------------
Eval num_timesteps=366000, episode_reward=176.90 +/- 50.25
Episode length: 44.58 +/- 12.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 366000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.09     |
|    n_updates        | 81499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 419      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4136     |
|    fps              | 266      |
|    time_elapsed     | 1377     |
|    total_timesteps  | 366472   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00469  |
|    n_updates        | 81617    |
----------------------------------
Eval num_timesteps=366500, episode_reward=175.82 +/- 43.59
Episode length: 44.38 +/- 10.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 366500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.034    |
|    n_updates        | 81624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 419      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4140     |
|    fps              | 265      |
|    time_elapsed     | 1379     |
|    total_timesteps  | 366795   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.61     |
|    n_updates        | 81698    |
----------------------------------
Eval num_timesteps=367000, episode_reward=185.90 +/- 47.42
Episode length: 46.82 +/- 11.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.8     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 367000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.09     |
|    n_updates        | 81749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4144     |
|    fps              | 265      |
|    time_elapsed     | 1381     |
|    total_timesteps  | 367157   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00168  |
|    n_updates        | 81789    |
----------------------------------
Eval num_timesteps=367500, episode_reward=186.90 +/- 53.83
Episode length: 47.02 +/- 13.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 367500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.862    |
|    n_updates        | 81874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4148     |
|    fps              | 265      |
|    time_elapsed     | 1383     |
|    total_timesteps  | 367527   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00349  |
|    n_updates        | 81881    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4152     |
|    fps              | 265      |
|    time_elapsed     | 1383     |
|    total_timesteps  | 367809   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.778    |
|    n_updates        | 81952    |
----------------------------------
Eval num_timesteps=368000, episode_reward=158.74 +/- 47.84
Episode length: 40.12 +/- 12.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.1     |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 368000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.86     |
|    n_updates        | 81999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4156     |
|    fps              | 265      |
|    time_elapsed     | 1385     |
|    total_timesteps  | 368125   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.027    |
|    n_updates        | 82031    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4160     |
|    fps              | 265      |
|    time_elapsed     | 1385     |
|    total_timesteps  | 368487   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0952   |
|    n_updates        | 82121    |
----------------------------------
Eval num_timesteps=368500, episode_reward=164.80 +/- 37.53
Episode length: 41.62 +/- 9.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 368500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.05     |
|    n_updates        | 82124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4164     |
|    fps              | 265      |
|    time_elapsed     | 1387     |
|    total_timesteps  | 368832   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.86     |
|    n_updates        | 82207    |
----------------------------------
Eval num_timesteps=369000, episode_reward=161.98 +/- 53.80
Episode length: 40.82 +/- 13.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.8     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 369000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08     |
|    n_updates        | 82249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.6     |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4168     |
|    fps              | 265      |
|    time_elapsed     | 1389     |
|    total_timesteps  | 369195   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0135   |
|    n_updates        | 82298    |
----------------------------------
Eval num_timesteps=369500, episode_reward=155.74 +/- 48.15
Episode length: 39.30 +/- 12.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.3     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 369500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.3      |
|    n_updates        | 82374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4172     |
|    fps              | 265      |
|    time_elapsed     | 1390     |
|    total_timesteps  | 369579   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.64     |
|    n_updates        | 82394    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4176     |
|    fps              | 265      |
|    time_elapsed     | 1391     |
|    total_timesteps  | 369960   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.165    |
|    n_updates        | 82489    |
----------------------------------
Eval num_timesteps=370000, episode_reward=154.74 +/- 40.67
Episode length: 39.08 +/- 10.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.1     |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 370000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.47     |
|    n_updates        | 82499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4180     |
|    fps              | 265      |
|    time_elapsed     | 1393     |
|    total_timesteps  | 370374   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0108   |
|    n_updates        | 82593    |
----------------------------------
Eval num_timesteps=370500, episode_reward=186.64 +/- 55.61
Episode length: 46.98 +/- 13.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 370500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.12     |
|    n_updates        | 82624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4184     |
|    fps              | 265      |
|    time_elapsed     | 1395     |
|    total_timesteps  | 370695   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.5      |
|    n_updates        | 82673    |
----------------------------------
Eval num_timesteps=371000, episode_reward=164.36 +/- 55.65
Episode length: 41.48 +/- 13.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 371000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.69     |
|    n_updates        | 82749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4188     |
|    fps              | 265      |
|    time_elapsed     | 1396     |
|    total_timesteps  | 371042   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47     |
|    n_updates        | 82760    |
----------------------------------
Eval num_timesteps=371500, episode_reward=171.86 +/- 56.16
Episode length: 43.28 +/- 14.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 371500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.04     |
|    n_updates        | 82874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4192     |
|    fps              | 265      |
|    time_elapsed     | 1398     |
|    total_timesteps  | 371519   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00184  |
|    n_updates        | 82879    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4196     |
|    fps              | 265      |
|    time_elapsed     | 1399     |
|    total_timesteps  | 371870   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.61     |
|    n_updates        | 82967    |
----------------------------------
Eval num_timesteps=372000, episode_reward=173.34 +/- 38.03
Episode length: 43.66 +/- 9.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 372000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.143    |
|    n_updates        | 82999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4200     |
|    fps              | 265      |
|    time_elapsed     | 1400     |
|    total_timesteps  | 372312   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0189   |
|    n_updates        | 83077    |
----------------------------------
Eval num_timesteps=372500, episode_reward=163.42 +/- 56.28
Episode length: 41.26 +/- 14.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.3     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 372500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.015    |
|    n_updates        | 83124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.9     |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4204     |
|    fps              | 265      |
|    time_elapsed     | 1402     |
|    total_timesteps  | 372643   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00196  |
|    n_updates        | 83160    |
----------------------------------
Eval num_timesteps=373000, episode_reward=172.92 +/- 48.91
Episode length: 43.68 +/- 12.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 373000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.59     |
|    n_updates        | 83249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.4     |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4208     |
|    fps              | 265      |
|    time_elapsed     | 1404     |
|    total_timesteps  | 373129   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.8      |
|    n_updates        | 83282    |
----------------------------------
Eval num_timesteps=373500, episode_reward=175.38 +/- 42.56
Episode length: 44.16 +/- 10.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 373500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.55     |
|    n_updates        | 83374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4212     |
|    fps              | 265      |
|    time_elapsed     | 1406     |
|    total_timesteps  | 373585   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0104   |
|    n_updates        | 83396    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4216     |
|    fps              | 265      |
|    time_elapsed     | 1406     |
|    total_timesteps  | 373780   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.5      |
|    n_updates        | 83444    |
----------------------------------
Eval num_timesteps=374000, episode_reward=163.38 +/- 57.38
Episode length: 41.28 +/- 14.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.3     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 374000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.43     |
|    n_updates        | 83499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4220     |
|    fps              | 265      |
|    time_elapsed     | 1408     |
|    total_timesteps  | 374311   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.7      |
|    n_updates        | 83577    |
----------------------------------
Eval num_timesteps=374500, episode_reward=179.42 +/- 38.39
Episode length: 45.22 +/- 9.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 374500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0186   |
|    n_updates        | 83624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4224     |
|    fps              | 265      |
|    time_elapsed     | 1410     |
|    total_timesteps  | 374611   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.051    |
|    n_updates        | 83652    |
----------------------------------
Eval num_timesteps=375000, episode_reward=158.58 +/- 41.65
Episode length: 40.00 +/- 10.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40       |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 375000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.902    |
|    n_updates        | 83749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4228     |
|    fps              | 265      |
|    time_elapsed     | 1412     |
|    total_timesteps  | 375236   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.31     |
|    n_updates        | 83808    |
----------------------------------
Eval num_timesteps=375500, episode_reward=161.02 +/- 57.92
Episode length: 40.68 +/- 14.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.7     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 375500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00759  |
|    n_updates        | 83874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4232     |
|    fps              | 265      |
|    time_elapsed     | 1414     |
|    total_timesteps  | 375525   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.46     |
|    n_updates        | 83881    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4236     |
|    fps              | 265      |
|    time_elapsed     | 1414     |
|    total_timesteps  | 375975   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00418  |
|    n_updates        | 83993    |
----------------------------------
Eval num_timesteps=376000, episode_reward=186.04 +/- 51.04
Episode length: 46.92 +/- 12.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 376000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.47     |
|    n_updates        | 83999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4240     |
|    fps              | 265      |
|    time_elapsed     | 1416     |
|    total_timesteps  | 376348   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.77     |
|    n_updates        | 84086    |
----------------------------------
Eval num_timesteps=376500, episode_reward=157.06 +/- 51.36
Episode length: 39.62 +/- 12.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.6     |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 376500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.16     |
|    n_updates        | 84124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.3     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4244     |
|    fps              | 265      |
|    time_elapsed     | 1418     |
|    total_timesteps  | 376791   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.103    |
|    n_updates        | 84197    |
----------------------------------
Eval num_timesteps=377000, episode_reward=171.02 +/- 41.35
Episode length: 43.10 +/- 10.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 377000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.87     |
|    n_updates        | 84249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4248     |
|    fps              | 265      |
|    time_elapsed     | 1420     |
|    total_timesteps  | 377147   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.71     |
|    n_updates        | 84286    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4252     |
|    fps              | 265      |
|    time_elapsed     | 1420     |
|    total_timesteps  | 377362   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.04     |
|    n_updates        | 84340    |
----------------------------------
Eval num_timesteps=377500, episode_reward=150.22 +/- 40.74
Episode length: 37.98 +/- 10.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38       |
|    mean_reward      | 150      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 377500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.54     |
|    n_updates        | 84374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4256     |
|    fps              | 265      |
|    time_elapsed     | 1422     |
|    total_timesteps  | 377720   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.55     |
|    n_updates        | 84429    |
----------------------------------
Eval num_timesteps=378000, episode_reward=186.86 +/- 44.58
Episode length: 47.14 +/- 11.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.1     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 378000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.212    |
|    n_updates        | 84499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4260     |
|    fps              | 265      |
|    time_elapsed     | 1423     |
|    total_timesteps  | 378062   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00205  |
|    n_updates        | 84515    |
----------------------------------
Eval num_timesteps=378500, episode_reward=151.48 +/- 59.76
Episode length: 38.20 +/- 14.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.2     |
|    mean_reward      | 151      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 378500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.5      |
|    n_updates        | 84624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4264     |
|    fps              | 265      |
|    time_elapsed     | 1425     |
|    total_timesteps  | 378582   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0556   |
|    n_updates        | 84645    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4268     |
|    fps              | 265      |
|    time_elapsed     | 1426     |
|    total_timesteps  | 378864   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00519  |
|    n_updates        | 84715    |
----------------------------------
Eval num_timesteps=379000, episode_reward=174.98 +/- 46.58
Episode length: 44.14 +/- 11.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 379000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0144   |
|    n_updates        | 84749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.2     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4272     |
|    fps              | 265      |
|    time_elapsed     | 1427     |
|    total_timesteps  | 379099   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.77     |
|    n_updates        | 84774    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.8     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4276     |
|    fps              | 265      |
|    time_elapsed     | 1428     |
|    total_timesteps  | 379445   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.13     |
|    n_updates        | 84861    |
----------------------------------
Eval num_timesteps=379500, episode_reward=179.12 +/- 50.50
Episode length: 45.18 +/- 12.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 379500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0361   |
|    n_updates        | 84874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.2     |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4280     |
|    fps              | 265      |
|    time_elapsed     | 1429     |
|    total_timesteps  | 379697   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.76     |
|    n_updates        | 84924    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93       |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4284     |
|    fps              | 265      |
|    time_elapsed     | 1430     |
|    total_timesteps  | 379991   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.77     |
|    n_updates        | 84997    |
----------------------------------
Eval num_timesteps=380000, episode_reward=165.14 +/- 48.38
Episode length: 41.60 +/- 12.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 380000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.8      |
|    n_updates        | 84999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.6     |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4288     |
|    fps              | 265      |
|    time_elapsed     | 1431     |
|    total_timesteps  | 380198   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.89     |
|    n_updates        | 85049    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.9     |
|    ep_rew_mean      | 354      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4292     |
|    fps              | 265      |
|    time_elapsed     | 1432     |
|    total_timesteps  | 380410   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00662  |
|    n_updates        | 85102    |
----------------------------------
Eval num_timesteps=380500, episode_reward=157.32 +/- 42.95
Episode length: 39.68 +/- 10.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.7     |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 380500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0124   |
|    n_updates        | 85124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.6     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4296     |
|    fps              | 265      |
|    time_elapsed     | 1433     |
|    total_timesteps  | 380733   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.81     |
|    n_updates        | 85183    |
----------------------------------
Eval num_timesteps=381000, episode_reward=177.26 +/- 38.86
Episode length: 44.70 +/- 9.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 381000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0122   |
|    n_updates        | 85249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.1     |
|    ep_rew_mean      | 351      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4300     |
|    fps              | 265      |
|    time_elapsed     | 1435     |
|    total_timesteps  | 381118   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86     |
|    n_updates        | 85279    |
----------------------------------
Eval num_timesteps=381500, episode_reward=177.84 +/- 42.31
Episode length: 44.90 +/- 10.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 381500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00188  |
|    n_updates        | 85374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.5     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4304     |
|    fps              | 265      |
|    time_elapsed     | 1437     |
|    total_timesteps  | 381593   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.59     |
|    n_updates        | 85398    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.3     |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4308     |
|    fps              | 265      |
|    time_elapsed     | 1437     |
|    total_timesteps  | 381961   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.81     |
|    n_updates        | 85490    |
----------------------------------
Eval num_timesteps=382000, episode_reward=166.48 +/- 50.81
Episode length: 41.90 +/- 12.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.9     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 382000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00736  |
|    n_updates        | 85499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.5     |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4312     |
|    fps              | 265      |
|    time_elapsed     | 1439     |
|    total_timesteps  | 382238   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0528   |
|    n_updates        | 85559    |
----------------------------------
Eval num_timesteps=382500, episode_reward=182.88 +/- 57.80
Episode length: 46.06 +/- 14.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 382500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.84     |
|    n_updates        | 85624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.5     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4316     |
|    fps              | 265      |
|    time_elapsed     | 1441     |
|    total_timesteps  | 382533   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.83     |
|    n_updates        | 85633    |
----------------------------------
Eval num_timesteps=383000, episode_reward=175.50 +/- 39.93
Episode length: 44.24 +/- 9.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 383000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000936 |
|    n_updates        | 85749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4320     |
|    fps              | 265      |
|    time_elapsed     | 1443     |
|    total_timesteps  | 383028   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00202  |
|    n_updates        | 85756    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4324     |
|    fps              | 265      |
|    time_elapsed     | 1443     |
|    total_timesteps  | 383333   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.79     |
|    n_updates        | 85833    |
----------------------------------
Eval num_timesteps=383500, episode_reward=172.12 +/- 41.48
Episode length: 43.44 +/- 10.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 383500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00248  |
|    n_updates        | 85874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.1     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4328     |
|    fps              | 265      |
|    time_elapsed     | 1445     |
|    total_timesteps  | 383943   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.82     |
|    n_updates        | 85985    |
----------------------------------
Eval num_timesteps=384000, episode_reward=173.48 +/- 42.00
Episode length: 43.72 +/- 10.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 384000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00085  |
|    n_updates        | 85999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.7     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4332     |
|    fps              | 265      |
|    time_elapsed     | 1447     |
|    total_timesteps  | 384298   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.82     |
|    n_updates        | 86074    |
----------------------------------
Eval num_timesteps=384500, episode_reward=153.02 +/- 47.04
Episode length: 38.60 +/- 11.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.6     |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 384500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.85     |
|    n_updates        | 86124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.4     |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4336     |
|    fps              | 265      |
|    time_elapsed     | 1449     |
|    total_timesteps  | 384812   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.64     |
|    n_updates        | 86202    |
----------------------------------
Eval num_timesteps=385000, episode_reward=174.60 +/- 43.34
Episode length: 44.08 +/- 10.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 385000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00403  |
|    n_updates        | 86249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.6     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4340     |
|    fps              | 265      |
|    time_elapsed     | 1451     |
|    total_timesteps  | 385212   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.8      |
|    n_updates        | 86302    |
----------------------------------
Eval num_timesteps=385500, episode_reward=185.34 +/- 49.60
Episode length: 46.66 +/- 12.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 385500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.77     |
|    n_updates        | 86374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 347      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4344     |
|    fps              | 265      |
|    time_elapsed     | 1453     |
|    total_timesteps  | 385512   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00621  |
|    n_updates        | 86377    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87       |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4348     |
|    fps              | 265      |
|    time_elapsed     | 1453     |
|    total_timesteps  | 385850   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.66     |
|    n_updates        | 86462    |
----------------------------------
Eval num_timesteps=386000, episode_reward=160.26 +/- 55.37
Episode length: 40.48 +/- 13.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.5     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 386000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 9.1      |
|    n_updates        | 86499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.7     |
|    ep_rew_mean      | 357      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4352     |
|    fps              | 265      |
|    time_elapsed     | 1455     |
|    total_timesteps  | 386331   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00646  |
|    n_updates        | 86582    |
----------------------------------
Eval num_timesteps=386500, episode_reward=160.14 +/- 51.43
Episode length: 40.42 +/- 12.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.4     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 386500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.8      |
|    n_updates        | 86624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.4     |
|    ep_rew_mean      | 360      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4356     |
|    fps              | 265      |
|    time_elapsed     | 1457     |
|    total_timesteps  | 386756   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0443   |
|    n_updates        | 86688    |
----------------------------------
Eval num_timesteps=387000, episode_reward=176.08 +/- 46.66
Episode length: 44.36 +/- 11.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 387000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.019    |
|    n_updates        | 86749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.3     |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4360     |
|    fps              | 265      |
|    time_elapsed     | 1459     |
|    total_timesteps  | 387196   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.74     |
|    n_updates        | 86798    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.7     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4364     |
|    fps              | 265      |
|    time_elapsed     | 1459     |
|    total_timesteps  | 387456   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.89     |
|    n_updates        | 86863    |
----------------------------------
Eval num_timesteps=387500, episode_reward=179.92 +/- 41.70
Episode length: 45.34 +/- 10.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 387500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0132   |
|    n_updates        | 86874    |
----------------------------------
Eval num_timesteps=388000, episode_reward=165.86 +/- 39.88
Episode length: 41.78 +/- 9.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.8     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 388000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0292   |
|    n_updates        | 86999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4368     |
|    fps              | 265      |
|    time_elapsed     | 1462     |
|    total_timesteps  | 388019   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0294   |
|    n_updates        | 87004    |
----------------------------------
Eval num_timesteps=388500, episode_reward=148.04 +/- 44.07
Episode length: 37.44 +/- 11.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.4     |
|    mean_reward      | 148      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 388500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2        |
|    n_updates        | 87124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4372     |
|    fps              | 265      |
|    time_elapsed     | 1464     |
|    total_timesteps  | 388646   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.136    |
|    n_updates        | 87161    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.8     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4376     |
|    fps              | 265      |
|    time_elapsed     | 1465     |
|    total_timesteps  | 388930   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.17     |
|    n_updates        | 87232    |
----------------------------------
Eval num_timesteps=389000, episode_reward=169.10 +/- 48.67
Episode length: 42.64 +/- 12.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 389000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.132    |
|    n_updates        | 87249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.6     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4380     |
|    fps              | 265      |
|    time_elapsed     | 1467     |
|    total_timesteps  | 389357   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.26     |
|    n_updates        | 87339    |
----------------------------------
Eval num_timesteps=389500, episode_reward=168.74 +/- 39.54
Episode length: 42.50 +/- 9.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 389500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00764  |
|    n_updates        | 87374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4384     |
|    fps              | 265      |
|    time_elapsed     | 1468     |
|    total_timesteps  | 389739   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00417  |
|    n_updates        | 87434    |
----------------------------------
Eval num_timesteps=390000, episode_reward=170.36 +/- 52.86
Episode length: 42.96 +/- 13.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 390000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.84     |
|    n_updates        | 87499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4388     |
|    fps              | 265      |
|    time_elapsed     | 1470     |
|    total_timesteps  | 390074   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.46     |
|    n_updates        | 87518    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4392     |
|    fps              | 265      |
|    time_elapsed     | 1471     |
|    total_timesteps  | 390486   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00151  |
|    n_updates        | 87621    |
----------------------------------
Eval num_timesteps=390500, episode_reward=173.98 +/- 38.99
Episode length: 43.86 +/- 9.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 390500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00104  |
|    n_updates        | 87624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4396     |
|    fps              | 265      |
|    time_elapsed     | 1472     |
|    total_timesteps  | 390878   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.337    |
|    n_updates        | 87719    |
----------------------------------
Eval num_timesteps=391000, episode_reward=153.40 +/- 43.89
Episode length: 38.76 +/- 11.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.8     |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 391000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00109  |
|    n_updates        | 87749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4400     |
|    fps              | 265      |
|    time_elapsed     | 1474     |
|    total_timesteps  | 391130   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0359   |
|    n_updates        | 87782    |
----------------------------------
Eval num_timesteps=391500, episode_reward=181.50 +/- 52.42
Episode length: 45.80 +/- 13.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 391500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00123  |
|    n_updates        | 87874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4404     |
|    fps              | 265      |
|    time_elapsed     | 1476     |
|    total_timesteps  | 391665   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.67     |
|    n_updates        | 87916    |
----------------------------------
Eval num_timesteps=392000, episode_reward=162.16 +/- 44.37
Episode length: 40.94 +/- 11.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.9     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 392000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.142    |
|    n_updates        | 87999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4408     |
|    fps              | 265      |
|    time_elapsed     | 1478     |
|    total_timesteps  | 392083   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.7      |
|    n_updates        | 88020    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4412     |
|    fps              | 265      |
|    time_elapsed     | 1478     |
|    total_timesteps  | 392421   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00106  |
|    n_updates        | 88105    |
----------------------------------
Eval num_timesteps=392500, episode_reward=186.08 +/- 50.22
Episode length: 46.86 +/- 12.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 392500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0105   |
|    n_updates        | 88124    |
----------------------------------
Eval num_timesteps=393000, episode_reward=164.14 +/- 46.37
Episode length: 41.40 +/- 11.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 393000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0149   |
|    n_updates        | 88249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 418      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4416     |
|    fps              | 265      |
|    time_elapsed     | 1482     |
|    total_timesteps  | 393008   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0164   |
|    n_updates        | 88251    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 414      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4420     |
|    fps              | 265      |
|    time_elapsed     | 1482     |
|    total_timesteps  | 393424   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.92     |
|    n_updates        | 88355    |
----------------------------------
Eval num_timesteps=393500, episode_reward=175.96 +/- 53.01
Episode length: 44.30 +/- 13.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 393500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.92     |
|    n_updates        | 88374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 421      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4424     |
|    fps              | 265      |
|    time_elapsed     | 1484     |
|    total_timesteps  | 393900   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9      |
|    n_updates        | 88474    |
----------------------------------
Eval num_timesteps=394000, episode_reward=169.94 +/- 52.10
Episode length: 42.84 +/- 13.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 394000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00783  |
|    n_updates        | 88499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 410      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4428     |
|    fps              | 265      |
|    time_elapsed     | 1486     |
|    total_timesteps  | 394228   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.84     |
|    n_updates        | 88556    |
----------------------------------
Eval num_timesteps=394500, episode_reward=175.46 +/- 46.75
Episode length: 44.22 +/- 11.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 394500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00226  |
|    n_updates        | 88624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4432     |
|    fps              | 265      |
|    time_elapsed     | 1488     |
|    total_timesteps  | 394561   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0142   |
|    n_updates        | 88640    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4436     |
|    fps              | 265      |
|    time_elapsed     | 1488     |
|    total_timesteps  | 394916   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00194  |
|    n_updates        | 88728    |
----------------------------------
Eval num_timesteps=395000, episode_reward=190.08 +/- 45.47
Episode length: 47.90 +/- 11.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.9     |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 395000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.84     |
|    n_updates        | 88749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4440     |
|    fps              | 265      |
|    time_elapsed     | 1490     |
|    total_timesteps  | 395284   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0188   |
|    n_updates        | 88820    |
----------------------------------
Eval num_timesteps=395500, episode_reward=183.76 +/- 46.91
Episode length: 46.30 +/- 11.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 395500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.85     |
|    n_updates        | 88874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4444     |
|    fps              | 265      |
|    time_elapsed     | 1492     |
|    total_timesteps  | 395644   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.89     |
|    n_updates        | 88910    |
----------------------------------
Eval num_timesteps=396000, episode_reward=187.44 +/- 60.42
Episode length: 47.24 +/- 15.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.2     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 396000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.89     |
|    n_updates        | 88999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4448     |
|    fps              | 264      |
|    time_elapsed     | 1494     |
|    total_timesteps  | 396022   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.631    |
|    n_updates        | 89005    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4452     |
|    fps              | 265      |
|    time_elapsed     | 1494     |
|    total_timesteps  | 396305   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.122    |
|    n_updates        | 89076    |
----------------------------------
Eval num_timesteps=396500, episode_reward=162.60 +/- 59.52
Episode length: 40.92 +/- 14.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.9     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 396500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.495    |
|    n_updates        | 89124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4456     |
|    fps              | 264      |
|    time_elapsed     | 1496     |
|    total_timesteps  | 396527   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86     |
|    n_updates        | 89131    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4460     |
|    fps              | 265      |
|    time_elapsed     | 1496     |
|    total_timesteps  | 396970   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.91     |
|    n_updates        | 89242    |
----------------------------------
Eval num_timesteps=397000, episode_reward=165.46 +/- 51.33
Episode length: 41.68 +/- 12.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.7     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 397000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0127   |
|    n_updates        | 89249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.9     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4464     |
|    fps              | 265      |
|    time_elapsed     | 1498     |
|    total_timesteps  | 397342   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86     |
|    n_updates        | 89335    |
----------------------------------
Eval num_timesteps=397500, episode_reward=173.66 +/- 45.32
Episode length: 43.80 +/- 11.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 397500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.85     |
|    n_updates        | 89374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4468     |
|    fps              | 265      |
|    time_elapsed     | 1500     |
|    total_timesteps  | 397814   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000418 |
|    n_updates        | 89453    |
----------------------------------
Eval num_timesteps=398000, episode_reward=169.90 +/- 45.46
Episode length: 42.84 +/- 11.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 398000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0208   |
|    n_updates        | 89499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4472     |
|    fps              | 265      |
|    time_elapsed     | 1502     |
|    total_timesteps  | 398243   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.88     |
|    n_updates        | 89560    |
----------------------------------
Eval num_timesteps=398500, episode_reward=173.12 +/- 43.88
Episode length: 43.66 +/- 10.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 398500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0242   |
|    n_updates        | 89624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4476     |
|    fps              | 265      |
|    time_elapsed     | 1504     |
|    total_timesteps  | 398649   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0168   |
|    n_updates        | 89662    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4480     |
|    fps              | 265      |
|    time_elapsed     | 1504     |
|    total_timesteps  | 398957   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.87     |
|    n_updates        | 89739    |
----------------------------------
Eval num_timesteps=399000, episode_reward=180.14 +/- 53.91
Episode length: 45.40 +/- 13.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 399000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.76     |
|    n_updates        | 89749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.7     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4484     |
|    fps              | 265      |
|    time_elapsed     | 1506     |
|    total_timesteps  | 399207   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00403  |
|    n_updates        | 89801    |
----------------------------------
Eval num_timesteps=399500, episode_reward=184.52 +/- 47.64
Episode length: 46.48 +/- 11.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 399500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00501  |
|    n_updates        | 89874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.6     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4488     |
|    fps              | 264      |
|    time_elapsed     | 1508     |
|    total_timesteps  | 399736   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0108   |
|    n_updates        | 89933    |
----------------------------------
Eval num_timesteps=400000, episode_reward=171.74 +/- 42.24
Episode length: 43.28 +/- 10.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 400000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.242    |
|    n_updates        | 89999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4492     |
|    fps              | 264      |
|    time_elapsed     | 1510     |
|    total_timesteps  | 400033   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.35     |
|    n_updates        | 90008    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.3     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4496     |
|    fps              | 265      |
|    time_elapsed     | 1510     |
|    total_timesteps  | 400411   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.31     |
|    n_updates        | 90102    |
----------------------------------
Eval num_timesteps=400500, episode_reward=179.44 +/- 48.31
Episode length: 45.24 +/- 12.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 400500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0335   |
|    n_updates        | 90124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4500     |
|    fps              | 264      |
|    time_elapsed     | 1512     |
|    total_timesteps  | 400735   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0507   |
|    n_updates        | 90183    |
----------------------------------
Eval num_timesteps=401000, episode_reward=170.76 +/- 40.78
Episode length: 43.10 +/- 10.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 401000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.53     |
|    n_updates        | 90249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | 376      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4504     |
|    fps              | 264      |
|    time_elapsed     | 1514     |
|    total_timesteps  | 401098   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.93     |
|    n_updates        | 90274    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.9     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4508     |
|    fps              | 265      |
|    time_elapsed     | 1514     |
|    total_timesteps  | 401471   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86     |
|    n_updates        | 90367    |
----------------------------------
Eval num_timesteps=401500, episode_reward=168.98 +/- 47.71
Episode length: 42.64 +/- 11.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 401500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.15     |
|    n_updates        | 90374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.1     |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4512     |
|    fps              | 264      |
|    time_elapsed     | 1516     |
|    total_timesteps  | 401834   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.92     |
|    n_updates        | 90458    |
----------------------------------
Eval num_timesteps=402000, episode_reward=161.36 +/- 43.85
Episode length: 40.72 +/- 10.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.7     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 402000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.52     |
|    n_updates        | 90499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.1     |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4516     |
|    fps              | 264      |
|    time_elapsed     | 1518     |
|    total_timesteps  | 402119   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.68     |
|    n_updates        | 90529    |
----------------------------------
Eval num_timesteps=402500, episode_reward=173.54 +/- 46.27
Episode length: 43.78 +/- 11.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 402500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.517    |
|    n_updates        | 90624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.6     |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4520     |
|    fps              | 264      |
|    time_elapsed     | 1520     |
|    total_timesteps  | 402583   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.366    |
|    n_updates        | 90645    |
----------------------------------
Eval num_timesteps=403000, episode_reward=186.34 +/- 48.23
Episode length: 46.98 +/- 12.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 403000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000459 |
|    n_updates        | 90749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.4     |
|    ep_rew_mean      | 364      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4524     |
|    fps              | 264      |
|    time_elapsed     | 1522     |
|    total_timesteps  | 403037   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.5      |
|    n_updates        | 90759    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.3     |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4528     |
|    fps              | 264      |
|    time_elapsed     | 1522     |
|    total_timesteps  | 403463   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.82     |
|    n_updates        | 90865    |
----------------------------------
Eval num_timesteps=403500, episode_reward=164.44 +/- 58.85
Episode length: 41.50 +/- 14.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 403500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0224   |
|    n_updates        | 90874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.5     |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4532     |
|    fps              | 264      |
|    time_elapsed     | 1524     |
|    total_timesteps  | 403908   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.128    |
|    n_updates        | 90976    |
----------------------------------
Eval num_timesteps=404000, episode_reward=171.56 +/- 31.61
Episode length: 43.24 +/- 7.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 404000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.43     |
|    n_updates        | 90999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4536     |
|    fps              | 264      |
|    time_elapsed     | 1526     |
|    total_timesteps  | 404425   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.32     |
|    n_updates        | 91106    |
----------------------------------
Eval num_timesteps=404500, episode_reward=175.76 +/- 53.84
Episode length: 44.32 +/- 13.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 404500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.54     |
|    n_updates        | 91124    |
----------------------------------
Eval num_timesteps=405000, episode_reward=166.10 +/- 58.43
Episode length: 41.86 +/- 14.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.9     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 405000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00371  |
|    n_updates        | 91249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.9     |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4540     |
|    fps              | 264      |
|    time_elapsed     | 1529     |
|    total_timesteps  | 405070   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0422   |
|    n_updates        | 91267    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4544     |
|    fps              | 264      |
|    time_elapsed     | 1530     |
|    total_timesteps  | 405441   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26     |
|    n_updates        | 91360    |
----------------------------------
Eval num_timesteps=405500, episode_reward=169.84 +/- 62.66
Episode length: 42.88 +/- 15.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 405500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.89     |
|    n_updates        | 91374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4548     |
|    fps              | 264      |
|    time_elapsed     | 1532     |
|    total_timesteps  | 405764   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29     |
|    n_updates        | 91440    |
----------------------------------
Eval num_timesteps=406000, episode_reward=159.50 +/- 64.17
Episode length: 40.22 +/- 16.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.2     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 406000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0725   |
|    n_updates        | 91499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.6     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4552     |
|    fps              | 264      |
|    time_elapsed     | 1533     |
|    total_timesteps  | 406164   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0142   |
|    n_updates        | 91540    |
----------------------------------
Eval num_timesteps=406500, episode_reward=182.06 +/- 46.88
Episode length: 45.92 +/- 11.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 406500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.91     |
|    n_updates        | 91624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4556     |
|    fps              | 264      |
|    time_elapsed     | 1535     |
|    total_timesteps  | 406714   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.75     |
|    n_updates        | 91678    |
----------------------------------
Eval num_timesteps=407000, episode_reward=165.76 +/- 66.19
Episode length: 41.90 +/- 16.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.9     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 407000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.5      |
|    n_updates        | 91749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 410      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4560     |
|    fps              | 264      |
|    time_elapsed     | 1537     |
|    total_timesteps  | 407245   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94     |
|    n_updates        | 91811    |
----------------------------------
Eval num_timesteps=407500, episode_reward=163.88 +/- 59.21
Episode length: 41.38 +/- 14.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 407500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.81     |
|    n_updates        | 91874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 410      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4564     |
|    fps              | 264      |
|    time_elapsed     | 1539     |
|    total_timesteps  | 407636   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.66     |
|    n_updates        | 91908    |
----------------------------------
Eval num_timesteps=408000, episode_reward=165.24 +/- 47.59
Episode length: 41.68 +/- 11.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.7     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 408000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.152    |
|    n_updates        | 91999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4568     |
|    fps              | 264      |
|    time_elapsed     | 1541     |
|    total_timesteps  | 408084   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.16     |
|    n_updates        | 92020    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4572     |
|    fps              | 264      |
|    time_elapsed     | 1541     |
|    total_timesteps  | 408405   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.08     |
|    n_updates        | 92101    |
----------------------------------
Eval num_timesteps=408500, episode_reward=163.16 +/- 46.49
Episode length: 41.16 +/- 11.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.2     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 408500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.126    |
|    n_updates        | 92124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4576     |
|    fps              | 264      |
|    time_elapsed     | 1543     |
|    total_timesteps  | 408688   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.91     |
|    n_updates        | 92171    |
----------------------------------
Eval num_timesteps=409000, episode_reward=178.66 +/- 47.66
Episode length: 45.10 +/- 11.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 409000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.67     |
|    n_updates        | 92249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4580     |
|    fps              | 264      |
|    time_elapsed     | 1545     |
|    total_timesteps  | 409284   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.56     |
|    n_updates        | 92320    |
----------------------------------
Eval num_timesteps=409500, episode_reward=171.00 +/- 45.03
Episode length: 43.14 +/- 11.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 409500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94     |
|    n_updates        | 92374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 414      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4584     |
|    fps              | 264      |
|    time_elapsed     | 1547     |
|    total_timesteps  | 409603   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.79     |
|    n_updates        | 92400    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4588     |
|    fps              | 264      |
|    time_elapsed     | 1547     |
|    total_timesteps  | 409880   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00853  |
|    n_updates        | 92469    |
----------------------------------
Eval num_timesteps=410000, episode_reward=158.44 +/- 54.25
Episode length: 39.88 +/- 13.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.9     |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 410000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.82     |
|    n_updates        | 92499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4592     |
|    fps              | 264      |
|    time_elapsed     | 1549     |
|    total_timesteps  | 410085   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.467    |
|    n_updates        | 92521    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4596     |
|    fps              | 264      |
|    time_elapsed     | 1549     |
|    total_timesteps  | 410315   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0115   |
|    n_updates        | 92578    |
----------------------------------
Eval num_timesteps=410500, episode_reward=176.94 +/- 47.36
Episode length: 44.68 +/- 11.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 410500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2        |
|    n_updates        | 92624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4600     |
|    fps              | 264      |
|    time_elapsed     | 1551     |
|    total_timesteps  | 410681   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0083   |
|    n_updates        | 92670    |
----------------------------------
Eval num_timesteps=411000, episode_reward=191.86 +/- 55.38
Episode length: 48.38 +/- 13.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.4     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 411000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.37     |
|    n_updates        | 92749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4604     |
|    fps              | 264      |
|    time_elapsed     | 1553     |
|    total_timesteps  | 411163   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94     |
|    n_updates        | 92790    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.9     |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4608     |
|    fps              | 264      |
|    time_elapsed     | 1553     |
|    total_timesteps  | 411462   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.118    |
|    n_updates        | 92865    |
----------------------------------
Eval num_timesteps=411500, episode_reward=182.66 +/- 51.64
Episode length: 46.00 +/- 12.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 411500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0217   |
|    n_updates        | 92874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.3     |
|    ep_rew_mean      | 396      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4612     |
|    fps              | 264      |
|    time_elapsed     | 1555     |
|    total_timesteps  | 411762   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94     |
|    n_updates        | 92940    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.2     |
|    ep_rew_mean      | 391      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4616     |
|    fps              | 264      |
|    time_elapsed     | 1555     |
|    total_timesteps  | 411941   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.134    |
|    n_updates        | 92985    |
----------------------------------
Eval num_timesteps=412000, episode_reward=175.90 +/- 47.96
Episode length: 44.34 +/- 12.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 412000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01     |
|    n_updates        | 92999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4620     |
|    fps              | 264      |
|    time_elapsed     | 1557     |
|    total_timesteps  | 412251   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.81     |
|    n_updates        | 93062    |
----------------------------------
Eval num_timesteps=412500, episode_reward=177.16 +/- 46.73
Episode length: 44.68 +/- 11.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 412500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.88     |
|    n_updates        | 93124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.4     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4624     |
|    fps              | 264      |
|    time_elapsed     | 1559     |
|    total_timesteps  | 412574   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0378   |
|    n_updates        | 93143    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.2     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4628     |
|    fps              | 264      |
|    time_elapsed     | 1559     |
|    total_timesteps  | 412983   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.35     |
|    n_updates        | 93245    |
----------------------------------
Eval num_timesteps=413000, episode_reward=186.30 +/- 53.33
Episode length: 46.96 +/- 13.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 413000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.95     |
|    n_updates        | 93249    |
----------------------------------
Eval num_timesteps=413500, episode_reward=174.80 +/- 44.39
Episode length: 44.10 +/- 11.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 413500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0356   |
|    n_updates        | 93374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.4     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4632     |
|    fps              | 264      |
|    time_elapsed     | 1563     |
|    total_timesteps  | 413551   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2        |
|    n_updates        | 93387    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.9     |
|    ep_rew_mean      | 378      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4636     |
|    fps              | 264      |
|    time_elapsed     | 1563     |
|    total_timesteps  | 413917   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.79     |
|    n_updates        | 93479    |
----------------------------------
Eval num_timesteps=414000, episode_reward=164.80 +/- 65.06
Episode length: 41.56 +/- 16.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 414000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.45     |
|    n_updates        | 93499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.5     |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4640     |
|    fps              | 264      |
|    time_elapsed     | 1565     |
|    total_timesteps  | 414324   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.95     |
|    n_updates        | 93580    |
----------------------------------
Eval num_timesteps=414500, episode_reward=190.28 +/- 89.01
Episode length: 47.92 +/- 22.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.9     |
|    mean_reward      | 190      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 414500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08     |
|    n_updates        | 93624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91       |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4644     |
|    fps              | 264      |
|    time_elapsed     | 1567     |
|    total_timesteps  | 414541   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.87     |
|    n_updates        | 93635    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.1     |
|    ep_rew_mean      | 363      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4648     |
|    fps              | 264      |
|    time_elapsed     | 1567     |
|    total_timesteps  | 414870   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.67     |
|    n_updates        | 93717    |
----------------------------------
Eval num_timesteps=415000, episode_reward=168.36 +/- 43.80
Episode length: 42.46 +/- 10.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 415000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.97     |
|    n_updates        | 93749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 365      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4652     |
|    fps              | 264      |
|    time_elapsed     | 1569     |
|    total_timesteps  | 415319   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.012    |
|    n_updates        | 93829    |
----------------------------------
Eval num_timesteps=415500, episode_reward=164.18 +/- 36.18
Episode length: 41.46 +/- 9.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 415500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0652   |
|    n_updates        | 93874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.3     |
|    ep_rew_mean      | 352      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4656     |
|    fps              | 264      |
|    time_elapsed     | 1571     |
|    total_timesteps  | 415545   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0122   |
|    n_updates        | 93886    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.3     |
|    ep_rew_mean      | 348      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4660     |
|    fps              | 264      |
|    time_elapsed     | 1571     |
|    total_timesteps  | 415976   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9      |
|    n_updates        | 93993    |
----------------------------------
Eval num_timesteps=416000, episode_reward=177.02 +/- 64.25
Episode length: 44.60 +/- 16.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 416000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.444    |
|    n_updates        | 93999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.4     |
|    ep_rew_mean      | 344      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4664     |
|    fps              | 264      |
|    time_elapsed     | 1573     |
|    total_timesteps  | 416275   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.1      |
|    n_updates        | 94068    |
----------------------------------
Eval num_timesteps=416500, episode_reward=168.12 +/- 55.40
Episode length: 42.42 +/- 13.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 416500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.79     |
|    n_updates        | 94124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.1     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4668     |
|    fps              | 264      |
|    time_elapsed     | 1575     |
|    total_timesteps  | 416594   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.462    |
|    n_updates        | 94148    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.8     |
|    ep_rew_mean      | 337      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4672     |
|    fps              | 264      |
|    time_elapsed     | 1575     |
|    total_timesteps  | 416881   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.143    |
|    n_updates        | 94220    |
----------------------------------
Eval num_timesteps=417000, episode_reward=181.38 +/- 76.81
Episode length: 45.68 +/- 19.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 417000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0831   |
|    n_updates        | 94249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 339      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4676     |
|    fps              | 264      |
|    time_elapsed     | 1577     |
|    total_timesteps  | 417211   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.92     |
|    n_updates        | 94302    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 325      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4680     |
|    fps              | 264      |
|    time_elapsed     | 1577     |
|    total_timesteps  | 417451   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.13     |
|    n_updates        | 94362    |
----------------------------------
Eval num_timesteps=417500, episode_reward=170.98 +/- 37.51
Episode length: 43.10 +/- 9.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 417500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.09     |
|    n_updates        | 94374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 321      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4684     |
|    fps              | 264      |
|    time_elapsed     | 1579     |
|    total_timesteps  | 417656   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.285    |
|    n_updates        | 94413    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 320      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4688     |
|    fps              | 264      |
|    time_elapsed     | 1579     |
|    total_timesteps  | 417928   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.89     |
|    n_updates        | 94481    |
----------------------------------
Eval num_timesteps=418000, episode_reward=177.12 +/- 72.81
Episode length: 44.64 +/- 18.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 418000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.82     |
|    n_updates        | 94499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 326      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4692     |
|    fps              | 264      |
|    time_elapsed     | 1581     |
|    total_timesteps  | 418280   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.93     |
|    n_updates        | 94569    |
----------------------------------
Eval num_timesteps=418500, episode_reward=182.88 +/- 72.91
Episode length: 46.08 +/- 18.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 418500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.106    |
|    n_updates        | 94624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.9     |
|    ep_rew_mean      | 338      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4696     |
|    fps              | 264      |
|    time_elapsed     | 1583     |
|    total_timesteps  | 418802   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.172    |
|    n_updates        | 94700    |
----------------------------------
Eval num_timesteps=419000, episode_reward=173.04 +/- 38.95
Episode length: 43.74 +/- 9.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 419000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.13     |
|    n_updates        | 94749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.5     |
|    ep_rew_mean      | 332      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4700     |
|    fps              | 264      |
|    time_elapsed     | 1585     |
|    total_timesteps  | 419027   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0357   |
|    n_updates        | 94756    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.2     |
|    ep_rew_mean      | 331      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4704     |
|    fps              | 264      |
|    time_elapsed     | 1585     |
|    total_timesteps  | 419488   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0322   |
|    n_updates        | 94871    |
----------------------------------
Eval num_timesteps=419500, episode_reward=173.52 +/- 49.50
Episode length: 43.76 +/- 12.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 419500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0145   |
|    n_updates        | 94874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 328      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4708     |
|    fps              | 264      |
|    time_elapsed     | 1587     |
|    total_timesteps  | 419701   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.55     |
|    n_updates        | 94925    |
----------------------------------
Eval num_timesteps=420000, episode_reward=174.22 +/- 52.97
Episode length: 43.86 +/- 13.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 420000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.92     |
|    n_updates        | 94999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 336      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4712     |
|    fps              | 264      |
|    time_elapsed     | 1589     |
|    total_timesteps  | 420196   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.06     |
|    n_updates        | 95048    |
----------------------------------
Eval num_timesteps=420500, episode_reward=175.94 +/- 47.38
Episode length: 44.34 +/- 11.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 420500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.154    |
|    n_updates        | 95124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87       |
|    ep_rew_mean      | 346      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4716     |
|    fps              | 264      |
|    time_elapsed     | 1591     |
|    total_timesteps  | 420638   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.315    |
|    n_updates        | 95159    |
----------------------------------
Eval num_timesteps=421000, episode_reward=165.70 +/- 51.98
Episode length: 41.74 +/- 13.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.7     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 421000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.99     |
|    n_updates        | 95249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.5     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4720     |
|    fps              | 264      |
|    time_elapsed     | 1593     |
|    total_timesteps  | 421006   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.67     |
|    n_updates        | 95251    |
----------------------------------
Eval num_timesteps=421500, episode_reward=170.96 +/- 43.92
Episode length: 43.16 +/- 10.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 421500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.9      |
|    n_updates        | 95374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.3     |
|    ep_rew_mean      | 360      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4724     |
|    fps              | 264      |
|    time_elapsed     | 1595     |
|    total_timesteps  | 421600   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.02     |
|    n_updates        | 95399    |
----------------------------------
Eval num_timesteps=422000, episode_reward=177.66 +/- 40.68
Episode length: 44.76 +/- 10.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 422000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.88     |
|    n_updates        | 95499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.6     |
|    ep_rew_mean      | 361      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4728     |
|    fps              | 264      |
|    time_elapsed     | 1597     |
|    total_timesteps  | 422040   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0213   |
|    n_updates        | 95509    |
----------------------------------
Eval num_timesteps=422500, episode_reward=169.40 +/- 39.26
Episode length: 42.76 +/- 9.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 422500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02     |
|    n_updates        | 95624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.4     |
|    ep_rew_mean      | 360      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4732     |
|    fps              | 264      |
|    time_elapsed     | 1599     |
|    total_timesteps  | 422591   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.12     |
|    n_updates        | 95647    |
----------------------------------
Eval num_timesteps=423000, episode_reward=172.86 +/- 46.93
Episode length: 43.56 +/- 11.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.6     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 423000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0838   |
|    n_updates        | 95749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.8     |
|    ep_rew_mean      | 370      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4736     |
|    fps              | 264      |
|    time_elapsed     | 1601     |
|    total_timesteps  | 423193   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.129    |
|    n_updates        | 95798    |
----------------------------------
Eval num_timesteps=423500, episode_reward=175.16 +/- 47.04
Episode length: 44.20 +/- 11.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 423500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.06     |
|    n_updates        | 95874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.5     |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4740     |
|    fps              | 264      |
|    time_elapsed     | 1603     |
|    total_timesteps  | 423674   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.61     |
|    n_updates        | 95918    |
----------------------------------
Eval num_timesteps=424000, episode_reward=190.84 +/- 48.79
Episode length: 48.10 +/- 12.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.1     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 424000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.906    |
|    n_updates        | 95999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4744     |
|    fps              | 264      |
|    time_elapsed     | 1605     |
|    total_timesteps  | 424156   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.99     |
|    n_updates        | 96038    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.3     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4748     |
|    fps              | 264      |
|    time_elapsed     | 1605     |
|    total_timesteps  | 424398   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0351   |
|    n_updates        | 96099    |
----------------------------------
Eval num_timesteps=424500, episode_reward=163.98 +/- 44.52
Episode length: 41.40 +/- 11.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 424500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.95     |
|    n_updates        | 96124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.4     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4752     |
|    fps              | 264      |
|    time_elapsed     | 1607     |
|    total_timesteps  | 424862   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.254    |
|    n_updates        | 96215    |
----------------------------------
Eval num_timesteps=425000, episode_reward=164.26 +/- 57.53
Episode length: 41.46 +/- 14.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 425000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0439   |
|    n_updates        | 96249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4756     |
|    fps              | 264      |
|    time_elapsed     | 1609     |
|    total_timesteps  | 425196   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0303   |
|    n_updates        | 96298    |
----------------------------------
Eval num_timesteps=425500, episode_reward=185.26 +/- 54.41
Episode length: 46.74 +/- 13.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 425500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.46     |
|    n_updates        | 96374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4760     |
|    fps              | 264      |
|    time_elapsed     | 1611     |
|    total_timesteps  | 425817   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0087   |
|    n_updates        | 96454    |
----------------------------------
Eval num_timesteps=426000, episode_reward=164.38 +/- 70.10
Episode length: 41.48 +/- 17.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 426000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00668  |
|    n_updates        | 96499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.9     |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4764     |
|    fps              | 264      |
|    time_elapsed     | 1613     |
|    total_timesteps  | 426062   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.05     |
|    n_updates        | 96515    |
----------------------------------
Eval num_timesteps=426500, episode_reward=170.48 +/- 62.67
Episode length: 43.00 +/- 15.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 426500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00975  |
|    n_updates        | 96624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4768     |
|    fps              | 264      |
|    time_elapsed     | 1615     |
|    total_timesteps  | 426652   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.7      |
|    n_updates        | 96662    |
----------------------------------
Eval num_timesteps=427000, episode_reward=163.46 +/- 48.09
Episode length: 41.20 +/- 12.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.2     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 427000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00951  |
|    n_updates        | 96749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4772     |
|    fps              | 264      |
|    time_elapsed     | 1616     |
|    total_timesteps  | 427042   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00645  |
|    n_updates        | 96760    |
----------------------------------
Eval num_timesteps=427500, episode_reward=182.18 +/- 46.66
Episode length: 45.90 +/- 11.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 427500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.1      |
|    n_updates        | 96874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4776     |
|    fps              | 264      |
|    time_elapsed     | 1618     |
|    total_timesteps  | 427534   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.1      |
|    n_updates        | 96883    |
----------------------------------
Eval num_timesteps=428000, episode_reward=171.32 +/- 41.25
Episode length: 43.16 +/- 10.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 428000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.5      |
|    n_updates        | 96999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 426      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4780     |
|    fps              | 264      |
|    time_elapsed     | 1621     |
|    total_timesteps  | 428141   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.87     |
|    n_updates        | 97035    |
----------------------------------
Eval num_timesteps=428500, episode_reward=178.38 +/- 42.02
Episode length: 44.98 +/- 10.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 428500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.164    |
|    n_updates        | 97124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 444      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4784     |
|    fps              | 264      |
|    time_elapsed     | 1623     |
|    total_timesteps  | 428788   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.75     |
|    n_updates        | 97196    |
----------------------------------
Eval num_timesteps=429000, episode_reward=174.94 +/- 43.75
Episode length: 44.10 +/- 10.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 429000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.38     |
|    n_updates        | 97249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 446      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4788     |
|    fps              | 264      |
|    time_elapsed     | 1625     |
|    total_timesteps  | 429106   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.26     |
|    n_updates        | 97276    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 447      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4792     |
|    fps              | 264      |
|    time_elapsed     | 1625     |
|    total_timesteps  | 429477   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08     |
|    n_updates        | 97369    |
----------------------------------
Eval num_timesteps=429500, episode_reward=182.24 +/- 45.77
Episode length: 45.92 +/- 11.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.9     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 429500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00663  |
|    n_updates        | 97374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 441      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4796     |
|    fps              | 264      |
|    time_elapsed     | 1627     |
|    total_timesteps  | 429868   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.96     |
|    n_updates        | 97466    |
----------------------------------
Eval num_timesteps=430000, episode_reward=183.78 +/- 44.13
Episode length: 46.30 +/- 11.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 430000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.55     |
|    n_updates        | 97499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 446      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4800     |
|    fps              | 264      |
|    time_elapsed     | 1629     |
|    total_timesteps  | 430209   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.92     |
|    n_updates        | 97552    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 439      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4804     |
|    fps              | 264      |
|    time_elapsed     | 1629     |
|    total_timesteps  | 430499   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.68     |
|    n_updates        | 97624    |
----------------------------------
Eval num_timesteps=430500, episode_reward=156.92 +/- 45.04
Episode length: 39.64 +/- 11.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.6     |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 430500   |
---------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 443      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4808     |
|    fps              | 264      |
|    time_elapsed     | 1631     |
|    total_timesteps  | 430812   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01     |
|    n_updates        | 97702    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 430      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4812     |
|    fps              | 264      |
|    time_elapsed     | 1631     |
|    total_timesteps  | 430972   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.21     |
|    n_updates        | 97742    |
----------------------------------
Eval num_timesteps=431000, episode_reward=178.68 +/- 45.96
Episode length: 45.04 +/- 11.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 431000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.49     |
|    n_updates        | 97749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 427      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4816     |
|    fps              | 264      |
|    time_elapsed     | 1633     |
|    total_timesteps  | 431357   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.1      |
|    n_updates        | 97839    |
----------------------------------
Eval num_timesteps=431500, episode_reward=181.34 +/- 44.27
Episode length: 45.70 +/- 11.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.7     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 431500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.19     |
|    n_updates        | 97874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 423      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4820     |
|    fps              | 263      |
|    time_elapsed     | 1635     |
|    total_timesteps  | 431608   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.96     |
|    n_updates        | 97901    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4824     |
|    fps              | 264      |
|    time_elapsed     | 1635     |
|    total_timesteps  | 431872   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.09     |
|    n_updates        | 97967    |
----------------------------------
Eval num_timesteps=432000, episode_reward=169.38 +/- 45.97
Episode length: 42.64 +/- 11.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 432000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01     |
|    n_updates        | 97999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4828     |
|    fps              | 263      |
|    time_elapsed     | 1636     |
|    total_timesteps  | 432064   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.09     |
|    n_updates        | 98015    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.5     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4832     |
|    fps              | 264      |
|    time_elapsed     | 1637     |
|    total_timesteps  | 432344   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0561   |
|    n_updates        | 98085    |
----------------------------------
Eval num_timesteps=432500, episode_reward=156.50 +/- 52.63
Episode length: 39.50 +/- 13.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.5     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 432500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.86     |
|    n_updates        | 98124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4836     |
|    fps              | 264      |
|    time_elapsed     | 1639     |
|    total_timesteps  | 432814   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.519    |
|    n_updates        | 98203    |
----------------------------------
Eval num_timesteps=433000, episode_reward=175.58 +/- 48.54
Episode length: 44.30 +/- 12.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 433000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0873   |
|    n_updates        | 98249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.3     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4840     |
|    fps              | 264      |
|    time_elapsed     | 1641     |
|    total_timesteps  | 433309   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.721    |
|    n_updates        | 98327    |
----------------------------------
Eval num_timesteps=433500, episode_reward=164.12 +/- 54.97
Episode length: 41.42 +/- 13.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 433500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.17     |
|    n_updates        | 98374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.3     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4844     |
|    fps              | 264      |
|    time_elapsed     | 1642     |
|    total_timesteps  | 433784   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0518   |
|    n_updates        | 98445    |
----------------------------------
Eval num_timesteps=434000, episode_reward=175.50 +/- 35.75
Episode length: 44.22 +/- 9.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 434000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0398   |
|    n_updates        | 98499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4848     |
|    fps              | 264      |
|    time_elapsed     | 1644     |
|    total_timesteps  | 434317   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23     |
|    n_updates        | 98579    |
----------------------------------
Eval num_timesteps=434500, episode_reward=169.70 +/- 49.17
Episode length: 42.84 +/- 12.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 434500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.96     |
|    n_updates        | 98624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.3     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4852     |
|    fps              | 263      |
|    time_elapsed     | 1646     |
|    total_timesteps  | 434588   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.03     |
|    n_updates        | 98646    |
----------------------------------
Eval num_timesteps=435000, episode_reward=163.82 +/- 35.28
Episode length: 41.30 +/- 8.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.3     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 435000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0359   |
|    n_updates        | 98749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4856     |
|    fps              | 263      |
|    time_elapsed     | 1648     |
|    total_timesteps  | 435033   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.07     |
|    n_updates        | 98758    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.1     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4860     |
|    fps              | 264      |
|    time_elapsed     | 1648     |
|    total_timesteps  | 435426   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.165    |
|    n_updates        | 98856    |
----------------------------------
Eval num_timesteps=435500, episode_reward=164.76 +/- 43.95
Episode length: 41.62 +/- 11.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 435500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.93     |
|    n_updates        | 98874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.3     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4864     |
|    fps              | 264      |
|    time_elapsed     | 1650     |
|    total_timesteps  | 435789   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.58     |
|    n_updates        | 98947    |
----------------------------------
Eval num_timesteps=436000, episode_reward=175.68 +/- 48.19
Episode length: 44.34 +/- 12.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 436000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00306  |
|    n_updates        | 98999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.1     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4868     |
|    fps              | 263      |
|    time_elapsed     | 1652     |
|    total_timesteps  | 436263   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.99     |
|    n_updates        | 99065    |
----------------------------------
Eval num_timesteps=436500, episode_reward=171.86 +/- 78.02
Episode length: 43.34 +/- 19.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 436500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.87     |
|    n_updates        | 99124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4872     |
|    fps              | 263      |
|    time_elapsed     | 1654     |
|    total_timesteps  | 436589   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.63     |
|    n_updates        | 99147    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4876     |
|    fps              | 264      |
|    time_elapsed     | 1654     |
|    total_timesteps  | 436916   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0119   |
|    n_updates        | 99228    |
----------------------------------
Eval num_timesteps=437000, episode_reward=164.76 +/- 57.38
Episode length: 41.52 +/- 14.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 437000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.15     |
|    n_updates        | 99249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.2     |
|    ep_rew_mean      | 367      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4880     |
|    fps              | 264      |
|    time_elapsed     | 1656     |
|    total_timesteps  | 437359   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.93     |
|    n_updates        | 99339    |
----------------------------------
Eval num_timesteps=437500, episode_reward=170.86 +/- 51.52
Episode length: 43.06 +/- 12.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 437500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.04     |
|    n_updates        | 99374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.8     |
|    ep_rew_mean      | 354      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4884     |
|    fps              | 263      |
|    time_elapsed     | 1658     |
|    total_timesteps  | 437670   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08     |
|    n_updates        | 99417    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.7     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4888     |
|    fps              | 264      |
|    time_elapsed     | 1658     |
|    total_timesteps  | 437980   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.86     |
|    n_updates        | 99494    |
----------------------------------
Eval num_timesteps=438000, episode_reward=176.84 +/- 43.32
Episode length: 44.56 +/- 10.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 438000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0782   |
|    n_updates        | 99499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.8     |
|    ep_rew_mean      | 354      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4892     |
|    fps              | 263      |
|    time_elapsed     | 1660     |
|    total_timesteps  | 438362   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08     |
|    n_updates        | 99590    |
----------------------------------
Eval num_timesteps=438500, episode_reward=157.52 +/- 51.14
Episode length: 39.74 +/- 12.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.7     |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 438500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4        |
|    n_updates        | 99624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.7     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4896     |
|    fps              | 263      |
|    time_elapsed     | 1662     |
|    total_timesteps  | 438740   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00992  |
|    n_updates        | 99684    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.5     |
|    ep_rew_mean      | 349      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4900     |
|    fps              | 264      |
|    time_elapsed     | 1662     |
|    total_timesteps  | 438963   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.08     |
|    n_updates        | 99740    |
----------------------------------
Eval num_timesteps=439000, episode_reward=170.24 +/- 42.53
Episode length: 42.96 +/- 10.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 439000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.76     |
|    n_updates        | 99749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.2     |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4904     |
|    fps              | 264      |
|    time_elapsed     | 1664     |
|    total_timesteps  | 439417   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.14     |
|    n_updates        | 99854    |
----------------------------------
Eval num_timesteps=439500, episode_reward=183.80 +/- 44.38
Episode length: 46.28 +/- 11.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.3     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 439500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.294    |
|    n_updates        | 99874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.5     |
|    ep_rew_mean      | 353      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4908     |
|    fps              | 263      |
|    time_elapsed     | 1666     |
|    total_timesteps  | 439667   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.01     |
|    n_updates        | 99916    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.9     |
|    ep_rew_mean      | 358      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4912     |
|    fps              | 264      |
|    time_elapsed     | 1666     |
|    total_timesteps  | 439963   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.688    |
|    n_updates        | 99990    |
----------------------------------
Eval num_timesteps=440000, episode_reward=176.50 +/- 45.99
Episode length: 44.48 +/- 11.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 440000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0365   |
|    n_updates        | 99999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.2     |
|    ep_rew_mean      | 355      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4916     |
|    fps              | 263      |
|    time_elapsed     | 1668     |
|    total_timesteps  | 440272   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0271   |
|    n_updates        | 100067   |
----------------------------------
Eval num_timesteps=440500, episode_reward=176.50 +/- 42.01
Episode length: 44.50 +/- 10.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.5     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 440500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.23     |
|    n_updates        | 100124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.4     |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4920     |
|    fps              | 263      |
|    time_elapsed     | 1670     |
|    total_timesteps  | 440849   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.56     |
|    n_updates        | 100212   |
----------------------------------
Eval num_timesteps=441000, episode_reward=187.44 +/- 65.94
Episode length: 47.26 +/- 16.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.3     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 441000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.468    |
|    n_updates        | 100249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.1     |
|    ep_rew_mean      | 375      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4924     |
|    fps              | 263      |
|    time_elapsed     | 1672     |
|    total_timesteps  | 441282   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 15       |
|    n_updates        | 100320   |
----------------------------------
Eval num_timesteps=441500, episode_reward=171.28 +/- 46.42
Episode length: 43.16 +/- 11.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 441500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.051    |
|    n_updates        | 100374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.5     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4928     |
|    fps              | 263      |
|    time_elapsed     | 1674     |
|    total_timesteps  | 441611   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.88     |
|    n_updates        | 100402   |
----------------------------------
Eval num_timesteps=442000, episode_reward=166.12 +/- 44.28
Episode length: 41.84 +/- 11.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.8     |
|    mean_reward      | 166      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 442000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.16     |
|    n_updates        | 100499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.3     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4932     |
|    fps              | 263      |
|    time_elapsed     | 1676     |
|    total_timesteps  | 442079   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.15     |
|    n_updates        | 100519   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.4     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4936     |
|    fps              | 263      |
|    time_elapsed     | 1676     |
|    total_timesteps  | 442355   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0544   |
|    n_updates        | 100588   |
----------------------------------
Eval num_timesteps=442500, episode_reward=173.16 +/- 47.80
Episode length: 43.66 +/- 11.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 442500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 5.81     |
|    n_updates        | 100624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4940     |
|    fps              | 263      |
|    time_elapsed     | 1678     |
|    total_timesteps  | 442876   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.54     |
|    n_updates        | 100718   |
----------------------------------
Eval num_timesteps=443000, episode_reward=167.38 +/- 45.65
Episode length: 42.26 +/- 11.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 443000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0262   |
|    n_updates        | 100749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.7     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4944     |
|    fps              | 263      |
|    time_elapsed     | 1680     |
|    total_timesteps  | 443355   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.74     |
|    n_updates        | 100838   |
----------------------------------
Eval num_timesteps=443500, episode_reward=178.58 +/- 46.44
Episode length: 45.02 +/- 11.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 443500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.636    |
|    n_updates        | 100874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4948     |
|    fps              | 263      |
|    time_elapsed     | 1682     |
|    total_timesteps  | 443963   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.12     |
|    n_updates        | 100990   |
----------------------------------
Eval num_timesteps=444000, episode_reward=171.82 +/- 44.13
Episode length: 43.30 +/- 11.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 444000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.7      |
|    n_updates        | 100999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4952     |
|    fps              | 263      |
|    time_elapsed     | 1684     |
|    total_timesteps  | 444392   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0713   |
|    n_updates        | 101097   |
----------------------------------
Eval num_timesteps=444500, episode_reward=158.60 +/- 47.78
Episode length: 40.04 +/- 11.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40       |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 444500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0181   |
|    n_updates        | 101124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.3     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4956     |
|    fps              | 263      |
|    time_elapsed     | 1685     |
|    total_timesteps  | 444663   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.981    |
|    n_updates        | 101165   |
----------------------------------
Eval num_timesteps=445000, episode_reward=158.10 +/- 35.24
Episode length: 39.92 +/- 8.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.9     |
|    mean_reward      | 158      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 445000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0184   |
|    n_updates        | 101249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4960     |
|    fps              | 263      |
|    time_elapsed     | 1687     |
|    total_timesteps  | 445013   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.63     |
|    n_updates        | 101253   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4964     |
|    fps              | 263      |
|    time_elapsed     | 1687     |
|    total_timesteps  | 445303   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.458    |
|    n_updates        | 101325   |
----------------------------------
Eval num_timesteps=445500, episode_reward=175.86 +/- 48.64
Episode length: 44.32 +/- 12.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.3     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 445500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0411   |
|    n_updates        | 101374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.6     |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4968     |
|    fps              | 263      |
|    time_elapsed     | 1689     |
|    total_timesteps  | 445620   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0381   |
|    n_updates        | 101404   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.7     |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4972     |
|    fps              | 263      |
|    time_elapsed     | 1689     |
|    total_timesteps  | 445963   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0423   |
|    n_updates        | 101490   |
----------------------------------
Eval num_timesteps=446000, episode_reward=174.50 +/- 42.69
Episode length: 44.02 +/- 10.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 446000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.35     |
|    n_updates        | 101499   |
----------------------------------
Eval num_timesteps=446500, episode_reward=166.90 +/- 57.20
Episode length: 42.04 +/- 14.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 446500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00276  |
|    n_updates        | 101624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4976     |
|    fps              | 263      |
|    time_elapsed     | 1693     |
|    total_timesteps  | 446507   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0168   |
|    n_updates        | 101626   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.6     |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4980     |
|    fps              | 263      |
|    time_elapsed     | 1693     |
|    total_timesteps  | 446717   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.27     |
|    n_updates        | 101679   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92       |
|    ep_rew_mean      | 366      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4984     |
|    fps              | 263      |
|    time_elapsed     | 1693     |
|    total_timesteps  | 446874   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.125    |
|    n_updates        | 101718   |
----------------------------------
Eval num_timesteps=447000, episode_reward=168.30 +/- 63.50
Episode length: 42.40 +/- 15.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.4     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 447000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.14     |
|    n_updates        | 101749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.3     |
|    ep_rew_mean      | 368      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4988     |
|    fps              | 263      |
|    time_elapsed     | 1695     |
|    total_timesteps  | 447213   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.09     |
|    n_updates        | 101803   |
----------------------------------
Eval num_timesteps=447500, episode_reward=177.66 +/- 42.41
Episode length: 44.80 +/- 10.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 447500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.28     |
|    n_updates        | 101874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.3     |
|    ep_rew_mean      | 372      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4992     |
|    fps              | 263      |
|    time_elapsed     | 1697     |
|    total_timesteps  | 447693   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.21     |
|    n_updates        | 101923   |
----------------------------------
Eval num_timesteps=448000, episode_reward=164.64 +/- 69.53
Episode length: 41.58 +/- 17.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 448000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00266  |
|    n_updates        | 101999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.7     |
|    ep_rew_mean      | 373      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 4996     |
|    fps              | 263      |
|    time_elapsed     | 1699     |
|    total_timesteps  | 448107   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23     |
|    n_updates        | 102026   |
----------------------------------
Eval num_timesteps=448500, episode_reward=168.82 +/- 62.98
Episode length: 42.56 +/- 15.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 448500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0344   |
|    n_updates        | 102124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5000     |
|    fps              | 263      |
|    time_elapsed     | 1701     |
|    total_timesteps  | 448543   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.12     |
|    n_updates        | 102135   |
----------------------------------
Eval num_timesteps=449000, episode_reward=166.62 +/- 61.71
Episode length: 42.06 +/- 15.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.1     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 449000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0208   |
|    n_updates        | 102249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5004     |
|    fps              | 263      |
|    time_elapsed     | 1703     |
|    total_timesteps  | 449067   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.1      |
|    n_updates        | 102266   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5008     |
|    fps              | 263      |
|    time_elapsed     | 1703     |
|    total_timesteps  | 449344   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.12     |
|    n_updates        | 102335   |
----------------------------------
Eval num_timesteps=449500, episode_reward=167.88 +/- 59.02
Episode length: 42.34 +/- 14.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.3     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 449500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.17     |
|    n_updates        | 102374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.4     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5012     |
|    fps              | 263      |
|    time_elapsed     | 1705     |
|    total_timesteps  | 449607   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.08     |
|    n_updates        | 102401   |
----------------------------------
Eval num_timesteps=450000, episode_reward=160.46 +/- 58.49
Episode length: 40.52 +/- 14.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.5     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 450000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0421   |
|    n_updates        | 102499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5016     |
|    fps              | 263      |
|    time_elapsed     | 1707     |
|    total_timesteps  | 450076   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.62     |
|    n_updates        | 102518   |
----------------------------------
Eval num_timesteps=450500, episode_reward=165.10 +/- 54.07
Episode length: 41.60 +/- 13.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.6     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 450500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.18     |
|    n_updates        | 102624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5020     |
|    fps              | 263      |
|    time_elapsed     | 1709     |
|    total_timesteps  | 450961   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.68     |
|    n_updates        | 102740   |
----------------------------------
Eval num_timesteps=451000, episode_reward=169.70 +/- 32.41
Episode length: 42.82 +/- 8.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.8     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 451000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.34     |
|    n_updates        | 102749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.8     |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5024     |
|    fps              | 263      |
|    time_elapsed     | 1711     |
|    total_timesteps  | 451259   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.15     |
|    n_updates        | 102814   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5028     |
|    fps              | 263      |
|    time_elapsed     | 1711     |
|    total_timesteps  | 451491   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00769  |
|    n_updates        | 102872   |
----------------------------------
Eval num_timesteps=451500, episode_reward=168.46 +/- 53.85
Episode length: 42.50 +/- 13.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 451500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.14     |
|    n_updates        | 102874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.3     |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5032     |
|    fps              | 263      |
|    time_elapsed     | 1713     |
|    total_timesteps  | 451805   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.112    |
|    n_updates        | 102951   |
----------------------------------
Eval num_timesteps=452000, episode_reward=161.78 +/- 50.66
Episode length: 40.78 +/- 12.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.8     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 452000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.16     |
|    n_updates        | 102999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5036     |
|    fps              | 263      |
|    time_elapsed     | 1714     |
|    total_timesteps  | 452270   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 7.45     |
|    n_updates        | 103067   |
----------------------------------
Eval num_timesteps=452500, episode_reward=159.44 +/- 42.44
Episode length: 40.28 +/- 10.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.3     |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 452500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.877    |
|    n_updates        | 103124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5040     |
|    fps              | 263      |
|    time_elapsed     | 1716     |
|    total_timesteps  | 452745   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.487    |
|    n_updates        | 103186   |
----------------------------------
Eval num_timesteps=453000, episode_reward=184.32 +/- 46.78
Episode length: 46.50 +/- 11.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 453000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.21     |
|    n_updates        | 103249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 392      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5044     |
|    fps              | 263      |
|    time_elapsed     | 1718     |
|    total_timesteps  | 453188   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.94     |
|    n_updates        | 103296   |
----------------------------------
Eval num_timesteps=453500, episode_reward=169.24 +/- 56.30
Episode length: 42.58 +/- 14.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.6     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 453500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0158   |
|    n_updates        | 103374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5048     |
|    fps              | 263      |
|    time_elapsed     | 1720     |
|    total_timesteps  | 453579   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.07     |
|    n_updates        | 103394   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.3     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5052     |
|    fps              | 263      |
|    time_elapsed     | 1720     |
|    total_timesteps  | 453926   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0838   |
|    n_updates        | 103481   |
----------------------------------
Eval num_timesteps=454000, episode_reward=186.00 +/- 53.23
Episode length: 46.90 +/- 13.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.9     |
|    mean_reward      | 186      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 454000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0488   |
|    n_updates        | 103499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.9     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5056     |
|    fps              | 263      |
|    time_elapsed     | 1722     |
|    total_timesteps  | 454355   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0225   |
|    n_updates        | 103588   |
----------------------------------
Eval num_timesteps=454500, episode_reward=168.34 +/- 43.53
Episode length: 42.48 +/- 10.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 454500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.02     |
|    n_updates        | 103624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.3     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5060     |
|    fps              | 263      |
|    time_elapsed     | 1724     |
|    total_timesteps  | 454646   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000661 |
|    n_updates        | 103661   |
----------------------------------
Eval num_timesteps=455000, episode_reward=200.86 +/- 57.01
Episode length: 50.58 +/- 14.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 50.6     |
|    mean_reward      | 201      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 455000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.98     |
|    n_updates        | 103749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5064     |
|    fps              | 263      |
|    time_elapsed     | 1726     |
|    total_timesteps  | 455187   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.8      |
|    n_updates        | 103796   |
----------------------------------
Eval num_timesteps=455500, episode_reward=191.96 +/- 56.31
Episode length: 48.38 +/- 14.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.4     |
|    mean_reward      | 192      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 455500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.67     |
|    n_updates        | 103874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5068     |
|    fps              | 263      |
|    time_elapsed     | 1729     |
|    total_timesteps  | 455845   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.391    |
|    n_updates        | 103961   |
----------------------------------
Eval num_timesteps=456000, episode_reward=166.58 +/- 50.09
Episode length: 42.04 +/- 12.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42       |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 456000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.95     |
|    n_updates        | 103999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5072     |
|    fps              | 263      |
|    time_elapsed     | 1730     |
|    total_timesteps  | 456150   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.8      |
|    n_updates        | 104037   |
----------------------------------
Eval num_timesteps=456500, episode_reward=178.06 +/- 45.54
Episode length: 44.92 +/- 11.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 456500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.126    |
|    n_updates        | 104124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 403      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5076     |
|    fps              | 263      |
|    time_elapsed     | 1732     |
|    total_timesteps  | 456626   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0116   |
|    n_updates        | 104156   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5080     |
|    fps              | 263      |
|    time_elapsed     | 1733     |
|    total_timesteps  | 456957   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.34     |
|    n_updates        | 104239   |
----------------------------------
Eval num_timesteps=457000, episode_reward=175.40 +/- 45.85
Episode length: 44.24 +/- 11.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 457000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.449    |
|    n_updates        | 104249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 423      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5084     |
|    fps              | 263      |
|    time_elapsed     | 1735     |
|    total_timesteps  | 457494   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.13     |
|    n_updates        | 104373   |
----------------------------------
Eval num_timesteps=457500, episode_reward=174.12 +/- 52.71
Episode length: 43.86 +/- 13.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 457500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.28     |
|    n_updates        | 104374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 423      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5088     |
|    fps              | 263      |
|    time_elapsed     | 1736     |
|    total_timesteps  | 457818   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.69     |
|    n_updates        | 104454   |
----------------------------------
Eval num_timesteps=458000, episode_reward=176.06 +/- 43.87
Episode length: 44.40 +/- 10.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 458000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.12     |
|    n_updates        | 104499   |
----------------------------------
Eval num_timesteps=458500, episode_reward=167.22 +/- 43.65
Episode length: 42.18 +/- 10.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.2     |
|    mean_reward      | 167      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 458500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.99     |
|    n_updates        | 104624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 432      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5092     |
|    fps              | 263      |
|    time_elapsed     | 1740     |
|    total_timesteps  | 458527   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.36     |
|    n_updates        | 104631   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 423      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5096     |
|    fps              | 263      |
|    time_elapsed     | 1740     |
|    total_timesteps  | 458707   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0432   |
|    n_updates        | 104676   |
----------------------------------
Eval num_timesteps=459000, episode_reward=165.40 +/- 47.12
Episode length: 41.80 +/- 11.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.8     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 459000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.64     |
|    n_updates        | 104749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 421      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5100     |
|    fps              | 263      |
|    time_elapsed     | 1742     |
|    total_timesteps  | 459114   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0257   |
|    n_updates        | 104778   |
----------------------------------
Eval num_timesteps=459500, episode_reward=184.36 +/- 50.27
Episode length: 46.48 +/- 12.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 459500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.123    |
|    n_updates        | 104874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 425      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5104     |
|    fps              | 263      |
|    time_elapsed     | 1744     |
|    total_timesteps  | 459727   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0834   |
|    n_updates        | 104931   |
----------------------------------
Eval num_timesteps=460000, episode_reward=173.62 +/- 38.90
Episode length: 43.84 +/- 9.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.8     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 460000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00278  |
|    n_updates        | 104999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 428      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5108     |
|    fps              | 263      |
|    time_elapsed     | 1746     |
|    total_timesteps  | 460072   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.63     |
|    n_updates        | 105017   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 431      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5112     |
|    fps              | 263      |
|    time_elapsed     | 1746     |
|    total_timesteps  | 460417   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.081    |
|    n_updates        | 105104   |
----------------------------------
Eval num_timesteps=460500, episode_reward=164.58 +/- 41.83
Episode length: 41.50 +/- 10.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.5     |
|    mean_reward      | 165      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 460500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.681    |
|    n_updates        | 105124   |
----------------------------------
Eval num_timesteps=461000, episode_reward=174.60 +/- 51.85
Episode length: 44.04 +/- 12.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 461000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0517   |
|    n_updates        | 105249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 439      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5116     |
|    fps              | 263      |
|    time_elapsed     | 1750     |
|    total_timesteps  | 461100   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0095   |
|    n_updates        | 105274   |
----------------------------------
Eval num_timesteps=461500, episode_reward=169.22 +/- 58.43
Episode length: 42.70 +/- 14.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.7     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 461500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.21     |
|    n_updates        | 105374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 421      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5120     |
|    fps              | 263      |
|    time_elapsed     | 1752     |
|    total_timesteps  | 461522   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.2      |
|    n_updates        | 105380   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 424      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5124     |
|    fps              | 263      |
|    time_elapsed     | 1752     |
|    total_timesteps  | 461890   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0213   |
|    n_updates        | 105472   |
----------------------------------
Eval num_timesteps=462000, episode_reward=192.82 +/- 54.52
Episode length: 48.64 +/- 13.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.6     |
|    mean_reward      | 193      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 462000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.07     |
|    n_updates        | 105499   |
----------------------------------
Eval num_timesteps=462500, episode_reward=182.58 +/- 45.43
Episode length: 46.04 +/- 11.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 462500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0642   |
|    n_updates        | 105624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 440      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5128     |
|    fps              | 263      |
|    time_elapsed     | 1756     |
|    total_timesteps  | 462518   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0202   |
|    n_updates        | 105629   |
----------------------------------
Eval num_timesteps=463000, episode_reward=176.82 +/- 43.72
Episode length: 44.58 +/- 10.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.6     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 463000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0888   |
|    n_updates        | 105749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 447      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5132     |
|    fps              | 263      |
|    time_elapsed     | 1758     |
|    total_timesteps  | 463024   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.24     |
|    n_updates        | 105755   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 443      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5136     |
|    fps              | 263      |
|    time_elapsed     | 1758     |
|    total_timesteps  | 463394   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.12     |
|    n_updates        | 105848   |
----------------------------------
Eval num_timesteps=463500, episode_reward=174.66 +/- 43.31
Episode length: 44.02 +/- 10.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44       |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 463500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.441    |
|    n_updates        | 105874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 110      |
|    ep_rew_mean      | 439      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5140     |
|    fps              | 263      |
|    time_elapsed     | 1760     |
|    total_timesteps  | 463753   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.34     |
|    n_updates        | 105938   |
----------------------------------
Eval num_timesteps=464000, episode_reward=159.04 +/- 51.87
Episode length: 40.08 +/- 12.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.1     |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 464000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.42     |
|    n_updates        | 105999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 435      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5144     |
|    fps              | 263      |
|    time_elapsed     | 1762     |
|    total_timesteps  | 464094   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29     |
|    n_updates        | 106023   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 431      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5148     |
|    fps              | 263      |
|    time_elapsed     | 1762     |
|    total_timesteps  | 464392   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.49     |
|    n_updates        | 106097   |
----------------------------------
Eval num_timesteps=464500, episode_reward=184.46 +/- 53.41
Episode length: 46.52 +/- 13.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 464500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.18     |
|    n_updates        | 106124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 432      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5152     |
|    fps              | 263      |
|    time_elapsed     | 1764     |
|    total_timesteps  | 464755   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23     |
|    n_updates        | 106188   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 422      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5156     |
|    fps              | 263      |
|    time_elapsed     | 1764     |
|    total_timesteps  | 464942   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00474  |
|    n_updates        | 106235   |
----------------------------------
Eval num_timesteps=465000, episode_reward=159.76 +/- 43.00
Episode length: 40.38 +/- 10.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.4     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 465000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.28     |
|    n_updates        | 106249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 423      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5160     |
|    fps              | 263      |
|    time_elapsed     | 1766     |
|    total_timesteps  | 465257   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.22     |
|    n_updates        | 106314   |
----------------------------------
Eval num_timesteps=465500, episode_reward=195.80 +/- 50.11
Episode length: 49.34 +/- 12.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 49.3     |
|    mean_reward      | 196      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 465500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0163   |
|    n_updates        | 106374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 420      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5164     |
|    fps              | 263      |
|    time_elapsed     | 1768     |
|    total_timesteps  | 465718   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0182   |
|    n_updates        | 106429   |
----------------------------------
Eval num_timesteps=466000, episode_reward=177.96 +/- 37.42
Episode length: 44.86 +/- 9.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 466000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00985  |
|    n_updates        | 106499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5168     |
|    fps              | 263      |
|    time_elapsed     | 1770     |
|    total_timesteps  | 466017   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.21     |
|    n_updates        | 106504   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 408      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5172     |
|    fps              | 263      |
|    time_elapsed     | 1770     |
|    total_timesteps  | 466380   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 8.54     |
|    n_updates        | 106594   |
----------------------------------
Eval num_timesteps=466500, episode_reward=178.28 +/- 43.57
Episode length: 44.94 +/- 10.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 466500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23     |
|    n_updates        | 106624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5176     |
|    fps              | 263      |
|    time_elapsed     | 1772     |
|    total_timesteps  | 466823   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00369  |
|    n_updates        | 106705   |
----------------------------------
Eval num_timesteps=467000, episode_reward=177.46 +/- 53.54
Episode length: 44.72 +/- 13.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.7     |
|    mean_reward      | 177      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 467000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.17     |
|    n_updates        | 106749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 407      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5180     |
|    fps              | 263      |
|    time_elapsed     | 1774     |
|    total_timesteps  | 467180   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0302   |
|    n_updates        | 106794   |
----------------------------------
Eval num_timesteps=467500, episode_reward=181.68 +/- 54.46
Episode length: 45.78 +/- 13.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 467500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0328   |
|    n_updates        | 106874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5184     |
|    fps              | 263      |
|    time_elapsed     | 1776     |
|    total_timesteps  | 467662   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.4      |
|    n_updates        | 106915   |
----------------------------------
Eval num_timesteps=468000, episode_reward=176.12 +/- 50.42
Episode length: 44.38 +/- 12.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 468000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.18     |
|    n_updates        | 106999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5188     |
|    fps              | 263      |
|    time_elapsed     | 1778     |
|    total_timesteps  | 468121   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0458   |
|    n_updates        | 107030   |
----------------------------------
Eval num_timesteps=468500, episode_reward=188.86 +/- 55.44
Episode length: 47.60 +/- 13.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.6     |
|    mean_reward      | 189      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 468500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.34     |
|    n_updates        | 107124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5192     |
|    fps              | 263      |
|    time_elapsed     | 1780     |
|    total_timesteps  | 468604   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.16     |
|    n_updates        | 107150   |
----------------------------------
Eval num_timesteps=469000, episode_reward=172.38 +/- 48.04
Episode length: 43.44 +/- 11.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 469000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.2      |
|    n_updates        | 107249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5196     |
|    fps              | 263      |
|    time_elapsed     | 1782     |
|    total_timesteps  | 469057   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.51     |
|    n_updates        | 107264   |
----------------------------------
Eval num_timesteps=469500, episode_reward=168.74 +/- 39.04
Episode length: 42.54 +/- 9.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 169      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 469500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0225   |
|    n_updates        | 107374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 420      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5200     |
|    fps              | 263      |
|    time_elapsed     | 1784     |
|    total_timesteps  | 469657   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.06     |
|    n_updates        | 107414   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 409      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5204     |
|    fps              | 263      |
|    time_elapsed     | 1784     |
|    total_timesteps  | 469990   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23     |
|    n_updates        | 107497   |
----------------------------------
Eval num_timesteps=470000, episode_reward=178.66 +/- 39.28
Episode length: 45.02 +/- 9.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 470000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.019    |
|    n_updates        | 107499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 414      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5208     |
|    fps              | 263      |
|    time_elapsed     | 1786     |
|    total_timesteps  | 470472   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.073    |
|    n_updates        | 107617   |
----------------------------------
Eval num_timesteps=470500, episode_reward=160.98 +/- 55.22
Episode length: 40.66 +/- 13.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.7     |
|    mean_reward      | 161      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 470500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.24     |
|    n_updates        | 107624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 413      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5212     |
|    fps              | 263      |
|    time_elapsed     | 1788     |
|    total_timesteps  | 470775   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.43     |
|    n_updates        | 107693   |
----------------------------------
Eval num_timesteps=471000, episode_reward=163.98 +/- 58.79
Episode length: 41.40 +/- 14.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.4     |
|    mean_reward      | 164      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 471000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0224   |
|    n_updates        | 107749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.5     |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5216     |
|    fps              | 263      |
|    time_elapsed     | 1790     |
|    total_timesteps  | 471054   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00683  |
|    n_updates        | 107763   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.6     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5220     |
|    fps              | 263      |
|    time_elapsed     | 1790     |
|    total_timesteps  | 471381   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00633  |
|    n_updates        | 107845   |
----------------------------------
Eval num_timesteps=471500, episode_reward=181.58 +/- 43.10
Episode length: 45.80 +/- 10.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 471500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.204    |
|    n_updates        | 107874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5224     |
|    fps              | 263      |
|    time_elapsed     | 1792     |
|    total_timesteps  | 471859   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00186  |
|    n_updates        | 107964   |
----------------------------------
Eval num_timesteps=472000, episode_reward=184.16 +/- 54.08
Episode length: 46.46 +/- 13.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 184      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 472000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 1.26     |
|    n_updates        | 107999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.8     |
|    ep_rew_mean      | 390      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5228     |
|    fps              | 263      |
|    time_elapsed     | 1794     |
|    total_timesteps  | 472301   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.24     |
|    n_updates        | 108075   |
----------------------------------
Eval num_timesteps=472500, episode_reward=160.40 +/- 52.26
Episode length: 40.40 +/- 13.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.4     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 472500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.62     |
|    n_updates        | 108124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.6     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5232     |
|    fps              | 263      |
|    time_elapsed     | 1796     |
|    total_timesteps  | 472587   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.24     |
|    n_updates        | 108146   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.6     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5236     |
|    fps              | 263      |
|    time_elapsed     | 1796     |
|    total_timesteps  | 472856   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.26     |
|    n_updates        | 108213   |
----------------------------------
Eval num_timesteps=473000, episode_reward=155.20 +/- 55.33
Episode length: 39.18 +/- 13.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.2     |
|    mean_reward      | 155      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 473000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0142   |
|    n_updates        | 108249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94       |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5240     |
|    fps              | 263      |
|    time_elapsed     | 1798     |
|    total_timesteps  | 473149   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.26     |
|    n_updates        | 108287   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.9     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5244     |
|    fps              | 263      |
|    time_elapsed     | 1798     |
|    total_timesteps  | 473487   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00932  |
|    n_updates        | 108371   |
----------------------------------
Eval num_timesteps=473500, episode_reward=177.82 +/- 41.70
Episode length: 44.80 +/- 10.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 473500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.64     |
|    n_updates        | 108374   |
----------------------------------
Eval num_timesteps=474000, episode_reward=174.78 +/- 61.22
Episode length: 44.12 +/- 15.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.1     |
|    mean_reward      | 175      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 474000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.62     |
|    n_updates        | 108499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.1     |
|    ep_rew_mean      | 387      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5248     |
|    fps              | 263      |
|    time_elapsed     | 1802     |
|    total_timesteps  | 474098   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0126   |
|    n_updates        | 108524   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5252     |
|    fps              | 263      |
|    time_elapsed     | 1802     |
|    total_timesteps  | 474435   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0513   |
|    n_updates        | 108608   |
----------------------------------
Eval num_timesteps=474500, episode_reward=179.04 +/- 38.52
Episode length: 45.14 +/- 9.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 474500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.42     |
|    n_updates        | 108624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.3     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5256     |
|    fps              | 263      |
|    time_elapsed     | 1804     |
|    total_timesteps  | 474673   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.131    |
|    n_updates        | 108668   |
----------------------------------
Eval num_timesteps=475000, episode_reward=173.20 +/- 47.15
Episode length: 43.70 +/- 11.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 475000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.34     |
|    n_updates        | 108749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.6     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5260     |
|    fps              | 263      |
|    time_elapsed     | 1806     |
|    total_timesteps  | 475017   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.136    |
|    n_updates        | 108754   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.6     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5264     |
|    fps              | 263      |
|    time_elapsed     | 1806     |
|    total_timesteps  | 475378   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.21     |
|    n_updates        | 108844   |
----------------------------------
Eval num_timesteps=475500, episode_reward=162.74 +/- 46.11
Episode length: 41.02 +/- 11.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41       |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 475500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0324   |
|    n_updates        | 108874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5268     |
|    fps              | 263      |
|    time_elapsed     | 1808     |
|    total_timesteps  | 475885   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.063    |
|    n_updates        | 108971   |
----------------------------------
Eval num_timesteps=476000, episode_reward=171.14 +/- 50.92
Episode length: 43.16 +/- 12.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.2     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 476000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.24     |
|    n_updates        | 108999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5272     |
|    fps              | 263      |
|    time_elapsed     | 1810     |
|    total_timesteps  | 476408   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.452    |
|    n_updates        | 109101   |
----------------------------------
Eval num_timesteps=476500, episode_reward=159.04 +/- 53.55
Episode length: 40.10 +/- 13.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.1     |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 476500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.33     |
|    n_updates        | 109124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 402      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5276     |
|    fps              | 263      |
|    time_elapsed     | 1812     |
|    total_timesteps  | 476917   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00978  |
|    n_updates        | 109229   |
----------------------------------
Eval num_timesteps=477000, episode_reward=181.06 +/- 44.50
Episode length: 45.64 +/- 11.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.6     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 477000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.23     |
|    n_updates        | 109249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5280     |
|    fps              | 263      |
|    time_elapsed     | 1814     |
|    total_timesteps  | 477216   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00304  |
|    n_updates        | 109303   |
----------------------------------
Eval num_timesteps=477500, episode_reward=153.30 +/- 51.36
Episode length: 38.72 +/- 12.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 38.7     |
|    mean_reward      | 153      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 477500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0283   |
|    n_updates        | 109374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5284     |
|    fps              | 263      |
|    time_elapsed     | 1816     |
|    total_timesteps  | 477702   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0288   |
|    n_updates        | 109425   |
----------------------------------
Eval num_timesteps=478000, episode_reward=186.56 +/- 48.21
Episode length: 46.98 +/- 12.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 478000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.192    |
|    n_updates        | 109499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5288     |
|    fps              | 263      |
|    time_elapsed     | 1818     |
|    total_timesteps  | 478472   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.48     |
|    n_updates        | 109617   |
----------------------------------
Eval num_timesteps=478500, episode_reward=145.88 +/- 33.92
Episode length: 36.84 +/- 8.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 36.8     |
|    mean_reward      | 146      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 478500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0441   |
|    n_updates        | 109624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 413      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5292     |
|    fps              | 263      |
|    time_elapsed     | 1820     |
|    total_timesteps  | 478963   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.36     |
|    n_updates        | 109740   |
----------------------------------
Eval num_timesteps=479000, episode_reward=178.64 +/- 49.07
Episode length: 45.00 +/- 12.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 479000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.151    |
|    n_updates        | 109749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 405      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5296     |
|    fps              | 263      |
|    time_elapsed     | 1822     |
|    total_timesteps  | 479218   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00496  |
|    n_updates        | 109804   |
----------------------------------
Eval num_timesteps=479500, episode_reward=178.28 +/- 48.56
Episode length: 44.94 +/- 12.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 479500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.25     |
|    n_updates        | 109874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.7     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5300     |
|    fps              | 262      |
|    time_elapsed     | 1823     |
|    total_timesteps  | 479530   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.42     |
|    n_updates        | 109882   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.1     |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5304     |
|    fps              | 263      |
|    time_elapsed     | 1824     |
|    total_timesteps  | 479903   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0516   |
|    n_updates        | 109975   |
----------------------------------
Eval num_timesteps=480000, episode_reward=177.80 +/- 46.85
Episode length: 44.88 +/- 11.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 480000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0284   |
|    n_updates        | 109999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 397      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5308     |
|    fps              | 263      |
|    time_elapsed     | 1826     |
|    total_timesteps  | 480444   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0334   |
|    n_updates        | 110110   |
----------------------------------
Eval num_timesteps=480500, episode_reward=148.20 +/- 43.91
Episode length: 37.38 +/- 11.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 37.4     |
|    mean_reward      | 148      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 480500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00463  |
|    n_updates        | 110124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5312     |
|    fps              | 263      |
|    time_elapsed     | 1827     |
|    total_timesteps  | 480803   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0276   |
|    n_updates        | 110200   |
----------------------------------
Eval num_timesteps=481000, episode_reward=182.82 +/- 43.39
Episode length: 46.14 +/- 10.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.1     |
|    mean_reward      | 183      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 481000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00329  |
|    n_updates        | 110249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 400      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5316     |
|    fps              | 262      |
|    time_elapsed     | 1829     |
|    total_timesteps  | 481084   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.26     |
|    n_updates        | 110270   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5320     |
|    fps              | 263      |
|    time_elapsed     | 1830     |
|    total_timesteps  | 481439   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.71     |
|    n_updates        | 110359   |
----------------------------------
Eval num_timesteps=481500, episode_reward=170.84 +/- 41.98
Episode length: 43.12 +/- 10.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 481500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.37     |
|    n_updates        | 110374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.9     |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5324     |
|    fps              | 263      |
|    time_elapsed     | 1832     |
|    total_timesteps  | 481850   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.28     |
|    n_updates        | 110462   |
----------------------------------
Eval num_timesteps=482000, episode_reward=170.76 +/- 62.22
Episode length: 43.12 +/- 15.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 482000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.262    |
|    n_updates        | 110499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 395      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5328     |
|    fps              | 262      |
|    time_elapsed     | 1833     |
|    total_timesteps  | 482216   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.25     |
|    n_updates        | 110553   |
----------------------------------
Eval num_timesteps=482500, episode_reward=180.30 +/- 43.17
Episode length: 45.44 +/- 10.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.4     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 482500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00444  |
|    n_updates        | 110624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 100      |
|    ep_rew_mean      | 399      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5332     |
|    fps              | 262      |
|    time_elapsed     | 1835     |
|    total_timesteps  | 482595   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.25     |
|    n_updates        | 110648   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5336     |
|    fps              | 263      |
|    time_elapsed     | 1836     |
|    total_timesteps  | 482994   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00703  |
|    n_updates        | 110748   |
----------------------------------
Eval num_timesteps=483000, episode_reward=159.78 +/- 42.95
Episode length: 40.30 +/- 10.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.3     |
|    mean_reward      | 160      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 483000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.56     |
|    n_updates        | 110749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5340     |
|    fps              | 262      |
|    time_elapsed     | 1837     |
|    total_timesteps  | 483343   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.66     |
|    n_updates        | 110835   |
----------------------------------
Eval num_timesteps=483500, episode_reward=170.28 +/- 47.18
Episode length: 42.96 +/- 11.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 483500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.54     |
|    n_updates        | 110874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5344     |
|    fps              | 262      |
|    time_elapsed     | 1839     |
|    total_timesteps  | 483548   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0315   |
|    n_updates        | 110886   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.9     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5348     |
|    fps              | 263      |
|    time_elapsed     | 1839     |
|    total_timesteps  | 483986   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00489  |
|    n_updates        | 110996   |
----------------------------------
Eval num_timesteps=484000, episode_reward=176.16 +/- 42.05
Episode length: 44.44 +/- 10.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.4     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 484000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0216   |
|    n_updates        | 110999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 389      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5352     |
|    fps              | 262      |
|    time_elapsed     | 1841     |
|    total_timesteps  | 484203   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.25     |
|    n_updates        | 111050   |
----------------------------------
Eval num_timesteps=484500, episode_reward=171.94 +/- 48.44
Episode length: 43.38 +/- 12.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.4     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 484500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.172    |
|    n_updates        | 111124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5356     |
|    fps              | 262      |
|    time_elapsed     | 1843     |
|    total_timesteps  | 484799   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0224   |
|    n_updates        | 111199   |
----------------------------------
Eval num_timesteps=485000, episode_reward=185.02 +/- 39.77
Episode length: 46.70 +/- 9.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.7     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 485000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0262   |
|    n_updates        | 111249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 404      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5360     |
|    fps              | 262      |
|    time_elapsed     | 1845     |
|    total_timesteps  | 485154   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0122   |
|    n_updates        | 111288   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.9     |
|    ep_rew_mean      | 398      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5364     |
|    fps              | 262      |
|    time_elapsed     | 1845     |
|    total_timesteps  | 485367   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0512   |
|    n_updates        | 111341   |
----------------------------------
Eval num_timesteps=485500, episode_reward=172.30 +/- 46.28
Episode length: 43.46 +/- 11.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 485500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.43     |
|    n_updates        | 111374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.8     |
|    ep_rew_mean      | 394      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5368     |
|    fps              | 262      |
|    time_elapsed     | 1847     |
|    total_timesteps  | 485760   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.58     |
|    n_updates        | 111439   |
----------------------------------
Eval num_timesteps=486000, episode_reward=163.20 +/- 53.96
Episode length: 41.18 +/- 13.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 41.2     |
|    mean_reward      | 163      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 486000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0305   |
|    n_updates        | 111499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5372     |
|    fps              | 262      |
|    time_elapsed     | 1849     |
|    total_timesteps  | 486132   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.36     |
|    n_updates        | 111532   |
----------------------------------
Eval num_timesteps=486500, episode_reward=173.44 +/- 50.76
Episode length: 43.70 +/- 12.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.7     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 486500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0386   |
|    n_updates        | 111624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 388      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5376     |
|    fps              | 262      |
|    time_elapsed     | 1851     |
|    total_timesteps  | 486655   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.142    |
|    n_updates        | 111663   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5380     |
|    fps              | 262      |
|    time_elapsed     | 1851     |
|    total_timesteps  | 486899   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29     |
|    n_updates        | 111724   |
----------------------------------
Eval num_timesteps=487000, episode_reward=191.06 +/- 49.42
Episode length: 48.20 +/- 12.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 48.2     |
|    mean_reward      | 191      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 487000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0432   |
|    n_updates        | 111749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.6     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5384     |
|    fps              | 262      |
|    time_elapsed     | 1853     |
|    total_timesteps  | 487259   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0372   |
|    n_updates        | 111814   |
----------------------------------
Eval num_timesteps=487500, episode_reward=184.70 +/- 44.71
Episode length: 46.50 +/- 11.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46.5     |
|    mean_reward      | 185      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 487500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.000445 |
|    n_updates        | 111874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.9     |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5388     |
|    fps              | 262      |
|    time_elapsed     | 1855     |
|    total_timesteps  | 487565   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0206   |
|    n_updates        | 111891   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.2     |
|    ep_rew_mean      | 356      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5392     |
|    fps              | 262      |
|    time_elapsed     | 1855     |
|    total_timesteps  | 487887   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.3      |
|    n_updates        | 111971   |
----------------------------------
Eval num_timesteps=488000, episode_reward=178.82 +/- 49.17
Episode length: 45.04 +/- 12.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45       |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 488000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.33     |
|    n_updates        | 111999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.2     |
|    ep_rew_mean      | 360      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5396     |
|    fps              | 262      |
|    time_elapsed     | 1857     |
|    total_timesteps  | 488240   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0103   |
|    n_updates        | 112059   |
----------------------------------
Eval num_timesteps=488500, episode_reward=181.70 +/- 57.48
Episode length: 45.80 +/- 14.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.8     |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 488500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.44     |
|    n_updates        | 112124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.9     |
|    ep_rew_mean      | 362      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5400     |
|    fps              | 262      |
|    time_elapsed     | 1859     |
|    total_timesteps  | 488619   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.27     |
|    n_updates        | 112154   |
----------------------------------
Eval num_timesteps=489000, episode_reward=170.38 +/- 43.62
Episode length: 42.94 +/- 10.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.9     |
|    mean_reward      | 170      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 489000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.25     |
|    n_updates        | 112249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.7     |
|    ep_rew_mean      | 369      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5404     |
|    fps              | 262      |
|    time_elapsed     | 1861     |
|    total_timesteps  | 489171   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.32     |
|    n_updates        | 112292   |
----------------------------------
Eval num_timesteps=489500, episode_reward=187.28 +/- 47.37
Episode length: 47.18 +/- 11.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47.2     |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 489500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.3      |
|    n_updates        | 112374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5408     |
|    fps              | 262      |
|    time_elapsed     | 1863     |
|    total_timesteps  | 489958   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0209   |
|    n_updates        | 112489   |
----------------------------------
Eval num_timesteps=490000, episode_reward=172.54 +/- 63.78
Episode length: 43.50 +/- 15.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.5     |
|    mean_reward      | 173      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 490000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.013    |
|    n_updates        | 112499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5412     |
|    fps              | 262      |
|    time_elapsed     | 1865     |
|    total_timesteps  | 490382   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0795   |
|    n_updates        | 112595   |
----------------------------------
Eval num_timesteps=490500, episode_reward=179.54 +/- 50.65
Episode length: 45.26 +/- 12.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.3     |
|    mean_reward      | 180      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 490500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.65     |
|    n_updates        | 112624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.6     |
|    ep_rew_mean      | 381      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5416     |
|    fps              | 262      |
|    time_elapsed     | 1867     |
|    total_timesteps  | 490644   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.95     |
|    n_updates        | 112660   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5420     |
|    fps              | 262      |
|    time_elapsed     | 1867     |
|    total_timesteps  | 490945   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.63     |
|    n_updates        | 112736   |
----------------------------------
Eval num_timesteps=491000, episode_reward=178.80 +/- 58.76
Episode length: 45.08 +/- 14.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.1     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 491000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.199    |
|    n_updates        | 112749   |
----------------------------------
Eval num_timesteps=491500, episode_reward=174.00 +/- 62.77
Episode length: 43.86 +/- 15.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.9     |
|    mean_reward      | 174      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 491500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.61     |
|    n_updates        | 112874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5424     |
|    fps              | 262      |
|    time_elapsed     | 1871     |
|    total_timesteps  | 491516   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0137   |
|    n_updates        | 112878   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5428     |
|    fps              | 262      |
|    time_elapsed     | 1871     |
|    total_timesteps  | 491898   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.92     |
|    n_updates        | 112974   |
----------------------------------
Eval num_timesteps=492000, episode_reward=158.98 +/- 49.74
Episode length: 39.98 +/- 12.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40       |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 492000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0307   |
|    n_updates        | 112999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.3     |
|    ep_rew_mean      | 380      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5432     |
|    fps              | 262      |
|    time_elapsed     | 1873     |
|    total_timesteps  | 492129   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0324   |
|    n_updates        | 113032   |
----------------------------------
Eval num_timesteps=492500, episode_reward=170.58 +/- 55.57
Episode length: 43.04 +/- 13.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43       |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 492500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.3      |
|    n_updates        | 113124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5436     |
|    fps              | 262      |
|    time_elapsed     | 1875     |
|    total_timesteps  | 492610   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0414   |
|    n_updates        | 113152   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.2     |
|    ep_rew_mean      | 379      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5440     |
|    fps              | 262      |
|    time_elapsed     | 1875     |
|    total_timesteps  | 492862   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.33     |
|    n_updates        | 113215   |
----------------------------------
Eval num_timesteps=493000, episode_reward=156.08 +/- 49.42
Episode length: 39.38 +/- 12.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.4     |
|    mean_reward      | 156      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 493000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00764  |
|    n_updates        | 113249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.3     |
|    ep_rew_mean      | 384      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5444     |
|    fps              | 262      |
|    time_elapsed     | 1876     |
|    total_timesteps  | 493183   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00316  |
|    n_updates        | 113295   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.7     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5448     |
|    fps              | 262      |
|    time_elapsed     | 1877     |
|    total_timesteps  | 493457   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00519  |
|    n_updates        | 113364   |
----------------------------------
Eval num_timesteps=493500, episode_reward=170.80 +/- 50.17
Episode length: 43.08 +/- 12.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.1     |
|    mean_reward      | 171      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 493500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0134   |
|    n_updates        | 113374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 383      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5452     |
|    fps              | 262      |
|    time_elapsed     | 1878     |
|    total_timesteps  | 493806   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.51     |
|    n_updates        | 113451   |
----------------------------------
Eval num_timesteps=494000, episode_reward=177.96 +/- 48.23
Episode length: 44.86 +/- 12.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.9     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 494000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00578  |
|    n_updates        | 113499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.1     |
|    ep_rew_mean      | 371      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5456     |
|    fps              | 262      |
|    time_elapsed     | 1880     |
|    total_timesteps  | 494113   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.615    |
|    n_updates        | 113528   |
----------------------------------
Eval num_timesteps=494500, episode_reward=186.70 +/- 47.10
Episode length: 47.02 +/- 11.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 47       |
|    mean_reward      | 187      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 494500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.32     |
|    n_updates        | 113624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.7     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5460     |
|    fps              | 262      |
|    time_elapsed     | 1882     |
|    total_timesteps  | 494623   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.25     |
|    n_updates        | 113655   |
----------------------------------
Eval num_timesteps=495000, episode_reward=171.50 +/- 43.37
Episode length: 43.26 +/- 10.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 495000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.4      |
|    n_updates        | 113749   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 385      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5464     |
|    fps              | 262      |
|    time_elapsed     | 1884     |
|    total_timesteps  | 495033   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00569  |
|    n_updates        | 113758   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.9     |
|    ep_rew_mean      | 382      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5468     |
|    fps              | 262      |
|    time_elapsed     | 1885     |
|    total_timesteps  | 495351   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.71     |
|    n_updates        | 113837   |
----------------------------------
Eval num_timesteps=495500, episode_reward=168.48 +/- 47.53
Episode length: 42.52 +/- 11.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 42.5     |
|    mean_reward      | 168      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 495500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0289   |
|    n_updates        | 113874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.6     |
|    ep_rew_mean      | 377      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5472     |
|    fps              | 262      |
|    time_elapsed     | 1886     |
|    total_timesteps  | 495595   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 6.94     |
|    n_updates        | 113898   |
----------------------------------
Eval num_timesteps=496000, episode_reward=158.98 +/- 57.81
Episode length: 40.06 +/- 14.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.1     |
|    mean_reward      | 159      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 496000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.46     |
|    n_updates        | 113999   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 374      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5476     |
|    fps              | 262      |
|    time_elapsed     | 1888     |
|    total_timesteps  | 496040   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.56     |
|    n_updates        | 114009   |
----------------------------------
Eval num_timesteps=496500, episode_reward=175.52 +/- 46.51
Episode length: 44.24 +/- 11.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.2     |
|    mean_reward      | 176      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 496500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.29     |
|    n_updates        | 114124   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.9     |
|    ep_rew_mean      | 386      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5480     |
|    fps              | 262      |
|    time_elapsed     | 1890     |
|    total_timesteps  | 496585   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.66     |
|    n_updates        | 114146   |
----------------------------------
Eval num_timesteps=497000, episode_reward=179.12 +/- 45.77
Episode length: 45.16 +/- 11.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.2     |
|    mean_reward      | 179      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 497000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0145   |
|    n_updates        | 114249   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.6     |
|    ep_rew_mean      | 393      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5484     |
|    fps              | 262      |
|    time_elapsed     | 1892     |
|    total_timesteps  | 497119   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.016    |
|    n_updates        | 114279   |
----------------------------------
Eval num_timesteps=497500, episode_reward=171.78 +/- 41.11
Episode length: 43.32 +/- 10.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 43.3     |
|    mean_reward      | 172      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 497500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.469    |
|    n_updates        | 114374   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 401      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5488     |
|    fps              | 262      |
|    time_elapsed     | 1894     |
|    total_timesteps  | 497631   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 3.38     |
|    n_updates        | 114407   |
----------------------------------
Eval num_timesteps=498000, episode_reward=162.16 +/- 39.86
Episode length: 40.92 +/- 9.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 40.9     |
|    mean_reward      | 162      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 498000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.28     |
|    n_updates        | 114499   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 406      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5492     |
|    fps              | 262      |
|    time_elapsed     | 1896     |
|    total_timesteps  | 498072   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.58     |
|    n_updates        | 114517   |
----------------------------------
Eval num_timesteps=498500, episode_reward=157.00 +/- 54.11
Episode length: 39.64 +/- 13.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 39.6     |
|    mean_reward      | 157      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 498500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.45     |
|    n_updates        | 114624   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 411      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5496     |
|    fps              | 262      |
|    time_elapsed     | 1898     |
|    total_timesteps  | 498557   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.0291   |
|    n_updates        | 114639   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 412      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5500     |
|    fps              | 262      |
|    time_elapsed     | 1898     |
|    total_timesteps  | 498962   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00131  |
|    n_updates        | 114740   |
----------------------------------
Eval num_timesteps=499000, episode_reward=177.52 +/- 39.70
Episode length: 44.78 +/- 9.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 44.8     |
|    mean_reward      | 178      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 499000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 4.56     |
|    n_updates        | 114749   |
----------------------------------
Eval num_timesteps=499500, episode_reward=180.56 +/- 51.31
Episode length: 45.46 +/- 12.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 45.5     |
|    mean_reward      | 181      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 499500   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.00652  |
|    n_updates        | 114874   |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 413      |
|    exploration_rate | 0.001    |
| time/               |          |
|    episodes         | 5504     |
|    fps              | 262      |
|    time_elapsed     | 1902     |
|    total_timesteps  | 499523   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 2.2      |
|    n_updates        | 114880   |
----------------------------------
Eval num_timesteps=500000, episode_reward=182.48 +/- 53.84
Episode length: 45.98 +/- 13.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 46       |
|    mean_reward      | 182      |
| rollout/            |          |
|    exploration_rate | 0.001    |
| time/               |          |
|    total_timesteps  | 500000   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.382    |
|    n_updates        | 114999   |
----------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/take-cover/dqn-4/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Parameters: {'batch_size': 64, 'learning_rate': 0.001, 'buffer_size': 15000, 'gamma': 0.99, 'exploration_fraction': 0.4, 'exploration_final_eps': 0.001, 'learning_starts': 40000.0, 'decay_start_steps': 40000.0, 'decay_end_steps': 200000.0}
Training steps: 500000
Frame skip: 4
Using cuda device
Eval num_timesteps=500, episode_reward=217.26 +/- 58.94
Episode length: 54.72 +/- 14.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=199.90 +/- 48.90
Episode length: 50.32 +/- 12.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=211.16 +/- 52.16
Episode length: 53.20 +/- 13.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | 211      |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=207.14 +/- 60.67
Episode length: 52.14 +/- 15.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.7     |
|    ep_rew_mean     | 317      |
| time/              |          |
|    fps             | 174      |
|    iterations      | 1        |
|    time_elapsed    | 11       |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=167.36 +/- 47.14
Episode length: 42.20 +/- 11.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 167         |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.008418152 |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.00097     |
|    learning_rate        | 0.0001      |
|    loss                 | 99.5        |
|    n_updates            | 10          |
|    policy_gradient_loss | 0.00741     |
|    value_loss           | 596         |
-----------------------------------------
Eval num_timesteps=3000, episode_reward=182.50 +/- 61.21
Episode length: 46.00 +/- 15.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=175.28 +/- 39.93
Episode length: 44.20 +/- 10.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.2     |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=175.04 +/- 47.08
Episode length: 44.18 +/- 11.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.2     |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.2     |
|    ep_rew_mean     | 275      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 2        |
|    time_elapsed    | 22       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=172.36 +/- 40.59
Episode length: 43.50 +/- 10.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.5        |
|    mean_reward          | 172         |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.007409284 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | 0.043       |
|    learning_rate        | 0.0001      |
|    loss                 | 519         |
|    n_updates            | 20          |
|    policy_gradient_loss | 0.00365     |
|    value_loss           | 846         |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=178.22 +/- 43.98
Episode length: 44.82 +/- 10.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=167.56 +/- 51.90
Episode length: 42.28 +/- 13.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=179.90 +/- 44.50
Episode length: 45.34 +/- 11.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.4     |
|    ep_rew_mean     | 292      |
| time/              |          |
|    fps             | 182      |
|    iterations      | 3        |
|    time_elapsed    | 33       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=168.64 +/- 40.03
Episode length: 42.48 +/- 9.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.5         |
|    mean_reward          | 169          |
| time/                   |              |
|    total_timesteps      | 6500         |
| train/                  |              |
|    approx_kl            | 0.0068340404 |
|    clip_fraction        | 0.0709       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.09        |
|    explained_variance   | 0.134        |
|    learning_rate        | 0.0001       |
|    loss                 | 425          |
|    n_updates            | 30           |
|    policy_gradient_loss | 0.00213      |
|    value_loss           | 968          |
------------------------------------------
Eval num_timesteps=7000, episode_reward=166.64 +/- 39.81
Episode length: 42.06 +/- 9.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=175.88 +/- 47.47
Episode length: 44.32 +/- 11.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.3     |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=160.12 +/- 32.45
Episode length: 40.42 +/- 8.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.5     |
|    ep_rew_mean     | 281      |
| time/              |          |
|    fps             | 183      |
|    iterations      | 4        |
|    time_elapsed    | 44       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=186.88 +/- 54.80
Episode length: 47.16 +/- 13.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.2        |
|    mean_reward          | 187         |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.008104694 |
|    clip_fraction        | 0.0689      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.323       |
|    learning_rate        | 0.0001      |
|    loss                 | 632         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.000834   |
|    value_loss           | 1.15e+03    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=172.30 +/- 36.33
Episode length: 43.46 +/- 9.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.5     |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=179.84 +/- 48.58
Episode length: 45.30 +/- 12.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=173.48 +/- 44.32
Episode length: 43.72 +/- 11.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.7     |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.4     |
|    ep_rew_mean     | 280      |
| time/              |          |
|    fps             | 183      |
|    iterations      | 5        |
|    time_elapsed    | 55       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=181.88 +/- 60.32
Episode length: 45.82 +/- 15.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.8        |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.009088237 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.0001      |
|    loss                 | 453         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00246    |
|    value_loss           | 1.05e+03    |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=182.02 +/- 65.40
Episode length: 45.84 +/- 16.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=190.12 +/- 69.26
Episode length: 47.88 +/- 17.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=180.06 +/- 58.15
Episode length: 45.36 +/- 14.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.5     |
|    ep_rew_mean     | 280      |
| time/              |          |
|    fps             | 182      |
|    iterations      | 6        |
|    time_elapsed    | 67       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=167.02 +/- 57.35
Episode length: 42.16 +/- 14.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 167         |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.012151543 |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0001      |
|    loss                 | 284         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00367    |
|    value_loss           | 643         |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=152.70 +/- 49.50
Episode length: 38.60 +/- 12.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.6     |
|    mean_reward     | 153      |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=163.82 +/- 56.39
Episode length: 41.34 +/- 14.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=155.22 +/- 44.49
Episode length: 39.16 +/- 11.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.2     |
|    mean_reward     | 155      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | 277      |
| time/              |          |
|    fps             | 184      |
|    iterations      | 7        |
|    time_elapsed    | 77       |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=167.68 +/- 53.40
Episode length: 42.38 +/- 13.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.4        |
|    mean_reward          | 168         |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.009502922 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0001      |
|    loss                 | 448         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00487    |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=173.32 +/- 77.50
Episode length: 43.70 +/- 19.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.7     |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=169.10 +/- 55.59
Episode length: 42.66 +/- 13.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=164.52 +/- 51.30
Episode length: 41.50 +/- 12.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73       |
|    ep_rew_mean     | 290      |
| time/              |          |
|    fps             | 184      |
|    iterations      | 8        |
|    time_elapsed    | 88       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=174.80 +/- 61.94
Episode length: 44.12 +/- 15.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.1        |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.006755532 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.0001      |
|    loss                 | 738         |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.00516     |
|    value_loss           | 1.41e+03    |
-----------------------------------------
Eval num_timesteps=17000, episode_reward=165.80 +/- 58.39
Episode length: 41.80 +/- 14.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=158.28 +/- 57.13
Episode length: 39.88 +/- 14.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.9     |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=154.84 +/- 40.72
Episode length: 39.10 +/- 10.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.1     |
|    mean_reward     | 155      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | 283      |
| time/              |          |
|    fps             | 185      |
|    iterations      | 9        |
|    time_elapsed    | 99       |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=166.70 +/- 53.91
Episode length: 42.02 +/- 13.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 167         |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.011316378 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.05e+03    |
|    n_updates            | 90          |
|    policy_gradient_loss | 0.00239     |
|    value_loss           | 1.27e+03    |
-----------------------------------------
Eval num_timesteps=19000, episode_reward=169.36 +/- 66.10
Episode length: 42.70 +/- 16.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=180.98 +/- 68.65
Episode length: 45.68 +/- 17.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=171.44 +/- 67.39
Episode length: 43.28 +/- 16.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.3     |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.6     |
|    ep_rew_mean     | 273      |
| time/              |          |
|    fps             | 185      |
|    iterations      | 10       |
|    time_elapsed    | 110      |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=161.26 +/- 49.77
Episode length: 40.68 +/- 12.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.7        |
|    mean_reward          | 161         |
| time/                   |             |
|    total_timesteps      | 20500       |
| train/                  |             |
|    approx_kl            | 0.008116795 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.0001      |
|    loss                 | 695         |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.003       |
|    value_loss           | 1.74e+03    |
-----------------------------------------
Eval num_timesteps=21000, episode_reward=179.62 +/- 61.28
Episode length: 45.26 +/- 15.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=179.72 +/- 67.05
Episode length: 45.30 +/- 16.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=168.34 +/- 58.86
Episode length: 42.48 +/- 14.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=177.04 +/- 66.18
Episode length: 44.62 +/- 16.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.6     |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 63.9     |
|    ep_rew_mean     | 254      |
| time/              |          |
|    fps             | 182      |
|    iterations      | 11       |
|    time_elapsed    | 123      |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=183.74 +/- 70.81
Episode length: 46.30 +/- 17.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.3        |
|    mean_reward          | 184         |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.012846525 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.477       |
|    learning_rate        | 0.0001      |
|    loss                 | 740         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.000982   |
|    value_loss           | 1.47e+03    |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=180.72 +/- 74.48
Episode length: 45.46 +/- 18.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=187.56 +/- 65.48
Episode length: 47.16 +/- 16.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=171.12 +/- 67.93
Episode length: 43.12 +/- 16.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.1     |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.1     |
|    ep_rew_mean     | 262      |
| time/              |          |
|    fps             | 182      |
|    iterations      | 12       |
|    time_elapsed    | 134      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=173.08 +/- 61.76
Episode length: 43.66 +/- 15.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.7        |
|    mean_reward          | 173         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.008396592 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | 0.377       |
|    learning_rate        | 0.0001      |
|    loss                 | 640         |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.00492     |
|    value_loss           | 1.6e+03     |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=177.62 +/- 72.68
Episode length: 44.82 +/- 18.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=209.38 +/- 79.99
Episode length: 52.74 +/- 20.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | 209      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=172.36 +/- 66.90
Episode length: 43.52 +/- 16.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.5     |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68       |
|    ep_rew_mean     | 270      |
| time/              |          |
|    fps             | 181      |
|    iterations      | 13       |
|    time_elapsed    | 146      |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=178.94 +/- 86.40
Episode length: 45.14 +/- 21.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.1        |
|    mean_reward          | 179         |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.008017443 |
|    clip_fraction        | 0.191       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.969      |
|    explained_variance   | 0.348       |
|    learning_rate        | 0.0001      |
|    loss                 | 840         |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.00474     |
|    value_loss           | 1.68e+03    |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=211.32 +/- 109.85
Episode length: 53.22 +/- 27.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | 211      |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=185.82 +/- 84.80
Episode length: 46.88 +/- 21.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=187.50 +/- 84.34
Episode length: 47.28 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | 276      |
| time/              |          |
|    fps             | 181      |
|    iterations      | 14       |
|    time_elapsed    | 158      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=191.40 +/- 76.22
Episode length: 48.24 +/- 19.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.2        |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.013285473 |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.907      |
|    explained_variance   | 0.382       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.11e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | 0.00383     |
|    value_loss           | 1.61e+03    |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=180.24 +/- 92.34
Episode length: 45.44 +/- 23.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=163.34 +/- 60.03
Episode length: 41.30 +/- 15.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 163      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=169.36 +/- 54.80
Episode length: 42.74 +/- 13.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | 273      |
| time/              |          |
|    fps             | 181      |
|    iterations      | 15       |
|    time_elapsed    | 169      |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=191.56 +/- 81.76
Episode length: 48.20 +/- 20.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.2        |
|    mean_reward          | 192         |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.040069047 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.859      |
|    explained_variance   | 0.435       |
|    learning_rate        | 0.0001      |
|    loss                 | 408         |
|    n_updates            | 150         |
|    policy_gradient_loss | 0.00544     |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=31500, episode_reward=167.90 +/- 64.22
Episode length: 42.36 +/- 16.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=172.72 +/- 58.96
Episode length: 43.52 +/- 14.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.5     |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=176.74 +/- 67.55
Episode length: 44.52 +/- 16.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 61.9     |
|    ep_rew_mean     | 246      |
| time/              |          |
|    fps             | 181      |
|    iterations      | 16       |
|    time_elapsed    | 180      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=161.66 +/- 50.78
Episode length: 40.80 +/- 12.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.8        |
|    mean_reward          | 162         |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.006500194 |
|    clip_fraction        | 0.0714      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.702      |
|    explained_variance   | 0.382       |
|    learning_rate        | 0.0001      |
|    loss                 | 572         |
|    n_updates            | 160         |
|    policy_gradient_loss | 0.00296     |
|    value_loss           | 1.98e+03    |
-----------------------------------------
Eval num_timesteps=33500, episode_reward=173.08 +/- 56.14
Episode length: 43.70 +/- 14.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.7     |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=177.48 +/- 70.08
Episode length: 44.80 +/- 17.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=179.30 +/- 56.93
Episode length: 45.24 +/- 14.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.2     |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 60.2     |
|    ep_rew_mean     | 239      |
| time/              |          |
|    fps             | 181      |
|    iterations      | 17       |
|    time_elapsed    | 191      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=176.00 +/- 66.59
Episode length: 44.34 +/- 16.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.3        |
|    mean_reward          | 176         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.018152252 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.467       |
|    learning_rate        | 0.0001      |
|    loss                 | 707         |
|    n_updates            | 170         |
|    policy_gradient_loss | 0.00553     |
|    value_loss           | 1.5e+03     |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=185.38 +/- 76.67
Episode length: 46.76 +/- 19.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=188.12 +/- 76.75
Episode length: 47.42 +/- 19.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=176.82 +/- 57.76
Episode length: 44.60 +/- 14.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.6     |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 60.8     |
|    ep_rew_mean     | 242      |
| time/              |          |
|    fps             | 181      |
|    iterations      | 18       |
|    time_elapsed    | 203      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=177.46 +/- 68.29
Episode length: 44.78 +/- 17.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.8        |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.008837096 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.847      |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.0001      |
|    loss                 | 773         |
|    n_updates            | 180         |
|    policy_gradient_loss | 0.0077      |
|    value_loss           | 1.64e+03    |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=196.66 +/- 76.78
Episode length: 49.54 +/- 19.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=184.44 +/- 73.07
Episode length: 46.54 +/- 18.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=180.46 +/- 78.80
Episode length: 45.38 +/- 19.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 65.5     |
|    ep_rew_mean     | 261      |
| time/              |          |
|    fps             | 180      |
|    iterations      | 19       |
|    time_elapsed    | 215      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=161.22 +/- 59.62
Episode length: 40.70 +/- 14.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 40.7       |
|    mean_reward          | 161        |
| time/                   |            |
|    total_timesteps      | 39000      |
| train/                  |            |
|    approx_kl            | 0.03359393 |
|    clip_fraction        | 0.176      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.774     |
|    explained_variance   | 0.39       |
|    learning_rate        | 0.0001     |
|    loss                 | 586        |
|    n_updates            | 190        |
|    policy_gradient_loss | 0.0092     |
|    value_loss           | 1.26e+03   |
----------------------------------------
Eval num_timesteps=39500, episode_reward=156.50 +/- 61.37
Episode length: 39.48 +/- 15.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.5     |
|    mean_reward     | 156      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=168.80 +/- 63.62
Episode length: 42.54 +/- 15.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=179.62 +/- 78.29
Episode length: 45.30 +/- 19.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.8     |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 181      |
|    iterations      | 20       |
|    time_elapsed    | 225      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=185.34 +/- 68.68
Episode length: 46.66 +/- 17.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.7       |
|    mean_reward          | 185        |
| time/                   |            |
|    total_timesteps      | 41000      |
| train/                  |            |
|    approx_kl            | 0.04889091 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.666     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.0001     |
|    loss                 | 468        |
|    n_updates            | 200        |
|    policy_gradient_loss | 0.00572    |
|    value_loss           | 801        |
----------------------------------------
Eval num_timesteps=41500, episode_reward=159.84 +/- 64.22
Episode length: 40.34 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.3     |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=162.04 +/- 56.78
Episode length: 40.90 +/- 14.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.9     |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=186.18 +/- 74.51
Episode length: 46.92 +/- 18.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.9     |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=160.72 +/- 62.52
Episode length: 40.56 +/- 15.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.6     |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.6     |
|    ep_rew_mean     | 293      |
| time/              |          |
|    fps             | 180      |
|    iterations      | 21       |
|    time_elapsed    | 238      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=206.82 +/- 73.55
Episode length: 52.12 +/- 18.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.1        |
|    mean_reward          | 207         |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.023952851 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.615      |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0001      |
|    loss                 | 344         |
|    n_updates            | 210         |
|    policy_gradient_loss | 0.00484     |
|    value_loss           | 947         |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=179.24 +/- 60.50
Episode length: 45.14 +/- 15.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=184.44 +/- 63.11
Episode length: 46.44 +/- 15.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=184.98 +/- 73.33
Episode length: 46.58 +/- 18.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.6     |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.8     |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 22       |
|    time_elapsed    | 250      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=182.04 +/- 60.30
Episode length: 45.94 +/- 15.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.9        |
|    mean_reward          | 182         |
| time/                   |             |
|    total_timesteps      | 45500       |
| train/                  |             |
|    approx_kl            | 0.015074916 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.511       |
|    learning_rate        | 0.0001      |
|    loss                 | 648         |
|    n_updates            | 220         |
|    policy_gradient_loss | 0.00511     |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=46000, episode_reward=182.68 +/- 69.65
Episode length: 46.06 +/- 17.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.1     |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=183.84 +/- 72.73
Episode length: 46.30 +/- 18.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=178.68 +/- 58.95
Episode length: 45.00 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76       |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 23       |
|    time_elapsed    | 262      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=171.08 +/- 54.36
Episode length: 43.04 +/- 13.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43          |
|    mean_reward          | 171         |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.020399563 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.575      |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0001      |
|    loss                 | 554         |
|    n_updates            | 230         |
|    policy_gradient_loss | 0.00233     |
|    value_loss           | 909         |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=178.02 +/- 70.53
Episode length: 44.92 +/- 17.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.9     |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=173.84 +/- 79.68
Episode length: 43.88 +/- 19.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.9     |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=176.32 +/- 69.76
Episode length: 44.48 +/- 17.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.8     |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 24       |
|    time_elapsed    | 273      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=171.34 +/- 49.98
Episode length: 43.22 +/- 12.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.2        |
|    mean_reward          | 171         |
| time/                   |             |
|    total_timesteps      | 49500       |
| train/                  |             |
|    approx_kl            | 0.021559943 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.559      |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0001      |
|    loss                 | 204         |
|    n_updates            | 240         |
|    policy_gradient_loss | -3.32e-05   |
|    value_loss           | 558         |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=161.74 +/- 46.93
Episode length: 40.80 +/- 11.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=164.18 +/- 57.14
Episode length: 41.42 +/- 14.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=159.52 +/- 58.66
Episode length: 40.24 +/- 14.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.2     |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.5     |
|    ep_rew_mean     | 268      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 25       |
|    time_elapsed    | 284      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=168.52 +/- 57.46
Episode length: 42.52 +/- 14.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.5        |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.014173787 |
|    clip_fraction        | 0.0602      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.344      |
|    explained_variance   | 0.384       |
|    learning_rate        | 0.0001      |
|    loss                 | 771         |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 1.69e+03    |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=180.32 +/- 63.37
Episode length: 45.50 +/- 15.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=167.32 +/- 57.84
Episode length: 42.24 +/- 14.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=171.98 +/- 57.61
Episode length: 43.34 +/- 14.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.3     |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | 278      |
| time/              |          |
|    fps             | 180      |
|    iterations      | 26       |
|    time_elapsed    | 295      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=150.64 +/- 52.70
Episode length: 37.98 +/- 13.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38          |
|    mean_reward          | 151         |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.021324221 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.51       |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0001      |
|    loss                 | 369         |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.0121      |
|    value_loss           | 626         |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=168.28 +/- 67.43
Episode length: 42.38 +/- 16.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=180.64 +/- 57.77
Episode length: 45.56 +/- 14.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=177.78 +/- 60.77
Episode length: 44.90 +/- 15.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.9     |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.6     |
|    ep_rew_mean     | 281      |
| time/              |          |
|    fps             | 180      |
|    iterations      | 27       |
|    time_elapsed    | 306      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=172.04 +/- 61.07
Episode length: 43.40 +/- 15.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.4        |
|    mean_reward          | 172         |
| time/                   |             |
|    total_timesteps      | 55500       |
| train/                  |             |
|    approx_kl            | 0.008518682 |
|    clip_fraction        | 0.0866      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.499      |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0001      |
|    loss                 | 510         |
|    n_updates            | 270         |
|    policy_gradient_loss | 0.00264     |
|    value_loss           | 755         |
-----------------------------------------
Eval num_timesteps=56000, episode_reward=161.78 +/- 57.61
Episode length: 40.82 +/- 14.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=164.86 +/- 68.01
Episode length: 41.56 +/- 17.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=189.08 +/- 66.81
Episode length: 47.56 +/- 16.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.4     |
|    ep_rew_mean     | 300      |
| time/              |          |
|    fps             | 180      |
|    iterations      | 28       |
|    time_elapsed    | 317      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=168.80 +/- 63.66
Episode length: 42.62 +/- 15.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.6        |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 57500       |
| train/                  |             |
|    approx_kl            | 0.014631486 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.382      |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.0001      |
|    loss                 | 577         |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.00296     |
|    value_loss           | 1.28e+03    |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=168.60 +/- 68.21
Episode length: 42.50 +/- 17.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=159.88 +/- 66.19
Episode length: 40.28 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.3     |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=172.04 +/- 64.52
Episode length: 43.34 +/- 16.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.3     |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 321      |
| time/              |          |
|    fps             | 180      |
|    iterations      | 29       |
|    time_elapsed    | 328      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=179.38 +/- 65.89
Episode length: 45.28 +/- 16.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 45.3         |
|    mean_reward          | 179          |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.0076070996 |
|    clip_fraction        | 0.0852       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.4         |
|    explained_variance   | 0.552        |
|    learning_rate        | 0.0001       |
|    loss                 | 845          |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 1.2e+03      |
------------------------------------------
Eval num_timesteps=60000, episode_reward=173.06 +/- 54.30
Episode length: 43.62 +/- 13.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.6     |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=165.30 +/- 57.13
Episode length: 41.68 +/- 14.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=173.70 +/- 58.97
Episode length: 43.82 +/- 14.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.8     |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.3     |
|    ep_rew_mean     | 304      |
| time/              |          |
|    fps             | 180      |
|    iterations      | 30       |
|    time_elapsed    | 339      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=180.22 +/- 75.23
Episode length: 45.48 +/- 18.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 45.5         |
|    mean_reward          | 180          |
| time/                   |              |
|    total_timesteps      | 61500        |
| train/                  |              |
|    approx_kl            | 0.0141280545 |
|    clip_fraction        | 0.142        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.447       |
|    explained_variance   | 0.496        |
|    learning_rate        | 0.0001       |
|    loss                 | 612          |
|    n_updates            | 300          |
|    policy_gradient_loss | 0.011        |
|    value_loss           | 1.19e+03     |
------------------------------------------
Eval num_timesteps=62000, episode_reward=178.82 +/- 68.68
Episode length: 45.08 +/- 17.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=178.68 +/- 54.30
Episode length: 45.00 +/- 13.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=190.86 +/- 66.80
Episode length: 48.10 +/- 16.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.2     |
|    ep_rew_mean     | 300      |
| time/              |          |
|    fps             | 180      |
|    iterations      | 31       |
|    time_elapsed    | 350      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=237.52 +/- 117.50
Episode length: 59.78 +/- 29.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 59.8       |
|    mean_reward          | 238        |
| time/                   |            |
|    total_timesteps      | 63500      |
| train/                  |            |
|    approx_kl            | 0.01326929 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.396     |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.0001     |
|    loss                 | 689        |
|    n_updates            | 310        |
|    policy_gradient_loss | 0.0111     |
|    value_loss           | 1.33e+03   |
----------------------------------------
New best mean reward!
Eval num_timesteps=64000, episode_reward=218.42 +/- 115.31
Episode length: 55.00 +/- 28.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | 218      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=232.80 +/- 121.32
Episode length: 58.56 +/- 30.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.6     |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=225.46 +/- 120.29
Episode length: 56.84 +/- 30.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.8     |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=211.66 +/- 112.82
Episode length: 53.24 +/- 28.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.2     |
|    ep_rew_mean     | 296      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 32       |
|    time_elapsed    | 366      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=174.68 +/- 65.20
Episode length: 44.04 +/- 16.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44          |
|    mean_reward          | 175         |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.029706776 |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.572      |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0001      |
|    loss                 | 320         |
|    n_updates            | 320         |
|    policy_gradient_loss | 0.00075     |
|    value_loss           | 555         |
-----------------------------------------
Eval num_timesteps=66500, episode_reward=204.58 +/- 89.34
Episode length: 51.58 +/- 22.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | 205      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=191.98 +/- 89.23
Episode length: 48.36 +/- 22.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=186.42 +/- 65.56
Episode length: 46.98 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.9     |
|    ep_rew_mean     | 306      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 33       |
|    time_elapsed    | 378      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=187.82 +/- 67.15
Episode length: 47.24 +/- 16.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47.2       |
|    mean_reward          | 188        |
| time/                   |            |
|    total_timesteps      | 68000      |
| train/                  |            |
|    approx_kl            | 0.01977105 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.471     |
|    explained_variance   | 0.68       |
|    learning_rate        | 0.0001     |
|    loss                 | 569        |
|    n_updates            | 330        |
|    policy_gradient_loss | 0.00575    |
|    value_loss           | 881        |
----------------------------------------
Eval num_timesteps=68500, episode_reward=190.88 +/- 82.68
Episode length: 48.12 +/- 20.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=191.18 +/- 77.60
Episode length: 48.18 +/- 19.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=184.78 +/- 83.58
Episode length: 46.48 +/- 20.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80       |
|    ep_rew_mean     | 319      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 34       |
|    time_elapsed    | 390      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=174.38 +/- 61.15
Episode length: 44.00 +/- 15.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 44        |
|    mean_reward          | 174       |
| time/                   |           |
|    total_timesteps      | 70000     |
| train/                  |           |
|    approx_kl            | 0.0196614 |
|    clip_fraction        | 0.146     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.499    |
|    explained_variance   | 0.822     |
|    learning_rate        | 0.0001    |
|    loss                 | 269       |
|    n_updates            | 340       |
|    policy_gradient_loss | 0.00673   |
|    value_loss           | 529       |
---------------------------------------
Eval num_timesteps=70500, episode_reward=161.62 +/- 65.19
Episode length: 40.80 +/- 16.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=176.08 +/- 73.05
Episode length: 44.38 +/- 18.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=173.78 +/- 74.27
Episode length: 43.84 +/- 18.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.8     |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.5     |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 35       |
|    time_elapsed    | 401      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=160.74 +/- 63.90
Episode length: 40.58 +/- 16.01
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 40.6     |
|    mean_reward          | 161      |
| time/                   |          |
|    total_timesteps      | 72000    |
| train/                  |          |
|    approx_kl            | 0.078843 |
|    clip_fraction        | 0.161    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.482   |
|    explained_variance   | 0.824    |
|    learning_rate        | 0.0001   |
|    loss                 | 258      |
|    n_updates            | 350      |
|    policy_gradient_loss | 0.00603  |
|    value_loss           | 587      |
--------------------------------------
Eval num_timesteps=72500, episode_reward=185.46 +/- 71.66
Episode length: 46.78 +/- 17.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=178.38 +/- 60.30
Episode length: 44.98 +/- 15.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=176.22 +/- 55.20
Episode length: 44.42 +/- 13.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.8     |
|    ep_rew_mean     | 290      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 36       |
|    time_elapsed    | 412      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=167.56 +/- 59.02
Episode length: 42.30 +/- 14.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.3        |
|    mean_reward          | 168         |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.014926136 |
|    clip_fraction        | 0.0841      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.3        |
|    explained_variance   | 0.378       |
|    learning_rate        | 0.0001      |
|    loss                 | 806         |
|    n_updates            | 360         |
|    policy_gradient_loss | 0.00302     |
|    value_loss           | 1.84e+03    |
-----------------------------------------
Eval num_timesteps=74500, episode_reward=180.94 +/- 66.03
Episode length: 45.60 +/- 16.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=167.24 +/- 69.21
Episode length: 42.16 +/- 17.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=164.66 +/- 54.80
Episode length: 41.50 +/- 13.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.6     |
|    ep_rew_mean     | 269      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 37       |
|    time_elapsed    | 423      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=166.62 +/- 60.42
Episode length: 41.96 +/- 15.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42          |
|    mean_reward          | 167         |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.014748646 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.351      |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.0001      |
|    loss                 | 522         |
|    n_updates            | 370         |
|    policy_gradient_loss | 0.0105      |
|    value_loss           | 1.14e+03    |
-----------------------------------------
Eval num_timesteps=76500, episode_reward=168.74 +/- 73.99
Episode length: 42.54 +/- 18.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=151.64 +/- 55.62
Episode length: 38.28 +/- 13.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38.3     |
|    mean_reward     | 152      |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=172.20 +/- 55.46
Episode length: 43.40 +/- 13.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.4     |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 60.5     |
|    ep_rew_mean     | 240      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 38       |
|    time_elapsed    | 433      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=173.08 +/- 55.67
Episode length: 43.62 +/- 13.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.6        |
|    mean_reward          | 173         |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.025151096 |
|    clip_fraction        | 0.0886      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.356      |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.0001      |
|    loss                 | 621         |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00685    |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=78500, episode_reward=178.16 +/- 53.93
Episode length: 44.90 +/- 13.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.9     |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=166.36 +/- 57.44
Episode length: 41.98 +/- 14.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=155.60 +/- 60.09
Episode length: 39.26 +/- 15.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.3     |
|    mean_reward     | 156      |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | 276      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 39       |
|    time_elapsed    | 444      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=180.94 +/- 64.08
Episode length: 45.60 +/- 16.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.6        |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.022208057 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.428      |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.0001      |
|    loss                 | 455         |
|    n_updates            | 390         |
|    policy_gradient_loss | 0.00874     |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=80500, episode_reward=161.38 +/- 61.22
Episode length: 40.72 +/- 15.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.7     |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=167.20 +/- 55.81
Episode length: 42.16 +/- 13.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=158.04 +/- 51.70
Episode length: 39.86 +/- 12.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.9     |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.9     |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 40       |
|    time_elapsed    | 455      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=176.88 +/- 69.70
Episode length: 44.56 +/- 17.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.6        |
|    mean_reward          | 177         |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.008528167 |
|    clip_fraction        | 0.0708      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.372      |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.0001      |
|    loss                 | 956         |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.000653   |
|    value_loss           | 1.68e+03    |
-----------------------------------------
Eval num_timesteps=82500, episode_reward=148.92 +/- 42.30
Episode length: 37.56 +/- 10.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.6     |
|    mean_reward     | 149      |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=158.02 +/- 48.21
Episode length: 39.96 +/- 12.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40       |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=155.46 +/- 51.48
Episode length: 39.30 +/- 12.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.3     |
|    mean_reward     | 155      |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.5     |
|    ep_rew_mean     | 292      |
| time/              |          |
|    fps             | 180      |
|    iterations      | 41       |
|    time_elapsed    | 466      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=180.40 +/- 71.34
Episode length: 45.48 +/- 17.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.5        |
|    mean_reward          | 180         |
| time/                   |             |
|    total_timesteps      | 84000       |
| train/                  |             |
|    approx_kl            | 0.013949475 |
|    clip_fraction        | 0.0775      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.366       |
|    learning_rate        | 0.0001      |
|    loss                 | 751         |
|    n_updates            | 410         |
|    policy_gradient_loss | 0.000625    |
|    value_loss           | 1.67e+03    |
-----------------------------------------
Eval num_timesteps=84500, episode_reward=168.56 +/- 62.43
Episode length: 42.48 +/- 15.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=161.76 +/- 55.40
Episode length: 40.74 +/- 13.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.7     |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=158.64 +/- 54.17
Episode length: 40.08 +/- 13.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.1     |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=160.52 +/- 52.71
Episode length: 40.54 +/- 13.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.5     |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.6     |
|    ep_rew_mean     | 301      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 42       |
|    time_elapsed    | 478      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=203.14 +/- 94.81
Episode length: 51.16 +/- 23.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.2        |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 86500       |
| train/                  |             |
|    approx_kl            | 0.021569151 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.393      |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.0001      |
|    loss                 | 592         |
|    n_updates            | 420         |
|    policy_gradient_loss | 0.0053      |
|    value_loss           | 1.22e+03    |
-----------------------------------------
Eval num_timesteps=87000, episode_reward=214.90 +/- 110.34
Episode length: 54.16 +/- 27.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=183.34 +/- 71.84
Episode length: 46.16 +/- 17.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=197.14 +/- 100.81
Episode length: 49.74 +/- 25.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.8     |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 179      |
|    iterations      | 43       |
|    time_elapsed    | 490      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=231.56 +/- 126.30
Episode length: 58.24 +/- 31.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 58.2        |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 88500       |
| train/                  |             |
|    approx_kl            | 0.024195094 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.454      |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0001      |
|    loss                 | 267         |
|    n_updates            | 430         |
|    policy_gradient_loss | 0.00163     |
|    value_loss           | 578         |
-----------------------------------------
Eval num_timesteps=89000, episode_reward=197.52 +/- 107.04
Episode length: 49.78 +/- 26.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=236.54 +/- 128.25
Episode length: 59.52 +/- 32.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.5     |
|    mean_reward     | 237      |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=214.56 +/- 95.51
Episode length: 54.02 +/- 23.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.2     |
|    ep_rew_mean     | 311      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 44       |
|    time_elapsed    | 504      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=168.28 +/- 48.95
Episode length: 42.44 +/- 12.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.4       |
|    mean_reward          | 168        |
| time/                   |            |
|    total_timesteps      | 90500      |
| train/                  |            |
|    approx_kl            | 0.15870586 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.44      |
|    explained_variance   | 0.873      |
|    learning_rate        | 0.0001     |
|    loss                 | 161        |
|    n_updates            | 440        |
|    policy_gradient_loss | 0.0153     |
|    value_loss           | 405        |
----------------------------------------
Eval num_timesteps=91000, episode_reward=162.04 +/- 58.48
Episode length: 40.90 +/- 14.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.9     |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=168.08 +/- 61.19
Episode length: 42.44 +/- 15.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=166.84 +/- 65.65
Episode length: 42.04 +/- 16.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.3     |
|    ep_rew_mean     | 312      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 45       |
|    time_elapsed    | 514      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=260.98 +/- 146.53
Episode length: 65.60 +/- 36.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 65.6        |
|    mean_reward          | 261         |
| time/                   |             |
|    total_timesteps      | 92500       |
| train/                  |             |
|    approx_kl            | 0.024553655 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.435      |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.0001      |
|    loss                 | 543         |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00458    |
|    value_loss           | 1.31e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=93000, episode_reward=243.00 +/- 117.03
Episode length: 61.10 +/- 29.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.1     |
|    mean_reward     | 243      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=215.08 +/- 103.99
Episode length: 54.18 +/- 25.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=216.62 +/- 118.73
Episode length: 54.54 +/- 29.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.7     |
|    ep_rew_mean     | 325      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 46       |
|    time_elapsed    | 528      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=176.64 +/- 82.77
Episode length: 44.56 +/- 20.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 44.6       |
|    mean_reward          | 177        |
| time/                   |            |
|    total_timesteps      | 94500      |
| train/                  |            |
|    approx_kl            | 0.01384877 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.515     |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0001     |
|    loss                 | 310        |
|    n_updates            | 460        |
|    policy_gradient_loss | 0.00677    |
|    value_loss           | 799        |
----------------------------------------
Eval num_timesteps=95000, episode_reward=181.62 +/- 76.51
Episode length: 45.76 +/- 19.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=195.48 +/- 82.20
Episode length: 49.28 +/- 20.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=184.44 +/- 76.32
Episode length: 46.50 +/- 18.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.5     |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.1     |
|    ep_rew_mean     | 331      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 47       |
|    time_elapsed    | 540      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=183.72 +/- 70.28
Episode length: 46.32 +/- 17.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.3        |
|    mean_reward          | 184         |
| time/                   |             |
|    total_timesteps      | 96500       |
| train/                  |             |
|    approx_kl            | 0.014360518 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.427      |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0001      |
|    loss                 | 539         |
|    n_updates            | 470         |
|    policy_gradient_loss | 0.00533     |
|    value_loss           | 1.21e+03    |
-----------------------------------------
Eval num_timesteps=97000, episode_reward=177.80 +/- 65.40
Episode length: 44.84 +/- 16.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=180.92 +/- 95.00
Episode length: 45.66 +/- 23.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=181.34 +/- 94.58
Episode length: 45.68 +/- 23.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83       |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 48       |
|    time_elapsed    | 551      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=181.20 +/- 65.31
Episode length: 45.58 +/- 16.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 45.6       |
|    mean_reward          | 181        |
| time/                   |            |
|    total_timesteps      | 98500      |
| train/                  |            |
|    approx_kl            | 0.01002311 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.423     |
|    explained_variance   | 0.693      |
|    learning_rate        | 0.0001     |
|    loss                 | 404        |
|    n_updates            | 480        |
|    policy_gradient_loss | 0.00988    |
|    value_loss           | 985        |
----------------------------------------
Eval num_timesteps=99000, episode_reward=183.94 +/- 92.92
Episode length: 46.32 +/- 23.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.3     |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=181.80 +/- 68.50
Episode length: 45.86 +/- 17.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.9     |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=179.88 +/- 102.82
Episode length: 45.32 +/- 25.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.9     |
|    ep_rew_mean     | 346      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 49       |
|    time_elapsed    | 562      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=173.50 +/- 46.94
Episode length: 43.74 +/- 11.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.7        |
|    mean_reward          | 174         |
| time/                   |             |
|    total_timesteps      | 100500      |
| train/                  |             |
|    approx_kl            | 0.014787339 |
|    clip_fraction        | 0.0791      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.415      |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0001      |
|    loss                 | 708         |
|    n_updates            | 490         |
|    policy_gradient_loss | 0.00446     |
|    value_loss           | 1.03e+03    |
-----------------------------------------
Eval num_timesteps=101000, episode_reward=168.60 +/- 64.27
Episode length: 42.56 +/- 16.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=187.22 +/- 73.89
Episode length: 47.24 +/- 18.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=162.16 +/- 47.43
Episode length: 40.96 +/- 11.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 162      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.4     |
|    ep_rew_mean     | 328      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 50       |
|    time_elapsed    | 573      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=159.82 +/- 62.62
Episode length: 40.34 +/- 15.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 40.3       |
|    mean_reward          | 160        |
| time/                   |            |
|    total_timesteps      | 102500     |
| train/                  |            |
|    approx_kl            | 0.01854201 |
|    clip_fraction        | 0.09       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.313     |
|    explained_variance   | 0.536      |
|    learning_rate        | 0.0001     |
|    loss                 | 471        |
|    n_updates            | 500        |
|    policy_gradient_loss | 0.000592   |
|    value_loss           | 1.52e+03   |
----------------------------------------
Eval num_timesteps=103000, episode_reward=171.86 +/- 65.31
Episode length: 43.24 +/- 16.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.2     |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=164.36 +/- 57.54
Episode length: 41.46 +/- 14.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=174.22 +/- 67.64
Episode length: 43.94 +/- 16.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.9     |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.8     |
|    ep_rew_mean     | 310      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 51       |
|    time_elapsed    | 584      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=191.94 +/- 89.22
Episode length: 48.36 +/- 22.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.4       |
|    mean_reward          | 192        |
| time/                   |            |
|    total_timesteps      | 104500     |
| train/                  |            |
|    approx_kl            | 0.01791468 |
|    clip_fraction        | 0.0835     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.323     |
|    explained_variance   | 0.372      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.04e+03   |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.00374   |
|    value_loss           | 1.82e+03   |
----------------------------------------
Eval num_timesteps=105000, episode_reward=169.44 +/- 62.85
Episode length: 42.82 +/- 15.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | 169      |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=191.08 +/- 78.46
Episode length: 48.12 +/- 19.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=201.00 +/- 77.69
Episode length: 50.56 +/- 19.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | 201      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.8     |
|    ep_rew_mean     | 306      |
| time/              |          |
|    fps             | 178      |
|    iterations      | 52       |
|    time_elapsed    | 596      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=211.96 +/- 83.47
Episode length: 53.38 +/- 20.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.4         |
|    mean_reward          | 212          |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0115808435 |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.386       |
|    explained_variance   | 0.476        |
|    learning_rate        | 0.0001       |
|    loss                 | 707          |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.000227    |
|    value_loss           | 1.31e+03     |
------------------------------------------
Eval num_timesteps=107000, episode_reward=224.14 +/- 123.43
Episode length: 56.40 +/- 30.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.4     |
|    mean_reward     | 224      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=215.54 +/- 108.67
Episode length: 54.30 +/- 27.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=199.90 +/- 105.30
Episode length: 50.30 +/- 26.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=227.70 +/- 120.12
Episode length: 57.24 +/- 30.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.2     |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.3     |
|    ep_rew_mean     | 304      |
| time/              |          |
|    fps             | 177      |
|    iterations      | 53       |
|    time_elapsed    | 611      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=196.46 +/- 78.68
Episode length: 49.54 +/- 19.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.5        |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.024626425 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.38       |
|    explained_variance   | 0.417       |
|    learning_rate        | 0.0001      |
|    loss                 | 700         |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.000231   |
|    value_loss           | 1.46e+03    |
-----------------------------------------
Eval num_timesteps=109500, episode_reward=186.92 +/- 68.62
Episode length: 47.14 +/- 17.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=168.04 +/- 63.30
Episode length: 42.40 +/- 15.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=191.60 +/- 88.88
Episode length: 48.26 +/- 22.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.3     |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.7     |
|    ep_rew_mean     | 313      |
| time/              |          |
|    fps             | 177      |
|    iterations      | 54       |
|    time_elapsed    | 623      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=190.32 +/- 71.60
Episode length: 47.98 +/- 17.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48          |
|    mean_reward          | 190         |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.021752197 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.397      |
|    explained_variance   | 0.541       |
|    learning_rate        | 0.0001      |
|    loss                 | 804         |
|    n_updates            | 540         |
|    policy_gradient_loss | 0.00213     |
|    value_loss           | 1.24e+03    |
-----------------------------------------
Eval num_timesteps=111500, episode_reward=180.42 +/- 73.66
Episode length: 45.48 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.5     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=200.26 +/- 76.40
Episode length: 50.52 +/- 18.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=201.56 +/- 88.49
Episode length: 50.82 +/- 22.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | 202      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.7     |
|    ep_rew_mean     | 325      |
| time/              |          |
|    fps             | 177      |
|    iterations      | 55       |
|    time_elapsed    | 635      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=211.36 +/- 80.43
Episode length: 53.36 +/- 20.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.4        |
|    mean_reward          | 211         |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.019035185 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.356      |
|    explained_variance   | 0.503       |
|    learning_rate        | 0.0001      |
|    loss                 | 633         |
|    n_updates            | 550         |
|    policy_gradient_loss | 0.0126      |
|    value_loss           | 1.41e+03    |
-----------------------------------------
Eval num_timesteps=113500, episode_reward=210.44 +/- 84.21
Episode length: 52.98 +/- 21.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=193.54 +/- 65.94
Episode length: 48.80 +/- 16.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=186.76 +/- 80.73
Episode length: 47.04 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.3     |
|    ep_rew_mean     | 323      |
| time/              |          |
|    fps             | 177      |
|    iterations      | 56       |
|    time_elapsed    | 647      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=214.54 +/- 82.11
Episode length: 54.02 +/- 20.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54          |
|    mean_reward          | 215         |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.017642332 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.38       |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.0001      |
|    loss                 | 922         |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.00449    |
|    value_loss           | 1.35e+03    |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=209.76 +/- 110.68
Episode length: 52.90 +/- 27.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=217.68 +/- 104.90
Episode length: 54.80 +/- 26.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.8     |
|    mean_reward     | 218      |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=201.56 +/- 84.55
Episode length: 50.78 +/- 21.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | 202      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.7     |
|    ep_rew_mean     | 337      |
| time/              |          |
|    fps             | 176      |
|    iterations      | 57       |
|    time_elapsed    | 660      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=300.80 +/- 147.44
Episode length: 75.62 +/- 36.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.6        |
|    mean_reward          | 301         |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.011416785 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.507      |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0001      |
|    loss                 | 371         |
|    n_updates            | 570         |
|    policy_gradient_loss | 0.000782    |
|    value_loss           | 1.16e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=117500, episode_reward=255.66 +/- 127.24
Episode length: 64.32 +/- 31.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.3     |
|    mean_reward     | 256      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=284.48 +/- 132.79
Episode length: 71.48 +/- 33.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.5     |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=258.66 +/- 135.96
Episode length: 64.98 +/- 33.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65       |
|    mean_reward     | 259      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.8     |
|    ep_rew_mean     | 342      |
| time/              |          |
|    fps             | 175      |
|    iterations      | 58       |
|    time_elapsed    | 676      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=240.76 +/- 150.17
Episode length: 60.54 +/- 37.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 60.5       |
|    mean_reward          | 241        |
| time/                   |            |
|    total_timesteps      | 119000     |
| train/                  |            |
|    approx_kl            | 0.03582389 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.469     |
|    explained_variance   | 0.739      |
|    learning_rate        | 0.0001     |
|    loss                 | 347        |
|    n_updates            | 580        |
|    policy_gradient_loss | 0.0188     |
|    value_loss           | 703        |
----------------------------------------
Eval num_timesteps=119500, episode_reward=209.62 +/- 106.94
Episode length: 52.80 +/- 26.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=220.54 +/- 118.65
Episode length: 55.46 +/- 29.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.5     |
|    mean_reward     | 221      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=213.42 +/- 112.53
Episode length: 53.76 +/- 28.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.3     |
|    ep_rew_mean     | 336      |
| time/              |          |
|    fps             | 175      |
|    iterations      | 59       |
|    time_elapsed    | 689      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=223.18 +/- 127.22
Episode length: 56.18 +/- 31.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.2        |
|    mean_reward          | 223         |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.020770652 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.471      |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0001      |
|    loss                 | 530         |
|    n_updates            | 590         |
|    policy_gradient_loss | 0.000724    |
|    value_loss           | 1.03e+03    |
-----------------------------------------
Eval num_timesteps=121500, episode_reward=219.94 +/- 123.16
Episode length: 55.36 +/- 30.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.4     |
|    mean_reward     | 220      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=227.44 +/- 113.22
Episode length: 57.16 +/- 28.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.2     |
|    mean_reward     | 227      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=193.40 +/- 95.85
Episode length: 48.74 +/- 23.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.4     |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 174      |
|    iterations      | 60       |
|    time_elapsed    | 702      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=218.90 +/- 84.24
Episode length: 55.14 +/- 21.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 55.1       |
|    mean_reward          | 219        |
| time/                   |            |
|    total_timesteps      | 123000     |
| train/                  |            |
|    approx_kl            | 0.01875229 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.496     |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0001     |
|    loss                 | 351        |
|    n_updates            | 600        |
|    policy_gradient_loss | 0.00309    |
|    value_loss           | 1.21e+03   |
----------------------------------------
Eval num_timesteps=123500, episode_reward=213.70 +/- 90.69
Episode length: 53.78 +/- 22.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=222.36 +/- 93.48
Episode length: 55.92 +/- 23.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.9     |
|    mean_reward     | 222      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=247.80 +/- 136.92
Episode length: 62.36 +/- 34.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.4     |
|    mean_reward     | 248      |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.9     |
|    ep_rew_mean     | 338      |
| time/              |          |
|    fps             | 174      |
|    iterations      | 61       |
|    time_elapsed    | 715      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=191.36 +/- 68.28
Episode length: 48.16 +/- 16.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.2        |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.025023764 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.519      |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0001      |
|    loss                 | 661         |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 1.06e+03    |
-----------------------------------------
Eval num_timesteps=125500, episode_reward=197.80 +/- 86.36
Episode length: 49.86 +/- 21.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=194.32 +/- 72.96
Episode length: 48.98 +/- 18.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=201.74 +/- 68.86
Episode length: 50.86 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.9     |
|    mean_reward     | 202      |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.7     |
|    ep_rew_mean     | 329      |
| time/              |          |
|    fps             | 174      |
|    iterations      | 62       |
|    time_elapsed    | 727      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=278.42 +/- 151.87
Episode length: 69.98 +/- 37.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 70         |
|    mean_reward          | 278        |
| time/                   |            |
|    total_timesteps      | 127000     |
| train/                  |            |
|    approx_kl            | 0.02692103 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.385     |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.0001     |
|    loss                 | 534        |
|    n_updates            | 620        |
|    policy_gradient_loss | 0.0104     |
|    value_loss           | 1.04e+03   |
----------------------------------------
Eval num_timesteps=127500, episode_reward=240.80 +/- 121.46
Episode length: 60.58 +/- 30.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.6     |
|    mean_reward     | 241      |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=229.90 +/- 127.32
Episode length: 57.86 +/- 31.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.9     |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=223.68 +/- 127.36
Episode length: 56.26 +/- 31.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.3     |
|    mean_reward     | 224      |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=260.80 +/- 140.01
Episode length: 65.68 +/- 35.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.7     |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.3     |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 173      |
|    iterations      | 63       |
|    time_elapsed    | 744      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=180.78 +/- 73.01
Episode length: 45.54 +/- 18.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.5        |
|    mean_reward          | 181         |
| time/                   |             |
|    total_timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.023805935 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.464      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0001      |
|    loss                 | 276         |
|    n_updates            | 630         |
|    policy_gradient_loss | 0.00702     |
|    value_loss           | 779         |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=200.26 +/- 76.89
Episode length: 50.48 +/- 19.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=196.44 +/- 80.96
Episode length: 49.46 +/- 20.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=180.62 +/- 81.23
Episode length: 45.58 +/- 20.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.7     |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 173      |
|    iterations      | 64       |
|    time_elapsed    | 756      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=223.00 +/- 121.55
Episode length: 56.12 +/- 30.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.1        |
|    mean_reward          | 223         |
| time/                   |             |
|    total_timesteps      | 131500      |
| train/                  |             |
|    approx_kl            | 0.020242017 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.333      |
|    explained_variance   | 0.453       |
|    learning_rate        | 0.0001      |
|    loss                 | 590         |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00207    |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=132000, episode_reward=280.74 +/- 133.66
Episode length: 70.56 +/- 33.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.6     |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=201.12 +/- 94.21
Episode length: 50.72 +/- 23.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | 201      |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=240.58 +/- 95.07
Episode length: 60.50 +/- 23.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.5     |
|    mean_reward     | 241      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.3     |
|    ep_rew_mean     | 304      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 65       |
|    time_elapsed    | 770      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=243.90 +/- 136.95
Episode length: 61.34 +/- 34.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 61.3       |
|    mean_reward          | 244        |
| time/                   |            |
|    total_timesteps      | 133500     |
| train/                  |            |
|    approx_kl            | 0.03891424 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.497     |
|    explained_variance   | 0.441      |
|    learning_rate        | 0.0001     |
|    loss                 | 383        |
|    n_updates            | 650        |
|    policy_gradient_loss | 0.00177    |
|    value_loss           | 1.2e+03    |
----------------------------------------
Eval num_timesteps=134000, episode_reward=243.84 +/- 135.72
Episode length: 61.26 +/- 33.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.3     |
|    mean_reward     | 244      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=240.00 +/- 144.03
Episode length: 60.44 +/- 36.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.4     |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=214.62 +/- 135.12
Episode length: 54.00 +/- 33.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.6     |
|    ep_rew_mean     | 329      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 66       |
|    time_elapsed    | 784      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=188.58 +/- 84.94
Episode length: 47.48 +/- 21.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.5        |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 135500      |
| train/                  |             |
|    approx_kl            | 0.017837763 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.0001      |
|    loss                 | 118         |
|    n_updates            | 660         |
|    policy_gradient_loss | 0.0184      |
|    value_loss           | 797         |
-----------------------------------------
Eval num_timesteps=136000, episode_reward=202.48 +/- 87.37
Episode length: 51.00 +/- 21.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | 202      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=199.86 +/- 107.97
Episode length: 50.34 +/- 26.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.3     |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=172.82 +/- 59.04
Episode length: 43.56 +/- 14.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.6     |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.9     |
|    ep_rew_mean     | 334      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 67       |
|    time_elapsed    | 795      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=201.44 +/- 99.45
Episode length: 50.70 +/- 24.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.7        |
|    mean_reward          | 201         |
| time/                   |             |
|    total_timesteps      | 137500      |
| train/                  |             |
|    approx_kl            | 0.013211143 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.422      |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.0001      |
|    loss                 | 606         |
|    n_updates            | 670         |
|    policy_gradient_loss | 0.00665     |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=205.86 +/- 103.81
Episode length: 51.82 +/- 25.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.8     |
|    mean_reward     | 206      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=198.92 +/- 89.93
Episode length: 50.14 +/- 22.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=196.98 +/- 74.45
Episode length: 49.68 +/- 18.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89       |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 68       |
|    time_elapsed    | 808      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=195.16 +/- 70.81
Episode length: 49.22 +/- 17.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 139500      |
| train/                  |             |
|    approx_kl            | 0.017231569 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.418      |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0001      |
|    loss                 | 463         |
|    n_updates            | 680         |
|    policy_gradient_loss | 0.00834     |
|    value_loss           | 1.4e+03     |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=212.66 +/- 109.46
Episode length: 53.48 +/- 27.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=211.86 +/- 86.37
Episode length: 53.32 +/- 21.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=213.20 +/- 88.47
Episode length: 53.68 +/- 22.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.8     |
|    ep_rew_mean     | 374      |
| time/              |          |
|    fps             | 172      |
|    iterations      | 69       |
|    time_elapsed    | 820      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=219.10 +/- 108.35
Episode length: 55.14 +/- 27.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.1        |
|    mean_reward          | 219         |
| time/                   |             |
|    total_timesteps      | 141500      |
| train/                  |             |
|    approx_kl            | 0.029393803 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.379      |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0001      |
|    loss                 | 458         |
|    n_updates            | 690         |
|    policy_gradient_loss | 0.0057      |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=208.90 +/- 98.39
Episode length: 52.66 +/- 24.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | 209      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=221.86 +/- 116.89
Episode length: 55.82 +/- 29.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | 222      |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=234.90 +/- 120.35
Episode length: 59.06 +/- 30.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.1     |
|    mean_reward     | 235      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92       |
|    ep_rew_mean     | 366      |
| time/              |          |
|    fps             | 171      |
|    iterations      | 70       |
|    time_elapsed    | 833      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=183.58 +/- 69.64
Episode length: 46.24 +/- 17.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.2        |
|    mean_reward          | 184         |
| time/                   |             |
|    total_timesteps      | 143500      |
| train/                  |             |
|    approx_kl            | 0.023230966 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.414      |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0001      |
|    loss                 | 819         |
|    n_updates            | 700         |
|    policy_gradient_loss | 0.00597     |
|    value_loss           | 1.19e+03    |
-----------------------------------------
Eval num_timesteps=144000, episode_reward=237.78 +/- 120.59
Episode length: 59.76 +/- 30.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.8     |
|    mean_reward     | 238      |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=214.10 +/- 95.88
Episode length: 53.88 +/- 23.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=230.60 +/- 105.46
Episode length: 57.98 +/- 26.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58       |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.1     |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 171      |
|    iterations      | 71       |
|    time_elapsed    | 846      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=300.88 +/- 151.32
Episode length: 75.60 +/- 37.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.6        |
|    mean_reward          | 301         |
| time/                   |             |
|    total_timesteps      | 145500      |
| train/                  |             |
|    approx_kl            | 0.029329268 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.443      |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0001      |
|    loss                 | 709         |
|    n_updates            | 710         |
|    policy_gradient_loss | 0.00106     |
|    value_loss           | 1.42e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=146000, episode_reward=245.56 +/- 121.39
Episode length: 61.70 +/- 30.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.7     |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=301.66 +/- 173.44
Episode length: 75.76 +/- 43.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.8     |
|    mean_reward     | 302      |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
New best mean reward!
Eval num_timesteps=147000, episode_reward=262.08 +/- 150.55
Episode length: 65.94 +/- 37.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.9     |
|    mean_reward     | 262      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.9     |
|    ep_rew_mean     | 358      |
| time/              |          |
|    fps             | 170      |
|    iterations      | 72       |
|    time_elapsed    | 862      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=178.02 +/- 61.78
Episode length: 44.86 +/- 15.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.9        |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 147500      |
| train/                  |             |
|    approx_kl            | 0.019148462 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.425      |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0001      |
|    loss                 | 443         |
|    n_updates            | 720         |
|    policy_gradient_loss | 0.0165      |
|    value_loss           | 821         |
-----------------------------------------
Eval num_timesteps=148000, episode_reward=188.22 +/- 71.41
Episode length: 47.42 +/- 17.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=205.22 +/- 74.08
Episode length: 51.72 +/- 18.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | 205      |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=203.58 +/- 87.99
Episode length: 51.24 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | 204      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=177.74 +/- 66.32
Episode length: 44.78 +/- 16.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.8     |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.8     |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 170      |
|    iterations      | 73       |
|    time_elapsed    | 876      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=194.26 +/- 87.86
Episode length: 48.90 +/- 21.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.9       |
|    mean_reward          | 194        |
| time/                   |            |
|    total_timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.01861402 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.349     |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.0001     |
|    loss                 | 829        |
|    n_updates            | 730        |
|    policy_gradient_loss | 0.000815   |
|    value_loss           | 1.5e+03    |
----------------------------------------
Eval num_timesteps=150500, episode_reward=196.64 +/- 91.96
Episode length: 49.58 +/- 22.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.6     |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=193.42 +/- 101.75
Episode length: 48.78 +/- 25.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.8     |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=190.12 +/- 88.47
Episode length: 47.94 +/- 22.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.7     |
|    ep_rew_mean     | 354      |
| time/              |          |
|    fps             | 170      |
|    iterations      | 74       |
|    time_elapsed    | 888      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=200.58 +/- 83.74
Episode length: 50.54 +/- 21.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50.5       |
|    mean_reward          | 201        |
| time/                   |            |
|    total_timesteps      | 152000     |
| train/                  |            |
|    approx_kl            | 0.01368897 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.692      |
|    learning_rate        | 0.0001     |
|    loss                 | 451        |
|    n_updates            | 740        |
|    policy_gradient_loss | 0.0128     |
|    value_loss           | 1e+03      |
----------------------------------------
Eval num_timesteps=152500, episode_reward=180.18 +/- 87.81
Episode length: 45.36 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.4     |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=178.20 +/- 83.89
Episode length: 44.96 +/- 21.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45       |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=190.18 +/- 97.76
Episode length: 47.92 +/- 24.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.7     |
|    ep_rew_mean     | 346      |
| time/              |          |
|    fps             | 170      |
|    iterations      | 75       |
|    time_elapsed    | 900      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=182.14 +/- 63.62
Episode length: 45.94 +/- 15.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 45.9       |
|    mean_reward          | 182        |
| time/                   |            |
|    total_timesteps      | 154000     |
| train/                  |            |
|    approx_kl            | 0.02231814 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.407     |
|    explained_variance   | 0.819      |
|    learning_rate        | 0.0001     |
|    loss                 | 350        |
|    n_updates            | 750        |
|    policy_gradient_loss | 0.00265    |
|    value_loss           | 594        |
----------------------------------------
Eval num_timesteps=154500, episode_reward=190.40 +/- 81.36
Episode length: 48.00 +/- 20.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=159.10 +/- 61.29
Episode length: 40.14 +/- 15.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.1     |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=157.80 +/- 61.01
Episode length: 39.82 +/- 15.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.8     |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | 321      |
| time/              |          |
|    fps             | 170      |
|    iterations      | 76       |
|    time_elapsed    | 911      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=172.04 +/- 65.81
Episode length: 43.42 +/- 16.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.4        |
|    mean_reward          | 172         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.021883085 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.33       |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0001      |
|    loss                 | 537         |
|    n_updates            | 760         |
|    policy_gradient_loss | 0.00634     |
|    value_loss           | 1.52e+03    |
-----------------------------------------
Eval num_timesteps=156500, episode_reward=194.46 +/- 74.34
Episode length: 49.00 +/- 18.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=190.06 +/- 65.09
Episode length: 47.92 +/- 16.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=197.92 +/- 83.29
Episode length: 49.88 +/- 20.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.7     |
|    ep_rew_mean     | 325      |
| time/              |          |
|    fps             | 170      |
|    iterations      | 77       |
|    time_elapsed    | 923      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=210.72 +/- 85.82
Episode length: 53.08 +/- 21.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.1        |
|    mean_reward          | 211         |
| time/                   |             |
|    total_timesteps      | 158000      |
| train/                  |             |
|    approx_kl            | 0.019563332 |
|    clip_fraction        | 0.0998      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.381      |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.0001      |
|    loss                 | 434         |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.000557   |
|    value_loss           | 1.12e+03    |
-----------------------------------------
Eval num_timesteps=158500, episode_reward=189.46 +/- 76.20
Episode length: 47.68 +/- 19.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=183.42 +/- 69.45
Episode length: 46.22 +/- 17.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=184.40 +/- 84.69
Episode length: 46.44 +/- 21.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.4     |
|    mean_reward     | 184      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.2     |
|    ep_rew_mean     | 331      |
| time/              |          |
|    fps             | 170      |
|    iterations      | 78       |
|    time_elapsed    | 935      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=205.30 +/- 106.57
Episode length: 51.72 +/- 26.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.7        |
|    mean_reward          | 205         |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.018040251 |
|    clip_fraction        | 0.0942      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.38       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0001      |
|    loss                 | 200         |
|    n_updates            | 780         |
|    policy_gradient_loss | 0.00158     |
|    value_loss           | 407         |
-----------------------------------------
Eval num_timesteps=160500, episode_reward=216.14 +/- 102.16
Episode length: 54.42 +/- 25.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=213.10 +/- 109.93
Episode length: 53.64 +/- 27.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=200.28 +/- 89.66
Episode length: 50.48 +/- 22.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.6     |
|    ep_rew_mean     | 321      |
| time/              |          |
|    fps             | 170      |
|    iterations      | 79       |
|    time_elapsed    | 947      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=228.00 +/- 100.73
Episode length: 57.40 +/- 25.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 57.4        |
|    mean_reward          | 228         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.021688627 |
|    clip_fraction        | 0.0931      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.401      |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0001      |
|    loss                 | 635         |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00656    |
|    value_loss           | 1.2e+03     |
-----------------------------------------
Eval num_timesteps=162500, episode_reward=213.54 +/- 93.73
Episode length: 53.66 +/- 23.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=189.20 +/- 79.07
Episode length: 47.70 +/- 19.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.7     |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=210.38 +/- 97.56
Episode length: 52.96 +/- 24.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.6     |
|    ep_rew_mean     | 329      |
| time/              |          |
|    fps             | 170      |
|    iterations      | 80       |
|    time_elapsed    | 960      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=304.18 +/- 129.33
Episode length: 76.42 +/- 32.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.4        |
|    mean_reward          | 304         |
| time/                   |             |
|    total_timesteps      | 164000      |
| train/                  |             |
|    approx_kl            | 0.019962039 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.38       |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0001      |
|    loss                 | 598         |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.00312    |
|    value_loss           | 1.13e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=164500, episode_reward=297.98 +/- 115.27
Episode length: 74.78 +/- 28.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.8     |
|    mean_reward     | 298      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=297.56 +/- 122.08
Episode length: 74.72 +/- 30.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 298      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=317.56 +/- 124.28
Episode length: 79.82 +/- 31.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.8     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.7     |
|    ep_rew_mean     | 329      |
| time/              |          |
|    fps             | 169      |
|    iterations      | 81       |
|    time_elapsed    | 977      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=279.00 +/- 131.56
Episode length: 70.06 +/- 32.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 70.1        |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.020785466 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.421      |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0001      |
|    loss                 | 148         |
|    n_updates            | 810         |
|    policy_gradient_loss | 0.00103     |
|    value_loss           | 447         |
-----------------------------------------
Eval num_timesteps=166500, episode_reward=258.18 +/- 112.88
Episode length: 64.90 +/- 28.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.9     |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=289.14 +/- 131.93
Episode length: 72.70 +/- 32.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=295.96 +/- 132.21
Episode length: 74.42 +/- 32.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.4     |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 169      |
|    iterations      | 82       |
|    time_elapsed    | 993      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=268.90 +/- 132.74
Episode length: 67.62 +/- 33.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 67.6        |
|    mean_reward          | 269         |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.023615083 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0001      |
|    loss                 | 126         |
|    n_updates            | 820         |
|    policy_gradient_loss | 0.00587     |
|    value_loss           | 340         |
-----------------------------------------
Eval num_timesteps=168500, episode_reward=242.50 +/- 128.93
Episode length: 61.06 +/- 32.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.1     |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=265.06 +/- 124.05
Episode length: 66.68 +/- 31.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.7     |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=236.58 +/- 110.90
Episode length: 59.58 +/- 27.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.6     |
|    mean_reward     | 237      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.7     |
|    ep_rew_mean     | 333      |
| time/              |          |
|    fps             | 168      |
|    iterations      | 83       |
|    time_elapsed    | 1007     |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=169.28 +/- 67.17
Episode length: 42.66 +/- 16.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.7        |
|    mean_reward          | 169         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.022053108 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.411      |
|    explained_variance   | 0.841       |
|    learning_rate        | 0.0001      |
|    loss                 | 155         |
|    n_updates            | 830         |
|    policy_gradient_loss | 0.000255    |
|    value_loss           | 514         |
-----------------------------------------
Eval num_timesteps=170500, episode_reward=161.30 +/- 58.94
Episode length: 40.74 +/- 14.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.7     |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=173.20 +/- 61.76
Episode length: 43.70 +/- 15.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.7     |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=176.58 +/- 63.28
Episode length: 44.54 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=186.14 +/- 59.04
Episode length: 46.96 +/- 14.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.1     |
|    ep_rew_mean     | 303      |
| time/              |          |
|    fps             | 168      |
|    iterations      | 84       |
|    time_elapsed    | 1021     |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=206.96 +/- 96.48
Episode length: 52.16 +/- 24.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.2        |
|    mean_reward          | 207         |
| time/                   |             |
|    total_timesteps      | 172500      |
| train/                  |             |
|    approx_kl            | 0.021301745 |
|    clip_fraction        | 0.0961      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.265      |
|    explained_variance   | 0.419       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.13e+03    |
|    n_updates            | 840         |
|    policy_gradient_loss | 5.12e-05    |
|    value_loss           | 2.06e+03    |
-----------------------------------------
Eval num_timesteps=173000, episode_reward=210.50 +/- 105.70
Episode length: 52.98 +/- 26.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=198.28 +/- 85.47
Episode length: 49.96 +/- 21.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=214.60 +/- 86.22
Episode length: 54.02 +/- 21.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.6     |
|    ep_rew_mean     | 305      |
| time/              |          |
|    fps             | 168      |
|    iterations      | 85       |
|    time_elapsed    | 1033     |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=199.84 +/- 77.37
Episode length: 50.34 +/- 19.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.3        |
|    mean_reward          | 200         |
| time/                   |             |
|    total_timesteps      | 174500      |
| train/                  |             |
|    approx_kl            | 0.026426654 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.398      |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.0001      |
|    loss                 | 340         |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.00324    |
|    value_loss           | 944         |
-----------------------------------------
Eval num_timesteps=175000, episode_reward=198.94 +/- 71.33
Episode length: 50.20 +/- 17.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=188.02 +/- 72.55
Episode length: 47.40 +/- 18.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.4     |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=201.70 +/- 80.47
Episode length: 50.82 +/- 20.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | 202      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.8     |
|    ep_rew_mean     | 298      |
| time/              |          |
|    fps             | 168      |
|    iterations      | 86       |
|    time_elapsed    | 1045     |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=185.22 +/- 75.31
Episode length: 46.66 +/- 18.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.7       |
|    mean_reward          | 185        |
| time/                   |            |
|    total_timesteps      | 176500     |
| train/                  |            |
|    approx_kl            | 0.02353467 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.426     |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0001     |
|    loss                 | 448        |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.00481   |
|    value_loss           | 1.27e+03   |
----------------------------------------
Eval num_timesteps=177000, episode_reward=199.42 +/- 99.00
Episode length: 50.20 +/- 24.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=209.28 +/- 101.11
Episode length: 52.64 +/- 25.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | 209      |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=186.06 +/- 92.10
Episode length: 46.82 +/- 23.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.8     |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77       |
|    ep_rew_mean     | 306      |
| time/              |          |
|    fps             | 168      |
|    iterations      | 87       |
|    time_elapsed    | 1058     |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=263.08 +/- 125.94
Episode length: 66.14 +/- 31.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 66.1        |
|    mean_reward          | 263         |
| time/                   |             |
|    total_timesteps      | 178500      |
| train/                  |             |
|    approx_kl            | 0.015553039 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.471      |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.0001      |
|    loss                 | 474         |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00326    |
|    value_loss           | 1.16e+03    |
-----------------------------------------
Eval num_timesteps=179000, episode_reward=230.78 +/- 104.52
Episode length: 58.08 +/- 26.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.1     |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=218.96 +/- 87.34
Episode length: 55.08 +/- 21.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=240.34 +/- 108.09
Episode length: 60.44 +/- 27.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.4     |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.7     |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 168      |
|    iterations      | 88       |
|    time_elapsed    | 1072     |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=277.54 +/- 130.50
Episode length: 69.76 +/- 32.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 69.8        |
|    mean_reward          | 278         |
| time/                   |             |
|    total_timesteps      | 180500      |
| train/                  |             |
|    approx_kl            | 0.017713517 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.514      |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0001      |
|    loss                 | 275         |
|    n_updates            | 880         |
|    policy_gradient_loss | 0.00683     |
|    value_loss           | 970         |
-----------------------------------------
Eval num_timesteps=181000, episode_reward=264.98 +/- 116.56
Episode length: 66.66 +/- 29.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.7     |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=266.08 +/- 119.65
Episode length: 66.92 +/- 29.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.9     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=251.88 +/- 106.10
Episode length: 63.38 +/- 26.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.4     |
|    mean_reward     | 252      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.7     |
|    ep_rew_mean     | 345      |
| time/              |          |
|    fps             | 167      |
|    iterations      | 89       |
|    time_elapsed    | 1087     |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=222.50 +/- 137.81
Episode length: 56.02 +/- 34.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56          |
|    mean_reward          | 222         |
| time/                   |             |
|    total_timesteps      | 182500      |
| train/                  |             |
|    approx_kl            | 0.017863547 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.475      |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0001      |
|    loss                 | 223         |
|    n_updates            | 890         |
|    policy_gradient_loss | 0.00313     |
|    value_loss           | 707         |
-----------------------------------------
Eval num_timesteps=183000, episode_reward=213.78 +/- 93.65
Episode length: 53.80 +/- 23.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=214.38 +/- 97.58
Episode length: 53.92 +/- 24.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=206.94 +/- 99.18
Episode length: 52.12 +/- 24.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92       |
|    ep_rew_mean     | 366      |
| time/              |          |
|    fps             | 167      |
|    iterations      | 90       |
|    time_elapsed    | 1100     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=180.46 +/- 84.39
Episode length: 45.46 +/- 21.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.5        |
|    mean_reward          | 180         |
| time/                   |             |
|    total_timesteps      | 184500      |
| train/                  |             |
|    approx_kl            | 0.027207889 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.396      |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0001      |
|    loss                 | 217         |
|    n_updates            | 900         |
|    policy_gradient_loss | 0.0108      |
|    value_loss           | 761         |
-----------------------------------------
Eval num_timesteps=185000, episode_reward=202.38 +/- 76.31
Episode length: 50.98 +/- 19.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | 202      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=186.28 +/- 67.18
Episode length: 46.98 +/- 16.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=167.56 +/- 68.75
Episode length: 42.30 +/- 17.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.5     |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 167      |
|    iterations      | 91       |
|    time_elapsed    | 1112     |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=178.94 +/- 65.38
Episode length: 45.12 +/- 16.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45.1        |
|    mean_reward          | 179         |
| time/                   |             |
|    total_timesteps      | 186500      |
| train/                  |             |
|    approx_kl            | 0.023726512 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.404      |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.18e+03    |
|    n_updates            | 910         |
|    policy_gradient_loss | 0.00822     |
|    value_loss           | 1.46e+03    |
-----------------------------------------
Eval num_timesteps=187000, episode_reward=175.54 +/- 81.63
Episode length: 44.24 +/- 20.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.2     |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=166.20 +/- 55.68
Episode length: 41.90 +/- 13.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=165.50 +/- 67.96
Episode length: 41.76 +/- 17.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.9     |
|    ep_rew_mean     | 342      |
| time/              |          |
|    fps             | 167      |
|    iterations      | 92       |
|    time_elapsed    | 1123     |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=209.26 +/- 90.42
Episode length: 52.64 +/- 22.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.6        |
|    mean_reward          | 209         |
| time/                   |             |
|    total_timesteps      | 188500      |
| train/                  |             |
|    approx_kl            | 0.011790212 |
|    clip_fraction        | 0.0772      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.359      |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.09e+03    |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00294    |
|    value_loss           | 1.84e+03    |
-----------------------------------------
Eval num_timesteps=189000, episode_reward=216.18 +/- 97.17
Episode length: 54.46 +/- 24.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=190.32 +/- 92.55
Episode length: 47.96 +/- 23.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=222.32 +/- 110.75
Episode length: 55.98 +/- 27.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 222      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.7     |
|    ep_rew_mean     | 353      |
| time/              |          |
|    fps             | 167      |
|    iterations      | 93       |
|    time_elapsed    | 1135     |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=296.96 +/- 143.01
Episode length: 74.64 +/- 35.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.6        |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 190500      |
| train/                  |             |
|    approx_kl            | 0.009621574 |
|    clip_fraction        | 0.0819      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.427      |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0001      |
|    loss                 | 275         |
|    n_updates            | 930         |
|    policy_gradient_loss | 0.00122     |
|    value_loss           | 770         |
-----------------------------------------
Eval num_timesteps=191000, episode_reward=260.14 +/- 146.64
Episode length: 65.42 +/- 36.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.4     |
|    mean_reward     | 260      |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=254.90 +/- 138.28
Episode length: 64.16 +/- 34.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.2     |
|    mean_reward     | 255      |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=245.84 +/- 124.13
Episode length: 61.82 +/- 31.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.8     |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=276.82 +/- 124.43
Episode length: 69.56 +/- 31.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.6     |
|    mean_reward     | 277      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.6     |
|    ep_rew_mean     | 349      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 94       |
|    time_elapsed    | 1153     |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=230.58 +/- 138.96
Episode length: 58.04 +/- 34.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 58          |
|    mean_reward          | 231         |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.044903535 |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.47       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0001      |
|    loss                 | 469         |
|    n_updates            | 940         |
|    policy_gradient_loss | 0.00505     |
|    value_loss           | 559         |
-----------------------------------------
Eval num_timesteps=193500, episode_reward=253.38 +/- 134.32
Episode length: 63.68 +/- 33.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.7     |
|    mean_reward     | 253      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=267.88 +/- 144.54
Episode length: 67.40 +/- 36.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.4     |
|    mean_reward     | 268      |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=286.08 +/- 135.71
Episode length: 71.94 +/- 33.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.9     |
|    mean_reward     | 286      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.2     |
|    ep_rew_mean     | 347      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 95       |
|    time_elapsed    | 1168     |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=195.80 +/- 68.67
Episode length: 49.24 +/- 17.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.2        |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.019755337 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0001      |
|    loss                 | 400         |
|    n_updates            | 950         |
|    policy_gradient_loss | 0.0113      |
|    value_loss           | 710         |
-----------------------------------------
Eval num_timesteps=195500, episode_reward=181.16 +/- 68.83
Episode length: 45.58 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.6     |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=188.88 +/- 63.72
Episode length: 47.58 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=174.20 +/- 63.48
Episode length: 43.92 +/- 15.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.9     |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.9     |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 96       |
|    time_elapsed    | 1180     |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=206.94 +/- 91.89
Episode length: 52.12 +/- 22.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.1        |
|    mean_reward          | 207         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.022778373 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.449      |
|    explained_variance   | 0.591       |
|    learning_rate        | 0.0001      |
|    loss                 | 472         |
|    n_updates            | 960         |
|    policy_gradient_loss | -0.00481    |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=197500, episode_reward=222.04 +/- 99.82
Episode length: 55.98 +/- 24.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 222      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=218.90 +/- 116.22
Episode length: 55.12 +/- 29.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=194.52 +/- 85.66
Episode length: 49.06 +/- 21.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.9     |
|    ep_rew_mean     | 358      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 97       |
|    time_elapsed    | 1193     |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=192.78 +/- 84.00
Episode length: 48.60 +/- 20.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.6       |
|    mean_reward          | 193        |
| time/                   |            |
|    total_timesteps      | 199000     |
| train/                  |            |
|    approx_kl            | 0.03311527 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.386     |
|    explained_variance   | 0.631      |
|    learning_rate        | 0.0001     |
|    loss                 | 417        |
|    n_updates            | 970        |
|    policy_gradient_loss | 0.0122     |
|    value_loss           | 976        |
----------------------------------------
Eval num_timesteps=199500, episode_reward=215.62 +/- 102.79
Episode length: 54.30 +/- 25.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=196.20 +/- 80.23
Episode length: 49.46 +/- 20.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=207.66 +/- 101.98
Episode length: 52.30 +/- 25.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | 208      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.5     |
|    ep_rew_mean     | 344      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 98       |
|    time_elapsed    | 1205     |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=203.04 +/- 93.55
Episode length: 51.14 +/- 23.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.1       |
|    mean_reward          | 203        |
| time/                   |            |
|    total_timesteps      | 201000     |
| train/                  |            |
|    approx_kl            | 0.02357787 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.538      |
|    learning_rate        | 0.0001     |
|    loss                 | 519        |
|    n_updates            | 980        |
|    policy_gradient_loss | 0.00859    |
|    value_loss           | 1.41e+03   |
----------------------------------------
Eval num_timesteps=201500, episode_reward=179.32 +/- 84.38
Episode length: 45.26 +/- 21.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.3     |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=192.18 +/- 75.22
Episode length: 48.44 +/- 18.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=210.44 +/- 93.89
Episode length: 52.98 +/- 23.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.2     |
|    ep_rew_mean     | 343      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 99       |
|    time_elapsed    | 1218     |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=184.40 +/- 82.88
Episode length: 46.46 +/- 20.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.5       |
|    mean_reward          | 184        |
| time/                   |            |
|    total_timesteps      | 203000     |
| train/                  |            |
|    approx_kl            | 0.01997113 |
|    clip_fraction        | 0.0822     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.448     |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.0001     |
|    loss                 | 497        |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.00509   |
|    value_loss           | 998        |
----------------------------------------
Eval num_timesteps=203500, episode_reward=197.10 +/- 91.66
Episode length: 49.66 +/- 22.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=187.84 +/- 72.74
Episode length: 47.30 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=194.62 +/- 82.73
Episode length: 49.02 +/- 20.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.5     |
|    ep_rew_mean     | 348      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 100      |
|    time_elapsed    | 1230     |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=251.94 +/- 122.14
Episode length: 63.38 +/- 30.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 63.4       |
|    mean_reward          | 252        |
| time/                   |            |
|    total_timesteps      | 205000     |
| train/                  |            |
|    approx_kl            | 0.01577284 |
|    clip_fraction        | 0.0833     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.398     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0001     |
|    loss                 | 484        |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.00343   |
|    value_loss           | 1.23e+03   |
----------------------------------------
Eval num_timesteps=205500, episode_reward=242.60 +/- 100.47
Episode length: 60.96 +/- 25.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61       |
|    mean_reward     | 243      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=235.52 +/- 113.22
Episode length: 59.24 +/- 28.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.2     |
|    mean_reward     | 236      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=227.78 +/- 102.45
Episode length: 57.30 +/- 25.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.3     |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86       |
|    ep_rew_mean     | 342      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 101      |
|    time_elapsed    | 1244     |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=173.02 +/- 73.44
Episode length: 43.66 +/- 18.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.7        |
|    mean_reward          | 173         |
| time/                   |             |
|    total_timesteps      | 207000      |
| train/                  |             |
|    approx_kl            | 0.025942817 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.375      |
|    explained_variance   | 0.856       |
|    learning_rate        | 0.0001      |
|    loss                 | 267         |
|    n_updates            | 1010        |
|    policy_gradient_loss | 0.0101      |
|    value_loss           | 288         |
-----------------------------------------
Eval num_timesteps=207500, episode_reward=190.54 +/- 82.86
Episode length: 48.04 +/- 20.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48       |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=175.28 +/- 69.50
Episode length: 44.20 +/- 17.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.2     |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=171.24 +/- 60.24
Episode length: 43.20 +/- 15.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.2     |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.2     |
|    ep_rew_mean     | 347      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 102      |
|    time_elapsed    | 1255     |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=196.10 +/- 90.88
Episode length: 49.40 +/- 22.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.4        |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 209000      |
| train/                  |             |
|    approx_kl            | 0.022309696 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.368      |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0001      |
|    loss                 | 397         |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00011    |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=209500, episode_reward=176.46 +/- 65.83
Episode length: 44.50 +/- 16.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=203.60 +/- 91.76
Episode length: 51.18 +/- 22.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | 204      |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=191.46 +/- 89.28
Episode length: 48.20 +/- 22.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.2     |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84       |
|    ep_rew_mean     | 334      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 103      |
|    time_elapsed    | 1267     |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=173.34 +/- 69.12
Episode length: 43.66 +/- 17.34
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 43.7      |
|    mean_reward          | 173       |
| time/                   |           |
|    total_timesteps      | 211000    |
| train/                  |           |
|    approx_kl            | 0.0264958 |
|    clip_fraction        | 0.0858    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.333    |
|    explained_variance   | 0.554     |
|    learning_rate        | 0.0001    |
|    loss                 | 620       |
|    n_updates            | 1030      |
|    policy_gradient_loss | -0.00403  |
|    value_loss           | 1.37e+03  |
---------------------------------------
Eval num_timesteps=211500, episode_reward=188.36 +/- 77.38
Episode length: 47.48 +/- 19.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=209.46 +/- 104.97
Episode length: 52.82 +/- 26.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | 209      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=194.24 +/- 80.00
Episode length: 49.00 +/- 20.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.3     |
|    ep_rew_mean     | 340      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 104      |
|    time_elapsed    | 1279     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=199.38 +/- 69.80
Episode length: 50.28 +/- 17.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.3        |
|    mean_reward          | 199         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.019242398 |
|    clip_fraction        | 0.0992      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.389      |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0001      |
|    loss                 | 204         |
|    n_updates            | 1040        |
|    policy_gradient_loss | 0.00364     |
|    value_loss           | 901         |
-----------------------------------------
Eval num_timesteps=213500, episode_reward=201.86 +/- 90.93
Episode length: 50.84 +/- 22.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | 202      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=178.92 +/- 82.51
Episode length: 45.08 +/- 20.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.1     |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=198.88 +/- 72.03
Episode length: 50.18 +/- 17.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=214.70 +/- 90.88
Episode length: 54.10 +/- 22.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.5     |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 105      |
|    time_elapsed    | 1293     |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=229.50 +/- 102.93
Episode length: 57.80 +/- 25.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 57.8        |
|    mean_reward          | 230         |
| time/                   |             |
|    total_timesteps      | 215500      |
| train/                  |             |
|    approx_kl            | 0.011743121 |
|    clip_fraction        | 0.0888      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.375      |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.0001      |
|    loss                 | 525         |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00995    |
|    value_loss           | 1.44e+03    |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=223.76 +/- 96.46
Episode length: 56.32 +/- 24.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.3     |
|    mean_reward     | 224      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=216.52 +/- 96.48
Episode length: 54.56 +/- 24.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=235.14 +/- 114.50
Episode length: 59.14 +/- 28.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.1     |
|    mean_reward     | 235      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.8     |
|    ep_rew_mean     | 342      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 106      |
|    time_elapsed    | 1306     |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=190.08 +/- 71.47
Episode length: 47.94 +/- 17.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47.9       |
|    mean_reward          | 190        |
| time/                   |            |
|    total_timesteps      | 217500     |
| train/                  |            |
|    approx_kl            | 0.09736091 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.397     |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.0001     |
|    loss                 | 134        |
|    n_updates            | 1060       |
|    policy_gradient_loss | 0.0155     |
|    value_loss           | 416        |
----------------------------------------
Eval num_timesteps=218000, episode_reward=200.18 +/- 88.10
Episode length: 50.38 +/- 22.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=220.16 +/- 113.36
Episode length: 55.36 +/- 28.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.4     |
|    mean_reward     | 220      |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=211.44 +/- 81.78
Episode length: 53.26 +/- 20.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | 211      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.2     |
|    ep_rew_mean     | 319      |
| time/              |          |
|    fps             | 166      |
|    iterations      | 107      |
|    time_elapsed    | 1319     |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=216.96 +/- 97.54
Episode length: 54.64 +/- 24.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.6        |
|    mean_reward          | 217         |
| time/                   |             |
|    total_timesteps      | 219500      |
| train/                  |             |
|    approx_kl            | 0.029384423 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.316      |
|    explained_variance   | 0.52        |
|    learning_rate        | 0.0001      |
|    loss                 | 914         |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00366    |
|    value_loss           | 1.48e+03    |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=214.16 +/- 85.77
Episode length: 53.84 +/- 21.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=232.58 +/- 98.36
Episode length: 58.52 +/- 24.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.5     |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=233.28 +/- 81.68
Episode length: 58.72 +/- 20.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.7     |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.1     |
|    ep_rew_mean     | 307      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 108      |
|    time_elapsed    | 1333     |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=206.72 +/- 78.29
Episode length: 52.04 +/- 19.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52         |
|    mean_reward          | 207        |
| time/                   |            |
|    total_timesteps      | 221500     |
| train/                  |            |
|    approx_kl            | 0.02650528 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.383     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.0001     |
|    loss                 | 525        |
|    n_updates            | 1080       |
|    policy_gradient_loss | 0.00364    |
|    value_loss           | 927        |
----------------------------------------
Eval num_timesteps=222000, episode_reward=241.74 +/- 89.14
Episode length: 60.80 +/- 22.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.8     |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=216.72 +/- 105.90
Episode length: 54.50 +/- 26.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=222.46 +/- 79.49
Episode length: 55.96 +/- 19.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 222      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.9     |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 109      |
|    time_elapsed    | 1346     |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=201.62 +/- 67.61
Episode length: 50.72 +/- 16.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.7        |
|    mean_reward          | 202         |
| time/                   |             |
|    total_timesteps      | 223500      |
| train/                  |             |
|    approx_kl            | 0.023651114 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.383      |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0001      |
|    loss                 | 383         |
|    n_updates            | 1090        |
|    policy_gradient_loss | 0.00264     |
|    value_loss           | 856         |
-----------------------------------------
Eval num_timesteps=224000, episode_reward=198.28 +/- 71.10
Episode length: 49.94 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=197.48 +/- 80.14
Episode length: 49.78 +/- 20.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=201.36 +/- 78.23
Episode length: 50.72 +/- 19.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | 201      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 322      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 110      |
|    time_elapsed    | 1358     |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=206.34 +/- 86.98
Episode length: 52.04 +/- 21.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52          |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 0.013638293 |
|    clip_fraction        | 0.0901      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.373      |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0001      |
|    loss                 | 429         |
|    n_updates            | 1100        |
|    policy_gradient_loss | 0.000172    |
|    value_loss           | 1e+03       |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=201.12 +/- 91.99
Episode length: 50.68 +/- 23.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | 201      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=233.50 +/- 101.50
Episode length: 58.78 +/- 25.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.8     |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=207.50 +/- 89.27
Episode length: 52.26 +/- 22.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | 208      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.8     |
|    ep_rew_mean     | 338      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 111      |
|    time_elapsed    | 1371     |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=208.10 +/- 98.84
Episode length: 52.36 +/- 24.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.4        |
|    mean_reward          | 208         |
| time/                   |             |
|    total_timesteps      | 227500      |
| train/                  |             |
|    approx_kl            | 0.016615443 |
|    clip_fraction        | 0.0858      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.382      |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0001      |
|    loss                 | 303         |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00269    |
|    value_loss           | 854         |
-----------------------------------------
Eval num_timesteps=228000, episode_reward=237.16 +/- 93.47
Episode length: 59.68 +/- 23.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.7     |
|    mean_reward     | 237      |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=200.40 +/- 83.25
Episode length: 50.48 +/- 20.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=235.24 +/- 109.64
Episode length: 59.22 +/- 27.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.2     |
|    mean_reward     | 235      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.5     |
|    ep_rew_mean     | 344      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 112      |
|    time_elapsed    | 1384     |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=185.30 +/- 63.13
Episode length: 46.68 +/- 15.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.7        |
|    mean_reward          | 185         |
| time/                   |             |
|    total_timesteps      | 229500      |
| train/                  |             |
|    approx_kl            | 0.013628245 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0001      |
|    loss                 | 447         |
|    n_updates            | 1120        |
|    policy_gradient_loss | 0.00223     |
|    value_loss           | 782         |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=171.02 +/- 65.71
Episode length: 43.12 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.1     |
|    mean_reward     | 171      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=188.74 +/- 61.79
Episode length: 47.54 +/- 15.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=182.66 +/- 58.60
Episode length: 46.02 +/- 14.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46       |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.8     |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 113      |
|    time_elapsed    | 1396     |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=221.72 +/- 87.25
Episode length: 55.80 +/- 21.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.8        |
|    mean_reward          | 222         |
| time/                   |             |
|    total_timesteps      | 231500      |
| train/                  |             |
|    approx_kl            | 0.012544572 |
|    clip_fraction        | 0.0784      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.27       |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0001      |
|    loss                 | 652         |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00346    |
|    value_loss           | 1.26e+03    |
-----------------------------------------
Eval num_timesteps=232000, episode_reward=198.10 +/- 73.92
Episode length: 49.86 +/- 18.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.9     |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=186.68 +/- 61.99
Episode length: 47.08 +/- 15.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=209.56 +/- 84.66
Episode length: 52.78 +/- 21.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.4     |
|    ep_rew_mean     | 320      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 114      |
|    time_elapsed    | 1408     |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=192.54 +/- 75.61
Episode length: 48.50 +/- 18.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.5       |
|    mean_reward          | 193        |
| time/                   |            |
|    total_timesteps      | 233500     |
| train/                  |            |
|    approx_kl            | 0.02576131 |
|    clip_fraction        | 0.0854     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.328     |
|    explained_variance   | 0.467      |
|    learning_rate        | 0.0001     |
|    loss                 | 534        |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.00435   |
|    value_loss           | 1.65e+03   |
----------------------------------------
Eval num_timesteps=234000, episode_reward=195.94 +/- 82.04
Episode length: 49.30 +/- 20.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.3     |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=203.40 +/- 87.50
Episode length: 51.30 +/- 21.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.3     |
|    mean_reward     | 203      |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=231.00 +/- 92.90
Episode length: 58.10 +/- 23.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.1     |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=203.68 +/- 71.11
Episode length: 51.38 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | 204      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.9     |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 115      |
|    time_elapsed    | 1423     |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=178.10 +/- 64.77
Episode length: 44.96 +/- 16.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 45          |
|    mean_reward          | 178         |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.016887998 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.366      |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0001      |
|    loss                 | 355         |
|    n_updates            | 1150        |
|    policy_gradient_loss | 0.00362     |
|    value_loss           | 780         |
-----------------------------------------
Eval num_timesteps=236500, episode_reward=174.12 +/- 73.14
Episode length: 43.88 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.9     |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=172.22 +/- 59.67
Episode length: 43.42 +/- 14.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.4     |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=177.48 +/- 76.90
Episode length: 44.72 +/- 19.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.7     |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.1     |
|    ep_rew_mean     | 327      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 116      |
|    time_elapsed    | 1434     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=234.62 +/- 96.39
Episode length: 59.02 +/- 24.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 59          |
|    mean_reward          | 235         |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.015805887 |
|    clip_fraction        | 0.0838      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.353      |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0001      |
|    loss                 | 282         |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.000871   |
|    value_loss           | 778         |
-----------------------------------------
Eval num_timesteps=238500, episode_reward=226.52 +/- 93.39
Episode length: 56.98 +/- 23.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57       |
|    mean_reward     | 227      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=214.68 +/- 81.08
Episode length: 54.08 +/- 20.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=213.06 +/- 79.93
Episode length: 53.70 +/- 19.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83       |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 117      |
|    time_elapsed    | 1447     |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=237.34 +/- 103.60
Episode length: 59.70 +/- 25.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 59.7        |
|    mean_reward          | 237         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.009713089 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.352      |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0001      |
|    loss                 | 267         |
|    n_updates            | 1170        |
|    policy_gradient_loss | -0.00105    |
|    value_loss           | 839         |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=283.04 +/- 124.06
Episode length: 71.12 +/- 30.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.1     |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=230.78 +/- 109.47
Episode length: 58.02 +/- 27.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58       |
|    mean_reward     | 231      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=257.72 +/- 119.87
Episode length: 64.88 +/- 29.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.9     |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83       |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 118      |
|    time_elapsed    | 1462     |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=290.08 +/- 135.71
Episode length: 72.90 +/- 33.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.9        |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.015383352 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.452      |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.0001      |
|    loss                 | 556         |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.007      |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=242500, episode_reward=306.94 +/- 108.00
Episode length: 77.10 +/- 27.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.1     |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=272.60 +/- 117.22
Episode length: 68.60 +/- 29.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.6     |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=320.66 +/- 134.95
Episode length: 80.58 +/- 33.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.6     |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.9     |
|    ep_rew_mean     | 334      |
| time/              |          |
|    fps             | 164      |
|    iterations      | 119      |
|    time_elapsed    | 1478     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=224.22 +/- 92.22
Episode length: 56.40 +/- 23.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.4        |
|    mean_reward          | 224         |
| time/                   |             |
|    total_timesteps      | 244000      |
| train/                  |             |
|    approx_kl            | 0.020994263 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.378      |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0001      |
|    loss                 | 155         |
|    n_updates            | 1190        |
|    policy_gradient_loss | 0.000999    |
|    value_loss           | 450         |
-----------------------------------------
Eval num_timesteps=244500, episode_reward=228.92 +/- 98.80
Episode length: 57.60 +/- 24.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.6     |
|    mean_reward     | 229      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=241.18 +/- 108.80
Episode length: 60.68 +/- 27.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.7     |
|    mean_reward     | 241      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=220.96 +/- 101.43
Episode length: 55.60 +/- 25.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | 221      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.5     |
|    ep_rew_mean     | 336      |
| time/              |          |
|    fps             | 164      |
|    iterations      | 120      |
|    time_elapsed    | 1492     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=218.54 +/- 105.68
Episode length: 55.06 +/- 26.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.1        |
|    mean_reward          | 219         |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.020523869 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.407      |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0001      |
|    loss                 | 414         |
|    n_updates            | 1200        |
|    policy_gradient_loss | -0.00772    |
|    value_loss           | 740         |
-----------------------------------------
Eval num_timesteps=246500, episode_reward=217.16 +/- 112.09
Episode length: 54.64 +/- 28.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=208.98 +/- 86.75
Episode length: 52.58 +/- 21.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | 209      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=233.92 +/- 99.74
Episode length: 58.80 +/- 24.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.8     |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.3     |
|    ep_rew_mean     | 340      |
| time/              |          |
|    fps             | 164      |
|    iterations      | 121      |
|    time_elapsed    | 1505     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=279.12 +/- 105.18
Episode length: 70.18 +/- 26.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 70.2        |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.038049765 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.44       |
|    explained_variance   | 0.585       |
|    learning_rate        | 0.0001      |
|    loss                 | 346         |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.00818    |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=248500, episode_reward=257.48 +/- 99.75
Episode length: 64.64 +/- 25.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.6     |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=278.34 +/- 124.57
Episode length: 69.94 +/- 31.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.9     |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=269.78 +/- 102.53
Episode length: 67.80 +/- 25.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.8     |
|    mean_reward     | 270      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91       |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 164      |
|    iterations      | 122      |
|    time_elapsed    | 1520     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=204.72 +/- 82.26
Episode length: 51.64 +/- 20.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.6        |
|    mean_reward          | 205         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.020608328 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.363      |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0001      |
|    loss                 | 114         |
|    n_updates            | 1220        |
|    policy_gradient_loss | 0.0141      |
|    value_loss           | 375         |
-----------------------------------------
Eval num_timesteps=250500, episode_reward=222.66 +/- 80.91
Episode length: 56.04 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=183.18 +/- 71.16
Episode length: 46.16 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=198.40 +/- 86.57
Episode length: 50.06 +/- 21.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.2     |
|    ep_rew_mean     | 351      |
| time/              |          |
|    fps             | 164      |
|    iterations      | 123      |
|    time_elapsed    | 1533     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=213.68 +/- 92.28
Episode length: 53.86 +/- 23.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.9        |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 0.027031817 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.403      |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0001      |
|    loss                 | 442         |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00568    |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=252500, episode_reward=219.52 +/- 72.60
Episode length: 55.30 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.3     |
|    mean_reward     | 220      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=229.90 +/- 106.34
Episode length: 57.88 +/- 26.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.9     |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=225.70 +/- 111.99
Episode length: 56.82 +/- 28.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.8     |
|    mean_reward     | 226      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.4     |
|    ep_rew_mean     | 348      |
| time/              |          |
|    fps             | 164      |
|    iterations      | 124      |
|    time_elapsed    | 1546     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=306.28 +/- 110.09
Episode length: 76.92 +/- 27.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.9        |
|    mean_reward          | 306         |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.033418193 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.363      |
|    explained_variance   | 0.633       |
|    learning_rate        | 0.0001      |
|    loss                 | 491         |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00612    |
|    value_loss           | 1.33e+03    |
-----------------------------------------
Eval num_timesteps=254500, episode_reward=336.18 +/- 114.85
Episode length: 84.34 +/- 28.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.3     |
|    mean_reward     | 336      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
New best mean reward!
Eval num_timesteps=255000, episode_reward=269.24 +/- 101.51
Episode length: 67.68 +/- 25.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.7     |
|    mean_reward     | 269      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=288.80 +/- 111.86
Episode length: 72.60 +/- 27.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.6     |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=282.94 +/- 119.89
Episode length: 71.04 +/- 29.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71       |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.8     |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 125      |
|    time_elapsed    | 1566     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=203.22 +/- 86.73
Episode length: 51.16 +/- 21.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.2        |
|    mean_reward          | 203         |
| time/                   |             |
|    total_timesteps      | 256500      |
| train/                  |             |
|    approx_kl            | 0.025634926 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.384      |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0001      |
|    loss                 | 382         |
|    n_updates            | 1250        |
|    policy_gradient_loss | 0.0142      |
|    value_loss           | 940         |
-----------------------------------------
Eval num_timesteps=257000, episode_reward=255.76 +/- 104.35
Episode length: 64.28 +/- 26.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.3     |
|    mean_reward     | 256      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=240.88 +/- 102.40
Episode length: 60.60 +/- 25.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.6     |
|    mean_reward     | 241      |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=179.32 +/- 68.53
Episode length: 45.24 +/- 17.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.2     |
|    mean_reward     | 179      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87       |
|    ep_rew_mean     | 346      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 126      |
|    time_elapsed    | 1579     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=274.10 +/- 131.26
Episode length: 68.88 +/- 32.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.9        |
|    mean_reward          | 274         |
| time/                   |             |
|    total_timesteps      | 258500      |
| train/                  |             |
|    approx_kl            | 0.019582769 |
|    clip_fraction        | 0.0927      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.397      |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0001      |
|    loss                 | 474         |
|    n_updates            | 1260        |
|    policy_gradient_loss | 0.000625    |
|    value_loss           | 845         |
-----------------------------------------
Eval num_timesteps=259000, episode_reward=283.46 +/- 110.00
Episode length: 71.26 +/- 27.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.3     |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=273.24 +/- 103.75
Episode length: 68.66 +/- 25.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.7     |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=287.12 +/- 107.01
Episode length: 72.14 +/- 26.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.1     |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.2     |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 127      |
|    time_elapsed    | 1595     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=271.86 +/- 139.80
Episode length: 68.34 +/- 34.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.3        |
|    mean_reward          | 272         |
| time/                   |             |
|    total_timesteps      | 260500      |
| train/                  |             |
|    approx_kl            | 0.009922577 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.393      |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0001      |
|    loss                 | 429         |
|    n_updates            | 1270        |
|    policy_gradient_loss | -0.00611    |
|    value_loss           | 806         |
-----------------------------------------
Eval num_timesteps=261000, episode_reward=272.88 +/- 132.30
Episode length: 68.58 +/- 33.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.6     |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=267.92 +/- 136.27
Episode length: 67.36 +/- 34.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.4     |
|    mean_reward     | 268      |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=263.88 +/- 137.09
Episode length: 66.34 +/- 34.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.3     |
|    mean_reward     | 264      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.7     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 128      |
|    time_elapsed    | 1610     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=285.20 +/- 126.05
Episode length: 71.68 +/- 31.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 71.7        |
|    mean_reward          | 285         |
| time/                   |             |
|    total_timesteps      | 262500      |
| train/                  |             |
|    approx_kl            | 0.014073286 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.358      |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0001      |
|    loss                 | 429         |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00367    |
|    value_loss           | 1.06e+03    |
-----------------------------------------
Eval num_timesteps=263000, episode_reward=276.36 +/- 113.41
Episode length: 69.48 +/- 28.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.5     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=311.60 +/- 127.23
Episode length: 78.26 +/- 31.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.3     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=273.60 +/- 111.39
Episode length: 68.74 +/- 27.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.7     |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.1     |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 129      |
|    time_elapsed    | 1627     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=192.34 +/- 79.98
Episode length: 48.42 +/- 19.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.4        |
|    mean_reward          | 192         |
| time/                   |             |
|    total_timesteps      | 264500      |
| train/                  |             |
|    approx_kl            | 0.026937261 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.39       |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.0001      |
|    loss                 | 207         |
|    n_updates            | 1290        |
|    policy_gradient_loss | 0.00765     |
|    value_loss           | 454         |
-----------------------------------------
Eval num_timesteps=265000, episode_reward=192.64 +/- 88.03
Episode length: 48.58 +/- 22.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.6     |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=218.10 +/- 91.51
Episode length: 54.84 +/- 22.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.8     |
|    mean_reward     | 218      |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=194.22 +/- 79.33
Episode length: 48.92 +/- 19.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.3     |
|    ep_rew_mean     | 356      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 130      |
|    time_elapsed    | 1639     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=297.66 +/- 126.77
Episode length: 74.76 +/- 31.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.8        |
|    mean_reward          | 298         |
| time/                   |             |
|    total_timesteps      | 266500      |
| train/                  |             |
|    approx_kl            | 0.019837096 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.348      |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0001      |
|    loss                 | 472         |
|    n_updates            | 1300        |
|    policy_gradient_loss | 0.00216     |
|    value_loss           | 1.3e+03     |
-----------------------------------------
Eval num_timesteps=267000, episode_reward=250.44 +/- 101.56
Episode length: 62.94 +/- 25.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.9     |
|    mean_reward     | 250      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=248.18 +/- 95.94
Episode length: 62.38 +/- 24.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.4     |
|    mean_reward     | 248      |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=281.20 +/- 116.29
Episode length: 70.76 +/- 29.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.8     |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83       |
|    ep_rew_mean     | 331      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 131      |
|    time_elapsed    | 1654     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=295.42 +/- 104.16
Episode length: 74.30 +/- 26.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.3        |
|    mean_reward          | 295         |
| time/                   |             |
|    total_timesteps      | 268500      |
| train/                  |             |
|    approx_kl            | 0.019601334 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.399      |
|    explained_variance   | 0.511       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.07e+03    |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00375    |
|    value_loss           | 1.73e+03    |
-----------------------------------------
Eval num_timesteps=269000, episode_reward=294.06 +/- 110.39
Episode length: 73.90 +/- 27.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=282.12 +/- 102.93
Episode length: 70.90 +/- 25.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.9     |
|    mean_reward     | 282      |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=266.40 +/- 119.07
Episode length: 66.98 +/- 29.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67       |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.3     |
|    ep_rew_mean     | 344      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 132      |
|    time_elapsed    | 1670     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=196.94 +/- 66.98
Episode length: 49.58 +/- 16.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.6        |
|    mean_reward          | 197         |
| time/                   |             |
|    total_timesteps      | 270500      |
| train/                  |             |
|    approx_kl            | 0.019254258 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.35       |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0001      |
|    loss                 | 297         |
|    n_updates            | 1320        |
|    policy_gradient_loss | 0.00702     |
|    value_loss           | 813         |
-----------------------------------------
Eval num_timesteps=271000, episode_reward=193.24 +/- 81.98
Episode length: 48.74 +/- 20.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.7     |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=201.20 +/- 93.08
Episode length: 50.68 +/- 23.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.7     |
|    mean_reward     | 201      |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=199.42 +/- 94.47
Episode length: 50.24 +/- 23.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.2     |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.7     |
|    ep_rew_mean     | 334      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 133      |
|    time_elapsed    | 1682     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=255.98 +/- 120.43
Episode length: 64.38 +/- 30.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 64.4        |
|    mean_reward          | 256         |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 0.015779927 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.378      |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.0001      |
|    loss                 | 519         |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00247    |
|    value_loss           | 1.25e+03    |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=266.44 +/- 111.93
Episode length: 66.92 +/- 27.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.9     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=266.20 +/- 113.63
Episode length: 66.88 +/- 28.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.9     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=277.00 +/- 149.20
Episode length: 69.56 +/- 37.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.6     |
|    mean_reward     | 277      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.6     |
|    ep_rew_mean     | 349      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 134      |
|    time_elapsed    | 1697     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=224.52 +/- 98.35
Episode length: 56.52 +/- 24.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.5        |
|    mean_reward          | 225         |
| time/                   |             |
|    total_timesteps      | 274500      |
| train/                  |             |
|    approx_kl            | 0.019675229 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.411      |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0001      |
|    loss                 | 190         |
|    n_updates            | 1340        |
|    policy_gradient_loss | 0.00163     |
|    value_loss           | 659         |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=220.58 +/- 75.56
Episode length: 55.60 +/- 18.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | 221      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=216.82 +/- 103.17
Episode length: 54.58 +/- 25.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=221.94 +/- 94.70
Episode length: 55.88 +/- 23.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.9     |
|    mean_reward     | 222      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89       |
|    ep_rew_mean     | 354      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 135      |
|    time_elapsed    | 1710     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=220.98 +/- 95.07
Episode length: 55.62 +/- 23.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.6        |
|    mean_reward          | 221         |
| time/                   |             |
|    total_timesteps      | 276500      |
| train/                  |             |
|    approx_kl            | 0.024323769 |
|    clip_fraction        | 0.0983      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.37       |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0001      |
|    loss                 | 806         |
|    n_updates            | 1350        |
|    policy_gradient_loss | 0.002       |
|    value_loss           | 935         |
-----------------------------------------
Eval num_timesteps=277000, episode_reward=253.02 +/- 116.38
Episode length: 63.60 +/- 29.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.6     |
|    mean_reward     | 253      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=234.30 +/- 109.59
Episode length: 58.98 +/- 27.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59       |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=242.98 +/- 109.58
Episode length: 61.14 +/- 27.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.1     |
|    mean_reward     | 243      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=214.12 +/- 99.19
Episode length: 54.00 +/- 24.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.5     |
|    ep_rew_mean     | 353      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 136      |
|    time_elapsed    | 1726     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=274.54 +/- 122.36
Episode length: 69.04 +/- 30.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 69          |
|    mean_reward          | 275         |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.026130494 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.417      |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.0001      |
|    loss                 | 612         |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.00401    |
|    value_loss           | 1.15e+03    |
-----------------------------------------
Eval num_timesteps=279500, episode_reward=301.78 +/- 132.22
Episode length: 75.84 +/- 33.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.8     |
|    mean_reward     | 302      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=293.00 +/- 121.75
Episode length: 73.64 +/- 30.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.6     |
|    mean_reward     | 293      |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=304.54 +/- 119.62
Episode length: 76.64 +/- 29.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 305      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92       |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 137      |
|    time_elapsed    | 1743     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=274.00 +/- 125.85
Episode length: 68.84 +/- 31.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.8        |
|    mean_reward          | 274         |
| time/                   |             |
|    total_timesteps      | 281000      |
| train/                  |             |
|    approx_kl            | 0.016175203 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.441      |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0001      |
|    loss                 | 379         |
|    n_updates            | 1370        |
|    policy_gradient_loss | 0.00373     |
|    value_loss           | 833         |
-----------------------------------------
Eval num_timesteps=281500, episode_reward=234.08 +/- 125.27
Episode length: 58.96 +/- 31.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59       |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=281.20 +/- 139.74
Episode length: 70.66 +/- 34.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.7     |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=266.02 +/- 130.22
Episode length: 66.88 +/- 32.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.9     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.7     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 138      |
|    time_elapsed    | 1758     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=256.00 +/- 141.37
Episode length: 64.42 +/- 35.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 64.4       |
|    mean_reward          | 256        |
| time/                   |            |
|    total_timesteps      | 283000     |
| train/                  |            |
|    approx_kl            | 0.02985795 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.379     |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.0001     |
|    loss                 | 686        |
|    n_updates            | 1380       |
|    policy_gradient_loss | 0.00395    |
|    value_loss           | 1.1e+03    |
----------------------------------------
Eval num_timesteps=283500, episode_reward=265.84 +/- 127.95
Episode length: 66.78 +/- 32.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.8     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=229.14 +/- 125.01
Episode length: 57.66 +/- 31.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.7     |
|    mean_reward     | 229      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=248.42 +/- 97.94
Episode length: 62.48 +/- 24.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.5     |
|    mean_reward     | 248      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.7     |
|    ep_rew_mean     | 377      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 139      |
|    time_elapsed    | 1772     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=230.70 +/- 111.96
Episode length: 58.02 +/- 28.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 58          |
|    mean_reward          | 231         |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.017118862 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.382      |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0001      |
|    loss                 | 345         |
|    n_updates            | 1390        |
|    policy_gradient_loss | 0.0164      |
|    value_loss           | 777         |
-----------------------------------------
Eval num_timesteps=285500, episode_reward=232.30 +/- 105.64
Episode length: 58.42 +/- 26.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.4     |
|    mean_reward     | 232      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=229.76 +/- 113.75
Episode length: 57.80 +/- 28.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.8     |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=240.64 +/- 132.54
Episode length: 60.52 +/- 33.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.5     |
|    mean_reward     | 241      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.9     |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 140      |
|    time_elapsed    | 1785     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=246.22 +/- 104.66
Episode length: 61.94 +/- 26.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 61.9        |
|    mean_reward          | 246         |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 0.021802235 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.332      |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0001      |
|    loss                 | 445         |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00737    |
|    value_loss           | 969         |
-----------------------------------------
Eval num_timesteps=287500, episode_reward=286.24 +/- 133.48
Episode length: 71.98 +/- 33.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72       |
|    mean_reward     | 286      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=259.12 +/- 123.67
Episode length: 65.18 +/- 30.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.2     |
|    mean_reward     | 259      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=265.96 +/- 130.13
Episode length: 66.82 +/- 32.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.8     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.3     |
|    ep_rew_mean     | 380      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 141      |
|    time_elapsed    | 1800     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=206.50 +/- 101.04
Episode length: 51.98 +/- 25.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52          |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.018063303 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.369      |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.0001      |
|    loss                 | 910         |
|    n_updates            | 1410        |
|    policy_gradient_loss | 0.00451     |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=289500, episode_reward=227.14 +/- 85.87
Episode length: 57.16 +/- 21.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.2     |
|    mean_reward     | 227      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=210.98 +/- 87.91
Episode length: 53.10 +/- 22.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | 211      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=241.82 +/- 116.19
Episode length: 60.90 +/- 29.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.9     |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.2     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 142      |
|    time_elapsed    | 1814     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=257.84 +/- 132.75
Episode length: 64.76 +/- 33.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 64.8        |
|    mean_reward          | 258         |
| time/                   |             |
|    total_timesteps      | 291000      |
| train/                  |             |
|    approx_kl            | 0.025750576 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.35       |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.0001      |
|    loss                 | 589         |
|    n_updates            | 1420        |
|    policy_gradient_loss | 0.00531     |
|    value_loss           | 1.23e+03    |
-----------------------------------------
Eval num_timesteps=291500, episode_reward=268.14 +/- 115.25
Episode length: 67.48 +/- 28.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.5     |
|    mean_reward     | 268      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=267.06 +/- 128.60
Episode length: 67.16 +/- 32.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.2     |
|    mean_reward     | 267      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=264.82 +/- 120.31
Episode length: 66.54 +/- 30.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.5     |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.5     |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 143      |
|    time_elapsed    | 1828     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=264.60 +/- 108.87
Episode length: 66.52 +/- 27.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 66.5        |
|    mean_reward          | 265         |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.018906062 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.381      |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0001      |
|    loss                 | 214         |
|    n_updates            | 1430        |
|    policy_gradient_loss | -0.000924   |
|    value_loss           | 609         |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=281.00 +/- 113.21
Episode length: 70.56 +/- 28.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.6     |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=285.82 +/- 135.85
Episode length: 71.78 +/- 33.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.8     |
|    mean_reward     | 286      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=287.92 +/- 131.08
Episode length: 72.34 +/- 32.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.3     |
|    ep_rew_mean     | 344      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 144      |
|    time_elapsed    | 1844     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=293.22 +/- 120.55
Episode length: 73.62 +/- 30.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73.6        |
|    mean_reward          | 293         |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.025836743 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.425      |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0001      |
|    loss                 | 297         |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 920         |
-----------------------------------------
Eval num_timesteps=295500, episode_reward=312.36 +/- 125.56
Episode length: 78.40 +/- 31.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=289.84 +/- 129.09
Episode length: 72.86 +/- 32.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.9     |
|    mean_reward     | 290      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=314.46 +/- 127.56
Episode length: 79.02 +/- 31.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79       |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.7     |
|    ep_rew_mean     | 345      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 145      |
|    time_elapsed    | 1861     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=319.60 +/- 137.26
Episode length: 80.32 +/- 34.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.3        |
|    mean_reward          | 320         |
| time/                   |             |
|    total_timesteps      | 297000      |
| train/                  |             |
|    approx_kl            | 0.021152847 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.404      |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0001      |
|    loss                 | 390         |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00337    |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=297500, episode_reward=320.70 +/- 123.36
Episode length: 80.54 +/- 30.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.5     |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=306.44 +/- 134.23
Episode length: 77.00 +/- 33.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77       |
|    mean_reward     | 306      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=328.40 +/- 130.35
Episode length: 82.46 +/- 32.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.5     |
|    mean_reward     | 328      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=316.58 +/- 121.99
Episode length: 79.56 +/- 30.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.6     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.5     |
|    ep_rew_mean     | 357      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 146      |
|    time_elapsed    | 1882     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=305.98 +/- 141.12
Episode length: 76.88 +/- 35.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.9        |
|    mean_reward          | 306         |
| time/                   |             |
|    total_timesteps      | 299500      |
| train/                  |             |
|    approx_kl            | 0.013485914 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.388      |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0001      |
|    loss                 | 401         |
|    n_updates            | 1460        |
|    policy_gradient_loss | 0.00348     |
|    value_loss           | 807         |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=328.82 +/- 135.27
Episode length: 82.56 +/- 33.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.6     |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=299.98 +/- 111.27
Episode length: 75.32 +/- 27.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.3     |
|    mean_reward     | 300      |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=293.86 +/- 134.74
Episode length: 73.86 +/- 33.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.5     |
|    ep_rew_mean     | 356      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 147      |
|    time_elapsed    | 1898     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=222.74 +/- 90.67
Episode length: 56.02 +/- 22.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56          |
|    mean_reward          | 223         |
| time/                   |             |
|    total_timesteps      | 301500      |
| train/                  |             |
|    approx_kl            | 0.020416563 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.392      |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0001      |
|    loss                 | 346         |
|    n_updates            | 1470        |
|    policy_gradient_loss | 0.00247     |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=302000, episode_reward=240.18 +/- 110.13
Episode length: 60.44 +/- 27.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.4     |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=235.06 +/- 91.67
Episode length: 59.06 +/- 22.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.1     |
|    mean_reward     | 235      |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=262.22 +/- 110.69
Episode length: 65.90 +/- 27.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.9     |
|    mean_reward     | 262      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88       |
|    ep_rew_mean     | 351      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 148      |
|    time_elapsed    | 1913     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=214.20 +/- 88.35
Episode length: 53.90 +/- 22.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.9        |
|    mean_reward          | 214         |
| time/                   |             |
|    total_timesteps      | 303500      |
| train/                  |             |
|    approx_kl            | 0.023158036 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.32       |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.0001      |
|    loss                 | 579         |
|    n_updates            | 1480        |
|    policy_gradient_loss | 0.00908     |
|    value_loss           | 1.21e+03    |
-----------------------------------------
Eval num_timesteps=304000, episode_reward=239.74 +/- 108.26
Episode length: 60.32 +/- 27.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.3     |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=218.22 +/- 88.04
Episode length: 54.92 +/- 22.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.9     |
|    mean_reward     | 218      |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=233.30 +/- 94.94
Episode length: 58.74 +/- 23.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.7     |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.5     |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 149      |
|    time_elapsed    | 1926     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=234.02 +/- 99.91
Episode length: 58.94 +/- 24.98
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 58.9     |
|    mean_reward          | 234      |
| time/                   |          |
|    total_timesteps      | 305500   |
| train/                  |          |
|    approx_kl            | 0.040551 |
|    clip_fraction        | 0.113    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.263   |
|    explained_variance   | 0.563    |
|    learning_rate        | 0.0001   |
|    loss                 | 518      |
|    n_updates            | 1490     |
|    policy_gradient_loss | -0.00136 |
|    value_loss           | 1.37e+03 |
--------------------------------------
Eval num_timesteps=306000, episode_reward=232.16 +/- 99.30
Episode length: 58.40 +/- 24.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.4     |
|    mean_reward     | 232      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=236.04 +/- 96.87
Episode length: 59.40 +/- 24.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.4     |
|    mean_reward     | 236      |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=231.86 +/- 103.03
Episode length: 58.28 +/- 25.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.3     |
|    mean_reward     | 232      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.4     |
|    ep_rew_mean     | 308      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 150      |
|    time_elapsed    | 1940     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=222.82 +/- 106.71
Episode length: 56.10 +/- 26.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.1        |
|    mean_reward          | 223         |
| time/                   |             |
|    total_timesteps      | 307500      |
| train/                  |             |
|    approx_kl            | 0.028653746 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.293      |
|    explained_variance   | 0.408       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.07e+03    |
|    n_updates            | 1500        |
|    policy_gradient_loss | 0.00734     |
|    value_loss           | 1.83e+03    |
-----------------------------------------
Eval num_timesteps=308000, episode_reward=229.38 +/- 99.47
Episode length: 57.68 +/- 24.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.7     |
|    mean_reward     | 229      |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=220.84 +/- 87.46
Episode length: 55.58 +/- 21.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | 221      |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=224.42 +/- 85.64
Episode length: 56.56 +/- 21.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.6     |
|    mean_reward     | 224      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 76.7     |
|    ep_rew_mean     | 305      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 151      |
|    time_elapsed    | 1953     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=204.32 +/- 101.76
Episode length: 51.44 +/- 25.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.4        |
|    mean_reward          | 204         |
| time/                   |             |
|    total_timesteps      | 309500      |
| train/                  |             |
|    approx_kl            | 0.029228956 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.31       |
|    explained_variance   | 0.504       |
|    learning_rate        | 0.0001      |
|    loss                 | 391         |
|    n_updates            | 1510        |
|    policy_gradient_loss | 0.000846    |
|    value_loss           | 1.12e+03    |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=194.72 +/- 83.25
Episode length: 49.04 +/- 20.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=196.70 +/- 84.42
Episode length: 49.52 +/- 21.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=208.08 +/- 103.62
Episode length: 52.40 +/- 25.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | 208      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 75.9     |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 152      |
|    time_elapsed    | 1965     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=187.54 +/- 68.92
Episode length: 47.28 +/- 17.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.3        |
|    mean_reward          | 188         |
| time/                   |             |
|    total_timesteps      | 311500      |
| train/                  |             |
|    approx_kl            | 0.032174762 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.332      |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.0001      |
|    loss                 | 903         |
|    n_updates            | 1520        |
|    policy_gradient_loss | -0.000834   |
|    value_loss           | 1.38e+03    |
-----------------------------------------
Eval num_timesteps=312000, episode_reward=206.10 +/- 71.54
Episode length: 51.86 +/- 17.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | 206      |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=204.20 +/- 92.02
Episode length: 51.44 +/- 22.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | 204      |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=199.84 +/- 83.26
Episode length: 50.42 +/- 20.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.4     |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.8     |
|    ep_rew_mean     | 298      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 153      |
|    time_elapsed    | 1977     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=245.82 +/- 85.27
Episode length: 61.86 +/- 21.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 61.9        |
|    mean_reward          | 246         |
| time/                   |             |
|    total_timesteps      | 313500      |
| train/                  |             |
|    approx_kl            | 0.030282073 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.292      |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.0001      |
|    loss                 | 591         |
|    n_updates            | 1530        |
|    policy_gradient_loss | -0.00609    |
|    value_loss           | 1.41e+03    |
-----------------------------------------
Eval num_timesteps=314000, episode_reward=224.14 +/- 79.78
Episode length: 56.36 +/- 19.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.4     |
|    mean_reward     | 224      |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=268.28 +/- 98.02
Episode length: 67.44 +/- 24.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.4     |
|    mean_reward     | 268      |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=256.62 +/- 102.74
Episode length: 64.54 +/- 25.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.5     |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.9     |
|    ep_rew_mean     | 322      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 154      |
|    time_elapsed    | 1992     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=213.18 +/- 102.68
Episode length: 53.66 +/- 25.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.7        |
|    mean_reward          | 213         |
| time/                   |             |
|    total_timesteps      | 315500      |
| train/                  |             |
|    approx_kl            | 0.023309615 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.387      |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0001      |
|    loss                 | 250         |
|    n_updates            | 1540        |
|    policy_gradient_loss | 0.00715     |
|    value_loss           | 709         |
-----------------------------------------
Eval num_timesteps=316000, episode_reward=215.76 +/- 94.73
Episode length: 54.32 +/- 23.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=219.44 +/- 89.25
Episode length: 55.18 +/- 22.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=206.94 +/- 99.88
Episode length: 52.16 +/- 25.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.3     |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 155      |
|    time_elapsed    | 2004     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=186.04 +/- 73.63
Episode length: 46.84 +/- 18.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.8        |
|    mean_reward          | 186         |
| time/                   |             |
|    total_timesteps      | 317500      |
| train/                  |             |
|    approx_kl            | 0.013278887 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.375      |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0001      |
|    loss                 | 395         |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00326    |
|    value_loss           | 638         |
-----------------------------------------
Eval num_timesteps=318000, episode_reward=188.88 +/- 93.68
Episode length: 47.58 +/- 23.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=208.04 +/- 86.63
Episode length: 52.42 +/- 21.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.4     |
|    mean_reward     | 208      |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=202.78 +/- 90.48
Episode length: 51.04 +/- 22.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | 203      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.2     |
|    ep_rew_mean     | 336      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 156      |
|    time_elapsed    | 2016     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=210.70 +/- 88.63
Episode length: 53.08 +/- 22.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.1        |
|    mean_reward          | 211         |
| time/                   |             |
|    total_timesteps      | 319500      |
| train/                  |             |
|    approx_kl            | 0.022587456 |
|    clip_fraction        | 0.0927      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.322      |
|    explained_variance   | 0.588       |
|    learning_rate        | 0.0001      |
|    loss                 | 810         |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 1.18e+03    |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=244.04 +/- 97.65
Episode length: 61.38 +/- 24.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.4     |
|    mean_reward     | 244      |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=228.50 +/- 91.55
Episode length: 57.48 +/- 22.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.5     |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=232.76 +/- 85.34
Episode length: 58.50 +/- 21.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.5     |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=234.96 +/- 104.03
Episode length: 59.12 +/- 25.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.1     |
|    mean_reward     | 235      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.3     |
|    ep_rew_mean     | 356      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 157      |
|    time_elapsed    | 2032     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=204.42 +/- 81.53
Episode length: 51.44 +/- 20.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 51.4       |
|    mean_reward          | 204        |
| time/                   |            |
|    total_timesteps      | 322000     |
| train/                  |            |
|    approx_kl            | 0.02643912 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.377     |
|    explained_variance   | 0.673      |
|    learning_rate        | 0.0001     |
|    loss                 | 222        |
|    n_updates            | 1570       |
|    policy_gradient_loss | 0.00285    |
|    value_loss           | 766        |
----------------------------------------
Eval num_timesteps=322500, episode_reward=229.18 +/- 131.95
Episode length: 57.74 +/- 32.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.7     |
|    mean_reward     | 229      |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=236.98 +/- 101.56
Episode length: 59.56 +/- 25.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.6     |
|    mean_reward     | 237      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=239.32 +/- 127.36
Episode length: 60.14 +/- 31.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.1     |
|    mean_reward     | 239      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.9     |
|    ep_rew_mean     | 370      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 158      |
|    time_elapsed    | 2045     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=218.46 +/- 103.91
Episode length: 55.08 +/- 26.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.1        |
|    mean_reward          | 218         |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.017714996 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.378      |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0001      |
|    loss                 | 193         |
|    n_updates            | 1580        |
|    policy_gradient_loss | 0.00402     |
|    value_loss           | 603         |
-----------------------------------------
Eval num_timesteps=324500, episode_reward=167.36 +/- 62.18
Episode length: 42.20 +/- 15.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=197.58 +/- 87.35
Episode length: 49.76 +/- 21.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.8     |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=194.40 +/- 98.30
Episode length: 48.98 +/- 24.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49       |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.7     |
|    ep_rew_mean     | 357      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 159      |
|    time_elapsed    | 2057     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=210.88 +/- 86.50
Episode length: 53.06 +/- 21.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 53.1       |
|    mean_reward          | 211        |
| time/                   |            |
|    total_timesteps      | 326000     |
| train/                  |            |
|    approx_kl            | 0.04044356 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.338     |
|    explained_variance   | 0.693      |
|    learning_rate        | 0.0001     |
|    loss                 | 307        |
|    n_updates            | 1590       |
|    policy_gradient_loss | -0.0104    |
|    value_loss           | 977        |
----------------------------------------
Eval num_timesteps=326500, episode_reward=238.78 +/- 110.46
Episode length: 60.02 +/- 27.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60       |
|    mean_reward     | 239      |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=241.08 +/- 115.62
Episode length: 60.66 +/- 28.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.7     |
|    mean_reward     | 241      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=238.34 +/- 100.36
Episode length: 59.98 +/- 25.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60       |
|    mean_reward     | 238      |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.8     |
|    ep_rew_mean     | 370      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 160      |
|    time_elapsed    | 2071     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=291.92 +/- 122.99
Episode length: 73.32 +/- 30.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73.3        |
|    mean_reward          | 292         |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.021804517 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.354      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.0001      |
|    loss                 | 703         |
|    n_updates            | 1600        |
|    policy_gradient_loss | -0.00115    |
|    value_loss           | 1.24e+03    |
-----------------------------------------
Eval num_timesteps=328500, episode_reward=256.68 +/- 105.73
Episode length: 64.60 +/- 26.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.6     |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=282.36 +/- 93.68
Episode length: 70.94 +/- 23.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.9     |
|    mean_reward     | 282      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=228.84 +/- 91.22
Episode length: 57.58 +/- 22.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.6     |
|    mean_reward     | 229      |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92       |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 161      |
|    time_elapsed    | 2086     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=222.64 +/- 81.43
Episode length: 56.02 +/- 20.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56          |
|    mean_reward          | 223         |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.018456688 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.344      |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0001      |
|    loss                 | 470         |
|    n_updates            | 1610        |
|    policy_gradient_loss | 0.0027      |
|    value_loss           | 644         |
-----------------------------------------
Eval num_timesteps=330500, episode_reward=226.58 +/- 86.02
Episode length: 56.94 +/- 21.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.9     |
|    mean_reward     | 227      |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=239.52 +/- 93.65
Episode length: 60.20 +/- 23.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.2     |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=222.76 +/- 97.69
Episode length: 56.10 +/- 24.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.1     |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91       |
|    ep_rew_mean     | 362      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 162      |
|    time_elapsed    | 2099     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=325.96 +/- 114.46
Episode length: 81.90 +/- 28.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.9        |
|    mean_reward          | 326         |
| time/                   |             |
|    total_timesteps      | 332000      |
| train/                  |             |
|    approx_kl            | 0.012242032 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.341      |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0001      |
|    loss                 | 411         |
|    n_updates            | 1620        |
|    policy_gradient_loss | 0.000517    |
|    value_loss           | 808         |
-----------------------------------------
Eval num_timesteps=332500, episode_reward=278.84 +/- 115.39
Episode length: 70.16 +/- 28.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.2     |
|    mean_reward     | 279      |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=327.74 +/- 135.67
Episode length: 82.28 +/- 33.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.3     |
|    mean_reward     | 328      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=331.82 +/- 135.99
Episode length: 83.36 +/- 33.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.4     |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.1     |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 163      |
|    time_elapsed    | 2116     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=333.50 +/- 135.73
Episode length: 83.72 +/- 33.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 83.7        |
|    mean_reward          | 334         |
| time/                   |             |
|    total_timesteps      | 334000      |
| train/                  |             |
|    approx_kl            | 0.022639927 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.334      |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.0001      |
|    loss                 | 223         |
|    n_updates            | 1630        |
|    policy_gradient_loss | 0.00497     |
|    value_loss           | 467         |
-----------------------------------------
Eval num_timesteps=334500, episode_reward=375.24 +/- 126.46
Episode length: 94.18 +/- 31.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.2     |
|    mean_reward     | 375      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
New best mean reward!
Eval num_timesteps=335000, episode_reward=343.72 +/- 126.41
Episode length: 86.32 +/- 31.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.3     |
|    mean_reward     | 344      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=339.34 +/- 135.32
Episode length: 85.22 +/- 33.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.2     |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.7     |
|    ep_rew_mean     | 357      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 164      |
|    time_elapsed    | 2135     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=317.36 +/- 130.38
Episode length: 79.68 +/- 32.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 79.7       |
|    mean_reward          | 317        |
| time/                   |            |
|    total_timesteps      | 336000     |
| train/                  |            |
|    approx_kl            | 0.02133135 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.422     |
|    explained_variance   | 0.92       |
|    learning_rate        | 0.0001     |
|    loss                 | 128        |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.00286   |
|    value_loss           | 272        |
----------------------------------------
Eval num_timesteps=336500, episode_reward=332.34 +/- 126.89
Episode length: 83.50 +/- 31.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.5     |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=296.32 +/- 128.19
Episode length: 74.40 +/- 32.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=289.26 +/- 138.83
Episode length: 72.70 +/- 34.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.8     |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 165      |
|    time_elapsed    | 2151     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=214.58 +/- 97.62
Episode length: 54.02 +/- 24.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54          |
|    mean_reward          | 215         |
| time/                   |             |
|    total_timesteps      | 338000      |
| train/                  |             |
|    approx_kl            | 0.008477095 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.402      |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0001      |
|    loss                 | 109         |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.0029     |
|    value_loss           | 318         |
-----------------------------------------
Eval num_timesteps=338500, episode_reward=246.30 +/- 106.71
Episode length: 61.96 +/- 26.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62       |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=224.00 +/- 109.96
Episode length: 56.32 +/- 27.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.3     |
|    mean_reward     | 224      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=220.84 +/- 105.91
Episode length: 55.64 +/- 26.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | 221      |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.1     |
|    ep_rew_mean     | 343      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 166      |
|    time_elapsed    | 2165     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=237.70 +/- 95.99
Episode length: 59.78 +/- 23.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 59.8        |
|    mean_reward          | 238         |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.015301901 |
|    clip_fraction        | 0.0998      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.369      |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0001      |
|    loss                 | 163         |
|    n_updates            | 1660        |
|    policy_gradient_loss | 0.00183     |
|    value_loss           | 580         |
-----------------------------------------
Eval num_timesteps=340500, episode_reward=233.60 +/- 134.84
Episode length: 58.76 +/- 33.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.8     |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=280.42 +/- 136.73
Episode length: 70.54 +/- 34.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.5     |
|    mean_reward     | 280      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=228.68 +/- 99.90
Episode length: 57.54 +/- 24.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.5     |
|    mean_reward     | 229      |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=261.56 +/- 129.18
Episode length: 65.76 +/- 32.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.8     |
|    mean_reward     | 262      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.3     |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 167      |
|    time_elapsed    | 2182     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=324.08 +/- 130.80
Episode length: 81.38 +/- 32.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 81.4       |
|    mean_reward          | 324        |
| time/                   |            |
|    total_timesteps      | 342500     |
| train/                  |            |
|    approx_kl            | 0.03426604 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.34      |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.0001     |
|    loss                 | 644        |
|    n_updates            | 1670       |
|    policy_gradient_loss | 0.00298    |
|    value_loss           | 1.31e+03   |
----------------------------------------
Eval num_timesteps=343000, episode_reward=311.08 +/- 123.76
Episode length: 78.14 +/- 30.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.1     |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=257.18 +/- 114.25
Episode length: 64.62 +/- 28.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.6     |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=262.72 +/- 95.37
Episode length: 66.04 +/- 23.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66       |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86       |
|    ep_rew_mean     | 343      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 168      |
|    time_elapsed    | 2198     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=272.46 +/- 101.39
Episode length: 68.48 +/- 25.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.5        |
|    mean_reward          | 272         |
| time/                   |             |
|    total_timesteps      | 344500      |
| train/                  |             |
|    approx_kl            | 0.014087952 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.375      |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0001      |
|    loss                 | 228         |
|    n_updates            | 1680        |
|    policy_gradient_loss | 0.000261    |
|    value_loss           | 694         |
-----------------------------------------
Eval num_timesteps=345000, episode_reward=275.84 +/- 99.68
Episode length: 69.34 +/- 24.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.3     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=300.14 +/- 127.95
Episode length: 75.40 +/- 31.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 300      |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=270.36 +/- 98.76
Episode length: 68.04 +/- 24.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68       |
|    mean_reward     | 270      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.3     |
|    ep_rew_mean     | 348      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 169      |
|    time_elapsed    | 2213     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=304.40 +/- 137.64
Episode length: 76.52 +/- 34.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.5        |
|    mean_reward          | 304         |
| time/                   |             |
|    total_timesteps      | 346500      |
| train/                  |             |
|    approx_kl            | 0.018663531 |
|    clip_fraction        | 0.0866      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.383      |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0001      |
|    loss                 | 294         |
|    n_updates            | 1690        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 668         |
-----------------------------------------
Eval num_timesteps=347000, episode_reward=239.94 +/- 122.97
Episode length: 60.34 +/- 30.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.3     |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=292.54 +/- 117.42
Episode length: 73.54 +/- 29.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 293      |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=281.18 +/- 119.35
Episode length: 70.64 +/- 29.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.6     |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.7     |
|    ep_rew_mean     | 353      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 170      |
|    time_elapsed    | 2229     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=315.60 +/- 142.40
Episode length: 79.28 +/- 35.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 79.3       |
|    mean_reward          | 316        |
| time/                   |            |
|    total_timesteps      | 348500     |
| train/                  |            |
|    approx_kl            | 0.03604795 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.392     |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0001     |
|    loss                 | 195        |
|    n_updates            | 1700       |
|    policy_gradient_loss | 0.00849    |
|    value_loss           | 659        |
----------------------------------------
Eval num_timesteps=349000, episode_reward=314.50 +/- 145.75
Episode length: 79.04 +/- 36.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79       |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=272.04 +/- 126.04
Episode length: 68.38 +/- 31.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.4     |
|    mean_reward     | 272      |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=299.96 +/- 136.76
Episode length: 75.40 +/- 34.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.4     |
|    mean_reward     | 300      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.6     |
|    ep_rew_mean     | 357      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 171      |
|    time_elapsed    | 2246     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=275.74 +/- 147.56
Episode length: 69.32 +/- 36.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 69.3        |
|    mean_reward          | 276         |
| time/                   |             |
|    total_timesteps      | 350500      |
| train/                  |             |
|    approx_kl            | 0.012153408 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.327      |
|    explained_variance   | 0.85        |
|    learning_rate        | 0.0001      |
|    loss                 | 201         |
|    n_updates            | 1710        |
|    policy_gradient_loss | 0.00292     |
|    value_loss           | 463         |
-----------------------------------------
Eval num_timesteps=351000, episode_reward=274.02 +/- 129.09
Episode length: 68.90 +/- 32.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.9     |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=252.04 +/- 117.39
Episode length: 63.42 +/- 29.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.4     |
|    mean_reward     | 252      |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=284.76 +/- 125.23
Episode length: 71.58 +/- 31.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.6     |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.7     |
|    ep_rew_mean     | 345      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 172      |
|    time_elapsed    | 2261     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=279.08 +/- 129.12
Episode length: 70.14 +/- 32.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 70.1        |
|    mean_reward          | 279         |
| time/                   |             |
|    total_timesteps      | 352500      |
| train/                  |             |
|    approx_kl            | 0.020039676 |
|    clip_fraction        | 0.0882      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.349      |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0001      |
|    loss                 | 458         |
|    n_updates            | 1720        |
|    policy_gradient_loss | -0.00841    |
|    value_loss           | 1.19e+03    |
-----------------------------------------
Eval num_timesteps=353000, episode_reward=318.86 +/- 123.56
Episode length: 80.08 +/- 30.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.1     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=302.12 +/- 110.49
Episode length: 75.86 +/- 27.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.9     |
|    mean_reward     | 302      |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=283.48 +/- 126.56
Episode length: 71.30 +/- 31.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.3     |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.6     |
|    ep_rew_mean     | 349      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 173      |
|    time_elapsed    | 2277     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=281.74 +/- 115.77
Episode length: 70.80 +/- 28.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 70.8        |
|    mean_reward          | 282         |
| time/                   |             |
|    total_timesteps      | 354500      |
| train/                  |             |
|    approx_kl            | 0.026850924 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.35       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0001      |
|    loss                 | 116         |
|    n_updates            | 1730        |
|    policy_gradient_loss | 0.00164     |
|    value_loss           | 541         |
-----------------------------------------
Eval num_timesteps=355000, episode_reward=237.08 +/- 118.90
Episode length: 59.62 +/- 29.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.6     |
|    mean_reward     | 237      |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=250.84 +/- 117.52
Episode length: 63.16 +/- 29.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.2     |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=270.46 +/- 113.95
Episode length: 67.92 +/- 28.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.9     |
|    mean_reward     | 270      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.9     |
|    ep_rew_mean     | 358      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 174      |
|    time_elapsed    | 2292     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=208.12 +/- 96.03
Episode length: 52.44 +/- 24.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.4        |
|    mean_reward          | 208         |
| time/                   |             |
|    total_timesteps      | 356500      |
| train/                  |             |
|    approx_kl            | 0.019934934 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.301      |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0001      |
|    loss                 | 241         |
|    n_updates            | 1740        |
|    policy_gradient_loss | 0.00603     |
|    value_loss           | 649         |
-----------------------------------------
Eval num_timesteps=357000, episode_reward=230.24 +/- 109.76
Episode length: 57.94 +/- 27.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.9     |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=246.46 +/- 125.13
Episode length: 61.98 +/- 31.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62       |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=204.68 +/- 136.99
Episode length: 51.62 +/- 34.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | 205      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90       |
|    ep_rew_mean     | 358      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 175      |
|    time_elapsed    | 2304     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=294.42 +/- 121.86
Episode length: 73.96 +/- 30.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74          |
|    mean_reward          | 294         |
| time/                   |             |
|    total_timesteps      | 358500      |
| train/                  |             |
|    approx_kl            | 0.023712574 |
|    clip_fraction        | 0.0838      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.31       |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0001      |
|    loss                 | 352         |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.00668    |
|    value_loss           | 913         |
-----------------------------------------
Eval num_timesteps=359000, episode_reward=303.80 +/- 134.45
Episode length: 76.34 +/- 33.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.3     |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=292.60 +/- 134.79
Episode length: 73.54 +/- 33.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | 293      |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=327.54 +/- 121.41
Episode length: 82.26 +/- 30.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.3     |
|    mean_reward     | 328      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.7     |
|    ep_rew_mean     | 373      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 176      |
|    time_elapsed    | 2317     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=286.66 +/- 118.74
Episode length: 72.04 +/- 29.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72          |
|    mean_reward          | 287         |
| time/                   |             |
|    total_timesteps      | 360500      |
| train/                  |             |
|    approx_kl            | 0.010724742 |
|    clip_fraction        | 0.0641      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0001      |
|    loss                 | 768         |
|    n_updates            | 1760        |
|    policy_gradient_loss | 0.00134     |
|    value_loss           | 932         |
-----------------------------------------
Eval num_timesteps=361000, episode_reward=264.66 +/- 117.60
Episode length: 66.48 +/- 29.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.5     |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=277.66 +/- 125.82
Episode length: 69.80 +/- 31.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.8     |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=272.28 +/- 101.82
Episode length: 68.40 +/- 25.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.4     |
|    mean_reward     | 272      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94       |
|    ep_rew_mean     | 374      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 177      |
|    time_elapsed    | 2328     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=291.84 +/- 115.44
Episode length: 73.34 +/- 28.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73.3        |
|    mean_reward          | 292         |
| time/                   |             |
|    total_timesteps      | 362500      |
| train/                  |             |
|    approx_kl            | 0.018390732 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.336      |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0001      |
|    loss                 | 315         |
|    n_updates            | 1770        |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=363000, episode_reward=277.80 +/- 119.46
Episode length: 69.78 +/- 29.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.8     |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=266.92 +/- 127.95
Episode length: 67.08 +/- 32.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.1     |
|    mean_reward     | 267      |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=260.82 +/- 123.61
Episode length: 65.52 +/- 30.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.5     |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=258.12 +/- 126.50
Episode length: 64.84 +/- 31.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.8     |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.9     |
|    ep_rew_mean     | 370      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 178      |
|    time_elapsed    | 2341     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=272.78 +/- 122.42
Episode length: 68.58 +/- 30.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.6        |
|    mean_reward          | 273         |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.022116687 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.312      |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0001      |
|    loss                 | 318         |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.00565    |
|    value_loss           | 776         |
-----------------------------------------
Eval num_timesteps=365500, episode_reward=276.22 +/- 110.44
Episode length: 69.46 +/- 27.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.5     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=269.64 +/- 106.10
Episode length: 67.76 +/- 26.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.8     |
|    mean_reward     | 270      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=272.08 +/- 107.58
Episode length: 68.40 +/- 26.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.4     |
|    mean_reward     | 272      |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.3     |
|    ep_rew_mean     | 356      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 179      |
|    time_elapsed    | 2352     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=273.32 +/- 126.62
Episode length: 68.68 +/- 31.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.7        |
|    mean_reward          | 273         |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.023127742 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.383      |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0001      |
|    loss                 | 343         |
|    n_updates            | 1790        |
|    policy_gradient_loss | 0.00445     |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=367500, episode_reward=254.36 +/- 100.51
Episode length: 64.04 +/- 25.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64       |
|    mean_reward     | 254      |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=271.68 +/- 98.28
Episode length: 68.30 +/- 24.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.3     |
|    mean_reward     | 272      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=276.32 +/- 116.95
Episode length: 69.46 +/- 29.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.5     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.2     |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 155      |
|    iterations      | 180      |
|    time_elapsed    | 2363     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=231.86 +/- 125.97
Episode length: 58.30 +/- 31.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 58.3        |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 369000      |
| train/                  |             |
|    approx_kl            | 0.021853428 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.359      |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0001      |
|    loss                 | 284         |
|    n_updates            | 1800        |
|    policy_gradient_loss | 0.00599     |
|    value_loss           | 632         |
-----------------------------------------
Eval num_timesteps=369500, episode_reward=239.54 +/- 133.90
Episode length: 60.30 +/- 33.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.3     |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=230.48 +/- 111.64
Episode length: 57.94 +/- 27.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.9     |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=275.90 +/- 144.98
Episode length: 69.40 +/- 36.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.4     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.4     |
|    ep_rew_mean     | 368      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 181      |
|    time_elapsed    | 2374     |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=255.56 +/- 115.52
Episode length: 64.26 +/- 28.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 64.3        |
|    mean_reward          | 256         |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.021065064 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.361      |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0001      |
|    loss                 | 457         |
|    n_updates            | 1810        |
|    policy_gradient_loss | 0.0057      |
|    value_loss           | 715         |
-----------------------------------------
Eval num_timesteps=371500, episode_reward=266.86 +/- 122.75
Episode length: 67.08 +/- 30.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.1     |
|    mean_reward     | 267      |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=227.78 +/- 116.58
Episode length: 57.34 +/- 29.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.3     |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=258.80 +/- 108.22
Episode length: 65.00 +/- 27.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65       |
|    mean_reward     | 259      |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.1     |
|    ep_rew_mean     | 371      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 182      |
|    time_elapsed    | 2384     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=312.74 +/- 116.66
Episode length: 78.50 +/- 29.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 78.5       |
|    mean_reward          | 313        |
| time/                   |            |
|    total_timesteps      | 373000     |
| train/                  |            |
|    approx_kl            | 0.01449115 |
|    clip_fraction        | 0.0879     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.398     |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.0001     |
|    loss                 | 447        |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.00481   |
|    value_loss           | 892        |
----------------------------------------
Eval num_timesteps=373500, episode_reward=309.18 +/- 103.92
Episode length: 77.68 +/- 25.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.7     |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=266.50 +/- 116.43
Episode length: 67.10 +/- 29.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.1     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=274.90 +/- 101.21
Episode length: 69.10 +/- 25.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.1     |
|    mean_reward     | 275      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.1     |
|    ep_rew_mean     | 371      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 183      |
|    time_elapsed    | 2396     |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=218.82 +/- 87.57
Episode length: 55.10 +/- 21.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.1        |
|    mean_reward          | 219         |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.019269409 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.392      |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0001      |
|    loss                 | 246         |
|    n_updates            | 1830        |
|    policy_gradient_loss | 0.0059      |
|    value_loss           | 488         |
-----------------------------------------
Eval num_timesteps=375500, episode_reward=215.56 +/- 82.78
Episode length: 54.24 +/- 20.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=222.70 +/- 86.02
Episode length: 56.02 +/- 21.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=214.80 +/- 101.41
Episode length: 54.08 +/- 25.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.3     |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 184      |
|    time_elapsed    | 2406     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=171.70 +/- 67.87
Episode length: 43.28 +/- 16.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.3        |
|    mean_reward          | 172         |
| time/                   |             |
|    total_timesteps      | 377000      |
| train/                  |             |
|    approx_kl            | 0.014644476 |
|    clip_fraction        | 0.0924      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.318      |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0001      |
|    loss                 | 253         |
|    n_updates            | 1840        |
|    policy_gradient_loss | 0.00299     |
|    value_loss           | 626         |
-----------------------------------------
Eval num_timesteps=377500, episode_reward=175.60 +/- 64.04
Episode length: 44.26 +/- 15.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.3     |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=186.32 +/- 77.74
Episode length: 47.00 +/- 19.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47       |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=196.16 +/- 64.79
Episode length: 49.36 +/- 16.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.4     |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.7     |
|    ep_rew_mean     | 353      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 185      |
|    time_elapsed    | 2414     |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=226.34 +/- 102.63
Episode length: 56.98 +/- 25.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 57          |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 379000      |
| train/                  |             |
|    approx_kl            | 0.022501241 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.298      |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.0001      |
|    loss                 | 810         |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.00982    |
|    value_loss           | 1.69e+03    |
-----------------------------------------
Eval num_timesteps=379500, episode_reward=196.34 +/- 88.35
Episode length: 49.46 +/- 22.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=181.24 +/- 67.49
Episode length: 45.72 +/- 16.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.7     |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=207.58 +/- 77.03
Episode length: 52.30 +/- 19.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | 208      |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.3     |
|    ep_rew_mean     | 364      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 186      |
|    time_elapsed    | 2423     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=245.22 +/- 128.93
Episode length: 61.70 +/- 32.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 61.7       |
|    mean_reward          | 245        |
| time/                   |            |
|    total_timesteps      | 381000     |
| train/                  |            |
|    approx_kl            | 0.02330407 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.343     |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.0001     |
|    loss                 | 443        |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.00761   |
|    value_loss           | 932        |
----------------------------------------
Eval num_timesteps=381500, episode_reward=244.26 +/- 122.70
Episode length: 61.46 +/- 30.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.5     |
|    mean_reward     | 244      |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=219.22 +/- 102.68
Episode length: 55.18 +/- 25.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=203.82 +/- 84.32
Episode length: 51.36 +/- 21.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | 204      |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.9     |
|    ep_rew_mean     | 354      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 187      |
|    time_elapsed    | 2433     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=249.70 +/- 121.11
Episode length: 62.80 +/- 30.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 62.8       |
|    mean_reward          | 250        |
| time/                   |            |
|    total_timesteps      | 383000     |
| train/                  |            |
|    approx_kl            | 0.01988398 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.317     |
|    explained_variance   | 0.701      |
|    learning_rate        | 0.0001     |
|    loss                 | 316        |
|    n_updates            | 1870       |
|    policy_gradient_loss | 0.00766    |
|    value_loss           | 977        |
----------------------------------------
Eval num_timesteps=383500, episode_reward=241.02 +/- 123.41
Episode length: 60.58 +/- 30.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.6     |
|    mean_reward     | 241      |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=263.62 +/- 133.50
Episode length: 66.30 +/- 33.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.3     |
|    mean_reward     | 264      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=246.94 +/- 128.02
Episode length: 62.06 +/- 31.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.1     |
|    mean_reward     | 247      |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=262.80 +/- 128.76
Episode length: 66.06 +/- 32.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.1     |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.7     |
|    ep_rew_mean     | 345      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 188      |
|    time_elapsed    | 2445     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=257.88 +/- 108.63
Episode length: 64.88 +/- 27.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 64.9       |
|    mean_reward          | 258        |
| time/                   |            |
|    total_timesteps      | 385500     |
| train/                  |            |
|    approx_kl            | 0.01718238 |
|    clip_fraction        | 0.0845     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.333     |
|    explained_variance   | 0.693      |
|    learning_rate        | 0.0001     |
|    loss                 | 548        |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.00415   |
|    value_loss           | 979        |
----------------------------------------
Eval num_timesteps=386000, episode_reward=241.54 +/- 103.67
Episode length: 60.78 +/- 25.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.8     |
|    mean_reward     | 242      |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=215.42 +/- 89.90
Episode length: 54.20 +/- 22.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=235.70 +/- 85.68
Episode length: 59.32 +/- 21.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.3     |
|    mean_reward     | 236      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.4     |
|    ep_rew_mean     | 344      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 189      |
|    time_elapsed    | 2455     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=282.84 +/- 100.60
Episode length: 71.08 +/- 25.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 71.1        |
|    mean_reward          | 283         |
| time/                   |             |
|    total_timesteps      | 387500      |
| train/                  |             |
|    approx_kl            | 0.022266638 |
|    clip_fraction        | 0.0933      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.355      |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.0001      |
|    loss                 | 411         |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00236    |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=388000, episode_reward=282.16 +/- 112.87
Episode length: 70.84 +/- 28.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.8     |
|    mean_reward     | 282      |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=271.80 +/- 116.82
Episode length: 68.34 +/- 29.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.3     |
|    mean_reward     | 272      |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=276.36 +/- 127.86
Episode length: 69.40 +/- 31.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.4     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.7     |
|    ep_rew_mean     | 345      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 190      |
|    time_elapsed    | 2467     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=206.48 +/- 67.97
Episode length: 52.04 +/- 17.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52          |
|    mean_reward          | 206         |
| time/                   |             |
|    total_timesteps      | 389500      |
| train/                  |             |
|    approx_kl            | 0.015029272 |
|    clip_fraction        | 0.0992      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.339      |
|    explained_variance   | 0.81        |
|    learning_rate        | 0.0001      |
|    loss                 | 408         |
|    n_updates            | 1900        |
|    policy_gradient_loss | 0.00131     |
|    value_loss           | 512         |
-----------------------------------------
Eval num_timesteps=390000, episode_reward=202.30 +/- 86.75
Episode length: 50.98 +/- 21.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | 202      |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=213.38 +/- 71.12
Episode length: 53.76 +/- 17.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=206.66 +/- 81.05
Episode length: 52.02 +/- 20.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.2     |
|    ep_rew_mean     | 335      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 191      |
|    time_elapsed    | 2476     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=240.34 +/- 90.85
Episode length: 60.46 +/- 22.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 60.5        |
|    mean_reward          | 240         |
| time/                   |             |
|    total_timesteps      | 391500      |
| train/                  |             |
|    approx_kl            | 0.025470586 |
|    clip_fraction        | 0.086       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.317      |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0001      |
|    loss                 | 504         |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00777    |
|    value_loss           | 1.09e+03    |
-----------------------------------------
Eval num_timesteps=392000, episode_reward=238.10 +/- 85.10
Episode length: 59.86 +/- 21.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.9     |
|    mean_reward     | 238      |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=224.88 +/- 88.51
Episode length: 56.64 +/- 22.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.6     |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=290.42 +/- 107.26
Episode length: 73.02 +/- 26.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73       |
|    mean_reward     | 290      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.9     |
|    ep_rew_mean     | 346      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 192      |
|    time_elapsed    | 2486     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=249.80 +/- 125.09
Episode length: 62.78 +/- 31.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 62.8        |
|    mean_reward          | 250         |
| time/                   |             |
|    total_timesteps      | 393500      |
| train/                  |             |
|    approx_kl            | 0.025544416 |
|    clip_fraction        | 0.0917      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.305      |
|    explained_variance   | 0.899       |
|    learning_rate        | 0.0001      |
|    loss                 | 195         |
|    n_updates            | 1920        |
|    policy_gradient_loss | 0.0041      |
|    value_loss           | 294         |
-----------------------------------------
Eval num_timesteps=394000, episode_reward=237.68 +/- 107.95
Episode length: 59.78 +/- 27.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.8     |
|    mean_reward     | 238      |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=243.32 +/- 124.14
Episode length: 61.22 +/- 30.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.2     |
|    mean_reward     | 243      |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=219.76 +/- 107.27
Episode length: 55.30 +/- 26.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.3     |
|    mean_reward     | 220      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.1     |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 193      |
|    time_elapsed    | 2496     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=242.60 +/- 94.61
Episode length: 61.02 +/- 23.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 61          |
|    mean_reward          | 243         |
| time/                   |             |
|    total_timesteps      | 395500      |
| train/                  |             |
|    approx_kl            | 0.018095843 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.375      |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0001      |
|    loss                 | 258         |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.00015    |
|    value_loss           | 579         |
-----------------------------------------
Eval num_timesteps=396000, episode_reward=275.74 +/- 123.42
Episode length: 69.28 +/- 30.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.3     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=264.42 +/- 119.81
Episode length: 66.52 +/- 29.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.5     |
|    mean_reward     | 264      |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=234.22 +/- 113.34
Episode length: 58.88 +/- 28.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.9     |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.6     |
|    ep_rew_mean     | 357      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 194      |
|    time_elapsed    | 2507     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=191.38 +/- 75.90
Episode length: 48.20 +/- 18.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 48.2        |
|    mean_reward          | 191         |
| time/                   |             |
|    total_timesteps      | 397500      |
| train/                  |             |
|    approx_kl            | 0.011148349 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.375      |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0001      |
|    loss                 | 251         |
|    n_updates            | 1940        |
|    policy_gradient_loss | 0.00547     |
|    value_loss           | 548         |
-----------------------------------------
Eval num_timesteps=398000, episode_reward=176.16 +/- 76.60
Episode length: 44.46 +/- 19.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=212.08 +/- 80.11
Episode length: 53.38 +/- 20.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=186.74 +/- 67.33
Episode length: 47.10 +/- 16.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.1     |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.5     |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 195      |
|    time_elapsed    | 2516     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=249.36 +/- 96.85
Episode length: 62.74 +/- 24.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 62.7       |
|    mean_reward          | 249        |
| time/                   |            |
|    total_timesteps      | 399500     |
| train/                  |            |
|    approx_kl            | 0.01959655 |
|    clip_fraction        | 0.0876     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.283     |
|    explained_variance   | 0.571      |
|    learning_rate        | 0.0001     |
|    loss                 | 388        |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.00298   |
|    value_loss           | 1.21e+03   |
----------------------------------------
Eval num_timesteps=400000, episode_reward=214.90 +/- 85.15
Episode length: 54.16 +/- 21.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=250.06 +/- 101.75
Episode length: 62.82 +/- 25.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.8     |
|    mean_reward     | 250      |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=250.64 +/- 96.60
Episode length: 63.06 +/- 24.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.1     |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.8     |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 196      |
|    time_elapsed    | 2526     |
|    total_timesteps | 401408   |
---------------------------------
Eval num_timesteps=401500, episode_reward=209.60 +/- 99.27
Episode length: 52.82 +/- 24.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.8       |
|    mean_reward          | 210        |
| time/                   |            |
|    total_timesteps      | 401500     |
| train/                  |            |
|    approx_kl            | 0.01734455 |
|    clip_fraction        | 0.0651     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.297     |
|    explained_variance   | 0.689      |
|    learning_rate        | 0.0001     |
|    loss                 | 438        |
|    n_updates            | 1960       |
|    policy_gradient_loss | -0.0023    |
|    value_loss           | 858        |
----------------------------------------
Eval num_timesteps=402000, episode_reward=230.18 +/- 95.38
Episode length: 57.92 +/- 23.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.9     |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=246.88 +/- 97.80
Episode length: 62.10 +/- 24.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.1     |
|    mean_reward     | 247      |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=218.56 +/- 107.96
Episode length: 55.06 +/- 26.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.1     |
|    ep_rew_mean     | 335      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 197      |
|    time_elapsed    | 2535     |
|    total_timesteps | 403456   |
---------------------------------
Eval num_timesteps=403500, episode_reward=240.40 +/- 118.65
Episode length: 60.52 +/- 29.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 60.5        |
|    mean_reward          | 240         |
| time/                   |             |
|    total_timesteps      | 403500      |
| train/                  |             |
|    approx_kl            | 0.026439149 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.353      |
|    explained_variance   | 0.521       |
|    learning_rate        | 0.0001      |
|    loss                 | 641         |
|    n_updates            | 1970        |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=404000, episode_reward=276.06 +/- 131.80
Episode length: 69.42 +/- 32.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.4     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=275.08 +/- 122.84
Episode length: 69.18 +/- 30.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.2     |
|    mean_reward     | 275      |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=258.26 +/- 117.39
Episode length: 65.00 +/- 29.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65       |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=283.06 +/- 131.32
Episode length: 71.18 +/- 32.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.2     |
|    mean_reward     | 283      |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.4     |
|    ep_rew_mean     | 336      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 198      |
|    time_elapsed    | 2548     |
|    total_timesteps | 405504   |
---------------------------------
Eval num_timesteps=406000, episode_reward=274.36 +/- 131.93
Episode length: 69.04 +/- 32.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 69         |
|    mean_reward          | 274        |
| time/                   |            |
|    total_timesteps      | 406000     |
| train/                  |            |
|    approx_kl            | 0.01644897 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.387     |
|    explained_variance   | 0.797      |
|    learning_rate        | 0.0001     |
|    loss                 | 117        |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.00148   |
|    value_loss           | 459        |
----------------------------------------
Eval num_timesteps=406500, episode_reward=267.08 +/- 120.68
Episode length: 67.16 +/- 30.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.2     |
|    mean_reward     | 267      |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=256.28 +/- 119.36
Episode length: 64.44 +/- 29.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.4     |
|    mean_reward     | 256      |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=252.64 +/- 110.63
Episode length: 63.48 +/- 27.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.5     |
|    mean_reward     | 253      |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.5     |
|    ep_rew_mean     | 349      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 199      |
|    time_elapsed    | 2559     |
|    total_timesteps | 407552   |
---------------------------------
Eval num_timesteps=408000, episode_reward=289.90 +/- 130.16
Episode length: 72.88 +/- 32.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.9        |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 408000      |
| train/                  |             |
|    approx_kl            | 0.020055186 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.421      |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0001      |
|    loss                 | 167         |
|    n_updates            | 1990        |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 516         |
-----------------------------------------
Eval num_timesteps=408500, episode_reward=270.84 +/- 129.78
Episode length: 68.08 +/- 32.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.1     |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=273.40 +/- 129.43
Episode length: 68.68 +/- 32.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.7     |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=287.34 +/- 126.11
Episode length: 72.20 +/- 31.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.2     |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.2     |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 200      |
|    time_elapsed    | 2571     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=410000, episode_reward=287.58 +/- 120.42
Episode length: 72.28 +/- 30.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.3        |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.023916945 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.411      |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0001      |
|    loss                 | 162         |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 715         |
-----------------------------------------
Eval num_timesteps=410500, episode_reward=269.26 +/- 123.27
Episode length: 67.70 +/- 30.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.7     |
|    mean_reward     | 269      |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=253.96 +/- 118.19
Episode length: 63.86 +/- 29.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.9     |
|    mean_reward     | 254      |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=266.98 +/- 109.72
Episode length: 67.08 +/- 27.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.1     |
|    mean_reward     | 267      |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.5     |
|    ep_rew_mean     | 356      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 201      |
|    time_elapsed    | 2582     |
|    total_timesteps | 411648   |
---------------------------------
Eval num_timesteps=412000, episode_reward=271.18 +/- 91.95
Episode length: 68.20 +/- 22.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.2        |
|    mean_reward          | 271         |
| time/                   |             |
|    total_timesteps      | 412000      |
| train/                  |             |
|    approx_kl            | 0.013051318 |
|    clip_fraction        | 0.097       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.404      |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0001      |
|    loss                 | 583         |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.00327    |
|    value_loss           | 931         |
-----------------------------------------
Eval num_timesteps=412500, episode_reward=316.40 +/- 126.47
Episode length: 79.46 +/- 31.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.5     |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=289.44 +/- 109.96
Episode length: 72.72 +/- 27.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=304.96 +/- 123.40
Episode length: 76.64 +/- 30.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 305      |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93       |
|    ep_rew_mean     | 370      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 202      |
|    time_elapsed    | 2593     |
|    total_timesteps | 413696   |
---------------------------------
Eval num_timesteps=414000, episode_reward=301.20 +/- 118.82
Episode length: 75.72 +/- 29.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.7        |
|    mean_reward          | 301         |
| time/                   |             |
|    total_timesteps      | 414000      |
| train/                  |             |
|    approx_kl            | 0.013274506 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.407      |
|    explained_variance   | 0.84        |
|    learning_rate        | 0.0001      |
|    loss                 | 312         |
|    n_updates            | 2020        |
|    policy_gradient_loss | 0.00538     |
|    value_loss           | 416         |
-----------------------------------------
Eval num_timesteps=414500, episode_reward=283.64 +/- 100.76
Episode length: 71.26 +/- 25.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.3     |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=270.46 +/- 88.15
Episode length: 68.04 +/- 22.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68       |
|    mean_reward     | 270      |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=303.18 +/- 133.50
Episode length: 76.18 +/- 33.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.2     |
|    mean_reward     | 303      |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.2     |
|    ep_rew_mean     | 360      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 203      |
|    time_elapsed    | 2605     |
|    total_timesteps | 415744   |
---------------------------------
Eval num_timesteps=416000, episode_reward=348.02 +/- 146.55
Episode length: 87.36 +/- 36.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 87.4       |
|    mean_reward          | 348        |
| time/                   |            |
|    total_timesteps      | 416000     |
| train/                  |            |
|    approx_kl            | 0.03278155 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.436     |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0001     |
|    loss                 | 187        |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.00714   |
|    value_loss           | 562        |
----------------------------------------
Eval num_timesteps=416500, episode_reward=341.70 +/- 136.28
Episode length: 85.74 +/- 34.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.7     |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=315.64 +/- 110.80
Episode length: 79.24 +/- 27.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.2     |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=364.32 +/- 135.88
Episode length: 91.44 +/- 33.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.4     |
|    mean_reward     | 364      |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.4     |
|    ep_rew_mean     | 364      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 204      |
|    time_elapsed    | 2618     |
|    total_timesteps | 417792   |
---------------------------------
Eval num_timesteps=418000, episode_reward=271.78 +/- 135.54
Episode length: 68.28 +/- 33.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.3        |
|    mean_reward          | 272         |
| time/                   |             |
|    total_timesteps      | 418000      |
| train/                  |             |
|    approx_kl            | 0.018915655 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.459      |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0001      |
|    loss                 | 389         |
|    n_updates            | 2040        |
|    policy_gradient_loss | 0.00623     |
|    value_loss           | 819         |
-----------------------------------------
Eval num_timesteps=418500, episode_reward=254.42 +/- 124.88
Episode length: 64.06 +/- 31.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.1     |
|    mean_reward     | 254      |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=257.28 +/- 102.71
Episode length: 64.70 +/- 25.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.7     |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=240.42 +/- 115.04
Episode length: 60.50 +/- 28.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.5     |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.3     |
|    ep_rew_mean     | 368      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 205      |
|    time_elapsed    | 2629     |
|    total_timesteps | 419840   |
---------------------------------
Eval num_timesteps=420000, episode_reward=305.00 +/- 126.95
Episode length: 76.60 +/- 31.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 76.6        |
|    mean_reward          | 305         |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.019410567 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.383      |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0001      |
|    loss                 | 339         |
|    n_updates            | 2050        |
|    policy_gradient_loss | 0.00391     |
|    value_loss           | 648         |
-----------------------------------------
Eval num_timesteps=420500, episode_reward=284.98 +/- 134.52
Episode length: 71.64 +/- 33.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.6     |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=273.84 +/- 130.65
Episode length: 68.84 +/- 32.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.8     |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=304.14 +/- 134.17
Episode length: 76.44 +/- 33.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.4     |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.3     |
|    ep_rew_mean     | 360      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 206      |
|    time_elapsed    | 2641     |
|    total_timesteps | 421888   |
---------------------------------
Eval num_timesteps=422000, episode_reward=260.82 +/- 117.96
Episode length: 65.60 +/- 29.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 65.6        |
|    mean_reward          | 261         |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.018364444 |
|    clip_fraction        | 0.0957      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.458      |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0001      |
|    loss                 | 255         |
|    n_updates            | 2060        |
|    policy_gradient_loss | -0.00908    |
|    value_loss           | 687         |
-----------------------------------------
Eval num_timesteps=422500, episode_reward=259.94 +/- 121.26
Episode length: 65.42 +/- 30.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.4     |
|    mean_reward     | 260      |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=244.82 +/- 123.57
Episode length: 61.58 +/- 30.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.6     |
|    mean_reward     | 245      |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=248.96 +/- 118.81
Episode length: 62.64 +/- 29.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.6     |
|    mean_reward     | 249      |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.7     |
|    ep_rew_mean     | 357      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 207      |
|    time_elapsed    | 2651     |
|    total_timesteps | 423936   |
---------------------------------
Eval num_timesteps=424000, episode_reward=296.74 +/- 109.48
Episode length: 74.60 +/- 27.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.6        |
|    mean_reward          | 297         |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.018112298 |
|    clip_fraction        | 0.099       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.385      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0001      |
|    loss                 | 303         |
|    n_updates            | 2070        |
|    policy_gradient_loss | 0.00406     |
|    value_loss           | 915         |
-----------------------------------------
Eval num_timesteps=424500, episode_reward=298.34 +/- 143.09
Episode length: 74.94 +/- 35.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.9     |
|    mean_reward     | 298      |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=300.00 +/- 134.31
Episode length: 75.34 +/- 33.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.3     |
|    mean_reward     | 300      |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=270.88 +/- 140.31
Episode length: 68.16 +/- 35.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.2     |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.8     |
|    ep_rew_mean     | 354      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 208      |
|    time_elapsed    | 2663     |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=426000, episode_reward=297.84 +/- 132.24
Episode length: 74.78 +/- 33.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 74.8        |
|    mean_reward          | 298         |
| time/                   |             |
|    total_timesteps      | 426000      |
| train/                  |             |
|    approx_kl            | 0.024181113 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.375      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0001      |
|    loss                 | 563         |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 1.04e+03    |
-----------------------------------------
Eval num_timesteps=426500, episode_reward=339.70 +/- 152.81
Episode length: 85.34 +/- 38.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.3     |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=284.14 +/- 136.34
Episode length: 71.38 +/- 34.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.4     |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=306.22 +/- 140.92
Episode length: 76.96 +/- 35.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77       |
|    mean_reward     | 306      |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=266.28 +/- 128.68
Episode length: 67.00 +/- 32.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67       |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.8     |
|    ep_rew_mean     | 362      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 209      |
|    time_elapsed    | 2678     |
|    total_timesteps | 428032   |
---------------------------------
Eval num_timesteps=428500, episode_reward=325.90 +/- 139.58
Episode length: 81.84 +/- 34.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.8        |
|    mean_reward          | 326         |
| time/                   |             |
|    total_timesteps      | 428500      |
| train/                  |             |
|    approx_kl            | 0.015186733 |
|    clip_fraction        | 0.0868      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.397      |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0001      |
|    loss                 | 209         |
|    n_updates            | 2090        |
|    policy_gradient_loss | 0.0014      |
|    value_loss           | 765         |
-----------------------------------------
Eval num_timesteps=429000, episode_reward=347.96 +/- 136.50
Episode length: 87.36 +/- 34.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.4     |
|    mean_reward     | 348      |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=348.94 +/- 134.06
Episode length: 87.64 +/- 33.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.6     |
|    mean_reward     | 349      |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=339.20 +/- 140.28
Episode length: 85.18 +/- 35.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.2     |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.4     |
|    ep_rew_mean     | 364      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 210      |
|    time_elapsed    | 2691     |
|    total_timesteps | 430080   |
---------------------------------
Eval num_timesteps=430500, episode_reward=279.92 +/- 131.84
Episode length: 70.42 +/- 32.95
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.4      |
|    mean_reward          | 280       |
| time/                   |           |
|    total_timesteps      | 430500    |
| train/                  |           |
|    approx_kl            | 0.0435472 |
|    clip_fraction        | 0.167     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.408    |
|    explained_variance   | 0.84      |
|    learning_rate        | 0.0001    |
|    loss                 | 260       |
|    n_updates            | 2100      |
|    policy_gradient_loss | 0.0104    |
|    value_loss           | 509       |
---------------------------------------
Eval num_timesteps=431000, episode_reward=315.68 +/- 151.40
Episode length: 79.30 +/- 37.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.3     |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=316.90 +/- 129.13
Episode length: 79.56 +/- 32.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.6     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=328.04 +/- 143.68
Episode length: 82.38 +/- 35.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.4     |
|    mean_reward     | 328      |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.2     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 211      |
|    time_elapsed    | 2703     |
|    total_timesteps | 432128   |
---------------------------------
Eval num_timesteps=432500, episode_reward=386.42 +/- 129.62
Episode length: 96.98 +/- 32.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97          |
|    mean_reward          | 386         |
| time/                   |             |
|    total_timesteps      | 432500      |
| train/                  |             |
|    approx_kl            | 0.025795281 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.482      |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0001      |
|    loss                 | 300         |
|    n_updates            | 2110        |
|    policy_gradient_loss | 0.0039      |
|    value_loss           | 632         |
-----------------------------------------
New best mean reward!
Eval num_timesteps=433000, episode_reward=354.84 +/- 143.54
Episode length: 89.10 +/- 35.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.1     |
|    mean_reward     | 355      |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=373.26 +/- 148.09
Episode length: 93.68 +/- 37.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.7     |
|    mean_reward     | 373      |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=369.22 +/- 133.84
Episode length: 92.68 +/- 33.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.7     |
|    mean_reward     | 369      |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.2     |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 212      |
|    time_elapsed    | 2718     |
|    total_timesteps | 434176   |
---------------------------------
Eval num_timesteps=434500, episode_reward=301.14 +/- 155.58
Episode length: 75.68 +/- 38.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.7        |
|    mean_reward          | 301         |
| time/                   |             |
|    total_timesteps      | 434500      |
| train/                  |             |
|    approx_kl            | 0.027035078 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.457      |
|    explained_variance   | 0.863       |
|    learning_rate        | 0.0001      |
|    loss                 | 192         |
|    n_updates            | 2120        |
|    policy_gradient_loss | 0.00682     |
|    value_loss           | 397         |
-----------------------------------------
Eval num_timesteps=435000, episode_reward=297.04 +/- 160.93
Episode length: 74.70 +/- 40.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 297      |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=310.14 +/- 151.05
Episode length: 77.90 +/- 37.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.9     |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=273.00 +/- 132.14
Episode length: 68.68 +/- 33.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.7     |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.2     |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 213      |
|    time_elapsed    | 2730     |
|    total_timesteps | 436224   |
---------------------------------
Eval num_timesteps=436500, episode_reward=248.08 +/- 124.17
Episode length: 62.40 +/- 31.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 62.4        |
|    mean_reward          | 248         |
| time/                   |             |
|    total_timesteps      | 436500      |
| train/                  |             |
|    approx_kl            | 0.017436318 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.412      |
|    explained_variance   | 0.885       |
|    learning_rate        | 0.0001      |
|    loss                 | 94.7        |
|    n_updates            | 2130        |
|    policy_gradient_loss | -0.00791    |
|    value_loss           | 368         |
-----------------------------------------
Eval num_timesteps=437000, episode_reward=234.94 +/- 112.66
Episode length: 59.14 +/- 28.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.1     |
|    mean_reward     | 235      |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=249.72 +/- 138.82
Episode length: 62.78 +/- 34.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.8     |
|    mean_reward     | 250      |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=250.24 +/- 142.83
Episode length: 62.98 +/- 35.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63       |
|    mean_reward     | 250      |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.7     |
|    ep_rew_mean     | 345      |
| time/              |          |
|    fps             | 159      |
|    iterations      | 214      |
|    time_elapsed    | 2740     |
|    total_timesteps | 438272   |
---------------------------------
Eval num_timesteps=438500, episode_reward=268.98 +/- 124.09
Episode length: 67.62 +/- 31.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 67.6        |
|    mean_reward          | 269         |
| time/                   |             |
|    total_timesteps      | 438500      |
| train/                  |             |
|    approx_kl            | 0.013494945 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.427      |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0001      |
|    loss                 | 312         |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.00343    |
|    value_loss           | 845         |
-----------------------------------------
Eval num_timesteps=439000, episode_reward=232.56 +/- 126.22
Episode length: 58.52 +/- 31.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.5     |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=272.36 +/- 139.82
Episode length: 68.42 +/- 34.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.4     |
|    mean_reward     | 272      |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=262.34 +/- 139.22
Episode length: 66.00 +/- 34.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66       |
|    mean_reward     | 262      |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.4     |
|    ep_rew_mean     | 348      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 215      |
|    time_elapsed    | 2751     |
|    total_timesteps | 440320   |
---------------------------------
Eval num_timesteps=440500, episode_reward=290.10 +/- 143.66
Episode length: 72.88 +/- 35.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.9        |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 440500      |
| train/                  |             |
|    approx_kl            | 0.015469677 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.491      |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0001      |
|    loss                 | 282         |
|    n_updates            | 2150        |
|    policy_gradient_loss | 0.000166    |
|    value_loss           | 560         |
-----------------------------------------
Eval num_timesteps=441000, episode_reward=274.12 +/- 151.15
Episode length: 68.96 +/- 37.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69       |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=251.48 +/- 156.46
Episode length: 63.26 +/- 39.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.3     |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=271.70 +/- 150.81
Episode length: 68.30 +/- 37.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.3     |
|    mean_reward     | 272      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.9     |
|    ep_rew_mean     | 326      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 216      |
|    time_elapsed    | 2762     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=442500, episode_reward=315.50 +/- 141.20
Episode length: 79.24 +/- 35.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.2        |
|    mean_reward          | 316         |
| time/                   |             |
|    total_timesteps      | 442500      |
| train/                  |             |
|    approx_kl            | 0.035603732 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.398      |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.0001      |
|    loss                 | 483         |
|    n_updates            | 2160        |
|    policy_gradient_loss | 0.000979    |
|    value_loss           | 1.17e+03    |
-----------------------------------------
Eval num_timesteps=443000, episode_reward=303.58 +/- 141.44
Episode length: 76.28 +/- 35.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.3     |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=272.06 +/- 127.55
Episode length: 68.38 +/- 31.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.4     |
|    mean_reward     | 272      |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=303.18 +/- 115.33
Episode length: 76.12 +/- 28.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.1     |
|    mean_reward     | 303      |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.6     |
|    ep_rew_mean     | 329      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 217      |
|    time_elapsed    | 2774     |
|    total_timesteps | 444416   |
---------------------------------
Eval num_timesteps=444500, episode_reward=186.72 +/- 79.49
Episode length: 47.08 +/- 19.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 47.1       |
|    mean_reward          | 187        |
| time/                   |            |
|    total_timesteps      | 444500     |
| train/                  |            |
|    approx_kl            | 0.03849353 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.523     |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.0001     |
|    loss                 | 276        |
|    n_updates            | 2170       |
|    policy_gradient_loss | -0.00155   |
|    value_loss           | 1.02e+03   |
----------------------------------------
Eval num_timesteps=445000, episode_reward=239.54 +/- 109.22
Episode length: 60.28 +/- 27.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.3     |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=222.04 +/- 104.95
Episode length: 55.84 +/- 26.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | 222      |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=220.76 +/- 107.37
Episode length: 55.60 +/- 26.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | 221      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.4     |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 218      |
|    time_elapsed    | 2783     |
|    total_timesteps | 446464   |
---------------------------------
Eval num_timesteps=446500, episode_reward=272.30 +/- 121.12
Episode length: 68.52 +/- 30.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.5        |
|    mean_reward          | 272         |
| time/                   |             |
|    total_timesteps      | 446500      |
| train/                  |             |
|    approx_kl            | 0.022812862 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.438      |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0001      |
|    loss                 | 664         |
|    n_updates            | 2180        |
|    policy_gradient_loss | 0.00416     |
|    value_loss           | 936         |
-----------------------------------------
Eval num_timesteps=447000, episode_reward=281.06 +/- 112.68
Episode length: 70.64 +/- 28.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.6     |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=264.50 +/- 122.37
Episode length: 66.52 +/- 30.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.5     |
|    mean_reward     | 264      |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=261.30 +/- 131.76
Episode length: 65.70 +/- 32.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.7     |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=279.66 +/- 155.42
Episode length: 70.28 +/- 38.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.3     |
|    mean_reward     | 280      |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.9     |
|    ep_rew_mean     | 334      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 219      |
|    time_elapsed    | 2797     |
|    total_timesteps | 448512   |
---------------------------------
Eval num_timesteps=449000, episode_reward=231.84 +/- 144.25
Episode length: 58.28 +/- 36.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 58.3        |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 449000      |
| train/                  |             |
|    approx_kl            | 0.019899542 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.409      |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.0001      |
|    loss                 | 530         |
|    n_updates            | 2190        |
|    policy_gradient_loss | 0.00226     |
|    value_loss           | 1.11e+03    |
-----------------------------------------
Eval num_timesteps=449500, episode_reward=266.36 +/- 131.68
Episode length: 66.96 +/- 32.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67       |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=246.50 +/- 115.29
Episode length: 61.98 +/- 28.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62       |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=285.46 +/- 145.11
Episode length: 71.74 +/- 36.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.7     |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.9     |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 220      |
|    time_elapsed    | 2807     |
|    total_timesteps | 450560   |
---------------------------------
Eval num_timesteps=451000, episode_reward=244.46 +/- 91.80
Episode length: 61.44 +/- 22.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 61.4       |
|    mean_reward          | 244        |
| time/                   |            |
|    total_timesteps      | 451000     |
| train/                  |            |
|    approx_kl            | 0.02191026 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.468     |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0001     |
|    loss                 | 304        |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.00169   |
|    value_loss           | 637        |
----------------------------------------
Eval num_timesteps=451500, episode_reward=210.44 +/- 99.22
Episode length: 52.96 +/- 24.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=217.76 +/- 103.86
Episode length: 54.78 +/- 25.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.8     |
|    mean_reward     | 218      |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=237.96 +/- 119.18
Episode length: 59.86 +/- 29.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.9     |
|    mean_reward     | 238      |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.5     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 221      |
|    time_elapsed    | 2817     |
|    total_timesteps | 452608   |
---------------------------------
Eval num_timesteps=453000, episode_reward=270.32 +/- 121.04
Episode length: 68.00 +/- 30.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68          |
|    mean_reward          | 270         |
| time/                   |             |
|    total_timesteps      | 453000      |
| train/                  |             |
|    approx_kl            | 0.024059333 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.383      |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0001      |
|    loss                 | 505         |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.00339    |
|    value_loss           | 1.21e+03    |
-----------------------------------------
Eval num_timesteps=453500, episode_reward=274.00 +/- 125.54
Episode length: 68.84 +/- 31.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.8     |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=276.50 +/- 135.45
Episode length: 69.48 +/- 33.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.5     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=290.04 +/- 137.98
Episode length: 72.84 +/- 34.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.8     |
|    mean_reward     | 290      |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.6     |
|    ep_rew_mean     | 365      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 222      |
|    time_elapsed    | 2828     |
|    total_timesteps | 454656   |
---------------------------------
Eval num_timesteps=455000, episode_reward=195.68 +/- 79.08
Episode length: 49.32 +/- 19.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.3        |
|    mean_reward          | 196         |
| time/                   |             |
|    total_timesteps      | 455000      |
| train/                  |             |
|    approx_kl            | 0.033424918 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.424      |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.0001      |
|    loss                 | 373         |
|    n_updates            | 2220        |
|    policy_gradient_loss | 0.0162      |
|    value_loss           | 784         |
-----------------------------------------
Eval num_timesteps=455500, episode_reward=187.38 +/- 67.75
Episode length: 47.20 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.2     |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=202.72 +/- 70.94
Episode length: 50.98 +/- 17.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51       |
|    mean_reward     | 203      |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=215.62 +/- 116.36
Episode length: 54.30 +/- 29.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.4     |
|    ep_rew_mean     | 328      |
| time/              |          |
|    fps             | 160      |
|    iterations      | 223      |
|    time_elapsed    | 2837     |
|    total_timesteps | 456704   |
---------------------------------
Eval num_timesteps=457000, episode_reward=185.72 +/- 68.71
Episode length: 46.78 +/- 17.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 46.8        |
|    mean_reward          | 186         |
| time/                   |             |
|    total_timesteps      | 457000      |
| train/                  |             |
|    approx_kl            | 0.054948114 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.353       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.09e+03    |
|    n_updates            | 2230        |
|    policy_gradient_loss | 0.00417     |
|    value_loss           | 2.25e+03    |
-----------------------------------------
Eval num_timesteps=457500, episode_reward=214.38 +/- 91.59
Episode length: 54.06 +/- 22.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=215.78 +/- 87.72
Episode length: 54.28 +/- 21.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=204.48 +/- 85.74
Episode length: 51.52 +/- 21.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | 204      |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69       |
|    ep_rew_mean     | 274      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 224      |
|    time_elapsed    | 2846     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=459000, episode_reward=209.38 +/- 77.02
Episode length: 52.74 +/- 19.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 52.7       |
|    mean_reward          | 209        |
| time/                   |            |
|    total_timesteps      | 459000     |
| train/                  |            |
|    approx_kl            | 0.03612935 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.234     |
|    explained_variance   | 0.382      |
|    learning_rate        | 0.0001     |
|    loss                 | 750        |
|    n_updates            | 2240       |
|    policy_gradient_loss | 0.00865    |
|    value_loss           | 1.89e+03   |
----------------------------------------
Eval num_timesteps=459500, episode_reward=196.34 +/- 76.61
Episode length: 49.54 +/- 19.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=195.42 +/- 78.99
Episode length: 49.14 +/- 19.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.1     |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=205.24 +/- 89.95
Episode length: 51.70 +/- 22.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.7     |
|    mean_reward     | 205      |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57.7     |
|    ep_rew_mean     | 229      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 225      |
|    time_elapsed    | 2855     |
|    total_timesteps | 460800   |
---------------------------------
Eval num_timesteps=461000, episode_reward=207.56 +/- 75.46
Episode length: 52.20 +/- 18.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 52.2        |
|    mean_reward          | 208         |
| time/                   |             |
|    total_timesteps      | 461000      |
| train/                  |             |
|    approx_kl            | 0.028211707 |
|    clip_fraction        | 0.0933      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.269      |
|    explained_variance   | 0.459       |
|    learning_rate        | 0.0001      |
|    loss                 | 651         |
|    n_updates            | 2250        |
|    policy_gradient_loss | 0.000298    |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=461500, episode_reward=206.74 +/- 90.63
Episode length: 52.06 +/- 22.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=188.82 +/- 74.41
Episode length: 47.60 +/- 18.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.6     |
|    mean_reward     | 189      |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=200.84 +/- 93.64
Episode length: 50.62 +/- 23.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | 201      |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 59.8     |
|    ep_rew_mean     | 238      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 226      |
|    time_elapsed    | 2864     |
|    total_timesteps | 462848   |
---------------------------------
Eval num_timesteps=463000, episode_reward=219.98 +/- 90.94
Episode length: 55.38 +/- 22.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.4        |
|    mean_reward          | 220         |
| time/                   |             |
|    total_timesteps      | 463000      |
| train/                  |             |
|    approx_kl            | 0.034667335 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.294      |
|    explained_variance   | 0.527       |
|    learning_rate        | 0.0001      |
|    loss                 | 438         |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00925    |
|    value_loss           | 1.03e+03    |
-----------------------------------------
Eval num_timesteps=463500, episode_reward=249.84 +/- 94.16
Episode length: 62.86 +/- 23.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.9     |
|    mean_reward     | 250      |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=262.02 +/- 110.04
Episode length: 65.94 +/- 27.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.9     |
|    mean_reward     | 262      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=257.30 +/- 88.83
Episode length: 64.66 +/- 22.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.7     |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66       |
|    ep_rew_mean     | 262      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 227      |
|    time_elapsed    | 2875     |
|    total_timesteps | 464896   |
---------------------------------
Eval num_timesteps=465000, episode_reward=258.12 +/- 116.60
Episode length: 64.92 +/- 29.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 64.9       |
|    mean_reward          | 258        |
| time/                   |            |
|    total_timesteps      | 465000     |
| train/                  |            |
|    approx_kl            | 0.03270751 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.317     |
|    explained_variance   | 0.627      |
|    learning_rate        | 0.0001     |
|    loss                 | 165        |
|    n_updates            | 2270       |
|    policy_gradient_loss | 0.00276    |
|    value_loss           | 552        |
----------------------------------------
Eval num_timesteps=465500, episode_reward=270.54 +/- 109.41
Episode length: 68.06 +/- 27.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.1     |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=288.24 +/- 109.39
Episode length: 72.38 +/- 27.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.4     |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=281.82 +/- 105.26
Episode length: 70.86 +/- 26.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.9     |
|    mean_reward     | 282      |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.4     |
|    ep_rew_mean     | 296      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 228      |
|    time_elapsed    | 2886     |
|    total_timesteps | 466944   |
---------------------------------
Eval num_timesteps=467000, episode_reward=287.68 +/- 121.82
Episode length: 72.26 +/- 30.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.3        |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 467000      |
| train/                  |             |
|    approx_kl            | 0.019607788 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.386      |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0001      |
|    loss                 | 92.7        |
|    n_updates            | 2280        |
|    policy_gradient_loss | 0.0039      |
|    value_loss           | 594         |
-----------------------------------------
Eval num_timesteps=467500, episode_reward=287.08 +/- 126.08
Episode length: 72.16 +/- 31.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.2     |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=273.94 +/- 126.32
Episode length: 68.96 +/- 31.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69       |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=320.28 +/- 132.14
Episode length: 80.44 +/- 33.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.4     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.7     |
|    ep_rew_mean     | 325      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 229      |
|    time_elapsed    | 2897     |
|    total_timesteps | 468992   |
---------------------------------
Eval num_timesteps=469000, episode_reward=315.54 +/- 121.36
Episode length: 79.20 +/- 30.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 79.2        |
|    mean_reward          | 316         |
| time/                   |             |
|    total_timesteps      | 469000      |
| train/                  |             |
|    approx_kl            | 0.024273943 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.457      |
|    explained_variance   | 0.531       |
|    learning_rate        | 0.0001      |
|    loss                 | 264         |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.000577   |
|    value_loss           | 997         |
-----------------------------------------
Eval num_timesteps=469500, episode_reward=319.20 +/- 104.86
Episode length: 80.20 +/- 26.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.2     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=337.04 +/- 109.24
Episode length: 84.62 +/- 27.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.6     |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=329.16 +/- 123.89
Episode length: 82.72 +/- 30.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.7     |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=337.64 +/- 124.67
Episode length: 84.76 +/- 31.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.8     |
|    mean_reward     | 338      |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.2     |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 230      |
|    time_elapsed    | 2913     |
|    total_timesteps | 471040   |
---------------------------------
Eval num_timesteps=471500, episode_reward=217.62 +/- 90.26
Episode length: 54.80 +/- 22.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54.8        |
|    mean_reward          | 218         |
| time/                   |             |
|    total_timesteps      | 471500      |
| train/                  |             |
|    approx_kl            | 0.025517909 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.418      |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.0001      |
|    loss                 | 278         |
|    n_updates            | 2300        |
|    policy_gradient_loss | 0.00815     |
|    value_loss           | 645         |
-----------------------------------------
Eval num_timesteps=472000, episode_reward=225.32 +/- 86.26
Episode length: 56.72 +/- 21.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.7     |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=222.72 +/- 100.89
Episode length: 56.04 +/- 25.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=223.64 +/- 107.46
Episode length: 56.28 +/- 26.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.3     |
|    mean_reward     | 224      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.8     |
|    ep_rew_mean     | 362      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 231      |
|    time_elapsed    | 2923     |
|    total_timesteps | 473088   |
---------------------------------
Eval num_timesteps=473500, episode_reward=221.34 +/- 101.33
Episode length: 55.70 +/- 25.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.7        |
|    mean_reward          | 221         |
| time/                   |             |
|    total_timesteps      | 473500      |
| train/                  |             |
|    approx_kl            | 0.023474816 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.359      |
|    explained_variance   | 0.565       |
|    learning_rate        | 0.0001      |
|    loss                 | 546         |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.0074     |
|    value_loss           | 1.21e+03    |
-----------------------------------------
Eval num_timesteps=474000, episode_reward=218.88 +/- 105.56
Episode length: 55.08 +/- 26.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=222.48 +/- 85.89
Episode length: 56.00 +/- 21.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 222      |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=231.94 +/- 108.88
Episode length: 58.34 +/- 27.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.3     |
|    mean_reward     | 232      |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.9     |
|    ep_rew_mean     | 326      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 232      |
|    time_elapsed    | 2932     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=475500, episode_reward=297.42 +/- 118.71
Episode length: 74.68 +/- 29.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.7      |
|    mean_reward          | 297       |
| time/                   |           |
|    total_timesteps      | 475500    |
| train/                  |           |
|    approx_kl            | 0.0273516 |
|    clip_fraction        | 0.117     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.316    |
|    explained_variance   | 0.435     |
|    learning_rate        | 0.0001    |
|    loss                 | 784       |
|    n_updates            | 2320      |
|    policy_gradient_loss | -0.0048   |
|    value_loss           | 1.71e+03  |
---------------------------------------
Eval num_timesteps=476000, episode_reward=296.16 +/- 115.02
Episode length: 74.48 +/- 28.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.5     |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=303.98 +/- 137.50
Episode length: 76.40 +/- 34.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.4     |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=312.06 +/- 135.51
Episode length: 78.38 +/- 33.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.4     |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 233      |
|    time_elapsed    | 2944     |
|    total_timesteps | 477184   |
---------------------------------
Eval num_timesteps=477500, episode_reward=227.92 +/- 102.94
Episode length: 57.36 +/- 25.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 57.4        |
|    mean_reward          | 228         |
| time/                   |             |
|    total_timesteps      | 477500      |
| train/                  |             |
|    approx_kl            | 0.031553805 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.383      |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0001      |
|    loss                 | 84.7        |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00267    |
|    value_loss           | 418         |
-----------------------------------------
Eval num_timesteps=478000, episode_reward=197.44 +/- 80.44
Episode length: 49.70 +/- 20.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.7     |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=224.76 +/- 99.04
Episode length: 56.56 +/- 24.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.6     |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=212.00 +/- 103.62
Episode length: 53.38 +/- 25.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.2     |
|    ep_rew_mean     | 295      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 234      |
|    time_elapsed    | 2953     |
|    total_timesteps | 479232   |
---------------------------------
Eval num_timesteps=479500, episode_reward=254.02 +/- 114.96
Episode length: 63.86 +/- 28.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 63.9       |
|    mean_reward          | 254        |
| time/                   |            |
|    total_timesteps      | 479500     |
| train/                  |            |
|    approx_kl            | 0.01727955 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.48       |
|    learning_rate        | 0.0001     |
|    loss                 | 484        |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.00154   |
|    value_loss           | 1.4e+03    |
----------------------------------------
Eval num_timesteps=480000, episode_reward=229.16 +/- 107.01
Episode length: 57.64 +/- 26.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.6     |
|    mean_reward     | 229      |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=246.06 +/- 132.88
Episode length: 61.88 +/- 33.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.9     |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=234.78 +/- 107.76
Episode length: 59.10 +/- 26.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.1     |
|    mean_reward     | 235      |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80       |
|    ep_rew_mean     | 318      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 235      |
|    time_elapsed    | 2964     |
|    total_timesteps | 481280   |
---------------------------------
Eval num_timesteps=481500, episode_reward=259.66 +/- 129.24
Episode length: 65.24 +/- 32.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 65.2        |
|    mean_reward          | 260         |
| time/                   |             |
|    total_timesteps      | 481500      |
| train/                  |             |
|    approx_kl            | 0.018961642 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.387      |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0001      |
|    loss                 | 300         |
|    n_updates            | 2350        |
|    policy_gradient_loss | 0.0125      |
|    value_loss           | 644         |
-----------------------------------------
Eval num_timesteps=482000, episode_reward=236.38 +/- 123.52
Episode length: 59.44 +/- 30.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.4     |
|    mean_reward     | 236      |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=231.94 +/- 100.56
Episode length: 58.30 +/- 25.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.3     |
|    mean_reward     | 232      |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=246.46 +/- 130.68
Episode length: 61.94 +/- 32.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.9     |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.9     |
|    ep_rew_mean     | 346      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 236      |
|    time_elapsed    | 2974     |
|    total_timesteps | 483328   |
---------------------------------
Eval num_timesteps=483500, episode_reward=226.12 +/- 104.14
Episode length: 56.92 +/- 26.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.9        |
|    mean_reward          | 226         |
| time/                   |             |
|    total_timesteps      | 483500      |
| train/                  |             |
|    approx_kl            | 0.017968468 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.407      |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0001      |
|    loss                 | 296         |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.00418    |
|    value_loss           | 736         |
-----------------------------------------
Eval num_timesteps=484000, episode_reward=258.16 +/- 117.75
Episode length: 64.98 +/- 29.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65       |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=236.12 +/- 119.14
Episode length: 59.44 +/- 29.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.4     |
|    mean_reward     | 236      |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=255.34 +/- 103.43
Episode length: 64.26 +/- 25.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.3     |
|    mean_reward     | 255      |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.1     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 237      |
|    time_elapsed    | 2984     |
|    total_timesteps | 485376   |
---------------------------------
Eval num_timesteps=485500, episode_reward=202.06 +/- 91.79
Episode length: 50.88 +/- 22.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.9        |
|    mean_reward          | 202         |
| time/                   |             |
|    total_timesteps      | 485500      |
| train/                  |             |
|    approx_kl            | 0.020211311 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.334      |
|    explained_variance   | 0.83        |
|    learning_rate        | 0.0001      |
|    loss                 | 248         |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00122    |
|    value_loss           | 463         |
-----------------------------------------
Eval num_timesteps=486000, episode_reward=213.94 +/- 101.58
Episode length: 53.84 +/- 25.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=207.00 +/- 86.48
Episode length: 52.16 +/- 21.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=200.78 +/- 85.78
Episode length: 50.54 +/- 21.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | 201      |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93       |
|    ep_rew_mean     | 370      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 238      |
|    time_elapsed    | 2993     |
|    total_timesteps | 487424   |
---------------------------------
Eval num_timesteps=487500, episode_reward=194.94 +/- 79.07
Episode length: 49.08 +/- 19.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 49.1        |
|    mean_reward          | 195         |
| time/                   |             |
|    total_timesteps      | 487500      |
| train/                  |             |
|    approx_kl            | 0.016076412 |
|    clip_fraction        | 0.0937      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.295      |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0001      |
|    loss                 | 279         |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 1.13e+03    |
-----------------------------------------
Eval num_timesteps=488000, episode_reward=192.10 +/- 103.77
Episode length: 48.42 +/- 25.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.4     |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=220.80 +/- 90.55
Episode length: 55.56 +/- 22.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | 221      |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=246.58 +/- 120.20
Episode length: 62.02 +/- 29.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62       |
|    mean_reward     | 247      |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.3     |
|    ep_rew_mean     | 380      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 239      |
|    time_elapsed    | 3003     |
|    total_timesteps | 489472   |
---------------------------------
Eval num_timesteps=489500, episode_reward=215.12 +/- 90.54
Episode length: 54.14 +/- 22.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 54.1       |
|    mean_reward          | 215        |
| time/                   |            |
|    total_timesteps      | 489500     |
| train/                  |            |
|    approx_kl            | 0.04557405 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.348     |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.0001     |
|    loss                 | 449        |
|    n_updates            | 2390       |
|    policy_gradient_loss | 0.028      |
|    value_loss           | 1.35e+03   |
----------------------------------------
Eval num_timesteps=490000, episode_reward=212.72 +/- 105.29
Episode length: 53.60 +/- 26.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.6     |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=223.66 +/- 118.65
Episode length: 56.30 +/- 29.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.3     |
|    mean_reward     | 224      |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=234.24 +/- 93.64
Episode length: 58.94 +/- 23.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.9     |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=214.72 +/- 97.04
Episode length: 54.12 +/- 24.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | 215      |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.6     |
|    ep_rew_mean     | 365      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 240      |
|    time_elapsed    | 3014     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=492000, episode_reward=281.48 +/- 120.42
Episode length: 70.68 +/- 30.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 70.7        |
|    mean_reward          | 281         |
| time/                   |             |
|    total_timesteps      | 492000      |
| train/                  |             |
|    approx_kl            | 0.026229745 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.381      |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0001      |
|    loss                 | 300         |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.00205    |
|    value_loss           | 915         |
-----------------------------------------
Eval num_timesteps=492500, episode_reward=264.92 +/- 121.26
Episode length: 66.52 +/- 30.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.5     |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=247.86 +/- 107.27
Episode length: 62.36 +/- 26.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.4     |
|    mean_reward     | 248      |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=294.06 +/- 132.95
Episode length: 73.88 +/- 33.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 90.7     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 241      |
|    time_elapsed    | 3025     |
|    total_timesteps | 493568   |
---------------------------------
Eval num_timesteps=494000, episode_reward=273.42 +/- 148.70
Episode length: 68.68 +/- 37.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 68.7        |
|    mean_reward          | 273         |
| time/                   |             |
|    total_timesteps      | 494000      |
| train/                  |             |
|    approx_kl            | 0.021494618 |
|    clip_fraction        | 0.0917      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.417      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0001      |
|    loss                 | 344         |
|    n_updates            | 2410        |
|    policy_gradient_loss | 5.28e-05    |
|    value_loss           | 975         |
-----------------------------------------
Eval num_timesteps=494500, episode_reward=259.02 +/- 131.65
Episode length: 65.06 +/- 33.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.1     |
|    mean_reward     | 259      |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=275.74 +/- 122.42
Episode length: 69.30 +/- 30.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.3     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=233.80 +/- 95.68
Episode length: 58.78 +/- 23.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.8     |
|    mean_reward     | 234      |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.1     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 242      |
|    time_elapsed    | 3036     |
|    total_timesteps | 495616   |
---------------------------------
Eval num_timesteps=496000, episode_reward=302.46 +/- 117.11
Episode length: 75.92 +/- 29.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 75.9        |
|    mean_reward          | 302         |
| time/                   |             |
|    total_timesteps      | 496000      |
| train/                  |             |
|    approx_kl            | 0.024978971 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.433      |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0001      |
|    loss                 | 328         |
|    n_updates            | 2420        |
|    policy_gradient_loss | 0.00182     |
|    value_loss           | 727         |
-----------------------------------------
Eval num_timesteps=496500, episode_reward=311.98 +/- 124.39
Episode length: 78.42 +/- 31.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.4     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=295.98 +/- 123.35
Episode length: 74.40 +/- 30.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.4     |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=261.18 +/- 112.18
Episode length: 65.68 +/- 27.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.7     |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.2     |
|    ep_rew_mean     | 371      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 243      |
|    time_elapsed    | 3048     |
|    total_timesteps | 497664   |
---------------------------------
Eval num_timesteps=498000, episode_reward=277.82 +/- 126.09
Episode length: 69.82 +/- 31.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 69.8        |
|    mean_reward          | 278         |
| time/                   |             |
|    total_timesteps      | 498000      |
| train/                  |             |
|    approx_kl            | 0.021921922 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.43       |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0001      |
|    loss                 | 326         |
|    n_updates            | 2430        |
|    policy_gradient_loss | 0.00349     |
|    value_loss           | 893         |
-----------------------------------------
Eval num_timesteps=498500, episode_reward=263.04 +/- 144.89
Episode length: 66.10 +/- 36.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.1     |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=290.40 +/- 134.02
Episode length: 73.02 +/- 33.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73       |
|    mean_reward     | 290      |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=272.58 +/- 135.34
Episode length: 68.44 +/- 33.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.4     |
|    mean_reward     | 273      |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.4     |
|    ep_rew_mean     | 380      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 244      |
|    time_elapsed    | 3060     |
|    total_timesteps | 499712   |
---------------------------------
Eval num_timesteps=500000, episode_reward=306.68 +/- 126.87
Episode length: 76.98 +/- 31.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 77          |
|    mean_reward          | 307         |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.016871922 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.396      |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0001      |
|    loss                 | 619         |
|    n_updates            | 2440        |
|    policy_gradient_loss | 0.00367     |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=500500, episode_reward=291.74 +/- 123.79
Episode length: 73.30 +/- 30.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.3     |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=280.68 +/- 133.15
Episode length: 70.60 +/- 33.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.6     |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=274.08 +/- 141.12
Episode length: 68.90 +/- 35.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.9     |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.6     |
|    ep_rew_mean     | 373      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 245      |
|    time_elapsed    | 3072     |
|    total_timesteps | 501760   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/take-cover/ppo-4/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 0.0001, 'gamma': 0.94, 'gae_lambda': 0.93}
Training steps: 500000
Frame skip: 4
