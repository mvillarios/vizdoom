/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Eval num_timesteps=500, episode_reward=481.43 +/- 671.18
Episode length: 36.38 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 481      |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=399.19 +/- 701.80
Episode length: 35.02 +/- 7.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 399      |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 417      |
| time/              |          |
|    fps             | 294      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
Eval num_timesteps=1500, episode_reward=526.91 +/- 834.86
Episode length: 35.20 +/- 7.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.2        |
|    mean_reward          | 527         |
| time/                   |             |
|    total_timesteps      | 1500        |
| train/                  |             |
|    approx_kl            | 0.017775673 |
|    clip_fraction        | 0.0043      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0163     |
|    explained_variance   | -0.214      |
|    learning_rate        | 0.001       |
|    loss                 | 3.44e+04    |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.00185    |
|    value_loss           | 9.38e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=409.60 +/- 681.37
Episode length: 35.72 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 410      |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=322.51 +/- 660.98
Episode length: 34.78 +/- 7.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 323           |
| time/                   |               |
|    total_timesteps      | 2500          |
| train/                  |               |
|    approx_kl            | 0.00056367967 |
|    clip_fraction        | 0.00215       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0151       |
|    explained_variance   | -0.021        |
|    learning_rate        | 0.001         |
|    loss                 | 4.62e+04      |
|    n_updates            | 5320          |
|    policy_gradient_loss | -0.000702     |
|    value_loss           | 9.7e+04       |
-------------------------------------------
Eval num_timesteps=3000, episode_reward=496.41 +/- 800.44
Episode length: 35.72 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 496      |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 284      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 3        |
|    time_elapsed    | 10       |
|    total_timesteps | 3072     |
---------------------------------
Eval num_timesteps=3500, episode_reward=396.41 +/- 690.45
Episode length: 35.44 +/- 6.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | 396         |
| time/                   |             |
|    total_timesteps      | 3500        |
| train/                  |             |
|    approx_kl            | 0.004394104 |
|    clip_fraction        | 0.00283     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0301     |
|    explained_variance   | -0.15       |
|    learning_rate        | 0.001       |
|    loss                 | 2.42e+04    |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.000751   |
|    value_loss           | 5.75e+04    |
-----------------------------------------
Eval num_timesteps=4000, episode_reward=574.06 +/- 769.74
Episode length: 36.42 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 574      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.2     |
|    ep_rew_mean     | 217      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 4        |
|    time_elapsed    | 14       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=376.32 +/- 669.59
Episode length: 36.10 +/- 5.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 376          |
| time/                   |              |
|    total_timesteps      | 4500         |
| train/                  |              |
|    approx_kl            | 0.0068802848 |
|    clip_fraction        | 0.00547      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0348      |
|    explained_variance   | 0.046        |
|    learning_rate        | 0.001        |
|    loss                 | 3.83e+04     |
|    n_updates            | 5340         |
|    policy_gradient_loss | 0.0044       |
|    value_loss           | 8.24e+04     |
------------------------------------------
Eval num_timesteps=5000, episode_reward=419.72 +/- 704.54
Episode length: 35.48 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 372      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 5        |
|    time_elapsed    | 18       |
|    total_timesteps | 5120     |
---------------------------------
Eval num_timesteps=5500, episode_reward=284.37 +/- 607.32
Episode length: 34.60 +/- 6.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 5500         |
| train/                  |              |
|    approx_kl            | 0.0018745302 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00275     |
|    explained_variance   | 0.0191       |
|    learning_rate        | 0.001        |
|    loss                 | 3.02e+04     |
|    n_updates            | 5350         |
|    policy_gradient_loss | -0.000583    |
|    value_loss           | 8.11e+04     |
------------------------------------------
Eval num_timesteps=6000, episode_reward=463.88 +/- 769.76
Episode length: 35.42 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 464      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 419      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 6        |
|    time_elapsed    | 21       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=300.50 +/- 618.60
Episode length: 34.98 +/- 6.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 300           |
| time/                   |               |
|    total_timesteps      | 6500          |
| train/                  |               |
|    approx_kl            | 1.3447541e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00303      |
|    explained_variance   | 0.0309        |
|    learning_rate        | 0.001         |
|    loss                 | 2.39e+04      |
|    n_updates            | 5360          |
|    policy_gradient_loss | -0.00054      |
|    value_loss           | 9.22e+04      |
-------------------------------------------
Eval num_timesteps=7000, episode_reward=453.43 +/- 737.04
Episode length: 35.38 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 499      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 7        |
|    time_elapsed    | 25       |
|    total_timesteps | 7168     |
---------------------------------
Eval num_timesteps=7500, episode_reward=430.02 +/- 711.03
Episode length: 34.56 +/- 5.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 430          |
| time/                   |              |
|    total_timesteps      | 7500         |
| train/                  |              |
|    approx_kl            | 0.0071275155 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00146     |
|    explained_variance   | 0.00804      |
|    learning_rate        | 0.001        |
|    loss                 | 3.25e+04     |
|    n_updates            | 5370         |
|    policy_gradient_loss | 0.000525     |
|    value_loss           | 9.93e+04     |
------------------------------------------
Eval num_timesteps=8000, episode_reward=402.63 +/- 728.27
Episode length: 34.90 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 543      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 8        |
|    time_elapsed    | 28       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=379.01 +/- 697.80
Episode length: 35.02 +/- 6.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 379          |
| time/                   |              |
|    total_timesteps      | 8500         |
| train/                  |              |
|    approx_kl            | 2.880348e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00248     |
|    explained_variance   | 0.0138       |
|    learning_rate        | 0.001        |
|    loss                 | 4.1e+04      |
|    n_updates            | 5380         |
|    policy_gradient_loss | -0.000151    |
|    value_loss           | 1.09e+05     |
------------------------------------------
Eval num_timesteps=9000, episode_reward=448.53 +/- 720.35
Episode length: 35.30 +/- 7.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 449      |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 500      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 9        |
|    time_elapsed    | 32       |
|    total_timesteps | 9216     |
---------------------------------
Eval num_timesteps=9500, episode_reward=428.05 +/- 793.96
Episode length: 34.58 +/- 7.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 428          |
| time/                   |              |
|    total_timesteps      | 9500         |
| train/                  |              |
|    approx_kl            | 0.0013619286 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0123      |
|    explained_variance   | -0.104       |
|    learning_rate        | 0.001        |
|    loss                 | 4.09e+04     |
|    n_updates            | 5390         |
|    policy_gradient_loss | 0.000105     |
|    value_loss           | 1.07e+05     |
------------------------------------------
Eval num_timesteps=10000, episode_reward=617.85 +/- 791.42
Episode length: 36.20 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 415      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 10       |
|    time_elapsed    | 36       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=619.95 +/- 850.29
Episode length: 35.68 +/- 7.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 620           |
| time/                   |               |
|    total_timesteps      | 10500         |
| train/                  |               |
|    approx_kl            | 2.2478576e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0189       |
|    explained_variance   | 0.00766       |
|    learning_rate        | 0.001         |
|    loss                 | 4.72e+04      |
|    n_updates            | 5400          |
|    policy_gradient_loss | -0.00106      |
|    value_loss           | 8.03e+04      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=11000, episode_reward=458.59 +/- 673.36
Episode length: 36.22 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 331      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 11       |
|    time_elapsed    | 39       |
|    total_timesteps | 11264    |
---------------------------------
Eval num_timesteps=11500, episode_reward=250.90 +/- 634.40
Episode length: 33.52 +/- 6.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.5          |
|    mean_reward          | 251           |
| time/                   |               |
|    total_timesteps      | 11500         |
| train/                  |               |
|    approx_kl            | 0.00011170737 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0467       |
|    explained_variance   | -0.128        |
|    learning_rate        | 0.001         |
|    loss                 | 2.26e+04      |
|    n_updates            | 5410          |
|    policy_gradient_loss | -0.00138      |
|    value_loss           | 7.08e+04      |
-------------------------------------------
Eval num_timesteps=12000, episode_reward=228.41 +/- 562.67
Episode length: 34.70 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 247      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 12       |
|    time_elapsed    | 43       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=516.70 +/- 786.92
Episode length: 35.60 +/- 6.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 517         |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.011891529 |
|    clip_fraction        | 0.00771     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0437     |
|    explained_variance   | 0.0809      |
|    learning_rate        | 0.001       |
|    loss                 | 1.63e+04    |
|    n_updates            | 5420        |
|    policy_gradient_loss | 0.00597     |
|    value_loss           | 5.77e+04    |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=331.60 +/- 648.87
Episode length: 34.92 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 238      |
| time/              |          |
|    fps             | 283      |
|    iterations      | 13       |
|    time_elapsed    | 46       |
|    total_timesteps | 13312    |
---------------------------------
Eval num_timesteps=13500, episode_reward=512.57 +/- 724.93
Episode length: 36.36 +/- 5.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.4        |
|    mean_reward          | 513         |
| time/                   |             |
|    total_timesteps      | 13500       |
| train/                  |             |
|    approx_kl            | 0.023815237 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0182     |
|    explained_variance   | -0.124      |
|    learning_rate        | 0.001       |
|    loss                 | 1.68e+04    |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.00161    |
|    value_loss           | 6.7e+04     |
-----------------------------------------
Eval num_timesteps=14000, episode_reward=303.54 +/- 638.92
Episode length: 33.96 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 334      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 14       |
|    time_elapsed    | 50       |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=455.90 +/- 707.63
Episode length: 35.78 +/- 6.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 456           |
| time/                   |               |
|    total_timesteps      | 14500         |
| train/                  |               |
|    approx_kl            | 1.5289464e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00845      |
|    explained_variance   | -0.151        |
|    learning_rate        | 0.001         |
|    loss                 | 2.75e+04      |
|    n_updates            | 5440          |
|    policy_gradient_loss | -0.00128      |
|    value_loss           | 7.92e+04      |
-------------------------------------------
Eval num_timesteps=15000, episode_reward=294.58 +/- 625.68
Episode length: 34.28 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 295      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 384      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 15       |
|    time_elapsed    | 54       |
|    total_timesteps | 15360    |
---------------------------------
Eval num_timesteps=15500, episode_reward=406.04 +/- 725.65
Episode length: 34.96 +/- 7.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 406          |
| time/                   |              |
|    total_timesteps      | 15500        |
| train/                  |              |
|    approx_kl            | 0.0022173412 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00807     |
|    explained_variance   | 0.0247       |
|    learning_rate        | 0.001        |
|    loss                 | 2.26e+04     |
|    n_updates            | 5450         |
|    policy_gradient_loss | 0.000353     |
|    value_loss           | 8.32e+04     |
------------------------------------------
Eval num_timesteps=16000, episode_reward=432.41 +/- 718.38
Episode length: 35.24 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 441      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 16       |
|    time_elapsed    | 57       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=511.11 +/- 775.01
Episode length: 35.38 +/- 6.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 511           |
| time/                   |               |
|    total_timesteps      | 16500         |
| train/                  |               |
|    approx_kl            | 5.9770537e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00432      |
|    explained_variance   | 0.128         |
|    learning_rate        | 0.001         |
|    loss                 | 4.1e+04       |
|    n_updates            | 5460          |
|    policy_gradient_loss | -0.000405     |
|    value_loss           | 9.41e+04      |
-------------------------------------------
Eval num_timesteps=17000, episode_reward=445.70 +/- 743.38
Episode length: 34.72 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 446      |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 365      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 17       |
|    time_elapsed    | 61       |
|    total_timesteps | 17408    |
---------------------------------
Eval num_timesteps=17500, episode_reward=522.24 +/- 807.37
Episode length: 35.08 +/- 7.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 522           |
| time/                   |               |
|    total_timesteps      | 17500         |
| train/                  |               |
|    approx_kl            | 4.6281493e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00392      |
|    explained_variance   | -0.0146       |
|    learning_rate        | 0.001         |
|    loss                 | 4.54e+04      |
|    n_updates            | 5470          |
|    policy_gradient_loss | -0.000296     |
|    value_loss           | 9.91e+04      |
-------------------------------------------
Eval num_timesteps=18000, episode_reward=397.62 +/- 702.13
Episode length: 35.50 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 398      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 372      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 18       |
|    time_elapsed    | 64       |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=410.76 +/- 755.82
Episode length: 35.00 +/- 6.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 411           |
| time/                   |               |
|    total_timesteps      | 18500         |
| train/                  |               |
|    approx_kl            | 1.4685153e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00757      |
|    explained_variance   | -0.0616       |
|    learning_rate        | 0.001         |
|    loss                 | 2.78e+04      |
|    n_updates            | 5480          |
|    policy_gradient_loss | -0.000655     |
|    value_loss           | 7.99e+04      |
-------------------------------------------
Eval num_timesteps=19000, episode_reward=492.35 +/- 750.68
Episode length: 35.30 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 492      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 431      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 19       |
|    time_elapsed    | 68       |
|    total_timesteps | 19456    |
---------------------------------
Eval num_timesteps=19500, episode_reward=497.01 +/- 777.51
Episode length: 35.62 +/- 6.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 497          |
| time/                   |              |
|    total_timesteps      | 19500        |
| train/                  |              |
|    approx_kl            | 0.0031020406 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00302     |
|    explained_variance   | 0.118        |
|    learning_rate        | 0.001        |
|    loss                 | 3.96e+04     |
|    n_updates            | 5490         |
|    policy_gradient_loss | -0.000326    |
|    value_loss           | 8.93e+04     |
------------------------------------------
Eval num_timesteps=20000, episode_reward=441.38 +/- 704.96
Episode length: 35.58 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 385      |
| time/              |          |
|    fps             | 284      |
|    iterations      | 20       |
|    time_elapsed    | 71       |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=495.97 +/- 714.76
Episode length: 35.80 +/- 5.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 496           |
| time/                   |               |
|    total_timesteps      | 20500         |
| train/                  |               |
|    approx_kl            | 1.2935721e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0115       |
|    explained_variance   | 0.0201        |
|    learning_rate        | 0.001         |
|    loss                 | 2.95e+04      |
|    n_updates            | 5500          |
|    policy_gradient_loss | -0.000566     |
|    value_loss           | 8.28e+04      |
-------------------------------------------
Eval num_timesteps=21000, episode_reward=477.35 +/- 757.18
Episode length: 35.20 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=357.81 +/- 657.83
Episode length: 35.00 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 406      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 21       |
|    time_elapsed    | 76       |
|    total_timesteps | 21504    |
---------------------------------
Eval num_timesteps=22000, episode_reward=510.31 +/- 760.95
Episode length: 35.40 +/- 7.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 510           |
| time/                   |               |
|    total_timesteps      | 22000         |
| train/                  |               |
|    approx_kl            | 2.7313537e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00791      |
|    explained_variance   | 0.0984        |
|    learning_rate        | 0.001         |
|    loss                 | 3.71e+04      |
|    n_updates            | 5510          |
|    policy_gradient_loss | -0.00108      |
|    value_loss           | 9.81e+04      |
-------------------------------------------
Eval num_timesteps=22500, episode_reward=458.69 +/- 700.39
Episode length: 35.62 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 22       |
|    time_elapsed    | 80       |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=448.79 +/- 668.39
Episode length: 35.70 +/- 5.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 449          |
| time/                   |              |
|    total_timesteps      | 23000        |
| train/                  |              |
|    approx_kl            | 6.735325e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00441     |
|    explained_variance   | -0.201       |
|    learning_rate        | 0.001        |
|    loss                 | 4.59e+04     |
|    n_updates            | 5520         |
|    policy_gradient_loss | -0.000436    |
|    value_loss           | 8.81e+04     |
------------------------------------------
Eval num_timesteps=23500, episode_reward=475.57 +/- 724.79
Episode length: 35.12 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 476      |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 23       |
|    time_elapsed    | 84       |
|    total_timesteps | 23552    |
---------------------------------
Eval num_timesteps=24000, episode_reward=417.67 +/- 780.96
Episode length: 33.78 +/- 7.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.8        |
|    mean_reward          | 418         |
| time/                   |             |
|    total_timesteps      | 24000       |
| train/                  |             |
|    approx_kl            | 1.75467e-05 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0108     |
|    explained_variance   | 0.0032      |
|    learning_rate        | 0.001       |
|    loss                 | 3.23e+04    |
|    n_updates            | 5530        |
|    policy_gradient_loss | -0.000383   |
|    value_loss           | 8.17e+04    |
-----------------------------------------
Eval num_timesteps=24500, episode_reward=445.07 +/- 769.30
Episode length: 35.22 +/- 7.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 273      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 24       |
|    time_elapsed    | 88       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=514.33 +/- 726.69
Episode length: 36.26 +/- 6.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 514           |
| time/                   |               |
|    total_timesteps      | 25000         |
| train/                  |               |
|    approx_kl            | 0.00040668814 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0105       |
|    explained_variance   | 0.0683        |
|    learning_rate        | 0.001         |
|    loss                 | 3.26e+04      |
|    n_updates            | 5540          |
|    policy_gradient_loss | -0.000977     |
|    value_loss           | 6.86e+04      |
-------------------------------------------
Eval num_timesteps=25500, episode_reward=437.23 +/- 678.76
Episode length: 36.00 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 295      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 25       |
|    time_elapsed    | 91       |
|    total_timesteps | 25600    |
---------------------------------
Eval num_timesteps=26000, episode_reward=421.80 +/- 687.83
Episode length: 35.88 +/- 6.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 422          |
| time/                   |              |
|    total_timesteps      | 26000        |
| train/                  |              |
|    approx_kl            | 0.0016068744 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00991     |
|    explained_variance   | -0.0446      |
|    learning_rate        | 0.001        |
|    loss                 | 4.77e+04     |
|    n_updates            | 5550         |
|    policy_gradient_loss | -0.000895    |
|    value_loss           | 9.62e+04     |
------------------------------------------
Eval num_timesteps=26500, episode_reward=515.44 +/- 751.09
Episode length: 35.84 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 515      |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 280      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 26       |
|    time_elapsed    | 95       |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=493.56 +/- 722.94
Episode length: 36.12 +/- 5.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 494          |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0023487965 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0179      |
|    explained_variance   | -0.0919      |
|    learning_rate        | 0.001        |
|    loss                 | 2.7e+04      |
|    n_updates            | 5560         |
|    policy_gradient_loss | 0.000656     |
|    value_loss           | 7.47e+04     |
------------------------------------------
Eval num_timesteps=27500, episode_reward=512.91 +/- 793.87
Episode length: 35.84 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 513      |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 368      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 27       |
|    time_elapsed    | 98       |
|    total_timesteps | 27648    |
---------------------------------
Eval num_timesteps=28000, episode_reward=474.74 +/- 711.16
Episode length: 35.64 +/- 6.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 475           |
| time/                   |               |
|    total_timesteps      | 28000         |
| train/                  |               |
|    approx_kl            | 1.8242456e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00365      |
|    explained_variance   | 0.0805        |
|    learning_rate        | 0.001         |
|    loss                 | 3.47e+04      |
|    n_updates            | 5570          |
|    policy_gradient_loss | -0.00111      |
|    value_loss           | 9.13e+04      |
-------------------------------------------
Eval num_timesteps=28500, episode_reward=531.81 +/- 761.91
Episode length: 35.28 +/- 7.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 532      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 472      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 28       |
|    time_elapsed    | 102      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=417.77 +/- 724.67
Episode length: 34.38 +/- 6.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 418          |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0066972533 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00199     |
|    explained_variance   | 0.013        |
|    learning_rate        | 0.001        |
|    loss                 | 3.46e+04     |
|    n_updates            | 5580         |
|    policy_gradient_loss | 0.00209      |
|    value_loss           | 7.73e+04     |
------------------------------------------
Eval num_timesteps=29500, episode_reward=358.91 +/- 673.67
Episode length: 34.40 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 550      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 29       |
|    time_elapsed    | 106      |
|    total_timesteps | 29696    |
---------------------------------
Eval num_timesteps=30000, episode_reward=463.72 +/- 735.75
Episode length: 36.00 +/- 6.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 464           |
| time/                   |               |
|    total_timesteps      | 30000         |
| train/                  |               |
|    approx_kl            | 1.1944794e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000977     |
|    explained_variance   | 0.0307        |
|    learning_rate        | 0.001         |
|    loss                 | 2.88e+04      |
|    n_updates            | 5590          |
|    policy_gradient_loss | -4.57e-05     |
|    value_loss           | 9.62e+04      |
-------------------------------------------
Eval num_timesteps=30500, episode_reward=396.04 +/- 718.83
Episode length: 35.54 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 565      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 30       |
|    time_elapsed    | 109      |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=438.04 +/- 766.48
Episode length: 35.28 +/- 6.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 438           |
| time/                   |               |
|    total_timesteps      | 31000         |
| train/                  |               |
|    approx_kl            | 0.00021951873 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00391      |
|    explained_variance   | -0.0883       |
|    learning_rate        | 0.001         |
|    loss                 | 4.59e+04      |
|    n_updates            | 5600          |
|    policy_gradient_loss | -0.00018      |
|    value_loss           | 1.13e+05      |
-------------------------------------------
Eval num_timesteps=31500, episode_reward=418.70 +/- 724.98
Episode length: 35.12 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 553      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 31       |
|    time_elapsed    | 113      |
|    total_timesteps | 31744    |
---------------------------------
Eval num_timesteps=32000, episode_reward=359.73 +/- 706.56
Episode length: 35.02 +/- 6.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 360          |
| time/                   |              |
|    total_timesteps      | 32000        |
| train/                  |              |
|    approx_kl            | 7.483311e-05 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00256     |
|    explained_variance   | -0.0136      |
|    learning_rate        | 0.001        |
|    loss                 | 3.28e+04     |
|    n_updates            | 5610         |
|    policy_gradient_loss | -0.000501    |
|    value_loss           | 9.09e+04     |
------------------------------------------
Eval num_timesteps=32500, episode_reward=484.47 +/- 746.85
Episode length: 35.86 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 484      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 487      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 32       |
|    time_elapsed    | 116      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=450.81 +/- 646.63
Episode length: 37.70 +/- 5.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.7         |
|    mean_reward          | 451          |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 7.611292e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0031      |
|    explained_variance   | -0.337       |
|    learning_rate        | 0.001        |
|    loss                 | 4.63e+04     |
|    n_updates            | 5620         |
|    policy_gradient_loss | -0.000384    |
|    value_loss           | 8.97e+04     |
------------------------------------------
Eval num_timesteps=33500, episode_reward=483.04 +/- 748.67
Episode length: 35.80 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 483      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 506      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 33       |
|    time_elapsed    | 120      |
|    total_timesteps | 33792    |
---------------------------------
Eval num_timesteps=34000, episode_reward=663.73 +/- 826.67
Episode length: 36.94 +/- 6.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.9         |
|    mean_reward          | 664          |
| time/                   |              |
|    total_timesteps      | 34000        |
| train/                  |              |
|    approx_kl            | 8.948322e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00624     |
|    explained_variance   | 0.0123       |
|    learning_rate        | 0.001        |
|    loss                 | 3.23e+04     |
|    n_updates            | 5630         |
|    policy_gradient_loss | -0.000499    |
|    value_loss           | 1.05e+05     |
------------------------------------------
New best mean reward!
Eval num_timesteps=34500, episode_reward=336.38 +/- 703.40
Episode length: 34.94 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 336      |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 372      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 34       |
|    time_elapsed    | 124      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=337.48 +/- 662.75
Episode length: 34.46 +/- 6.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 337           |
| time/                   |               |
|    total_timesteps      | 35000         |
| train/                  |               |
|    approx_kl            | 0.00054512284 |
|    clip_fraction        | 0.00342       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0385       |
|    explained_variance   | -0.139        |
|    learning_rate        | 0.001         |
|    loss                 | 2.48e+04      |
|    n_updates            | 5640          |
|    policy_gradient_loss | -5.15e-05     |
|    value_loss           | 6.85e+04      |
-------------------------------------------
Eval num_timesteps=35500, episode_reward=361.28 +/- 693.44
Episode length: 34.76 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 340      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 35       |
|    time_elapsed    | 127      |
|    total_timesteps | 35840    |
---------------------------------
Eval num_timesteps=36000, episode_reward=355.41 +/- 676.84
Episode length: 34.52 +/- 6.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.5         |
|    mean_reward          | 355          |
| time/                   |              |
|    total_timesteps      | 36000        |
| train/                  |              |
|    approx_kl            | 0.0041966755 |
|    clip_fraction        | 0.00283      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0096      |
|    explained_variance   | 0.0206       |
|    learning_rate        | 0.001        |
|    loss                 | 2.94e+04     |
|    n_updates            | 5650         |
|    policy_gradient_loss | -0.00305     |
|    value_loss           | 6.53e+04     |
------------------------------------------
Eval num_timesteps=36500, episode_reward=647.87 +/- 800.32
Episode length: 36.56 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 648      |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 263      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 36       |
|    time_elapsed    | 131      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=435.07 +/- 755.30
Episode length: 35.66 +/- 6.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 435          |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 0.0005812977 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.022       |
|    explained_variance   | -0.0029      |
|    learning_rate        | 0.001        |
|    loss                 | 4.02e+04     |
|    n_updates            | 5660         |
|    policy_gradient_loss | -0.000445    |
|    value_loss           | 9.13e+04     |
------------------------------------------
Eval num_timesteps=37500, episode_reward=480.75 +/- 725.51
Episode length: 35.04 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 481      |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 37       |
|    time_elapsed    | 134      |
|    total_timesteps | 37888    |
---------------------------------
Eval num_timesteps=38000, episode_reward=321.39 +/- 690.09
Episode length: 34.30 +/- 6.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.3        |
|    mean_reward          | 321         |
| time/                   |             |
|    total_timesteps      | 38000       |
| train/                  |             |
|    approx_kl            | 0.002349722 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00811    |
|    explained_variance   | -0.0601     |
|    learning_rate        | 0.001       |
|    loss                 | 3.5e+04     |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.000671   |
|    value_loss           | 9.05e+04    |
-----------------------------------------
Eval num_timesteps=38500, episode_reward=533.03 +/- 775.60
Episode length: 35.70 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 306      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 38       |
|    time_elapsed    | 138      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=461.36 +/- 726.87
Episode length: 35.42 +/- 7.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 461           |
| time/                   |               |
|    total_timesteps      | 39000         |
| train/                  |               |
|    approx_kl            | 5.9716403e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00905      |
|    explained_variance   | 0.049         |
|    learning_rate        | 0.001         |
|    loss                 | 2.55e+04      |
|    n_updates            | 5680          |
|    policy_gradient_loss | -0.000446     |
|    value_loss           | 7.01e+04      |
-------------------------------------------
Eval num_timesteps=39500, episode_reward=425.46 +/- 668.54
Episode length: 35.80 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 280      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 39       |
|    time_elapsed    | 141      |
|    total_timesteps | 39936    |
---------------------------------
Eval num_timesteps=40000, episode_reward=442.83 +/- 741.91
Episode length: 35.12 +/- 7.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.1        |
|    mean_reward          | 443         |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.003507645 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00918    |
|    explained_variance   | -0.0308     |
|    learning_rate        | 0.001       |
|    loss                 | 2.88e+04    |
|    n_updates            | 5690        |
|    policy_gradient_loss | -6.97e-05   |
|    value_loss           | 7.91e+04    |
-----------------------------------------
Eval num_timesteps=40500, episode_reward=318.97 +/- 593.37
Episode length: 35.10 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 326      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 40       |
|    time_elapsed    | 145      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=396.85 +/- 722.55
Episode length: 35.58 +/- 6.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 397         |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.005228551 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00474    |
|    explained_variance   | 0.0737      |
|    learning_rate        | 0.001       |
|    loss                 | 4.92e+04    |
|    n_updates            | 5700        |
|    policy_gradient_loss | -0.000937   |
|    value_loss           | 9.16e+04    |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=620.34 +/- 770.65
Episode length: 37.00 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 299      |
| time/              |          |
|    fps             | 281      |
|    iterations      | 41       |
|    time_elapsed    | 149      |
|    total_timesteps | 41984    |
---------------------------------
Eval num_timesteps=42000, episode_reward=432.24 +/- 682.62
Episode length: 35.66 +/- 6.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.7       |
|    mean_reward          | 432        |
| time/                   |            |
|    total_timesteps      | 42000      |
| train/                  |            |
|    approx_kl            | 2.9488e-06 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00611   |
|    explained_variance   | 0.0623     |
|    learning_rate        | 0.001      |
|    loss                 | 5.44e+04   |
|    n_updates            | 5710       |
|    policy_gradient_loss | -0.000246  |
|    value_loss           | 9.42e+04   |
----------------------------------------
Eval num_timesteps=42500, episode_reward=488.18 +/- 775.92
Episode length: 34.98 +/- 7.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 488      |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=452.94 +/- 739.86
Episode length: 35.44 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 385      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 42       |
|    time_elapsed    | 153      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=387.13 +/- 679.03
Episode length: 35.68 +/- 5.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 387           |
| time/                   |               |
|    total_timesteps      | 43500         |
| train/                  |               |
|    approx_kl            | 2.0199222e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00378      |
|    explained_variance   | 0.069         |
|    learning_rate        | 0.001         |
|    loss                 | 4.04e+04      |
|    n_updates            | 5720          |
|    policy_gradient_loss | -0.000159     |
|    value_loss           | 9.19e+04      |
-------------------------------------------
Eval num_timesteps=44000, episode_reward=457.67 +/- 753.73
Episode length: 35.42 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 458      |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 420      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 43       |
|    time_elapsed    | 157      |
|    total_timesteps | 44032    |
---------------------------------
Eval num_timesteps=44500, episode_reward=446.16 +/- 732.22
Episode length: 35.58 +/- 6.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 446           |
| time/                   |               |
|    total_timesteps      | 44500         |
| train/                  |               |
|    approx_kl            | 0.00039993058 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00207      |
|    explained_variance   | -0.0583       |
|    learning_rate        | 0.001         |
|    loss                 | 2.73e+04      |
|    n_updates            | 5730          |
|    policy_gradient_loss | -0.000193     |
|    value_loss           | 8.26e+04      |
-------------------------------------------
Eval num_timesteps=45000, episode_reward=426.51 +/- 676.48
Episode length: 36.30 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 501      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 44       |
|    time_elapsed    | 161      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=497.24 +/- 750.09
Episode length: 36.00 +/- 6.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 497           |
| time/                   |               |
|    total_timesteps      | 45500         |
| train/                  |               |
|    approx_kl            | 3.6765705e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00641      |
|    explained_variance   | 0.0123        |
|    learning_rate        | 0.001         |
|    loss                 | 4.98e+04      |
|    n_updates            | 5740          |
|    policy_gradient_loss | -0.000252     |
|    value_loss           | 9.87e+04      |
-------------------------------------------
Eval num_timesteps=46000, episode_reward=334.23 +/- 617.92
Episode length: 34.34 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 334      |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 448      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 45       |
|    time_elapsed    | 164      |
|    total_timesteps | 46080    |
---------------------------------
Eval num_timesteps=46500, episode_reward=538.91 +/- 838.75
Episode length: 34.84 +/- 7.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 539          |
| time/                   |              |
|    total_timesteps      | 46500        |
| train/                  |              |
|    approx_kl            | 6.408547e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00821     |
|    explained_variance   | 0.11         |
|    learning_rate        | 0.001        |
|    loss                 | 3.81e+04     |
|    n_updates            | 5750         |
|    policy_gradient_loss | -0.000258    |
|    value_loss           | 9.04e+04     |
------------------------------------------
Eval num_timesteps=47000, episode_reward=370.16 +/- 646.40
Episode length: 35.48 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 370      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 420      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 46       |
|    time_elapsed    | 168      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=260.45 +/- 611.20
Episode length: 34.54 +/- 5.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.5         |
|    mean_reward          | 260          |
| time/                   |              |
|    total_timesteps      | 47500        |
| train/                  |              |
|    approx_kl            | 0.0039571617 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00902     |
|    explained_variance   | -0.272       |
|    learning_rate        | 0.001        |
|    loss                 | 4.1e+04      |
|    n_updates            | 5760         |
|    policy_gradient_loss | -6.03e-05    |
|    value_loss           | 8.9e+04      |
------------------------------------------
Eval num_timesteps=48000, episode_reward=485.84 +/- 745.40
Episode length: 36.02 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 47       |
|    time_elapsed    | 171      |
|    total_timesteps | 48128    |
---------------------------------
Eval num_timesteps=48500, episode_reward=526.84 +/- 734.85
Episode length: 36.54 +/- 6.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.5          |
|    mean_reward          | 527           |
| time/                   |               |
|    total_timesteps      | 48500         |
| train/                  |               |
|    approx_kl            | 0.00018873397 |
|    clip_fraction        | 0.00127       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0146       |
|    explained_variance   | 0.0756        |
|    learning_rate        | 0.001         |
|    loss                 | 2.31e+04      |
|    n_updates            | 5770          |
|    policy_gradient_loss | -1.41e-05     |
|    value_loss           | 7.59e+04      |
-------------------------------------------
Eval num_timesteps=49000, episode_reward=595.16 +/- 754.03
Episode length: 36.30 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 595      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 399      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 48       |
|    time_elapsed    | 175      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=414.96 +/- 733.43
Episode length: 34.94 +/- 7.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 415          |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 0.0009181426 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00927     |
|    explained_variance   | -0.0957      |
|    learning_rate        | 0.001        |
|    loss                 | 4.39e+04     |
|    n_updates            | 5780         |
|    policy_gradient_loss | -0.000938    |
|    value_loss           | 8.8e+04      |
------------------------------------------
Eval num_timesteps=50000, episode_reward=613.59 +/- 786.02
Episode length: 36.16 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 614      |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 49       |
|    time_elapsed    | 179      |
|    total_timesteps | 50176    |
---------------------------------
Eval num_timesteps=50500, episode_reward=232.14 +/- 635.74
Episode length: 33.60 +/- 6.92
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.6         |
|    mean_reward          | 232          |
| time/                   |              |
|    total_timesteps      | 50500        |
| train/                  |              |
|    approx_kl            | 8.494419e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00569     |
|    explained_variance   | 0.0333       |
|    learning_rate        | 0.001        |
|    loss                 | 4.27e+04     |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.001       |
|    value_loss           | 7.72e+04     |
------------------------------------------
Eval num_timesteps=51000, episode_reward=850.00 +/- 800.78
Episode length: 37.84 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.8     |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 442      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 50       |
|    time_elapsed    | 183      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=459.45 +/- 692.25
Episode length: 35.70 +/- 6.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 459          |
| time/                   |              |
|    total_timesteps      | 51500        |
| train/                  |              |
|    approx_kl            | 0.0003786931 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00318     |
|    explained_variance   | 0.0315       |
|    learning_rate        | 0.001        |
|    loss                 | 2.47e+04     |
|    n_updates            | 5800         |
|    policy_gradient_loss | -0.000141    |
|    value_loss           | 1.02e+05     |
------------------------------------------
Eval num_timesteps=52000, episode_reward=516.96 +/- 721.18
Episode length: 35.96 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 517      |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 404      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 51       |
|    time_elapsed    | 186      |
|    total_timesteps | 52224    |
---------------------------------
Eval num_timesteps=52500, episode_reward=574.53 +/- 770.91
Episode length: 36.14 +/- 6.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 575           |
| time/                   |               |
|    total_timesteps      | 52500         |
| train/                  |               |
|    approx_kl            | 1.2354343e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00941      |
|    explained_variance   | 0.0123        |
|    learning_rate        | 0.001         |
|    loss                 | 1.81e+04      |
|    n_updates            | 5810          |
|    policy_gradient_loss | -0.000943     |
|    value_loss           | 6.95e+04      |
-------------------------------------------
Eval num_timesteps=53000, episode_reward=421.38 +/- 760.03
Episode length: 34.44 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 407      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 52       |
|    time_elapsed    | 190      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=612.34 +/- 744.90
Episode length: 36.76 +/- 5.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.8          |
|    mean_reward          | 612           |
| time/                   |               |
|    total_timesteps      | 53500         |
| train/                  |               |
|    approx_kl            | 0.00025186536 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00925      |
|    explained_variance   | -0.161        |
|    learning_rate        | 0.001         |
|    loss                 | 2.77e+04      |
|    n_updates            | 5820          |
|    policy_gradient_loss | -0.000574     |
|    value_loss           | 8.25e+04      |
-------------------------------------------
Eval num_timesteps=54000, episode_reward=491.33 +/- 714.56
Episode length: 36.32 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 491      |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 415      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 53       |
|    time_elapsed    | 194      |
|    total_timesteps | 54272    |
---------------------------------
Eval num_timesteps=54500, episode_reward=344.89 +/- 683.62
Episode length: 34.28 +/- 6.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 345          |
| time/                   |              |
|    total_timesteps      | 54500        |
| train/                  |              |
|    approx_kl            | 5.617214e-06 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00927     |
|    explained_variance   | 0.0738       |
|    learning_rate        | 0.001        |
|    loss                 | 3.07e+04     |
|    n_updates            | 5830         |
|    policy_gradient_loss | -0.000271    |
|    value_loss           | 7.96e+04     |
------------------------------------------
Eval num_timesteps=55000, episode_reward=349.24 +/- 715.10
Episode length: 33.48 +/- 8.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 349      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 333      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 54       |
|    time_elapsed    | 197      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=296.17 +/- 582.47
Episode length: 34.62 +/- 6.11
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.6           |
|    mean_reward          | 296            |
| time/                   |                |
|    total_timesteps      | 55500          |
| train/                  |                |
|    approx_kl            | 1.23931095e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0121        |
|    explained_variance   | -0.147         |
|    learning_rate        | 0.001          |
|    loss                 | 2.43e+04       |
|    n_updates            | 5840           |
|    policy_gradient_loss | -0.00112       |
|    value_loss           | 6.86e+04       |
--------------------------------------------
Eval num_timesteps=56000, episode_reward=486.77 +/- 777.22
Episode length: 35.72 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 364      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 55       |
|    time_elapsed    | 201      |
|    total_timesteps | 56320    |
---------------------------------
Eval num_timesteps=56500, episode_reward=350.79 +/- 722.70
Episode length: 33.90 +/- 7.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.9         |
|    mean_reward          | 351          |
| time/                   |              |
|    total_timesteps      | 56500        |
| train/                  |              |
|    approx_kl            | 0.0007127674 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0142      |
|    explained_variance   | 0.0895       |
|    learning_rate        | 0.001        |
|    loss                 | 3.15e+04     |
|    n_updates            | 5850         |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 7.89e+04     |
------------------------------------------
Eval num_timesteps=57000, episode_reward=468.22 +/- 688.14
Episode length: 35.70 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 468      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 56       |
|    time_elapsed    | 204      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=410.49 +/- 722.45
Episode length: 35.10 +/- 6.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 410          |
| time/                   |              |
|    total_timesteps      | 57500        |
| train/                  |              |
|    approx_kl            | 1.489307e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0118      |
|    explained_variance   | 0.0462       |
|    learning_rate        | 0.001        |
|    loss                 | 2.24e+04     |
|    n_updates            | 5860         |
|    policy_gradient_loss | -0.000814    |
|    value_loss           | 7.78e+04     |
------------------------------------------
Eval num_timesteps=58000, episode_reward=571.32 +/- 815.62
Episode length: 35.42 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 571      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 389      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 57       |
|    time_elapsed    | 208      |
|    total_timesteps | 58368    |
---------------------------------
Eval num_timesteps=58500, episode_reward=509.21 +/- 804.61
Episode length: 35.56 +/- 6.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 509         |
| time/                   |             |
|    total_timesteps      | 58500       |
| train/                  |             |
|    approx_kl            | 0.000824692 |
|    clip_fraction        | 0.00186     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0147     |
|    explained_variance   | -0.0832     |
|    learning_rate        | 0.001       |
|    loss                 | 4.97e+04    |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.000495   |
|    value_loss           | 9e+04       |
-----------------------------------------
Eval num_timesteps=59000, episode_reward=492.74 +/- 752.02
Episode length: 36.00 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 493      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 434      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 58       |
|    time_elapsed    | 212      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=491.04 +/- 765.64
Episode length: 34.88 +/- 7.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 491           |
| time/                   |               |
|    total_timesteps      | 59500         |
| train/                  |               |
|    approx_kl            | 0.00088951946 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | 0.0538        |
|    learning_rate        | 0.001         |
|    loss                 | 2.93e+04      |
|    n_updates            | 5880          |
|    policy_gradient_loss | -0.000165     |
|    value_loss           | 8.78e+04      |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=364.96 +/- 709.69
Episode length: 34.42 +/- 7.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 365      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 521      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 59       |
|    time_elapsed    | 215      |
|    total_timesteps | 60416    |
---------------------------------
Eval num_timesteps=60500, episode_reward=345.28 +/- 659.39
Episode length: 35.34 +/- 6.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.3        |
|    mean_reward          | 345         |
| time/                   |             |
|    total_timesteps      | 60500       |
| train/                  |             |
|    approx_kl            | 0.001000239 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00284    |
|    explained_variance   | 0.0534      |
|    learning_rate        | 0.001       |
|    loss                 | 3.36e+04    |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 1.04e+05    |
-----------------------------------------
Eval num_timesteps=61000, episode_reward=487.92 +/- 737.58
Episode length: 35.62 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 488      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 547      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 60       |
|    time_elapsed    | 219      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=460.53 +/- 755.74
Episode length: 34.70 +/- 7.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 461           |
| time/                   |               |
|    total_timesteps      | 61500         |
| train/                  |               |
|    approx_kl            | 0.00012380548 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00488      |
|    explained_variance   | 0.125         |
|    learning_rate        | 0.001         |
|    loss                 | 4.13e+04      |
|    n_updates            | 5900          |
|    policy_gradient_loss | -0.000114     |
|    value_loss           | 1.03e+05      |
-------------------------------------------
Eval num_timesteps=62000, episode_reward=367.92 +/- 666.89
Episode length: 35.16 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 639      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 61       |
|    time_elapsed    | 222      |
|    total_timesteps | 62464    |
---------------------------------
Eval num_timesteps=62500, episode_reward=319.81 +/- 668.92
Episode length: 33.70 +/- 7.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.7        |
|    mean_reward          | 320         |
| time/                   |             |
|    total_timesteps      | 62500       |
| train/                  |             |
|    approx_kl            | 0.002567745 |
|    clip_fraction        | 0.00244     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00348    |
|    explained_variance   | 0.0317      |
|    learning_rate        | 0.001       |
|    loss                 | 3.21e+04    |
|    n_updates            | 5910        |
|    policy_gradient_loss | 0.00015     |
|    value_loss           | 9.94e+04    |
-----------------------------------------
Eval num_timesteps=63000, episode_reward=542.05 +/- 788.27
Episode length: 36.00 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 542      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 551      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 62       |
|    time_elapsed    | 226      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=398.27 +/- 687.37
Episode length: 35.20 +/- 6.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 398           |
| time/                   |               |
|    total_timesteps      | 63500         |
| train/                  |               |
|    approx_kl            | 5.4038246e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00594      |
|    explained_variance   | -0.0476       |
|    learning_rate        | 0.001         |
|    loss                 | 4.76e+04      |
|    n_updates            | 5920          |
|    policy_gradient_loss | -0.000302     |
|    value_loss           | 1.1e+05       |
-------------------------------------------
Eval num_timesteps=64000, episode_reward=541.18 +/- 777.42
Episode length: 35.66 +/- 7.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 541      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=323.37 +/- 613.37
Episode length: 35.14 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 323      |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 498      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 63       |
|    time_elapsed    | 231      |
|    total_timesteps | 64512    |
---------------------------------
Eval num_timesteps=65000, episode_reward=420.66 +/- 693.87
Episode length: 35.02 +/- 6.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 421          |
| time/                   |              |
|    total_timesteps      | 65000        |
| train/                  |              |
|    approx_kl            | 3.172725e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00313     |
|    explained_variance   | -0.227       |
|    learning_rate        | 0.001        |
|    loss                 | 3.62e+04     |
|    n_updates            | 5930         |
|    policy_gradient_loss | -0.000321    |
|    value_loss           | 8.3e+04      |
------------------------------------------
Eval num_timesteps=65500, episode_reward=232.47 +/- 643.02
Episode length: 33.88 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 232      |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 373      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 64       |
|    time_elapsed    | 235      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=426.92 +/- 724.66
Episode length: 35.20 +/- 7.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 427           |
| time/                   |               |
|    total_timesteps      | 66000         |
| train/                  |               |
|    approx_kl            | 0.00080320396 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00493      |
|    explained_variance   | 0.0894        |
|    learning_rate        | 0.001         |
|    loss                 | 3.1e+04       |
|    n_updates            | 5940          |
|    policy_gradient_loss | -0.000422     |
|    value_loss           | 7.19e+04      |
-------------------------------------------
Eval num_timesteps=66500, episode_reward=420.38 +/- 714.17
Episode length: 35.88 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 305      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 65       |
|    time_elapsed    | 238      |
|    total_timesteps | 66560    |
---------------------------------
Eval num_timesteps=67000, episode_reward=387.73 +/- 714.63
Episode length: 34.62 +/- 7.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 388           |
| time/                   |               |
|    total_timesteps      | 67000         |
| train/                  |               |
|    approx_kl            | 0.00011227082 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0115       |
|    explained_variance   | -0.129        |
|    learning_rate        | 0.001         |
|    loss                 | 3.51e+04      |
|    n_updates            | 5950          |
|    policy_gradient_loss | -4.68e-05     |
|    value_loss           | 9.1e+04       |
-------------------------------------------
Eval num_timesteps=67500, episode_reward=333.24 +/- 697.61
Episode length: 34.28 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 333      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 301      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 66       |
|    time_elapsed    | 242      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=446.06 +/- 751.07
Episode length: 34.72 +/- 7.41
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 446           |
| time/                   |               |
|    total_timesteps      | 68000         |
| train/                  |               |
|    approx_kl            | 1.3325305e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0115       |
|    explained_variance   | -0.0275       |
|    learning_rate        | 0.001         |
|    loss                 | 4.95e+04      |
|    n_updates            | 5960          |
|    policy_gradient_loss | -0.000649     |
|    value_loss           | 8.33e+04      |
-------------------------------------------
Eval num_timesteps=68500, episode_reward=315.23 +/- 691.89
Episode length: 34.28 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 294      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 67       |
|    time_elapsed    | 245      |
|    total_timesteps | 68608    |
---------------------------------
Eval num_timesteps=69000, episode_reward=389.13 +/- 710.64
Episode length: 34.74 +/- 6.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.7        |
|    mean_reward          | 389         |
| time/                   |             |
|    total_timesteps      | 69000       |
| train/                  |             |
|    approx_kl            | 0.006977202 |
|    clip_fraction        | 0.00469     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00782    |
|    explained_variance   | 0.133       |
|    learning_rate        | 0.001       |
|    loss                 | 3.86e+04    |
|    n_updates            | 5970        |
|    policy_gradient_loss | -5.87e-05   |
|    value_loss           | 8.14e+04    |
-----------------------------------------
Eval num_timesteps=69500, episode_reward=359.12 +/- 665.07
Episode length: 34.28 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 68       |
|    time_elapsed    | 249      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=257.60 +/- 618.45
Episode length: 34.40 +/- 6.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 258          |
| time/                   |              |
|    total_timesteps      | 70000        |
| train/                  |              |
|    approx_kl            | 0.0006732516 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00575     |
|    explained_variance   | 0.0476       |
|    learning_rate        | 0.001        |
|    loss                 | 4.5e+04      |
|    n_updates            | 5980         |
|    policy_gradient_loss | -0.000102    |
|    value_loss           | 9.36e+04     |
------------------------------------------
Eval num_timesteps=70500, episode_reward=400.10 +/- 744.30
Episode length: 34.90 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 400      |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 429      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 69       |
|    time_elapsed    | 253      |
|    total_timesteps | 70656    |
---------------------------------
Eval num_timesteps=71000, episode_reward=344.81 +/- 710.63
Episode length: 34.06 +/- 6.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.1          |
|    mean_reward          | 345           |
| time/                   |               |
|    total_timesteps      | 71000         |
| train/                  |               |
|    approx_kl            | 0.00019152067 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00478      |
|    explained_variance   | 0.2           |
|    learning_rate        | 0.001         |
|    loss                 | 5.11e+04      |
|    n_updates            | 5990          |
|    policy_gradient_loss | -0.000687     |
|    value_loss           | 1.17e+05      |
-------------------------------------------
Eval num_timesteps=71500, episode_reward=194.49 +/- 574.31
Episode length: 33.32 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 511      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 70       |
|    time_elapsed    | 256      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=325.69 +/- 669.17
Episode length: 35.12 +/- 6.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 326           |
| time/                   |               |
|    total_timesteps      | 72000         |
| train/                  |               |
|    approx_kl            | 8.7049557e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00257      |
|    explained_variance   | 0.0662        |
|    learning_rate        | 0.001         |
|    loss                 | 3.85e+04      |
|    n_updates            | 6000          |
|    policy_gradient_loss | -0.000134     |
|    value_loss           | 1.16e+05      |
-------------------------------------------
Eval num_timesteps=72500, episode_reward=339.21 +/- 666.91
Episode length: 34.48 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 515      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 71       |
|    time_elapsed    | 260      |
|    total_timesteps | 72704    |
---------------------------------
Eval num_timesteps=73000, episode_reward=211.50 +/- 564.12
Episode length: 34.08 +/- 5.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.1          |
|    mean_reward          | 211           |
| time/                   |               |
|    total_timesteps      | 73000         |
| train/                  |               |
|    approx_kl            | 2.9418152e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00271      |
|    explained_variance   | -0.0524       |
|    learning_rate        | 0.001         |
|    loss                 | 4.18e+04      |
|    n_updates            | 6010          |
|    policy_gradient_loss | -0.000172     |
|    value_loss           | 1.06e+05      |
-------------------------------------------
Eval num_timesteps=73500, episode_reward=612.40 +/- 770.00
Episode length: 36.52 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 612      |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 484      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 72       |
|    time_elapsed    | 263      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=314.95 +/- 644.12
Episode length: 34.56 +/- 5.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 315          |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 2.350018e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00763     |
|    explained_variance   | 0.0558       |
|    learning_rate        | 0.001        |
|    loss                 | 4.1e+04      |
|    n_updates            | 6020         |
|    policy_gradient_loss | -0.000145    |
|    value_loss           | 9.09e+04     |
------------------------------------------
Eval num_timesteps=74500, episode_reward=442.62 +/- 728.17
Episode length: 35.72 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 464      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 73       |
|    time_elapsed    | 267      |
|    total_timesteps | 74752    |
---------------------------------
Eval num_timesteps=75000, episode_reward=463.04 +/- 703.44
Episode length: 35.52 +/- 6.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 463           |
| time/                   |               |
|    total_timesteps      | 75000         |
| train/                  |               |
|    approx_kl            | 2.0522857e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00645      |
|    explained_variance   | -0.149        |
|    learning_rate        | 0.001         |
|    loss                 | 2.85e+04      |
|    n_updates            | 6030          |
|    policy_gradient_loss | -0.000184     |
|    value_loss           | 9.88e+04      |
-------------------------------------------
Eval num_timesteps=75500, episode_reward=387.48 +/- 701.23
Episode length: 35.28 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 387      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 465      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 74       |
|    time_elapsed    | 270      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=500.22 +/- 701.58
Episode length: 36.16 +/- 6.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.2          |
|    mean_reward          | 500           |
| time/                   |               |
|    total_timesteps      | 76000         |
| train/                  |               |
|    approx_kl            | 0.00062749285 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00849      |
|    explained_variance   | 0.0984        |
|    learning_rate        | 0.001         |
|    loss                 | 4.21e+04      |
|    n_updates            | 6040          |
|    policy_gradient_loss | 0.000286      |
|    value_loss           | 9.52e+04      |
-------------------------------------------
Eval num_timesteps=76500, episode_reward=275.78 +/- 644.06
Episode length: 33.64 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 442      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 75       |
|    time_elapsed    | 274      |
|    total_timesteps | 76800    |
---------------------------------
Eval num_timesteps=77000, episode_reward=499.58 +/- 687.60
Episode length: 36.60 +/- 6.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 500           |
| time/                   |               |
|    total_timesteps      | 77000         |
| train/                  |               |
|    approx_kl            | 4.0804152e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00872      |
|    explained_variance   | 0.0383        |
|    learning_rate        | 0.001         |
|    loss                 | 3.5e+04       |
|    n_updates            | 6050          |
|    policy_gradient_loss | -0.000209     |
|    value_loss           | 8.72e+04      |
-------------------------------------------
Eval num_timesteps=77500, episode_reward=324.71 +/- 669.33
Episode length: 33.92 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 325      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 438      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 76       |
|    time_elapsed    | 277      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=352.48 +/- 684.00
Episode length: 35.00 +/- 7.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 352          |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 0.0013957617 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0067      |
|    explained_variance   | 0.072        |
|    learning_rate        | 0.001        |
|    loss                 | 4.32e+04     |
|    n_updates            | 6060         |
|    policy_gradient_loss | -8.1e-05     |
|    value_loss           | 1.03e+05     |
------------------------------------------
Eval num_timesteps=78500, episode_reward=291.82 +/- 661.39
Episode length: 34.58 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 339      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 77       |
|    time_elapsed    | 281      |
|    total_timesteps | 78848    |
---------------------------------
Eval num_timesteps=79000, episode_reward=425.26 +/- 735.99
Episode length: 35.88 +/- 7.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.9        |
|    mean_reward          | 425         |
| time/                   |             |
|    total_timesteps      | 79000       |
| train/                  |             |
|    approx_kl            | 0.003554213 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0203     |
|    explained_variance   | 0.153       |
|    learning_rate        | 0.001       |
|    loss                 | 3.12e+04    |
|    n_updates            | 6070        |
|    policy_gradient_loss | 0.000806    |
|    value_loss           | 9.12e+04    |
-----------------------------------------
Eval num_timesteps=79500, episode_reward=396.41 +/- 667.06
Episode length: 35.90 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 78       |
|    time_elapsed    | 285      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=523.83 +/- 726.60
Episode length: 36.64 +/- 5.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.6         |
|    mean_reward          | 524          |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0033529704 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0176      |
|    explained_variance   | 0.0727       |
|    learning_rate        | 0.001        |
|    loss                 | 3.03e+04     |
|    n_updates            | 6080         |
|    policy_gradient_loss | -0.000743    |
|    value_loss           | 9.15e+04     |
------------------------------------------
Eval num_timesteps=80500, episode_reward=367.16 +/- 683.95
Episode length: 36.16 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 367      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 319      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 79       |
|    time_elapsed    | 288      |
|    total_timesteps | 80896    |
---------------------------------
Eval num_timesteps=81000, episode_reward=541.51 +/- 806.76
Episode length: 36.50 +/- 6.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.5         |
|    mean_reward          | 542          |
| time/                   |              |
|    total_timesteps      | 81000        |
| train/                  |              |
|    approx_kl            | 0.0066327406 |
|    clip_fraction        | 0.00381      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00617     |
|    explained_variance   | -0.135       |
|    learning_rate        | 0.001        |
|    loss                 | 4.09e+04     |
|    n_updates            | 6090         |
|    policy_gradient_loss | -0.000115    |
|    value_loss           | 8.59e+04     |
------------------------------------------
Eval num_timesteps=81500, episode_reward=338.68 +/- 622.08
Episode length: 35.78 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 322      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 80       |
|    time_elapsed    | 292      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=487.10 +/- 764.96
Episode length: 35.82 +/- 5.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 487           |
| time/                   |               |
|    total_timesteps      | 82000         |
| train/                  |               |
|    approx_kl            | 2.2315653e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00698      |
|    explained_variance   | -0.0666       |
|    learning_rate        | 0.001         |
|    loss                 | 3.22e+04      |
|    n_updates            | 6100          |
|    policy_gradient_loss | -0.000172     |
|    value_loss           | 8.49e+04      |
-------------------------------------------
Eval num_timesteps=82500, episode_reward=357.60 +/- 684.41
Episode length: 34.06 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 339      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 81       |
|    time_elapsed    | 295      |
|    total_timesteps | 82944    |
---------------------------------
Eval num_timesteps=83000, episode_reward=381.62 +/- 694.78
Episode length: 34.68 +/- 6.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 382          |
| time/                   |              |
|    total_timesteps      | 83000        |
| train/                  |              |
|    approx_kl            | 0.0014568806 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0226      |
|    explained_variance   | 0.0748       |
|    learning_rate        | 0.001        |
|    loss                 | 3.49e+04     |
|    n_updates            | 6110         |
|    policy_gradient_loss | -5.02e-05    |
|    value_loss           | 8.38e+04     |
------------------------------------------
Eval num_timesteps=83500, episode_reward=643.13 +/- 730.02
Episode length: 36.76 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 643      |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 258      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 82       |
|    time_elapsed    | 299      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=623.54 +/- 790.54
Episode length: 36.68 +/- 5.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.7         |
|    mean_reward          | 624          |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0005452187 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0183      |
|    explained_variance   | 0.00313      |
|    learning_rate        | 0.001        |
|    loss                 | 3.15e+04     |
|    n_updates            | 6120         |
|    policy_gradient_loss | -7.27e-05    |
|    value_loss           | 7.95e+04     |
------------------------------------------
Eval num_timesteps=84500, episode_reward=358.66 +/- 748.58
Episode length: 34.44 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 302      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 83       |
|    time_elapsed    | 303      |
|    total_timesteps | 84992    |
---------------------------------
Eval num_timesteps=85000, episode_reward=369.96 +/- 708.35
Episode length: 33.94 +/- 6.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.9          |
|    mean_reward          | 370           |
| time/                   |               |
|    total_timesteps      | 85000         |
| train/                  |               |
|    approx_kl            | 0.00070371927 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0116       |
|    explained_variance   | 0.112         |
|    learning_rate        | 0.001         |
|    loss                 | 3.24e+04      |
|    n_updates            | 6130          |
|    policy_gradient_loss | -0.000401     |
|    value_loss           | 8.35e+04      |
-------------------------------------------
Eval num_timesteps=85500, episode_reward=466.11 +/- 754.92
Episode length: 34.70 +/- 7.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=476.73 +/- 684.73
Episode length: 35.66 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 356      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 84       |
|    time_elapsed    | 307      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=565.42 +/- 790.99
Episode length: 36.22 +/- 6.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.2          |
|    mean_reward          | 565           |
| time/                   |               |
|    total_timesteps      | 86500         |
| train/                  |               |
|    approx_kl            | 0.00045050483 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00955      |
|    explained_variance   | 0.157         |
|    learning_rate        | 0.001         |
|    loss                 | 3.67e+04      |
|    n_updates            | 6140          |
|    policy_gradient_loss | -0.000299     |
|    value_loss           | 1.05e+05      |
-------------------------------------------
Eval num_timesteps=87000, episode_reward=356.35 +/- 666.68
Episode length: 35.12 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 356      |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 85       |
|    time_elapsed    | 311      |
|    total_timesteps | 87040    |
---------------------------------
Eval num_timesteps=87500, episode_reward=544.84 +/- 708.89
Episode length: 37.08 +/- 5.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.1          |
|    mean_reward          | 545           |
| time/                   |               |
|    total_timesteps      | 87500         |
| train/                  |               |
|    approx_kl            | 1.8465274e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0101       |
|    explained_variance   | -0.0218       |
|    learning_rate        | 0.001         |
|    loss                 | 5.48e+04      |
|    n_updates            | 6150          |
|    policy_gradient_loss | -0.000274     |
|    value_loss           | 1.2e+05       |
-------------------------------------------
Eval num_timesteps=88000, episode_reward=509.95 +/- 685.60
Episode length: 36.60 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 510      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 358      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 86       |
|    time_elapsed    | 315      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=335.58 +/- 696.73
Episode length: 34.34 +/- 6.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 336          |
| time/                   |              |
|    total_timesteps      | 88500        |
| train/                  |              |
|    approx_kl            | 0.0018361503 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00859     |
|    explained_variance   | -0.15        |
|    learning_rate        | 0.001        |
|    loss                 | 5.52e+04     |
|    n_updates            | 6160         |
|    policy_gradient_loss | -0.000389    |
|    value_loss           | 9.1e+04      |
------------------------------------------
Eval num_timesteps=89000, episode_reward=432.56 +/- 720.70
Episode length: 35.34 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 87       |
|    time_elapsed    | 319      |
|    total_timesteps | 89088    |
---------------------------------
Eval num_timesteps=89500, episode_reward=645.21 +/- 691.44
Episode length: 37.60 +/- 4.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.6        |
|    mean_reward          | 645         |
| time/                   |             |
|    total_timesteps      | 89500       |
| train/                  |             |
|    approx_kl            | 0.000191423 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00876    |
|    explained_variance   | 0.0762      |
|    learning_rate        | 0.001       |
|    loss                 | 4.1e+04     |
|    n_updates            | 6170        |
|    policy_gradient_loss | -0.000102   |
|    value_loss           | 9.16e+04    |
-----------------------------------------
Eval num_timesteps=90000, episode_reward=508.70 +/- 735.83
Episode length: 36.46 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 509      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 425      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 88       |
|    time_elapsed    | 322      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=295.56 +/- 684.00
Episode length: 33.88 +/- 6.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.9         |
|    mean_reward          | 296          |
| time/                   |              |
|    total_timesteps      | 90500        |
| train/                  |              |
|    approx_kl            | 0.0020912837 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.015       |
|    explained_variance   | 0.115        |
|    learning_rate        | 0.001        |
|    loss                 | 5.25e+04     |
|    n_updates            | 6180         |
|    policy_gradient_loss | -0.000973    |
|    value_loss           | 9.3e+04      |
------------------------------------------
Eval num_timesteps=91000, episode_reward=472.84 +/- 758.90
Episode length: 36.00 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 493      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 89       |
|    time_elapsed    | 326      |
|    total_timesteps | 91136    |
---------------------------------
Eval num_timesteps=91500, episode_reward=377.25 +/- 745.17
Episode length: 33.74 +/- 7.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.7          |
|    mean_reward          | 377           |
| time/                   |               |
|    total_timesteps      | 91500         |
| train/                  |               |
|    approx_kl            | 8.0333615e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00476      |
|    explained_variance   | 0.108         |
|    learning_rate        | 0.001         |
|    loss                 | 4.75e+04      |
|    n_updates            | 6190          |
|    policy_gradient_loss | -0.000609     |
|    value_loss           | 1.05e+05      |
-------------------------------------------
Eval num_timesteps=92000, episode_reward=419.93 +/- 684.46
Episode length: 35.46 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 377      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 90       |
|    time_elapsed    | 329      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=291.37 +/- 637.32
Episode length: 34.52 +/- 5.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.5         |
|    mean_reward          | 291          |
| time/                   |              |
|    total_timesteps      | 92500        |
| train/                  |              |
|    approx_kl            | 7.145986e-05 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0103      |
|    explained_variance   | 0.00973      |
|    learning_rate        | 0.001        |
|    loss                 | 4.26e+04     |
|    n_updates            | 6200         |
|    policy_gradient_loss | -0.000552    |
|    value_loss           | 8.39e+04     |
------------------------------------------
Eval num_timesteps=93000, episode_reward=610.96 +/- 868.40
Episode length: 35.58 +/- 7.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 611      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 468      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 91       |
|    time_elapsed    | 333      |
|    total_timesteps | 93184    |
---------------------------------
Eval num_timesteps=93500, episode_reward=231.68 +/- 592.92
Episode length: 33.58 +/- 6.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.6        |
|    mean_reward          | 232         |
| time/                   |             |
|    total_timesteps      | 93500       |
| train/                  |             |
|    approx_kl            | 0.004370158 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00352    |
|    explained_variance   | 0.0436      |
|    learning_rate        | 0.001       |
|    loss                 | 2.92e+04    |
|    n_updates            | 6210        |
|    policy_gradient_loss | 0.000186    |
|    value_loss           | 9.65e+04    |
-----------------------------------------
Eval num_timesteps=94000, episode_reward=422.98 +/- 796.62
Episode length: 34.94 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 438      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 92       |
|    time_elapsed    | 337      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=425.82 +/- 715.16
Episode length: 35.58 +/- 6.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 426         |
| time/                   |             |
|    total_timesteps      | 94500       |
| train/                  |             |
|    approx_kl            | 6.32368e-07 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00201    |
|    explained_variance   | -0.0802     |
|    learning_rate        | 0.001       |
|    loss                 | 3.13e+04    |
|    n_updates            | 6220        |
|    policy_gradient_loss | -0.000117   |
|    value_loss           | 8.57e+04    |
-----------------------------------------
Eval num_timesteps=95000, episode_reward=274.83 +/- 631.43
Episode length: 34.18 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 275      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 528      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 93       |
|    time_elapsed    | 340      |
|    total_timesteps | 95232    |
---------------------------------
Eval num_timesteps=95500, episode_reward=318.46 +/- 625.40
Episode length: 35.12 +/- 6.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 318           |
| time/                   |               |
|    total_timesteps      | 95500         |
| train/                  |               |
|    approx_kl            | 1.4890102e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00229      |
|    explained_variance   | 0.104         |
|    learning_rate        | 0.001         |
|    loss                 | 5.83e+04      |
|    n_updates            | 6230          |
|    policy_gradient_loss | -0.000172     |
|    value_loss           | 1.07e+05      |
-------------------------------------------
Eval num_timesteps=96000, episode_reward=289.39 +/- 695.93
Episode length: 33.86 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 476      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 94       |
|    time_elapsed    | 344      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=490.03 +/- 737.32
Episode length: 35.12 +/- 7.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 490           |
| time/                   |               |
|    total_timesteps      | 96500         |
| train/                  |               |
|    approx_kl            | 2.1097367e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00588      |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.001         |
|    loss                 | 3.18e+04      |
|    n_updates            | 6240          |
|    policy_gradient_loss | -0.000224     |
|    value_loss           | 1e+05         |
-------------------------------------------
Eval num_timesteps=97000, episode_reward=387.22 +/- 700.92
Episode length: 35.02 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 387      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 430      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 95       |
|    time_elapsed    | 347      |
|    total_timesteps | 97280    |
---------------------------------
Eval num_timesteps=97500, episode_reward=238.78 +/- 601.85
Episode length: 34.32 +/- 6.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 239          |
| time/                   |              |
|    total_timesteps      | 97500        |
| train/                  |              |
|    approx_kl            | 0.0021768208 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00777     |
|    explained_variance   | -0.105       |
|    learning_rate        | 0.001        |
|    loss                 | 3.12e+04     |
|    n_updates            | 6250         |
|    policy_gradient_loss | -0.00021     |
|    value_loss           | 9.32e+04     |
------------------------------------------
Eval num_timesteps=98000, episode_reward=474.71 +/- 740.57
Episode length: 35.60 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 465      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 96       |
|    time_elapsed    | 351      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=405.97 +/- 699.39
Episode length: 35.42 +/- 8.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 406           |
| time/                   |               |
|    total_timesteps      | 98500         |
| train/                  |               |
|    approx_kl            | 0.00058746646 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0104       |
|    explained_variance   | 0.0949        |
|    learning_rate        | 0.001         |
|    loss                 | 4.34e+04      |
|    n_updates            | 6260          |
|    policy_gradient_loss | 0.000155      |
|    value_loss           | 9.67e+04      |
-------------------------------------------
Eval num_timesteps=99000, episode_reward=483.59 +/- 760.96
Episode length: 35.06 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 484      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 429      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 97       |
|    time_elapsed    | 354      |
|    total_timesteps | 99328    |
---------------------------------
Eval num_timesteps=99500, episode_reward=385.18 +/- 761.68
Episode length: 34.40 +/- 7.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 385          |
| time/                   |              |
|    total_timesteps      | 99500        |
| train/                  |              |
|    approx_kl            | 0.0023860638 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00519     |
|    explained_variance   | -0.0693      |
|    learning_rate        | 0.001        |
|    loss                 | 2.64e+04     |
|    n_updates            | 6270         |
|    policy_gradient_loss | 1.36e-05     |
|    value_loss           | 6.51e+04     |
------------------------------------------
Eval num_timesteps=100000, episode_reward=479.59 +/- 758.68
Episode length: 35.82 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 480      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 521      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 98       |
|    time_elapsed    | 358      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=497.47 +/- 755.95
Episode length: 36.28 +/- 6.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 497           |
| time/                   |               |
|    total_timesteps      | 100500        |
| train/                  |               |
|    approx_kl            | 0.00021549629 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00373      |
|    explained_variance   | 0.0294        |
|    learning_rate        | 0.001         |
|    loss                 | 4.24e+04      |
|    n_updates            | 6280          |
|    policy_gradient_loss | 0.000774      |
|    value_loss           | 1.2e+05       |
-------------------------------------------
Eval num_timesteps=101000, episode_reward=576.58 +/- 752.45
Episode length: 35.88 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 577      |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 487      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 99       |
|    time_elapsed    | 362      |
|    total_timesteps | 101376   |
---------------------------------
Eval num_timesteps=101500, episode_reward=373.74 +/- 705.52
Episode length: 35.06 +/- 6.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 374           |
| time/                   |               |
|    total_timesteps      | 101500        |
| train/                  |               |
|    approx_kl            | 5.7865633e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0147       |
|    explained_variance   | -0.00971      |
|    learning_rate        | 0.001         |
|    loss                 | 2.84e+04      |
|    n_updates            | 6290          |
|    policy_gradient_loss | -0.000902     |
|    value_loss           | 8.77e+04      |
-------------------------------------------
Eval num_timesteps=102000, episode_reward=340.95 +/- 687.39
Episode length: 33.76 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 341      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 458      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 100      |
|    time_elapsed    | 365      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=532.49 +/- 768.73
Episode length: 36.22 +/- 5.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.2         |
|    mean_reward          | 532          |
| time/                   |              |
|    total_timesteps      | 102500       |
| train/                  |              |
|    approx_kl            | 0.0061503923 |
|    clip_fraction        | 0.00283      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0117      |
|    explained_variance   | -0.29        |
|    learning_rate        | 0.001        |
|    loss                 | 2.98e+04     |
|    n_updates            | 6300         |
|    policy_gradient_loss | 0.000588     |
|    value_loss           | 8.11e+04     |
------------------------------------------
Eval num_timesteps=103000, episode_reward=328.49 +/- 649.56
Episode length: 34.98 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 328      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 371      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 101      |
|    time_elapsed    | 369      |
|    total_timesteps | 103424   |
---------------------------------
Eval num_timesteps=103500, episode_reward=363.11 +/- 648.12
Episode length: 35.28 +/- 5.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 363           |
| time/                   |               |
|    total_timesteps      | 103500        |
| train/                  |               |
|    approx_kl            | 4.2387983e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0078       |
|    explained_variance   | 0.000683      |
|    learning_rate        | 0.001         |
|    loss                 | 2.8e+04       |
|    n_updates            | 6310          |
|    policy_gradient_loss | -0.000389     |
|    value_loss           | 8.31e+04      |
-------------------------------------------
Eval num_timesteps=104000, episode_reward=229.75 +/- 637.79
Episode length: 33.46 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 377      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 102      |
|    time_elapsed    | 372      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=416.31 +/- 737.58
Episode length: 35.20 +/- 6.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 416          |
| time/                   |              |
|    total_timesteps      | 104500       |
| train/                  |              |
|    approx_kl            | 0.0003373957 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0101      |
|    explained_variance   | 0.0963       |
|    learning_rate        | 0.001        |
|    loss                 | 3.79e+04     |
|    n_updates            | 6320         |
|    policy_gradient_loss | 0.00119      |
|    value_loss           | 9.8e+04      |
------------------------------------------
Eval num_timesteps=105000, episode_reward=540.94 +/- 749.89
Episode length: 36.78 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 541      |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 431      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 103      |
|    time_elapsed    | 376      |
|    total_timesteps | 105472   |
---------------------------------
Eval num_timesteps=105500, episode_reward=512.65 +/- 748.33
Episode length: 36.00 +/- 6.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 513          |
| time/                   |              |
|    total_timesteps      | 105500       |
| train/                  |              |
|    approx_kl            | 0.0008569946 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00494     |
|    explained_variance   | -0.0196      |
|    learning_rate        | 0.001        |
|    loss                 | 3.69e+04     |
|    n_updates            | 6330         |
|    policy_gradient_loss | -0.000363    |
|    value_loss           | 9.72e+04     |
------------------------------------------
Eval num_timesteps=106000, episode_reward=453.13 +/- 742.53
Episode length: 34.72 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 428      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 104      |
|    time_elapsed    | 380      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=420.67 +/- 689.56
Episode length: 36.12 +/- 6.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 421           |
| time/                   |               |
|    total_timesteps      | 106500        |
| train/                  |               |
|    approx_kl            | 6.9647795e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0107       |
|    explained_variance   | 0.013         |
|    learning_rate        | 0.001         |
|    loss                 | 2.87e+04      |
|    n_updates            | 6340          |
|    policy_gradient_loss | -0.000679     |
|    value_loss           | 9.08e+04      |
-------------------------------------------
Eval num_timesteps=107000, episode_reward=372.95 +/- 673.97
Episode length: 34.74 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 373      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=493.28 +/- 787.01
Episode length: 35.42 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 493      |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 523      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 105      |
|    time_elapsed    | 384      |
|    total_timesteps | 107520   |
---------------------------------
Eval num_timesteps=108000, episode_reward=607.05 +/- 764.70
Episode length: 36.58 +/- 6.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.6         |
|    mean_reward          | 607          |
| time/                   |              |
|    total_timesteps      | 108000       |
| train/                  |              |
|    approx_kl            | 0.0011386673 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00496     |
|    explained_variance   | 0.0275       |
|    learning_rate        | 0.001        |
|    loss                 | 3.9e+04      |
|    n_updates            | 6350         |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 1.02e+05     |
------------------------------------------
Eval num_timesteps=108500, episode_reward=431.29 +/- 713.63
Episode length: 35.96 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 589      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 106      |
|    time_elapsed    | 388      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=429.96 +/- 746.26
Episode length: 35.26 +/- 5.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 430          |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 1.308159e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00312     |
|    explained_variance   | 0.145        |
|    learning_rate        | 0.001        |
|    loss                 | 3.66e+04     |
|    n_updates            | 6360         |
|    policy_gradient_loss | -0.000139    |
|    value_loss           | 9.75e+04     |
------------------------------------------
Eval num_timesteps=109500, episode_reward=371.10 +/- 679.93
Episode length: 35.32 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 518      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 107      |
|    time_elapsed    | 392      |
|    total_timesteps | 109568   |
---------------------------------
Eval num_timesteps=110000, episode_reward=391.98 +/- 715.52
Episode length: 35.32 +/- 6.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 392           |
| time/                   |               |
|    total_timesteps      | 110000        |
| train/                  |               |
|    approx_kl            | 1.4968682e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00306      |
|    explained_variance   | -0.0298       |
|    learning_rate        | 0.001         |
|    loss                 | 4.6e+04       |
|    n_updates            | 6370          |
|    policy_gradient_loss | -0.000232     |
|    value_loss           | 9.2e+04       |
-------------------------------------------
Eval num_timesteps=110500, episode_reward=550.85 +/- 747.48
Episode length: 36.60 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 551      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 507      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 108      |
|    time_elapsed    | 395      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=422.06 +/- 747.05
Episode length: 35.00 +/- 6.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 422          |
| time/                   |              |
|    total_timesteps      | 111000       |
| train/                  |              |
|    approx_kl            | 9.674695e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00303     |
|    explained_variance   | 0.153        |
|    learning_rate        | 0.001        |
|    loss                 | 3.36e+04     |
|    n_updates            | 6380         |
|    policy_gradient_loss | -0.000259    |
|    value_loss           | 8.82e+04     |
------------------------------------------
Eval num_timesteps=111500, episode_reward=450.70 +/- 740.59
Episode length: 35.80 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 496      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 109      |
|    time_elapsed    | 399      |
|    total_timesteps | 111616   |
---------------------------------
Eval num_timesteps=112000, episode_reward=504.39 +/- 782.89
Episode length: 36.26 +/- 6.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 504          |
| time/                   |              |
|    total_timesteps      | 112000       |
| train/                  |              |
|    approx_kl            | 3.995956e-05 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00276     |
|    explained_variance   | 0.0688       |
|    learning_rate        | 0.001        |
|    loss                 | 4.62e+04     |
|    n_updates            | 6390         |
|    policy_gradient_loss | -0.000139    |
|    value_loss           | 1.03e+05     |
------------------------------------------
Eval num_timesteps=112500, episode_reward=296.22 +/- 678.91
Episode length: 33.84 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 487      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 110      |
|    time_elapsed    | 402      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=189.15 +/- 592.46
Episode length: 33.08 +/- 6.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.1        |
|    mean_reward          | 189         |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.010995469 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00129    |
|    explained_variance   | -0.0783     |
|    learning_rate        | 0.001       |
|    loss                 | 2.72e+04    |
|    n_updates            | 6400        |
|    policy_gradient_loss | 0.00168     |
|    value_loss           | 8.6e+04     |
-----------------------------------------
Eval num_timesteps=113500, episode_reward=502.00 +/- 747.32
Episode length: 35.56 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 450      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 111      |
|    time_elapsed    | 406      |
|    total_timesteps | 113664   |
---------------------------------
Eval num_timesteps=114000, episode_reward=277.08 +/- 652.68
Episode length: 34.00 +/- 7.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 277           |
| time/                   |               |
|    total_timesteps      | 114000        |
| train/                  |               |
|    approx_kl            | 1.6097329e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0047       |
|    explained_variance   | -0.0444       |
|    learning_rate        | 0.001         |
|    loss                 | 3.26e+04      |
|    n_updates            | 6410          |
|    policy_gradient_loss | -0.000305     |
|    value_loss           | 9.52e+04      |
-------------------------------------------
Eval num_timesteps=114500, episode_reward=376.32 +/- 717.33
Episode length: 33.94 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 460      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 112      |
|    time_elapsed    | 410      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=396.48 +/- 752.92
Episode length: 34.28 +/- 7.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 396          |
| time/                   |              |
|    total_timesteps      | 115000       |
| train/                  |              |
|    approx_kl            | 2.238783e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00732     |
|    explained_variance   | 0.0344       |
|    learning_rate        | 0.001        |
|    loss                 | 3.03e+04     |
|    n_updates            | 6420         |
|    policy_gradient_loss | -0.000228    |
|    value_loss           | 8.86e+04     |
------------------------------------------
Eval num_timesteps=115500, episode_reward=405.41 +/- 760.16
Episode length: 34.90 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 113      |
|    time_elapsed    | 413      |
|    total_timesteps | 115712   |
---------------------------------
Eval num_timesteps=116000, episode_reward=319.64 +/- 635.93
Episode length: 34.74 +/- 6.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 320          |
| time/                   |              |
|    total_timesteps      | 116000       |
| train/                  |              |
|    approx_kl            | 9.871728e-06 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0167      |
|    explained_variance   | -0.00158     |
|    learning_rate        | 0.001        |
|    loss                 | 2.61e+04     |
|    n_updates            | 6430         |
|    policy_gradient_loss | -0.000615    |
|    value_loss           | 9.03e+04     |
------------------------------------------
Eval num_timesteps=116500, episode_reward=490.25 +/- 709.00
Episode length: 35.90 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 490      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 491      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 114      |
|    time_elapsed    | 417      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=324.42 +/- 676.04
Episode length: 35.22 +/- 6.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 324          |
| time/                   |              |
|    total_timesteps      | 117000       |
| train/                  |              |
|    approx_kl            | 0.0015275857 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0041      |
|    explained_variance   | -0.121       |
|    learning_rate        | 0.001        |
|    loss                 | 4.38e+04     |
|    n_updates            | 6440         |
|    policy_gradient_loss | -0.000537    |
|    value_loss           | 9.66e+04     |
------------------------------------------
Eval num_timesteps=117500, episode_reward=451.18 +/- 685.02
Episode length: 36.46 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 455      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 115      |
|    time_elapsed    | 420      |
|    total_timesteps | 117760   |
---------------------------------
Eval num_timesteps=118000, episode_reward=447.55 +/- 796.68
Episode length: 33.80 +/- 7.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 448           |
| time/                   |               |
|    total_timesteps      | 118000        |
| train/                  |               |
|    approx_kl            | 3.1513046e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00686      |
|    explained_variance   | -0.0987       |
|    learning_rate        | 0.001         |
|    loss                 | 4.84e+04      |
|    n_updates            | 6450          |
|    policy_gradient_loss | -0.000391     |
|    value_loss           | 9.38e+04      |
-------------------------------------------
Eval num_timesteps=118500, episode_reward=176.68 +/- 628.24
Episode length: 32.64 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 177      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 480      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 116      |
|    time_elapsed    | 424      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=485.17 +/- 791.62
Episode length: 34.76 +/- 6.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 485          |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 0.0023926683 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00747     |
|    explained_variance   | 0.0639       |
|    learning_rate        | 0.001        |
|    loss                 | 2.68e+04     |
|    n_updates            | 6460         |
|    policy_gradient_loss | 0.00122      |
|    value_loss           | 9.6e+04      |
------------------------------------------
Eval num_timesteps=119500, episode_reward=477.77 +/- 767.20
Episode length: 35.14 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 372      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 117      |
|    time_elapsed    | 427      |
|    total_timesteps | 119808   |
---------------------------------
Eval num_timesteps=120000, episode_reward=616.63 +/- 747.90
Episode length: 37.78 +/- 6.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.8         |
|    mean_reward          | 617          |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0007433173 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0173      |
|    explained_variance   | 0.114        |
|    learning_rate        | 0.001        |
|    loss                 | 4.64e+04     |
|    n_updates            | 6470         |
|    policy_gradient_loss | -0.000597    |
|    value_loss           | 9.86e+04     |
------------------------------------------
Eval num_timesteps=120500, episode_reward=460.04 +/- 734.52
Episode length: 35.70 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 413      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 118      |
|    time_elapsed    | 431      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=423.04 +/- 793.91
Episode length: 34.18 +/- 6.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 423           |
| time/                   |               |
|    total_timesteps      | 121000        |
| train/                  |               |
|    approx_kl            | 2.6178313e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00859      |
|    explained_variance   | 0.0471        |
|    learning_rate        | 0.001         |
|    loss                 | 3.6e+04       |
|    n_updates            | 6480          |
|    policy_gradient_loss | -0.000399     |
|    value_loss           | 8.32e+04      |
-------------------------------------------
Eval num_timesteps=121500, episode_reward=446.75 +/- 708.66
Episode length: 35.56 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 424      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 119      |
|    time_elapsed    | 435      |
|    total_timesteps | 121856   |
---------------------------------
Eval num_timesteps=122000, episode_reward=399.04 +/- 674.15
Episode length: 35.44 +/- 6.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 399          |
| time/                   |              |
|    total_timesteps      | 122000       |
| train/                  |              |
|    approx_kl            | 0.0001231439 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.004       |
|    explained_variance   | 0.184        |
|    learning_rate        | 0.001        |
|    loss                 | 3.74e+04     |
|    n_updates            | 6490         |
|    policy_gradient_loss | -0.00058     |
|    value_loss           | 1.05e+05     |
------------------------------------------
Eval num_timesteps=122500, episode_reward=408.56 +/- 717.17
Episode length: 35.04 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 502      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 120      |
|    time_elapsed    | 438      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=322.27 +/- 650.45
Episode length: 34.24 +/- 6.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.2         |
|    mean_reward          | 322          |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0024142344 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00444     |
|    explained_variance   | 0.0548       |
|    learning_rate        | 0.001        |
|    loss                 | 5.16e+04     |
|    n_updates            | 6500         |
|    policy_gradient_loss | -0.000308    |
|    value_loss           | 1.32e+05     |
------------------------------------------
Eval num_timesteps=123500, episode_reward=587.30 +/- 745.51
Episode length: 36.98 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 587      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 495      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 121      |
|    time_elapsed    | 442      |
|    total_timesteps | 123904   |
---------------------------------
Eval num_timesteps=124000, episode_reward=448.08 +/- 748.34
Episode length: 34.48 +/- 7.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 448           |
| time/                   |               |
|    total_timesteps      | 124000        |
| train/                  |               |
|    approx_kl            | 1.4104298e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00378      |
|    explained_variance   | 0.036         |
|    learning_rate        | 0.001         |
|    loss                 | 3.71e+04      |
|    n_updates            | 6510          |
|    policy_gradient_loss | -0.000267     |
|    value_loss           | 1.2e+05       |
-------------------------------------------
Eval num_timesteps=124500, episode_reward=617.37 +/- 783.02
Episode length: 36.92 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 617      |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 389      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 122      |
|    time_elapsed    | 445      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=410.07 +/- 770.19
Episode length: 34.78 +/- 6.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 410          |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0006353763 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00799     |
|    explained_variance   | -0.0978      |
|    learning_rate        | 0.001        |
|    loss                 | 4.48e+04     |
|    n_updates            | 6520         |
|    policy_gradient_loss | -6.28e-05    |
|    value_loss           | 8.44e+04     |
------------------------------------------
Eval num_timesteps=125500, episode_reward=390.06 +/- 750.93
Episode length: 34.90 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 390      |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 123      |
|    time_elapsed    | 449      |
|    total_timesteps | 125952   |
---------------------------------
Eval num_timesteps=126000, episode_reward=435.44 +/- 755.16
Episode length: 34.54 +/- 6.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 435           |
| time/                   |               |
|    total_timesteps      | 126000        |
| train/                  |               |
|    approx_kl            | 5.6446297e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0136       |
|    explained_variance   | 0.0259        |
|    learning_rate        | 0.001         |
|    loss                 | 5.64e+04      |
|    n_updates            | 6530          |
|    policy_gradient_loss | -0.000613     |
|    value_loss           | 9.54e+04      |
-------------------------------------------
Eval num_timesteps=126500, episode_reward=346.66 +/- 731.91
Episode length: 34.42 +/- 7.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 347      |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 124      |
|    time_elapsed    | 453      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=511.80 +/- 774.26
Episode length: 35.86 +/- 7.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 512          |
| time/                   |              |
|    total_timesteps      | 127000       |
| train/                  |              |
|    approx_kl            | 0.0058619305 |
|    clip_fraction        | 0.00449      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0121      |
|    explained_variance   | -0.0556      |
|    learning_rate        | 0.001        |
|    loss                 | 3.83e+04     |
|    n_updates            | 6540         |
|    policy_gradient_loss | -0.00138     |
|    value_loss           | 9.52e+04     |
------------------------------------------
Eval num_timesteps=127500, episode_reward=532.70 +/- 777.23
Episode length: 35.14 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=355.07 +/- 661.93
Episode length: 35.00 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 355      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 125      |
|    time_elapsed    | 457      |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=399.71 +/- 743.17
Episode length: 34.76 +/- 7.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.8        |
|    mean_reward          | 400         |
| time/                   |             |
|    total_timesteps      | 128500      |
| train/                  |             |
|    approx_kl            | 0.001771199 |
|    clip_fraction        | 0.00225     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0102     |
|    explained_variance   | -0.115      |
|    learning_rate        | 0.001       |
|    loss                 | 5.77e+04    |
|    n_updates            | 6550        |
|    policy_gradient_loss | 0.000564    |
|    value_loss           | 9.75e+04    |
-----------------------------------------
Eval num_timesteps=129000, episode_reward=374.81 +/- 644.31
Episode length: 35.30 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 375      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 383      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 126      |
|    time_elapsed    | 461      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=418.60 +/- 726.58
Episode length: 35.22 +/- 7.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 419           |
| time/                   |               |
|    total_timesteps      | 129500        |
| train/                  |               |
|    approx_kl            | 0.00079852005 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00904      |
|    explained_variance   | 0.0697        |
|    learning_rate        | 0.001         |
|    loss                 | 3.7e+04       |
|    n_updates            | 6560          |
|    policy_gradient_loss | 0.000768      |
|    value_loss           | 9.5e+04       |
-------------------------------------------
Eval num_timesteps=130000, episode_reward=430.30 +/- 735.86
Episode length: 35.44 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 430      |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 500      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 127      |
|    time_elapsed    | 465      |
|    total_timesteps | 130048   |
---------------------------------
Eval num_timesteps=130500, episode_reward=576.87 +/- 793.22
Episode length: 36.14 +/- 6.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 577           |
| time/                   |               |
|    total_timesteps      | 130500        |
| train/                  |               |
|    approx_kl            | 3.2000826e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.011        |
|    explained_variance   | 0.0927        |
|    learning_rate        | 0.001         |
|    loss                 | 2.77e+04      |
|    n_updates            | 6570          |
|    policy_gradient_loss | -0.000353     |
|    value_loss           | 8.64e+04      |
-------------------------------------------
Eval num_timesteps=131000, episode_reward=339.14 +/- 657.27
Episode length: 34.86 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 496      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 128      |
|    time_elapsed    | 468      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=616.40 +/- 810.83
Episode length: 36.10 +/- 7.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.1        |
|    mean_reward          | 616         |
| time/                   |             |
|    total_timesteps      | 131500      |
| train/                  |             |
|    approx_kl            | 0.002313666 |
|    clip_fraction        | 0.00166     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0106     |
|    explained_variance   | -0.00052    |
|    learning_rate        | 0.001       |
|    loss                 | 3.3e+04     |
|    n_updates            | 6580        |
|    policy_gradient_loss | -0.000406   |
|    value_loss           | 1.02e+05    |
-----------------------------------------
Eval num_timesteps=132000, episode_reward=419.66 +/- 786.59
Episode length: 34.54 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 535      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 129      |
|    time_elapsed    | 472      |
|    total_timesteps | 132096   |
---------------------------------
Eval num_timesteps=132500, episode_reward=372.37 +/- 641.23
Episode length: 35.20 +/- 6.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 372           |
| time/                   |               |
|    total_timesteps      | 132500        |
| train/                  |               |
|    approx_kl            | 0.00027864822 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00753      |
|    explained_variance   | 0.085         |
|    learning_rate        | 0.001         |
|    loss                 | 2.98e+04      |
|    n_updates            | 6590          |
|    policy_gradient_loss | -0.000853     |
|    value_loss           | 9.93e+04      |
-------------------------------------------
Eval num_timesteps=133000, episode_reward=409.48 +/- 664.29
Episode length: 36.04 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 588      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 130      |
|    time_elapsed    | 475      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=438.62 +/- 760.28
Episode length: 35.46 +/- 6.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 439           |
| time/                   |               |
|    total_timesteps      | 133500        |
| train/                  |               |
|    approx_kl            | 2.5012414e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00553      |
|    explained_variance   | 0.0538        |
|    learning_rate        | 0.001         |
|    loss                 | 5.09e+04      |
|    n_updates            | 6600          |
|    policy_gradient_loss | -0.000336     |
|    value_loss           | 1.11e+05      |
-------------------------------------------
Eval num_timesteps=134000, episode_reward=430.90 +/- 716.21
Episode length: 35.38 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 563      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 131      |
|    time_elapsed    | 479      |
|    total_timesteps | 134144   |
---------------------------------
Eval num_timesteps=134500, episode_reward=375.31 +/- 682.14
Episode length: 34.74 +/- 6.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.7        |
|    mean_reward          | 375         |
| time/                   |             |
|    total_timesteps      | 134500      |
| train/                  |             |
|    approx_kl            | 0.006841204 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00635    |
|    explained_variance   | -0.0512     |
|    learning_rate        | 0.001       |
|    loss                 | 3.9e+04     |
|    n_updates            | 6610        |
|    policy_gradient_loss | -0.00037    |
|    value_loss           | 1.23e+05    |
-----------------------------------------
Eval num_timesteps=135000, episode_reward=369.84 +/- 649.85
Episode length: 34.76 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 370      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 484      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 132      |
|    time_elapsed    | 483      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=349.84 +/- 657.04
Episode length: 34.20 +/- 7.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.2         |
|    mean_reward          | 350          |
| time/                   |              |
|    total_timesteps      | 135500       |
| train/                  |              |
|    approx_kl            | 0.0022470306 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00759     |
|    explained_variance   | -0.0278      |
|    learning_rate        | 0.001        |
|    loss                 | 6.22e+04     |
|    n_updates            | 6620         |
|    policy_gradient_loss | 0.000266     |
|    value_loss           | 1.1e+05      |
------------------------------------------
Eval num_timesteps=136000, episode_reward=582.01 +/- 776.33
Episode length: 36.08 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 582      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 499      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 133      |
|    time_elapsed    | 486      |
|    total_timesteps | 136192   |
---------------------------------
Eval num_timesteps=136500, episode_reward=311.24 +/- 616.37
Episode length: 34.62 +/- 5.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 311           |
| time/                   |               |
|    total_timesteps      | 136500        |
| train/                  |               |
|    approx_kl            | 4.3346663e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0104       |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.001         |
|    loss                 | 3.59e+04      |
|    n_updates            | 6630          |
|    policy_gradient_loss | -0.000471     |
|    value_loss           | 9.94e+04      |
-------------------------------------------
Eval num_timesteps=137000, episode_reward=454.37 +/- 754.63
Episode length: 35.22 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 477      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 134      |
|    time_elapsed    | 490      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=466.35 +/- 705.24
Episode length: 35.80 +/- 6.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 466           |
| time/                   |               |
|    total_timesteps      | 137500        |
| train/                  |               |
|    approx_kl            | 0.00024014339 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0102       |
|    explained_variance   | -0.0455       |
|    learning_rate        | 0.001         |
|    loss                 | 3.38e+04      |
|    n_updates            | 6640          |
|    policy_gradient_loss | -0.000374     |
|    value_loss           | 9.85e+04      |
-------------------------------------------
Eval num_timesteps=138000, episode_reward=358.02 +/- 685.95
Episode length: 34.08 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 408      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 135      |
|    time_elapsed    | 493      |
|    total_timesteps | 138240   |
---------------------------------
Eval num_timesteps=138500, episode_reward=582.49 +/- 768.19
Episode length: 36.60 +/- 6.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 582           |
| time/                   |               |
|    total_timesteps      | 138500        |
| train/                  |               |
|    approx_kl            | 3.1611999e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0111       |
|    explained_variance   | -0.0179       |
|    learning_rate        | 0.001         |
|    loss                 | 4.56e+04      |
|    n_updates            | 6650          |
|    policy_gradient_loss | -0.000524     |
|    value_loss           | 1.16e+05      |
-------------------------------------------
Eval num_timesteps=139000, episode_reward=317.82 +/- 653.07
Episode length: 34.76 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 399      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 136      |
|    time_elapsed    | 497      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=280.50 +/- 641.95
Episode length: 34.62 +/- 6.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 280          |
| time/                   |              |
|    total_timesteps      | 139500       |
| train/                  |              |
|    approx_kl            | 0.0025049476 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0195      |
|    explained_variance   | -0.0939      |
|    learning_rate        | 0.001        |
|    loss                 | 2.6e+04      |
|    n_updates            | 6660         |
|    policy_gradient_loss | -0.000876    |
|    value_loss           | 7.95e+04     |
------------------------------------------
Eval num_timesteps=140000, episode_reward=276.40 +/- 599.33
Episode length: 34.80 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 373      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 137      |
|    time_elapsed    | 501      |
|    total_timesteps | 140288   |
---------------------------------
Eval num_timesteps=140500, episode_reward=465.65 +/- 680.53
Episode length: 36.54 +/- 4.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.5          |
|    mean_reward          | 466           |
| time/                   |               |
|    total_timesteps      | 140500        |
| train/                  |               |
|    approx_kl            | 0.00013220974 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.026        |
|    explained_variance   | 0.0254        |
|    learning_rate        | 0.001         |
|    loss                 | 4.76e+04      |
|    n_updates            | 6670          |
|    policy_gradient_loss | -0.00107      |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=141000, episode_reward=583.50 +/- 754.56
Episode length: 36.16 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 584      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 430      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 138      |
|    time_elapsed    | 504      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=455.68 +/- 737.99
Episode length: 35.24 +/- 7.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.2        |
|    mean_reward          | 456         |
| time/                   |             |
|    total_timesteps      | 141500      |
| train/                  |             |
|    approx_kl            | 0.000648756 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.016      |
|    explained_variance   | -0.148      |
|    learning_rate        | 0.001       |
|    loss                 | 4.56e+04    |
|    n_updates            | 6680        |
|    policy_gradient_loss | -0.00135    |
|    value_loss           | 9.38e+04    |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=481.34 +/- 794.75
Episode length: 34.68 +/- 7.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 481      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 467      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 139      |
|    time_elapsed    | 508      |
|    total_timesteps | 142336   |
---------------------------------
Eval num_timesteps=142500, episode_reward=405.69 +/- 678.84
Episode length: 35.80 +/- 6.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 406          |
| time/                   |              |
|    total_timesteps      | 142500       |
| train/                  |              |
|    approx_kl            | 0.0030954424 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0172      |
|    explained_variance   | -0.0413      |
|    learning_rate        | 0.001        |
|    loss                 | 4.61e+04     |
|    n_updates            | 6690         |
|    policy_gradient_loss | 3.73e-06     |
|    value_loss           | 1.06e+05     |
------------------------------------------
Eval num_timesteps=143000, episode_reward=306.90 +/- 758.60
Episode length: 33.32 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 475      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 140      |
|    time_elapsed    | 511      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=558.76 +/- 772.11
Episode length: 35.62 +/- 7.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 559           |
| time/                   |               |
|    total_timesteps      | 143500        |
| train/                  |               |
|    approx_kl            | 0.00012706628 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.025        |
|    explained_variance   | -0.117        |
|    learning_rate        | 0.001         |
|    loss                 | 3.07e+04      |
|    n_updates            | 6700          |
|    policy_gradient_loss | -0.000726     |
|    value_loss           | 1e+05         |
-------------------------------------------
Eval num_timesteps=144000, episode_reward=248.93 +/- 632.42
Episode length: 34.10 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 249      |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 530      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 141      |
|    time_elapsed    | 515      |
|    total_timesteps | 144384   |
---------------------------------
Eval num_timesteps=144500, episode_reward=530.67 +/- 696.87
Episode length: 36.26 +/- 6.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 531           |
| time/                   |               |
|    total_timesteps      | 144500        |
| train/                  |               |
|    approx_kl            | 0.00036994868 |
|    clip_fraction        | 0.00293       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0195       |
|    explained_variance   | 0.0372        |
|    learning_rate        | 0.001         |
|    loss                 | 4.63e+04      |
|    n_updates            | 6710          |
|    policy_gradient_loss | -0.00071      |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=145000, episode_reward=459.82 +/- 685.12
Episode length: 36.86 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.8     |
|    ep_rew_mean     | 604      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 142      |
|    time_elapsed    | 519      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=543.72 +/- 773.42
Episode length: 36.46 +/- 6.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.5         |
|    mean_reward          | 544          |
| time/                   |              |
|    total_timesteps      | 145500       |
| train/                  |              |
|    approx_kl            | 0.0032475227 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.01        |
|    explained_variance   | 0.0139       |
|    learning_rate        | 0.001        |
|    loss                 | 4.18e+04     |
|    n_updates            | 6720         |
|    policy_gradient_loss | 0.00102      |
|    value_loss           | 1.02e+05     |
------------------------------------------
Eval num_timesteps=146000, episode_reward=415.49 +/- 693.49
Episode length: 35.18 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 505      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 143      |
|    time_elapsed    | 522      |
|    total_timesteps | 146432   |
---------------------------------
Eval num_timesteps=146500, episode_reward=542.20 +/- 753.72
Episode length: 36.56 +/- 6.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.6         |
|    mean_reward          | 542          |
| time/                   |              |
|    total_timesteps      | 146500       |
| train/                  |              |
|    approx_kl            | 6.137532e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0278      |
|    explained_variance   | -0.228       |
|    learning_rate        | 0.001        |
|    loss                 | 2.64e+04     |
|    n_updates            | 6730         |
|    policy_gradient_loss | -0.000454    |
|    value_loss           | 9.03e+04     |
------------------------------------------
Eval num_timesteps=147000, episode_reward=569.62 +/- 810.86
Episode length: 36.02 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 570      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 462      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 144      |
|    time_elapsed    | 526      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=460.13 +/- 712.09
Episode length: 36.04 +/- 6.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 460           |
| time/                   |               |
|    total_timesteps      | 147500        |
| train/                  |               |
|    approx_kl            | 1.3150333e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0172       |
|    explained_variance   | 0.068         |
|    learning_rate        | 0.001         |
|    loss                 | 4.69e+04      |
|    n_updates            | 6740          |
|    policy_gradient_loss | -0.00112      |
|    value_loss           | 9.39e+04      |
-------------------------------------------
Eval num_timesteps=148000, episode_reward=169.95 +/- 621.28
Episode length: 32.44 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 473      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 145      |
|    time_elapsed    | 529      |
|    total_timesteps | 148480   |
---------------------------------
Eval num_timesteps=148500, episode_reward=376.34 +/- 684.93
Episode length: 34.92 +/- 6.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 376          |
| time/                   |              |
|    total_timesteps      | 148500       |
| train/                  |              |
|    approx_kl            | 3.870309e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.02        |
|    explained_variance   | 0.0809       |
|    learning_rate        | 0.001        |
|    loss                 | 4.54e+04     |
|    n_updates            | 6750         |
|    policy_gradient_loss | -0.000698    |
|    value_loss           | 9.25e+04     |
------------------------------------------
Eval num_timesteps=149000, episode_reward=469.64 +/- 764.98
Episode length: 35.30 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 470      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=528.04 +/- 758.41
Episode length: 36.28 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 528      |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 450      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 146      |
|    time_elapsed    | 534      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=325.78 +/- 706.91
Episode length: 33.90 +/- 7.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.9          |
|    mean_reward          | 326           |
| time/                   |               |
|    total_timesteps      | 150000        |
| train/                  |               |
|    approx_kl            | 0.00057002564 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0347       |
|    explained_variance   | -0.0428       |
|    learning_rate        | 0.001         |
|    loss                 | 4.42e+04      |
|    n_updates            | 6760          |
|    policy_gradient_loss | 0.000716      |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=150500, episode_reward=490.30 +/- 801.94
Episode length: 34.42 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 490      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 354      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 147      |
|    time_elapsed    | 538      |
|    total_timesteps | 150528   |
---------------------------------
Eval num_timesteps=151000, episode_reward=479.65 +/- 744.66
Episode length: 36.12 +/- 6.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 480          |
| time/                   |              |
|    total_timesteps      | 151000       |
| train/                  |              |
|    approx_kl            | 7.374969e-06 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0413      |
|    explained_variance   | 0.0411       |
|    learning_rate        | 0.001        |
|    loss                 | 3.65e+04     |
|    n_updates            | 6770         |
|    policy_gradient_loss | -0.000377    |
|    value_loss           | 8.26e+04     |
------------------------------------------
Eval num_timesteps=151500, episode_reward=377.99 +/- 688.94
Episode length: 35.20 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 378      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 285      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 148      |
|    time_elapsed    | 541      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=374.64 +/- 710.88
Episode length: 34.76 +/- 7.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 375          |
| time/                   |              |
|    total_timesteps      | 152000       |
| train/                  |              |
|    approx_kl            | 0.0003241804 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0468      |
|    explained_variance   | 0.00658      |
|    learning_rate        | 0.001        |
|    loss                 | 3.83e+04     |
|    n_updates            | 6780         |
|    policy_gradient_loss | -0.000529    |
|    value_loss           | 7.49e+04     |
------------------------------------------
Eval num_timesteps=152500, episode_reward=573.74 +/- 741.69
Episode length: 36.94 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 574      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 263      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 149      |
|    time_elapsed    | 545      |
|    total_timesteps | 152576   |
---------------------------------
Eval num_timesteps=153000, episode_reward=368.95 +/- 636.51
Episode length: 35.20 +/- 6.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 369          |
| time/                   |              |
|    total_timesteps      | 153000       |
| train/                  |              |
|    approx_kl            | 0.0006154873 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0521      |
|    explained_variance   | -0.0029      |
|    learning_rate        | 0.001        |
|    loss                 | 3.32e+04     |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.000578    |
|    value_loss           | 8.04e+04     |
------------------------------------------
Eval num_timesteps=153500, episode_reward=473.92 +/- 762.09
Episode length: 35.24 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 474      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 150      |
|    time_elapsed    | 549      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=290.67 +/- 611.52
Episode length: 34.92 +/- 6.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.9        |
|    mean_reward          | 291         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.008565657 |
|    clip_fraction        | 0.00439     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0316     |
|    explained_variance   | 0.115       |
|    learning_rate        | 0.001       |
|    loss                 | 4e+04       |
|    n_updates            | 6800        |
|    policy_gradient_loss | -0.00166    |
|    value_loss           | 9.2e+04     |
-----------------------------------------
Eval num_timesteps=154500, episode_reward=476.68 +/- 770.63
Episode length: 35.20 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 425      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 151      |
|    time_elapsed    | 552      |
|    total_timesteps | 154624   |
---------------------------------
Eval num_timesteps=155000, episode_reward=429.59 +/- 660.79
Episode length: 35.48 +/- 6.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.5       |
|    mean_reward          | 430        |
| time/                   |            |
|    total_timesteps      | 155000     |
| train/                  |            |
|    approx_kl            | 0.00180548 |
|    clip_fraction        | 0.00186    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0243    |
|    explained_variance   | 0.0293     |
|    learning_rate        | 0.001      |
|    loss                 | 3.98e+04   |
|    n_updates            | 6810       |
|    policy_gradient_loss | -0.000871  |
|    value_loss           | 8.29e+04   |
----------------------------------------
Eval num_timesteps=155500, episode_reward=443.26 +/- 729.61
Episode length: 35.62 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 419      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 152      |
|    time_elapsed    | 556      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=425.34 +/- 706.61
Episode length: 35.20 +/- 6.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 425           |
| time/                   |               |
|    total_timesteps      | 156000        |
| train/                  |               |
|    approx_kl            | 0.00014321739 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0306       |
|    explained_variance   | -0.0203       |
|    learning_rate        | 0.001         |
|    loss                 | 3.19e+04      |
|    n_updates            | 6820          |
|    policy_gradient_loss | -0.000799     |
|    value_loss           | 7.69e+04      |
-------------------------------------------
Eval num_timesteps=156500, episode_reward=453.41 +/- 756.60
Episode length: 34.74 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 153      |
|    time_elapsed    | 560      |
|    total_timesteps | 156672   |
---------------------------------
Eval num_timesteps=157000, episode_reward=395.20 +/- 672.64
Episode length: 35.28 +/- 7.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 395          |
| time/                   |              |
|    total_timesteps      | 157000       |
| train/                  |              |
|    approx_kl            | 0.0018768435 |
|    clip_fraction        | 0.00107      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0344      |
|    explained_variance   | 0.187        |
|    learning_rate        | 0.001        |
|    loss                 | 3.86e+04     |
|    n_updates            | 6830         |
|    policy_gradient_loss | -0.000564    |
|    value_loss           | 9.75e+04     |
------------------------------------------
Eval num_timesteps=157500, episode_reward=159.11 +/- 448.76
Episode length: 33.94 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 407      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 154      |
|    time_elapsed    | 563      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=390.49 +/- 762.87
Episode length: 33.98 +/- 7.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 390          |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0015033131 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0228      |
|    explained_variance   | 0.0219       |
|    learning_rate        | 0.001        |
|    loss                 | 4.65e+04     |
|    n_updates            | 6840         |
|    policy_gradient_loss | -0.00105     |
|    value_loss           | 9.24e+04     |
------------------------------------------
Eval num_timesteps=158500, episode_reward=502.99 +/- 726.75
Episode length: 35.52 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 503      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 432      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 155      |
|    time_elapsed    | 567      |
|    total_timesteps | 158720   |
---------------------------------
Eval num_timesteps=159000, episode_reward=431.83 +/- 714.65
Episode length: 35.20 +/- 6.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 432          |
| time/                   |              |
|    total_timesteps      | 159000       |
| train/                  |              |
|    approx_kl            | 0.0006921176 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0139      |
|    explained_variance   | 0.105        |
|    learning_rate        | 0.001        |
|    loss                 | 5.55e+04     |
|    n_updates            | 6850         |
|    policy_gradient_loss | -0.000905    |
|    value_loss           | 1.18e+05     |
------------------------------------------
Eval num_timesteps=159500, episode_reward=452.91 +/- 726.05
Episode length: 35.34 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 454      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 156      |
|    time_elapsed    | 570      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=344.25 +/- 709.93
Episode length: 33.42 +/- 7.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 344           |
| time/                   |               |
|    total_timesteps      | 160000        |
| train/                  |               |
|    approx_kl            | 0.00035547442 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0206       |
|    explained_variance   | -0.192        |
|    learning_rate        | 0.001         |
|    loss                 | 4.18e+04      |
|    n_updates            | 6860          |
|    policy_gradient_loss | -0.000893     |
|    value_loss           | 9.71e+04      |
-------------------------------------------
Eval num_timesteps=160500, episode_reward=567.78 +/- 748.19
Episode length: 35.94 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 568      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 479      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 157      |
|    time_elapsed    | 574      |
|    total_timesteps | 160768   |
---------------------------------
Eval num_timesteps=161000, episode_reward=309.26 +/- 667.99
Episode length: 34.80 +/- 6.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 309          |
| time/                   |              |
|    total_timesteps      | 161000       |
| train/                  |              |
|    approx_kl            | 0.0044085686 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00998     |
|    explained_variance   | 0.0783       |
|    learning_rate        | 0.001        |
|    loss                 | 3.67e+04     |
|    n_updates            | 6870         |
|    policy_gradient_loss | 0.00201      |
|    value_loss           | 1.04e+05     |
------------------------------------------
Eval num_timesteps=161500, episode_reward=463.02 +/- 761.53
Episode length: 35.44 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 158      |
|    time_elapsed    | 577      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=357.71 +/- 730.78
Episode length: 34.30 +/- 7.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 358           |
| time/                   |               |
|    total_timesteps      | 162000        |
| train/                  |               |
|    approx_kl            | 0.00028828013 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0195       |
|    explained_variance   | -0.0138       |
|    learning_rate        | 0.001         |
|    loss                 | 3.91e+04      |
|    n_updates            | 6880          |
|    policy_gradient_loss | -0.000499     |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=162500, episode_reward=438.60 +/- 724.42
Episode length: 35.40 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 452      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 159      |
|    time_elapsed    | 581      |
|    total_timesteps | 162816   |
---------------------------------
Eval num_timesteps=163000, episode_reward=385.18 +/- 724.18
Episode length: 34.70 +/- 6.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 385           |
| time/                   |               |
|    total_timesteps      | 163000        |
| train/                  |               |
|    approx_kl            | 1.1708413e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0129       |
|    explained_variance   | 0.173         |
|    learning_rate        | 0.001         |
|    loss                 | 5.22e+04      |
|    n_updates            | 6890          |
|    policy_gradient_loss | -0.000302     |
|    value_loss           | 9.23e+04      |
-------------------------------------------
Eval num_timesteps=163500, episode_reward=453.92 +/- 751.67
Episode length: 35.40 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 472      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 160      |
|    time_elapsed    | 585      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=365.58 +/- 689.04
Episode length: 34.86 +/- 6.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 366           |
| time/                   |               |
|    total_timesteps      | 164000        |
| train/                  |               |
|    approx_kl            | 1.8440187e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0145       |
|    explained_variance   | 0.12          |
|    learning_rate        | 0.001         |
|    loss                 | 5.18e+04      |
|    n_updates            | 6900          |
|    policy_gradient_loss | -0.000246     |
|    value_loss           | 9.82e+04      |
-------------------------------------------
Eval num_timesteps=164500, episode_reward=393.11 +/- 638.27
Episode length: 35.00 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 393      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 545      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 161      |
|    time_elapsed    | 588      |
|    total_timesteps | 164864   |
---------------------------------
Eval num_timesteps=165000, episode_reward=494.69 +/- 782.36
Episode length: 34.98 +/- 6.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 495           |
| time/                   |               |
|    total_timesteps      | 165000        |
| train/                  |               |
|    approx_kl            | 0.00037283293 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0113       |
|    explained_variance   | 0.0324        |
|    learning_rate        | 0.001         |
|    loss                 | 4.32e+04      |
|    n_updates            | 6910          |
|    policy_gradient_loss | -0.000227     |
|    value_loss           | 1.17e+05      |
-------------------------------------------
Eval num_timesteps=165500, episode_reward=549.18 +/- 811.61
Episode length: 34.82 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 525      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 162      |
|    time_elapsed    | 592      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=330.98 +/- 700.58
Episode length: 34.02 +/- 6.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 331          |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 0.0040590446 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0121      |
|    explained_variance   | 0.0955       |
|    learning_rate        | 0.001        |
|    loss                 | 4.16e+04     |
|    n_updates            | 6920         |
|    policy_gradient_loss | 0.000723     |
|    value_loss           | 9.47e+04     |
------------------------------------------
Eval num_timesteps=166500, episode_reward=423.14 +/- 705.14
Episode length: 35.80 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 457      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 163      |
|    time_elapsed    | 595      |
|    total_timesteps | 166912   |
---------------------------------
Eval num_timesteps=167000, episode_reward=551.87 +/- 729.98
Episode length: 36.54 +/- 5.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 36.5       |
|    mean_reward          | 552        |
| time/                   |            |
|    total_timesteps      | 167000     |
| train/                  |            |
|    approx_kl            | 0.00041251 |
|    clip_fraction        | 0.000488   |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0175    |
|    explained_variance   | 0.0763     |
|    learning_rate        | 0.001      |
|    loss                 | 3.77e+04   |
|    n_updates            | 6930       |
|    policy_gradient_loss | -0.000302  |
|    value_loss           | 1.13e+05   |
----------------------------------------
Eval num_timesteps=167500, episode_reward=508.78 +/- 768.94
Episode length: 35.78 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 509      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 439      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 164      |
|    time_elapsed    | 599      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=477.75 +/- 732.12
Episode length: 36.18 +/- 5.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.2         |
|    mean_reward          | 478          |
| time/                   |              |
|    total_timesteps      | 168000       |
| train/                  |              |
|    approx_kl            | 0.0011391719 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00828     |
|    explained_variance   | 0.0173       |
|    learning_rate        | 0.001        |
|    loss                 | 4.54e+04     |
|    n_updates            | 6940         |
|    policy_gradient_loss | -0.000182    |
|    value_loss           | 1.08e+05     |
------------------------------------------
Eval num_timesteps=168500, episode_reward=421.37 +/- 703.42
Episode length: 36.24 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 439      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 165      |
|    time_elapsed    | 603      |
|    total_timesteps | 168960   |
---------------------------------
Eval num_timesteps=169000, episode_reward=448.25 +/- 727.31
Episode length: 35.40 +/- 6.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 448           |
| time/                   |               |
|    total_timesteps      | 169000        |
| train/                  |               |
|    approx_kl            | 3.4220284e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0184       |
|    explained_variance   | 0.069         |
|    learning_rate        | 0.001         |
|    loss                 | 3.46e+04      |
|    n_updates            | 6950          |
|    policy_gradient_loss | -0.000365     |
|    value_loss           | 8.78e+04      |
-------------------------------------------
Eval num_timesteps=169500, episode_reward=501.35 +/- 739.27
Episode length: 35.86 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 501      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 487      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 166      |
|    time_elapsed    | 606      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=611.39 +/- 767.75
Episode length: 36.16 +/- 6.88
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.2           |
|    mean_reward          | 611            |
| time/                   |                |
|    total_timesteps      | 170000         |
| train/                  |                |
|    approx_kl            | 0.000105715124 |
|    clip_fraction        | 0.00117        |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0237        |
|    explained_variance   | 0.169          |
|    learning_rate        | 0.001          |
|    loss                 | 3.55e+04       |
|    n_updates            | 6960           |
|    policy_gradient_loss | -0.000417      |
|    value_loss           | 1.02e+05       |
--------------------------------------------
Eval num_timesteps=170500, episode_reward=319.43 +/- 679.37
Episode length: 33.76 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=344.71 +/- 650.54
Episode length: 35.70 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 345      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 450      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 167      |
|    time_elapsed    | 611      |
|    total_timesteps | 171008   |
---------------------------------
Eval num_timesteps=171500, episode_reward=460.61 +/- 799.24
Episode length: 34.40 +/- 6.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.4        |
|    mean_reward          | 461         |
| time/                   |             |
|    total_timesteps      | 171500      |
| train/                  |             |
|    approx_kl            | 0.000881323 |
|    clip_fraction        | 0.00186     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0216     |
|    explained_variance   | -0.04       |
|    learning_rate        | 0.001       |
|    loss                 | 6.5e+04     |
|    n_updates            | 6970        |
|    policy_gradient_loss | 0.00146     |
|    value_loss           | 1.05e+05    |
-----------------------------------------
Eval num_timesteps=172000, episode_reward=346.29 +/- 615.75
Episode length: 35.42 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 377      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 168      |
|    time_elapsed    | 615      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=644.62 +/- 752.50
Episode length: 37.60 +/- 5.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.6         |
|    mean_reward          | 645          |
| time/                   |              |
|    total_timesteps      | 172500       |
| train/                  |              |
|    approx_kl            | 0.0005980724 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0271      |
|    explained_variance   | -0.14        |
|    learning_rate        | 0.001        |
|    loss                 | 3e+04        |
|    n_updates            | 6980         |
|    policy_gradient_loss | -0.000736    |
|    value_loss           | 9.49e+04     |
------------------------------------------
Eval num_timesteps=173000, episode_reward=330.76 +/- 694.02
Episode length: 34.32 +/- 7.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 331      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 328      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 169      |
|    time_elapsed    | 619      |
|    total_timesteps | 173056   |
---------------------------------
Eval num_timesteps=173500, episode_reward=343.26 +/- 709.05
Episode length: 33.78 +/- 6.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 343          |
| time/                   |              |
|    total_timesteps      | 173500       |
| train/                  |              |
|    approx_kl            | 0.0004997706 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0292      |
|    explained_variance   | -0.078       |
|    learning_rate        | 0.001        |
|    loss                 | 2.85e+04     |
|    n_updates            | 6990         |
|    policy_gradient_loss | 0.00241      |
|    value_loss           | 6.53e+04     |
------------------------------------------
Eval num_timesteps=174000, episode_reward=470.27 +/- 750.10
Episode length: 34.96 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 470      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 170      |
|    time_elapsed    | 622      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=572.67 +/- 802.07
Episode length: 35.88 +/- 6.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 573           |
| time/                   |               |
|    total_timesteps      | 174500        |
| train/                  |               |
|    approx_kl            | 0.00020797871 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0293       |
|    explained_variance   | -0.119        |
|    learning_rate        | 0.001         |
|    loss                 | 5.05e+04      |
|    n_updates            | 7000          |
|    policy_gradient_loss | -0.000703     |
|    value_loss           | 1.06e+05      |
-------------------------------------------
Eval num_timesteps=175000, episode_reward=417.38 +/- 684.77
Episode length: 35.44 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 409      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 171      |
|    time_elapsed    | 626      |
|    total_timesteps | 175104   |
---------------------------------
Eval num_timesteps=175500, episode_reward=530.67 +/- 766.19
Episode length: 35.82 +/- 6.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 531          |
| time/                   |              |
|    total_timesteps      | 175500       |
| train/                  |              |
|    approx_kl            | 0.0053766337 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0261      |
|    explained_variance   | 0.0452       |
|    learning_rate        | 0.001        |
|    loss                 | 3.57e+04     |
|    n_updates            | 7010         |
|    policy_gradient_loss | -2.6e-05     |
|    value_loss           | 1e+05        |
------------------------------------------
Eval num_timesteps=176000, episode_reward=393.50 +/- 683.39
Episode length: 34.36 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 444      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 172      |
|    time_elapsed    | 629      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=460.12 +/- 694.75
Episode length: 35.80 +/- 6.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 460          |
| time/                   |              |
|    total_timesteps      | 176500       |
| train/                  |              |
|    approx_kl            | 0.0068776878 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0227      |
|    explained_variance   | 0.091        |
|    learning_rate        | 0.001        |
|    loss                 | 3.76e+04     |
|    n_updates            | 7020         |
|    policy_gradient_loss | 0.00143      |
|    value_loss           | 9.16e+04     |
------------------------------------------
Eval num_timesteps=177000, episode_reward=440.53 +/- 761.14
Episode length: 35.02 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 486      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 173      |
|    time_elapsed    | 633      |
|    total_timesteps | 177152   |
---------------------------------
Eval num_timesteps=177500, episode_reward=600.84 +/- 737.97
Episode length: 37.10 +/- 5.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.1         |
|    mean_reward          | 601          |
| time/                   |              |
|    total_timesteps      | 177500       |
| train/                  |              |
|    approx_kl            | 0.0069835335 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0228      |
|    explained_variance   | 0.0536       |
|    learning_rate        | 0.001        |
|    loss                 | 5.39e+04     |
|    n_updates            | 7030         |
|    policy_gradient_loss | 0.00122      |
|    value_loss           | 1.08e+05     |
------------------------------------------
Eval num_timesteps=178000, episode_reward=493.00 +/- 732.77
Episode length: 35.84 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 493      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 174      |
|    time_elapsed    | 637      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=528.82 +/- 762.02
Episode length: 36.12 +/- 6.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 529          |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0021754424 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0257      |
|    explained_variance   | -0.13        |
|    learning_rate        | 0.001        |
|    loss                 | 3.44e+04     |
|    n_updates            | 7040         |
|    policy_gradient_loss | 0.0101       |
|    value_loss           | 1.01e+05     |
------------------------------------------
Eval num_timesteps=179000, episode_reward=407.76 +/- 629.32
Episode length: 36.12 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 386      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 175      |
|    time_elapsed    | 640      |
|    total_timesteps | 179200   |
---------------------------------
Eval num_timesteps=179500, episode_reward=418.54 +/- 728.39
Episode length: 34.86 +/- 7.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.9        |
|    mean_reward          | 419         |
| time/                   |             |
|    total_timesteps      | 179500      |
| train/                  |             |
|    approx_kl            | 0.001403958 |
|    clip_fraction        | 0.00156     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0127     |
|    explained_variance   | 0.089       |
|    learning_rate        | 0.001       |
|    loss                 | 4.05e+04    |
|    n_updates            | 7050        |
|    policy_gradient_loss | -0.000211   |
|    value_loss           | 8.75e+04    |
-----------------------------------------
Eval num_timesteps=180000, episode_reward=397.93 +/- 772.35
Episode length: 34.40 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 398      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 176      |
|    time_elapsed    | 644      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=402.26 +/- 679.66
Episode length: 35.24 +/- 6.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.2        |
|    mean_reward          | 402         |
| time/                   |             |
|    total_timesteps      | 180500      |
| train/                  |             |
|    approx_kl            | 0.003993886 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0117     |
|    explained_variance   | 0.0562      |
|    learning_rate        | 0.001       |
|    loss                 | 5.09e+04    |
|    n_updates            | 7060        |
|    policy_gradient_loss | 0.000141    |
|    value_loss           | 1.1e+05     |
-----------------------------------------
Eval num_timesteps=181000, episode_reward=450.96 +/- 747.53
Episode length: 34.86 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 177      |
|    time_elapsed    | 647      |
|    total_timesteps | 181248   |
---------------------------------
Eval num_timesteps=181500, episode_reward=502.06 +/- 767.09
Episode length: 36.04 +/- 6.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 502          |
| time/                   |              |
|    total_timesteps      | 181500       |
| train/                  |              |
|    approx_kl            | 5.802722e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0136      |
|    explained_variance   | 0.0416       |
|    learning_rate        | 0.001        |
|    loss                 | 3.46e+04     |
|    n_updates            | 7070         |
|    policy_gradient_loss | -0.000217    |
|    value_loss           | 7.92e+04     |
------------------------------------------
Eval num_timesteps=182000, episode_reward=544.26 +/- 786.56
Episode length: 35.90 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 544      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 508      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 178      |
|    time_elapsed    | 651      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=447.06 +/- 711.51
Episode length: 35.82 +/- 6.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 447          |
| time/                   |              |
|    total_timesteps      | 182500       |
| train/                  |              |
|    approx_kl            | 6.009941e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00623     |
|    explained_variance   | 0.0637       |
|    learning_rate        | 0.001        |
|    loss                 | 4.38e+04     |
|    n_updates            | 7080         |
|    policy_gradient_loss | -0.000198    |
|    value_loss           | 1.02e+05     |
------------------------------------------
Eval num_timesteps=183000, episode_reward=395.40 +/- 713.68
Episode length: 34.26 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 482      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 179      |
|    time_elapsed    | 655      |
|    total_timesteps | 183296   |
---------------------------------
Eval num_timesteps=183500, episode_reward=499.84 +/- 740.14
Episode length: 35.80 +/- 6.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 500           |
| time/                   |               |
|    total_timesteps      | 183500        |
| train/                  |               |
|    approx_kl            | 4.8035057e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00853      |
|    explained_variance   | 0.0782        |
|    learning_rate        | 0.001         |
|    loss                 | 4.04e+04      |
|    n_updates            | 7090          |
|    policy_gradient_loss | -0.000255     |
|    value_loss           | 9.21e+04      |
-------------------------------------------
Eval num_timesteps=184000, episode_reward=348.34 +/- 645.89
Episode length: 34.66 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 348      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 435      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 180      |
|    time_elapsed    | 658      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=417.44 +/- 674.19
Episode length: 35.60 +/- 6.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 417          |
| time/                   |              |
|    total_timesteps      | 184500       |
| train/                  |              |
|    approx_kl            | 7.424387e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0155      |
|    explained_variance   | 0.0454       |
|    learning_rate        | 0.001        |
|    loss                 | 3.58e+04     |
|    n_updates            | 7100         |
|    policy_gradient_loss | -0.000197    |
|    value_loss           | 8.73e+04     |
------------------------------------------
Eval num_timesteps=185000, episode_reward=454.17 +/- 733.29
Episode length: 35.72 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 446      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 181      |
|    time_elapsed    | 662      |
|    total_timesteps | 185344   |
---------------------------------
Eval num_timesteps=185500, episode_reward=557.76 +/- 771.61
Episode length: 35.92 +/- 6.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 558          |
| time/                   |              |
|    total_timesteps      | 185500       |
| train/                  |              |
|    approx_kl            | 4.879257e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0123      |
|    explained_variance   | 0.0901       |
|    learning_rate        | 0.001        |
|    loss                 | 4.31e+04     |
|    n_updates            | 7110         |
|    policy_gradient_loss | -0.000236    |
|    value_loss           | 1.09e+05     |
------------------------------------------
Eval num_timesteps=186000, episode_reward=436.21 +/- 713.40
Episode length: 34.80 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 436      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 416      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 182      |
|    time_elapsed    | 666      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=497.47 +/- 727.27
Episode length: 36.30 +/- 6.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 497           |
| time/                   |               |
|    total_timesteps      | 186500        |
| train/                  |               |
|    approx_kl            | 3.3273245e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0125       |
|    explained_variance   | 0.027         |
|    learning_rate        | 0.001         |
|    loss                 | 5.04e+04      |
|    n_updates            | 7120          |
|    policy_gradient_loss | -0.000179     |
|    value_loss           | 1.07e+05      |
-------------------------------------------
Eval num_timesteps=187000, episode_reward=323.74 +/- 672.96
Episode length: 34.08 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 324      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 395      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 183      |
|    time_elapsed    | 669      |
|    total_timesteps | 187392   |
---------------------------------
Eval num_timesteps=187500, episode_reward=489.98 +/- 757.53
Episode length: 35.56 +/- 6.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 490          |
| time/                   |              |
|    total_timesteps      | 187500       |
| train/                  |              |
|    approx_kl            | 2.643792e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0147      |
|    explained_variance   | 0.05         |
|    learning_rate        | 0.001        |
|    loss                 | 5.2e+04      |
|    n_updates            | 7130         |
|    policy_gradient_loss | -0.000172    |
|    value_loss           | 7.82e+04     |
------------------------------------------
Eval num_timesteps=188000, episode_reward=429.33 +/- 704.81
Episode length: 35.92 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 429      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 452      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 184      |
|    time_elapsed    | 673      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=337.43 +/- 675.88
Episode length: 34.88 +/- 6.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 337           |
| time/                   |               |
|    total_timesteps      | 188500        |
| train/                  |               |
|    approx_kl            | 1.6856939e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0133       |
|    explained_variance   | -0.0722       |
|    learning_rate        | 0.001         |
|    loss                 | 5.49e+04      |
|    n_updates            | 7140          |
|    policy_gradient_loss | -0.000161     |
|    value_loss           | 1.04e+05      |
-------------------------------------------
Eval num_timesteps=189000, episode_reward=403.35 +/- 660.26
Episode length: 36.30 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 503      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 185      |
|    time_elapsed    | 676      |
|    total_timesteps | 189440   |
---------------------------------
Eval num_timesteps=189500, episode_reward=369.95 +/- 662.01
Episode length: 34.86 +/- 6.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 370          |
| time/                   |              |
|    total_timesteps      | 189500       |
| train/                  |              |
|    approx_kl            | 2.628076e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00967     |
|    explained_variance   | 0.0212       |
|    learning_rate        | 0.001        |
|    loss                 | 4.73e+04     |
|    n_updates            | 7150         |
|    policy_gradient_loss | -0.000218    |
|    value_loss           | 1.06e+05     |
------------------------------------------
Eval num_timesteps=190000, episode_reward=345.89 +/- 695.77
Episode length: 34.80 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 533      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 186      |
|    time_elapsed    | 680      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=442.38 +/- 737.68
Episode length: 35.38 +/- 6.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 442           |
| time/                   |               |
|    total_timesteps      | 190500        |
| train/                  |               |
|    approx_kl            | 5.4825796e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00973      |
|    explained_variance   | -0.0743       |
|    learning_rate        | 0.001         |
|    loss                 | 4.01e+04      |
|    n_updates            | 7160          |
|    policy_gradient_loss | -0.000255     |
|    value_loss           | 1.02e+05      |
-------------------------------------------
Eval num_timesteps=191000, episode_reward=432.39 +/- 739.51
Episode length: 34.80 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 509      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 187      |
|    time_elapsed    | 684      |
|    total_timesteps | 191488   |
---------------------------------
Eval num_timesteps=191500, episode_reward=242.63 +/- 645.05
Episode length: 33.00 +/- 7.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 243           |
| time/                   |               |
|    total_timesteps      | 191500        |
| train/                  |               |
|    approx_kl            | 8.8475645e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0104       |
|    explained_variance   | 0.15          |
|    learning_rate        | 0.001         |
|    loss                 | 4e+04         |
|    n_updates            | 7170          |
|    policy_gradient_loss | -9.05e-05     |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=192000, episode_reward=381.37 +/- 686.41
Episode length: 34.88 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=511.19 +/- 712.54
Episode length: 36.10 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 511      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 499      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 188      |
|    time_elapsed    | 688      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=271.44 +/- 604.82
Episode length: 34.46 +/- 6.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.5         |
|    mean_reward          | 271          |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 2.766028e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00725     |
|    explained_variance   | -0.102       |
|    learning_rate        | 0.001        |
|    loss                 | 4.98e+04     |
|    n_updates            | 7180         |
|    policy_gradient_loss | -0.000221    |
|    value_loss           | 1.06e+05     |
------------------------------------------
Eval num_timesteps=193500, episode_reward=417.32 +/- 737.72
Episode length: 34.88 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 189      |
|    time_elapsed    | 692      |
|    total_timesteps | 193536   |
---------------------------------
Eval num_timesteps=194000, episode_reward=629.46 +/- 771.56
Episode length: 36.26 +/- 6.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 629           |
| time/                   |               |
|    total_timesteps      | 194000        |
| train/                  |               |
|    approx_kl            | 0.00058749516 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00898      |
|    explained_variance   | 0.0126        |
|    learning_rate        | 0.001         |
|    loss                 | 3.62e+04      |
|    n_updates            | 7190          |
|    policy_gradient_loss | -0.000207     |
|    value_loss           | 8.33e+04      |
-------------------------------------------
Eval num_timesteps=194500, episode_reward=423.30 +/- 821.67
Episode length: 34.86 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 190      |
|    time_elapsed    | 696      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=441.28 +/- 710.36
Episode length: 36.00 +/- 5.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 441           |
| time/                   |               |
|    total_timesteps      | 195000        |
| train/                  |               |
|    approx_kl            | 2.1595042e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00948      |
|    explained_variance   | -0.0123       |
|    learning_rate        | 0.001         |
|    loss                 | 2.07e+04      |
|    n_updates            | 7200          |
|    policy_gradient_loss | -0.000217     |
|    value_loss           | 7.55e+04      |
-------------------------------------------
Eval num_timesteps=195500, episode_reward=484.38 +/- 752.95
Episode length: 35.86 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 484      |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 191      |
|    time_elapsed    | 699      |
|    total_timesteps | 195584   |
---------------------------------
Eval num_timesteps=196000, episode_reward=344.30 +/- 708.85
Episode length: 34.32 +/- 7.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 344           |
| time/                   |               |
|    total_timesteps      | 196000        |
| train/                  |               |
|    approx_kl            | 2.1437882e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0115       |
|    explained_variance   | -0.0175       |
|    learning_rate        | 0.001         |
|    loss                 | 3.64e+04      |
|    n_updates            | 7210          |
|    policy_gradient_loss | -0.000172     |
|    value_loss           | 9.21e+04      |
-------------------------------------------
Eval num_timesteps=196500, episode_reward=455.51 +/- 698.64
Episode length: 36.18 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 348      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 192      |
|    time_elapsed    | 703      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=437.93 +/- 784.22
Episode length: 34.42 +/- 7.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 438           |
| time/                   |               |
|    total_timesteps      | 197000        |
| train/                  |               |
|    approx_kl            | 0.00026793132 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00938      |
|    explained_variance   | 0.123         |
|    learning_rate        | 0.001         |
|    loss                 | 3.63e+04      |
|    n_updates            | 7220          |
|    policy_gradient_loss | -0.00024      |
|    value_loss           | 9e+04         |
-------------------------------------------
Eval num_timesteps=197500, episode_reward=402.32 +/- 662.83
Episode length: 35.96 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 402      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 393      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 193      |
|    time_elapsed    | 706      |
|    total_timesteps | 197632   |
---------------------------------
Eval num_timesteps=198000, episode_reward=455.79 +/- 687.77
Episode length: 35.90 +/- 6.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 456          |
| time/                   |              |
|    total_timesteps      | 198000       |
| train/                  |              |
|    approx_kl            | 0.0010382829 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0086      |
|    explained_variance   | 0.258        |
|    learning_rate        | 0.001        |
|    loss                 | 4.37e+04     |
|    n_updates            | 7230         |
|    policy_gradient_loss | 0.00093      |
|    value_loss           | 9.5e+04      |
------------------------------------------
Eval num_timesteps=198500, episode_reward=464.61 +/- 773.55
Episode length: 34.96 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 465      |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 424      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 194      |
|    time_elapsed    | 710      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=589.97 +/- 806.22
Episode length: 35.02 +/- 7.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 590           |
| time/                   |               |
|    total_timesteps      | 199000        |
| train/                  |               |
|    approx_kl            | 6.2555773e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00789      |
|    explained_variance   | 0.0643        |
|    learning_rate        | 0.001         |
|    loss                 | 5.74e+04      |
|    n_updates            | 7240          |
|    policy_gradient_loss | -0.000154     |
|    value_loss           | 1.18e+05      |
-------------------------------------------
Eval num_timesteps=199500, episode_reward=405.36 +/- 802.07
Episode length: 33.62 +/- 7.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 445      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 195      |
|    time_elapsed    | 714      |
|    total_timesteps | 199680   |
---------------------------------
Eval num_timesteps=200000, episode_reward=608.94 +/- 766.58
Episode length: 36.42 +/- 6.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 609           |
| time/                   |               |
|    total_timesteps      | 200000        |
| train/                  |               |
|    approx_kl            | 1.8056016e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00769      |
|    explained_variance   | 0.0747        |
|    learning_rate        | 0.001         |
|    loss                 | 3.71e+04      |
|    n_updates            | 7250          |
|    policy_gradient_loss | -0.000165     |
|    value_loss           | 9.45e+04      |
-------------------------------------------
Eval num_timesteps=200500, episode_reward=580.92 +/- 787.13
Episode length: 35.88 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 581      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 443      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 196      |
|    time_elapsed    | 717      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=391.13 +/- 741.36
Episode length: 34.80 +/- 6.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 391           |
| time/                   |               |
|    total_timesteps      | 201000        |
| train/                  |               |
|    approx_kl            | 1.3882527e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00597      |
|    explained_variance   | -0.15         |
|    learning_rate        | 0.001         |
|    loss                 | 3.63e+04      |
|    n_updates            | 7260          |
|    policy_gradient_loss | -0.000124     |
|    value_loss           | 1.16e+05      |
-------------------------------------------
Eval num_timesteps=201500, episode_reward=527.00 +/- 725.50
Episode length: 36.20 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 527      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 440      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 197      |
|    time_elapsed    | 721      |
|    total_timesteps | 201728   |
---------------------------------
Eval num_timesteps=202000, episode_reward=524.78 +/- 850.66
Episode length: 34.98 +/- 7.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 525           |
| time/                   |               |
|    total_timesteps      | 202000        |
| train/                  |               |
|    approx_kl            | 1.1723023e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00718      |
|    explained_variance   | 0.0659        |
|    learning_rate        | 0.001         |
|    loss                 | 4.41e+04      |
|    n_updates            | 7270          |
|    policy_gradient_loss | -0.000148     |
|    value_loss           | 9.92e+04      |
-------------------------------------------
Eval num_timesteps=202500, episode_reward=567.36 +/- 796.16
Episode length: 36.42 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 567      |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 513      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 198      |
|    time_elapsed    | 724      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=570.75 +/- 758.30
Episode length: 36.74 +/- 5.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.7         |
|    mean_reward          | 571          |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 7.905532e-06 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00862     |
|    explained_variance   | -0.21        |
|    learning_rate        | 0.001        |
|    loss                 | 3.25e+04     |
|    n_updates            | 7280         |
|    policy_gradient_loss | -0.000141    |
|    value_loss           | 1.1e+05      |
------------------------------------------
Eval num_timesteps=203500, episode_reward=339.95 +/- 704.78
Episode length: 34.44 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 472      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 199      |
|    time_elapsed    | 728      |
|    total_timesteps | 203776   |
---------------------------------
Eval num_timesteps=204000, episode_reward=225.03 +/- 653.70
Episode length: 32.70 +/- 7.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.7         |
|    mean_reward          | 225          |
| time/                   |              |
|    total_timesteps      | 204000       |
| train/                  |              |
|    approx_kl            | 0.0009836538 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00734     |
|    explained_variance   | 0.0308       |
|    learning_rate        | 0.001        |
|    loss                 | 3.06e+04     |
|    n_updates            | 7290         |
|    policy_gradient_loss | 0.00129      |
|    value_loss           | 1e+05        |
------------------------------------------
Eval num_timesteps=204500, episode_reward=317.04 +/- 661.05
Episode length: 34.64 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 200      |
|    time_elapsed    | 732      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=443.52 +/- 771.17
Episode length: 35.64 +/- 6.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 444           |
| time/                   |               |
|    total_timesteps      | 205000        |
| train/                  |               |
|    approx_kl            | 0.00033837563 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00787      |
|    explained_variance   | 0.0491        |
|    learning_rate        | 0.001         |
|    loss                 | 3.92e+04      |
|    n_updates            | 7300          |
|    policy_gradient_loss | -7.04e-05     |
|    value_loss           | 9.98e+04      |
-------------------------------------------
Eval num_timesteps=205500, episode_reward=404.82 +/- 660.90
Episode length: 36.22 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 474      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 201      |
|    time_elapsed    | 735      |
|    total_timesteps | 205824   |
---------------------------------
Eval num_timesteps=206000, episode_reward=464.64 +/- 677.90
Episode length: 37.14 +/- 5.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.1          |
|    mean_reward          | 465           |
| time/                   |               |
|    total_timesteps      | 206000        |
| train/                  |               |
|    approx_kl            | 2.0320294e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00705      |
|    explained_variance   | -0.0359       |
|    learning_rate        | 0.001         |
|    loss                 | 3.87e+04      |
|    n_updates            | 7310          |
|    policy_gradient_loss | -0.00011      |
|    value_loss           | 9.63e+04      |
-------------------------------------------
Eval num_timesteps=206500, episode_reward=431.64 +/- 754.03
Episode length: 34.44 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 471      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 202      |
|    time_elapsed    | 739      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=409.94 +/- 697.11
Episode length: 35.58 +/- 6.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 410          |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 1.231092e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00443     |
|    explained_variance   | 0.0695       |
|    learning_rate        | 0.001        |
|    loss                 | 5.09e+04     |
|    n_updates            | 7320         |
|    policy_gradient_loss | -0.000107    |
|    value_loss           | 1.12e+05     |
------------------------------------------
Eval num_timesteps=207500, episode_reward=457.21 +/- 693.44
Episode length: 35.28 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 506      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 203      |
|    time_elapsed    | 742      |
|    total_timesteps | 207872   |
---------------------------------
Eval num_timesteps=208000, episode_reward=445.03 +/- 652.05
Episode length: 36.78 +/- 5.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.8          |
|    mean_reward          | 445           |
| time/                   |               |
|    total_timesteps      | 208000        |
| train/                  |               |
|    approx_kl            | 1.2444798e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00416      |
|    explained_variance   | 0.107         |
|    learning_rate        | 0.001         |
|    loss                 | 5.19e+04      |
|    n_updates            | 7330          |
|    policy_gradient_loss | -7.54e-05     |
|    value_loss           | 1.08e+05      |
-------------------------------------------
Eval num_timesteps=208500, episode_reward=518.68 +/- 703.18
Episode length: 35.84 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 519      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 492      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 204      |
|    time_elapsed    | 746      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=350.12 +/- 683.52
Episode length: 33.82 +/- 6.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 350          |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 1.268927e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00589     |
|    explained_variance   | 0.0245       |
|    learning_rate        | 0.001        |
|    loss                 | 4.5e+04      |
|    n_updates            | 7340         |
|    policy_gradient_loss | -7.56e-05    |
|    value_loss           | 9.1e+04      |
------------------------------------------
Eval num_timesteps=209500, episode_reward=533.23 +/- 828.95
Episode length: 34.86 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 520      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 205      |
|    time_elapsed    | 750      |
|    total_timesteps | 209920   |
---------------------------------
Eval num_timesteps=210000, episode_reward=362.65 +/- 675.50
Episode length: 34.64 +/- 7.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 363           |
| time/                   |               |
|    total_timesteps      | 210000        |
| train/                  |               |
|    approx_kl            | 1.2042234e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0033       |
|    explained_variance   | -0.04         |
|    learning_rate        | 0.001         |
|    loss                 | 4.15e+04      |
|    n_updates            | 7350          |
|    policy_gradient_loss | -0.000276     |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=210500, episode_reward=484.00 +/- 738.44
Episode length: 35.68 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 484      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 509      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 206      |
|    time_elapsed    | 753      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=442.20 +/- 763.98
Episode length: 35.06 +/- 6.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 442          |
| time/                   |              |
|    total_timesteps      | 211000       |
| train/                  |              |
|    approx_kl            | 8.522242e-05 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00489     |
|    explained_variance   | -0.00195     |
|    learning_rate        | 0.001        |
|    loss                 | 3.29e+04     |
|    n_updates            | 7360         |
|    policy_gradient_loss | 0.000579     |
|    value_loss           | 8.6e+04      |
------------------------------------------
Eval num_timesteps=211500, episode_reward=351.79 +/- 645.61
Episode length: 34.72 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 352      |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 528      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 207      |
|    time_elapsed    | 757      |
|    total_timesteps | 211968   |
---------------------------------
Eval num_timesteps=212000, episode_reward=629.85 +/- 815.07
Episode length: 36.88 +/- 6.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.9         |
|    mean_reward          | 630          |
| time/                   |              |
|    total_timesteps      | 212000       |
| train/                  |              |
|    approx_kl            | 9.377254e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00349     |
|    explained_variance   | -0.0346      |
|    learning_rate        | 0.001        |
|    loss                 | 3.57e+04     |
|    n_updates            | 7370         |
|    policy_gradient_loss | -7.14e-05    |
|    value_loss           | 9.41e+04     |
------------------------------------------
Eval num_timesteps=212500, episode_reward=527.00 +/- 776.83
Episode length: 36.26 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 527      |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 554      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 208      |
|    time_elapsed    | 761      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=451.96 +/- 693.29
Episode length: 36.06 +/- 6.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 452           |
| time/                   |               |
|    total_timesteps      | 213000        |
| train/                  |               |
|    approx_kl            | 6.1059836e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00382      |
|    explained_variance   | 0.0872        |
|    learning_rate        | 0.001         |
|    loss                 | 3.4e+04       |
|    n_updates            | 7380          |
|    policy_gradient_loss | -7.73e-05     |
|    value_loss           | 8.67e+04      |
-------------------------------------------
Eval num_timesteps=213500, episode_reward=558.57 +/- 763.89
Episode length: 36.40 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 559      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=507.22 +/- 776.85
Episode length: 35.52 +/- 7.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 507      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 462      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 209      |
|    time_elapsed    | 766      |
|    total_timesteps | 214016   |
---------------------------------
Eval num_timesteps=214500, episode_reward=416.34 +/- 682.55
Episode length: 35.30 +/- 6.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 416          |
| time/                   |              |
|    total_timesteps      | 214500       |
| train/                  |              |
|    approx_kl            | 0.0001294926 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00575     |
|    explained_variance   | -0.0641      |
|    learning_rate        | 0.001        |
|    loss                 | 3.38e+04     |
|    n_updates            | 7390         |
|    policy_gradient_loss | -6.11e-05    |
|    value_loss           | 8.07e+04     |
------------------------------------------
Eval num_timesteps=215000, episode_reward=584.22 +/- 797.49
Episode length: 35.58 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 584      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 365      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 210      |
|    time_elapsed    | 769      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=616.90 +/- 868.12
Episode length: 34.96 +/- 8.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 617          |
| time/                   |              |
|    total_timesteps      | 215500       |
| train/                  |              |
|    approx_kl            | 0.0028724894 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00649     |
|    explained_variance   | -0.102       |
|    learning_rate        | 0.001        |
|    loss                 | 3.98e+04     |
|    n_updates            | 7400         |
|    policy_gradient_loss | 0.00287      |
|    value_loss           | 8.58e+04     |
------------------------------------------
Eval num_timesteps=216000, episode_reward=413.21 +/- 733.94
Episode length: 34.68 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 413      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 295      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 211      |
|    time_elapsed    | 773      |
|    total_timesteps | 216064   |
---------------------------------
Eval num_timesteps=216500, episode_reward=511.75 +/- 734.86
Episode length: 36.70 +/- 6.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.7         |
|    mean_reward          | 512          |
| time/                   |              |
|    total_timesteps      | 216500       |
| train/                  |              |
|    approx_kl            | 8.312054e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00449     |
|    explained_variance   | 0.0849       |
|    learning_rate        | 0.001        |
|    loss                 | 3.3e+04      |
|    n_updates            | 7410         |
|    policy_gradient_loss | -2.52e-05    |
|    value_loss           | 8.23e+04     |
------------------------------------------
Eval num_timesteps=217000, episode_reward=442.85 +/- 732.78
Episode length: 35.84 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 345      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 212      |
|    time_elapsed    | 776      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=482.61 +/- 820.66
Episode length: 34.56 +/- 7.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 483          |
| time/                   |              |
|    total_timesteps      | 217500       |
| train/                  |              |
|    approx_kl            | 8.934876e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00363     |
|    explained_variance   | -0.0564      |
|    learning_rate        | 0.001        |
|    loss                 | 3.55e+04     |
|    n_updates            | 7420         |
|    policy_gradient_loss | -4.37e-05    |
|    value_loss           | 8.54e+04     |
------------------------------------------
Eval num_timesteps=218000, episode_reward=413.79 +/- 753.24
Episode length: 34.56 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 213      |
|    time_elapsed    | 780      |
|    total_timesteps | 218112   |
---------------------------------
Eval num_timesteps=218500, episode_reward=434.05 +/- 768.98
Episode length: 34.46 +/- 6.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 434           |
| time/                   |               |
|    total_timesteps      | 218500        |
| train/                  |               |
|    approx_kl            | 4.2724423e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00338      |
|    explained_variance   | 0.0771        |
|    learning_rate        | 0.001         |
|    loss                 | 4.13e+04      |
|    n_updates            | 7430          |
|    policy_gradient_loss | -4.48e-05     |
|    value_loss           | 9.44e+04      |
-------------------------------------------
Eval num_timesteps=219000, episode_reward=385.81 +/- 713.39
Episode length: 34.70 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 473      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 214      |
|    time_elapsed    | 783      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=359.68 +/- 704.40
Episode length: 34.54 +/- 7.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.5         |
|    mean_reward          | 360          |
| time/                   |              |
|    total_timesteps      | 219500       |
| train/                  |              |
|    approx_kl            | 6.146729e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00355     |
|    explained_variance   | -0.217       |
|    learning_rate        | 0.001        |
|    loss                 | 3.49e+04     |
|    n_updates            | 7440         |
|    policy_gradient_loss | -6.95e-05    |
|    value_loss           | 1.01e+05     |
------------------------------------------
Eval num_timesteps=220000, episode_reward=537.46 +/- 793.39
Episode length: 35.30 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 537      |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 474      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 215      |
|    time_elapsed    | 787      |
|    total_timesteps | 220160   |
---------------------------------
Eval num_timesteps=220500, episode_reward=458.19 +/- 723.38
Episode length: 34.62 +/- 8.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 458           |
| time/                   |               |
|    total_timesteps      | 220500        |
| train/                  |               |
|    approx_kl            | 5.6053977e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00399      |
|    explained_variance   | -0.0359       |
|    learning_rate        | 0.001         |
|    loss                 | 5.27e+04      |
|    n_updates            | 7450          |
|    policy_gradient_loss | -5.12e-05     |
|    value_loss           | 1.08e+05      |
-------------------------------------------
Eval num_timesteps=221000, episode_reward=359.56 +/- 731.41
Episode length: 34.04 +/- 7.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 409      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 216      |
|    time_elapsed    | 791      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=456.78 +/- 714.23
Episode length: 36.10 +/- 6.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 457          |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 4.854519e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.005       |
|    explained_variance   | -0.152       |
|    learning_rate        | 0.001        |
|    loss                 | 3.46e+04     |
|    n_updates            | 7460         |
|    policy_gradient_loss | -3.71e-05    |
|    value_loss           | 7.12e+04     |
------------------------------------------
Eval num_timesteps=222000, episode_reward=497.65 +/- 727.15
Episode length: 35.90 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 498      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 217      |
|    time_elapsed    | 794      |
|    total_timesteps | 222208   |
---------------------------------
Eval num_timesteps=222500, episode_reward=258.24 +/- 596.98
Episode length: 33.80 +/- 6.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 258           |
| time/                   |               |
|    total_timesteps      | 222500        |
| train/                  |               |
|    approx_kl            | 4.0861778e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00448      |
|    explained_variance   | -0.0696       |
|    learning_rate        | 0.001         |
|    loss                 | 6.42e+04      |
|    n_updates            | 7470          |
|    policy_gradient_loss | -7.17e-05     |
|    value_loss           | 9.95e+04      |
-------------------------------------------
Eval num_timesteps=223000, episode_reward=374.23 +/- 702.93
Episode length: 34.38 +/- 7.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 374      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 266      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 218      |
|    time_elapsed    | 798      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=390.22 +/- 651.58
Episode length: 35.92 +/- 5.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 390           |
| time/                   |               |
|    total_timesteps      | 223500        |
| train/                  |               |
|    approx_kl            | 0.00013187126 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00582      |
|    explained_variance   | 0.00408       |
|    learning_rate        | 0.001         |
|    loss                 | 3.05e+04      |
|    n_updates            | 7480          |
|    policy_gradient_loss | 0.000144      |
|    value_loss           | 7.39e+04      |
-------------------------------------------
Eval num_timesteps=224000, episode_reward=439.39 +/- 786.48
Episode length: 33.68 +/- 8.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 406      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 219      |
|    time_elapsed    | 801      |
|    total_timesteps | 224256   |
---------------------------------
Eval num_timesteps=224500, episode_reward=418.17 +/- 655.95
Episode length: 36.16 +/- 5.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.2         |
|    mean_reward          | 418          |
| time/                   |              |
|    total_timesteps      | 224500       |
| train/                  |              |
|    approx_kl            | 0.0011145927 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00301     |
|    explained_variance   | 0.129        |
|    learning_rate        | 0.001        |
|    loss                 | 4.7e+04      |
|    n_updates            | 7490         |
|    policy_gradient_loss | 0.00132      |
|    value_loss           | 9.57e+04     |
------------------------------------------
Eval num_timesteps=225000, episode_reward=495.98 +/- 764.42
Episode length: 35.66 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 496      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 403      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 220      |
|    time_elapsed    | 805      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=377.98 +/- 676.18
Episode length: 34.92 +/- 7.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 378          |
| time/                   |              |
|    total_timesteps      | 225500       |
| train/                  |              |
|    approx_kl            | 6.420305e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00236     |
|    explained_variance   | 0.0278       |
|    learning_rate        | 0.001        |
|    loss                 | 4.77e+04     |
|    n_updates            | 7500         |
|    policy_gradient_loss | -9.23e-05    |
|    value_loss           | 1.02e+05     |
------------------------------------------
Eval num_timesteps=226000, episode_reward=443.94 +/- 712.05
Episode length: 36.06 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 444      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 434      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 221      |
|    time_elapsed    | 809      |
|    total_timesteps | 226304   |
---------------------------------
Eval num_timesteps=226500, episode_reward=422.13 +/- 712.32
Episode length: 34.88 +/- 6.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 422           |
| time/                   |               |
|    total_timesteps      | 226500        |
| train/                  |               |
|    approx_kl            | 5.3318217e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00361      |
|    explained_variance   | -0.189        |
|    learning_rate        | 0.001         |
|    loss                 | 4.13e+04      |
|    n_updates            | 7510          |
|    policy_gradient_loss | -5.6e-05      |
|    value_loss           | 8.92e+04      |
-------------------------------------------
Eval num_timesteps=227000, episode_reward=464.01 +/- 726.23
Episode length: 35.78 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 464      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 412      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 222      |
|    time_elapsed    | 812      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=351.02 +/- 634.22
Episode length: 35.82 +/- 5.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 351          |
| time/                   |              |
|    total_timesteps      | 227500       |
| train/                  |              |
|    approx_kl            | 3.731111e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00413     |
|    explained_variance   | -0.0749      |
|    learning_rate        | 0.001        |
|    loss                 | 5.14e+04     |
|    n_updates            | 7520         |
|    policy_gradient_loss | -3.68e-05    |
|    value_loss           | 9.3e+04      |
------------------------------------------
Eval num_timesteps=228000, episode_reward=492.76 +/- 735.23
Episode length: 36.08 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 493      |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 290      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 223      |
|    time_elapsed    | 816      |
|    total_timesteps | 228352   |
---------------------------------
Eval num_timesteps=228500, episode_reward=335.36 +/- 703.49
Episode length: 34.02 +/- 7.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 335           |
| time/                   |               |
|    total_timesteps      | 228500        |
| train/                  |               |
|    approx_kl            | 4.7206413e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00534      |
|    explained_variance   | 0.0268        |
|    learning_rate        | 0.001         |
|    loss                 | 4.33e+04      |
|    n_updates            | 7530          |
|    policy_gradient_loss | -3.39e-05     |
|    value_loss           | 8.49e+04      |
-------------------------------------------
Eval num_timesteps=229000, episode_reward=334.08 +/- 654.04
Episode length: 34.70 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 334      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 336      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 224      |
|    time_elapsed    | 819      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=574.86 +/- 765.37
Episode length: 36.32 +/- 6.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 575          |
| time/                   |              |
|    total_timesteps      | 229500       |
| train/                  |              |
|    approx_kl            | 6.495975e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00476     |
|    explained_variance   | 0.0877       |
|    learning_rate        | 0.001        |
|    loss                 | 4.24e+04     |
|    n_updates            | 7540         |
|    policy_gradient_loss | -8.73e-05    |
|    value_loss           | 9e+04        |
------------------------------------------
Eval num_timesteps=230000, episode_reward=495.45 +/- 767.93
Episode length: 35.92 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 225      |
|    time_elapsed    | 823      |
|    total_timesteps | 230400   |
---------------------------------
Eval num_timesteps=230500, episode_reward=621.58 +/- 800.86
Episode length: 36.46 +/- 6.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.5          |
|    mean_reward          | 622           |
| time/                   |               |
|    total_timesteps      | 230500        |
| train/                  |               |
|    approx_kl            | 2.6309863e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00522      |
|    explained_variance   | -0.0577       |
|    learning_rate        | 0.001         |
|    loss                 | 3.18e+04      |
|    n_updates            | 7550          |
|    policy_gradient_loss | -6.05e-05     |
|    value_loss           | 8.36e+04      |
-------------------------------------------
Eval num_timesteps=231000, episode_reward=645.13 +/- 803.61
Episode length: 36.40 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 645      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 387      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 226      |
|    time_elapsed    | 827      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=337.81 +/- 748.76
Episode length: 32.96 +/- 8.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 338           |
| time/                   |               |
|    total_timesteps      | 231500        |
| train/                  |               |
|    approx_kl            | 1.3763725e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00476      |
|    explained_variance   | 0.0207        |
|    learning_rate        | 0.001         |
|    loss                 | 3.99e+04      |
|    n_updates            | 7560          |
|    policy_gradient_loss | -0.000129     |
|    value_loss           | 8.55e+04      |
-------------------------------------------
Eval num_timesteps=232000, episode_reward=397.53 +/- 705.50
Episode length: 34.46 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 398      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 227      |
|    time_elapsed    | 830      |
|    total_timesteps | 232448   |
---------------------------------
Eval num_timesteps=232500, episode_reward=492.28 +/- 680.57
Episode length: 35.68 +/- 5.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 492          |
| time/                   |              |
|    total_timesteps      | 232500       |
| train/                  |              |
|    approx_kl            | 6.426126e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00382     |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.001        |
|    loss                 | 3.9e+04      |
|    n_updates            | 7570         |
|    policy_gradient_loss | -4.77e-05    |
|    value_loss           | 9.76e+04     |
------------------------------------------
Eval num_timesteps=233000, episode_reward=520.16 +/- 740.64
Episode length: 35.96 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 520      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 439      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 228      |
|    time_elapsed    | 834      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=522.82 +/- 762.21
Episode length: 36.34 +/- 6.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 523           |
| time/                   |               |
|    total_timesteps      | 233500        |
| train/                  |               |
|    approx_kl            | 3.4749974e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00318      |
|    explained_variance   | -0.0235       |
|    learning_rate        | 0.001         |
|    loss                 | 5.18e+04      |
|    n_updates            | 7580          |
|    policy_gradient_loss | -6.63e-05     |
|    value_loss           | 1.13e+05      |
-------------------------------------------
Eval num_timesteps=234000, episode_reward=442.47 +/- 747.01
Episode length: 35.10 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 479      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 229      |
|    time_elapsed    | 837      |
|    total_timesteps | 234496   |
---------------------------------
Eval num_timesteps=234500, episode_reward=485.09 +/- 701.04
Episode length: 35.90 +/- 6.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 485           |
| time/                   |               |
|    total_timesteps      | 234500        |
| train/                  |               |
|    approx_kl            | 3.7485734e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0034       |
|    explained_variance   | -0.0277       |
|    learning_rate        | 0.001         |
|    loss                 | 3.4e+04       |
|    n_updates            | 7590          |
|    policy_gradient_loss | -4.97e-05     |
|    value_loss           | 1.05e+05      |
-------------------------------------------
Eval num_timesteps=235000, episode_reward=575.49 +/- 761.95
Episode length: 36.22 +/- 7.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 575      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=380.86 +/- 713.50
Episode length: 34.06 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 428      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 230      |
|    time_elapsed    | 842      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=377.76 +/- 634.82
Episode length: 35.22 +/- 5.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 378           |
| time/                   |               |
|    total_timesteps      | 236000        |
| train/                  |               |
|    approx_kl            | 2.6542693e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00391      |
|    explained_variance   | 0.0293        |
|    learning_rate        | 0.001         |
|    loss                 | 4.94e+04      |
|    n_updates            | 7600          |
|    policy_gradient_loss | -5.31e-05     |
|    value_loss           | 1.18e+05      |
-------------------------------------------
Eval num_timesteps=236500, episode_reward=374.86 +/- 686.58
Episode length: 35.50 +/- 5.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 375      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 415      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 231      |
|    time_elapsed    | 846      |
|    total_timesteps | 236544   |
---------------------------------
Eval num_timesteps=237000, episode_reward=384.87 +/- 764.78
Episode length: 33.50 +/- 8.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.5          |
|    mean_reward          | 385           |
| time/                   |               |
|    total_timesteps      | 237000        |
| train/                  |               |
|    approx_kl            | 2.5960617e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00295      |
|    explained_variance   | -0.0737       |
|    learning_rate        | 0.001         |
|    loss                 | 3.51e+04      |
|    n_updates            | 7610          |
|    policy_gradient_loss | -4.89e-05     |
|    value_loss           | 9.48e+04      |
-------------------------------------------
Eval num_timesteps=237500, episode_reward=549.83 +/- 738.22
Episode length: 36.06 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 550      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 444      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 232      |
|    time_elapsed    | 849      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=487.47 +/- 749.37
Episode length: 36.40 +/- 6.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 487           |
| time/                   |               |
|    total_timesteps      | 238000        |
| train/                  |               |
|    approx_kl            | 2.6018824e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00302      |
|    explained_variance   | 0.0824        |
|    learning_rate        | 0.001         |
|    loss                 | 4.69e+04      |
|    n_updates            | 7620          |
|    policy_gradient_loss | -5.63e-05     |
|    value_loss           | 1.09e+05      |
-------------------------------------------
Eval num_timesteps=238500, episode_reward=358.39 +/- 700.25
Episode length: 34.30 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 453      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 233      |
|    time_elapsed    | 853      |
|    total_timesteps | 238592   |
---------------------------------
Eval num_timesteps=239000, episode_reward=342.82 +/- 690.04
Episode length: 34.64 +/- 6.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 343          |
| time/                   |              |
|    total_timesteps      | 239000       |
| train/                  |              |
|    approx_kl            | 3.085006e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00308     |
|    explained_variance   | 0.104        |
|    learning_rate        | 0.001        |
|    loss                 | 3.57e+04     |
|    n_updates            | 7630         |
|    policy_gradient_loss | -5.41e-05    |
|    value_loss           | 9.82e+04     |
------------------------------------------
Eval num_timesteps=239500, episode_reward=666.91 +/- 793.40
Episode length: 35.96 +/- 7.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 667      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 501      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 234      |
|    time_elapsed    | 857      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=396.50 +/- 719.86
Episode length: 34.80 +/- 7.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 397           |
| time/                   |               |
|    total_timesteps      | 240000        |
| train/                  |               |
|    approx_kl            | 2.1012966e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00254      |
|    explained_variance   | 0.0518        |
|    learning_rate        | 0.001         |
|    loss                 | 4.92e+04      |
|    n_updates            | 7640          |
|    policy_gradient_loss | -4.46e-05     |
|    value_loss           | 1.13e+05      |
-------------------------------------------
Eval num_timesteps=240500, episode_reward=227.02 +/- 547.82
Episode length: 34.10 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 227      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 483      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 235      |
|    time_elapsed    | 860      |
|    total_timesteps | 240640   |
---------------------------------
Eval num_timesteps=241000, episode_reward=422.48 +/- 693.73
Episode length: 36.38 +/- 6.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 422           |
| time/                   |               |
|    total_timesteps      | 241000        |
| train/                  |               |
|    approx_kl            | 2.2584572e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00261      |
|    explained_variance   | 0.101         |
|    learning_rate        | 0.001         |
|    loss                 | 3.9e+04       |
|    n_updates            | 7650          |
|    policy_gradient_loss | -4.57e-05     |
|    value_loss           | 1.1e+05       |
-------------------------------------------
Eval num_timesteps=241500, episode_reward=416.59 +/- 678.64
Episode length: 35.78 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 531      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 236      |
|    time_elapsed    | 864      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=368.99 +/- 740.32
Episode length: 35.12 +/- 7.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 369           |
| time/                   |               |
|    total_timesteps      | 242000        |
| train/                  |               |
|    approx_kl            | 3.6729034e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00204      |
|    explained_variance   | 0.00384       |
|    learning_rate        | 0.001         |
|    loss                 | 4.49e+04      |
|    n_updates            | 7660          |
|    policy_gradient_loss | -7.12e-05     |
|    value_loss           | 1.21e+05      |
-------------------------------------------
Eval num_timesteps=242500, episode_reward=652.20 +/- 816.60
Episode length: 36.92 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 581      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 237      |
|    time_elapsed    | 867      |
|    total_timesteps | 242688   |
---------------------------------
Eval num_timesteps=243000, episode_reward=534.01 +/- 710.19
Episode length: 36.58 +/- 6.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 534           |
| time/                   |               |
|    total_timesteps      | 243000        |
| train/                  |               |
|    approx_kl            | 2.1304004e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00149      |
|    explained_variance   | -0.00504      |
|    learning_rate        | 0.001         |
|    loss                 | 5.11e+04      |
|    n_updates            | 7670          |
|    policy_gradient_loss | -5.73e-05     |
|    value_loss           | 1.15e+05      |
-------------------------------------------
Eval num_timesteps=243500, episode_reward=363.53 +/- 709.32
Episode length: 34.72 +/- 7.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 364      |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 602      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 238      |
|    time_elapsed    | 871      |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=332.43 +/- 704.68
Episode length: 34.36 +/- 6.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 332          |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 7.859257e-06 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00198     |
|    explained_variance   | -0.0614      |
|    learning_rate        | 0.001        |
|    loss                 | 5e+04        |
|    n_updates            | 7680         |
|    policy_gradient_loss | 4.65e-05     |
|    value_loss           | 1.09e+05     |
------------------------------------------
Eval num_timesteps=244500, episode_reward=307.65 +/- 753.76
Episode length: 33.14 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 551      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 239      |
|    time_elapsed    | 874      |
|    total_timesteps | 244736   |
---------------------------------
Eval num_timesteps=245000, episode_reward=650.76 +/- 846.54
Episode length: 36.82 +/- 6.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.8         |
|    mean_reward          | 651          |
| time/                   |              |
|    total_timesteps      | 245000       |
| train/                  |              |
|    approx_kl            | 7.858034e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00333     |
|    explained_variance   | -0.186       |
|    learning_rate        | 0.001        |
|    loss                 | 3.62e+04     |
|    n_updates            | 7690         |
|    policy_gradient_loss | -3.14e-05    |
|    value_loss           | 1.01e+05     |
------------------------------------------
Eval num_timesteps=245500, episode_reward=446.42 +/- 701.20
Episode length: 36.36 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 446      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 515      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 240      |
|    time_elapsed    | 878      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=557.77 +/- 754.55
Episode length: 36.34 +/- 6.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 558           |
| time/                   |               |
|    total_timesteps      | 246000        |
| train/                  |               |
|    approx_kl            | 2.5373301e-06 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00289      |
|    explained_variance   | 0.0271        |
|    learning_rate        | 0.001         |
|    loss                 | 4.32e+04      |
|    n_updates            | 7700          |
|    policy_gradient_loss | -0.000113     |
|    value_loss           | 1.03e+05      |
-------------------------------------------
Eval num_timesteps=246500, episode_reward=270.51 +/- 637.67
Episode length: 33.96 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 435      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 241      |
|    time_elapsed    | 882      |
|    total_timesteps | 246784   |
---------------------------------
Eval num_timesteps=247000, episode_reward=428.62 +/- 718.12
Episode length: 35.08 +/- 6.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 429           |
| time/                   |               |
|    total_timesteps      | 247000        |
| train/                  |               |
|    approx_kl            | 1.2477394e-06 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00242      |
|    explained_variance   | -0.072        |
|    learning_rate        | 0.001         |
|    loss                 | 3.67e+04      |
|    n_updates            | 7710          |
|    policy_gradient_loss | -4.78e-05     |
|    value_loss           | 9.43e+04      |
-------------------------------------------
Eval num_timesteps=247500, episode_reward=280.40 +/- 609.44
Episode length: 34.36 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 280      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 447      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 242      |
|    time_elapsed    | 885      |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=382.31 +/- 771.88
Episode length: 33.96 +/- 7.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 382           |
| time/                   |               |
|    total_timesteps      | 248000        |
| train/                  |               |
|    approx_kl            | 3.2538082e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00265      |
|    explained_variance   | 0.124         |
|    learning_rate        | 0.001         |
|    loss                 | 4.22e+04      |
|    n_updates            | 7720          |
|    policy_gradient_loss | -2.78e-05     |
|    value_loss           | 9.71e+04      |
-------------------------------------------
Eval num_timesteps=248500, episode_reward=512.72 +/- 769.90
Episode length: 36.46 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 513      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 243      |
|    time_elapsed    | 889      |
|    total_timesteps | 248832   |
---------------------------------
Eval num_timesteps=249000, episode_reward=450.09 +/- 725.02
Episode length: 35.16 +/- 6.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 450           |
| time/                   |               |
|    total_timesteps      | 249000        |
| train/                  |               |
|    approx_kl            | 2.8172508e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00305      |
|    explained_variance   | 0.108         |
|    learning_rate        | 0.001         |
|    loss                 | 2.67e+04      |
|    n_updates            | 7730          |
|    policy_gradient_loss | -3.13e-05     |
|    value_loss           | 8.03e+04      |
-------------------------------------------
Eval num_timesteps=249500, episode_reward=294.18 +/- 640.25
Episode length: 34.72 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 334      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 244      |
|    time_elapsed    | 892      |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=351.21 +/- 711.53
Episode length: 34.32 +/- 6.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 351          |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 9.334728e-05 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00318     |
|    explained_variance   | 0.072        |
|    learning_rate        | 0.001        |
|    loss                 | 3.12e+04     |
|    n_updates            | 7740         |
|    policy_gradient_loss | 2.36e-05     |
|    value_loss           | 8.77e+04     |
------------------------------------------
Eval num_timesteps=250500, episode_reward=361.03 +/- 644.20
Episode length: 34.82 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 323      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 245      |
|    time_elapsed    | 896      |
|    total_timesteps | 250880   |
---------------------------------
Eval num_timesteps=251000, episode_reward=321.91 +/- 703.43
Episode length: 33.84 +/- 7.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 322           |
| time/                   |               |
|    total_timesteps      | 251000        |
| train/                  |               |
|    approx_kl            | 1.8218998e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00288      |
|    explained_variance   | 0.0857        |
|    learning_rate        | 0.001         |
|    loss                 | 3.21e+04      |
|    n_updates            | 7750          |
|    policy_gradient_loss | -3.24e-05     |
|    value_loss           | 9.29e+04      |
-------------------------------------------
Eval num_timesteps=251500, episode_reward=341.99 +/- 622.71
Episode length: 35.00 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 358      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 246      |
|    time_elapsed    | 899      |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=528.43 +/- 709.48
Episode length: 36.04 +/- 5.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36          |
|    mean_reward          | 528         |
| time/                   |             |
|    total_timesteps      | 252000      |
| train/                  |             |
|    approx_kl            | 1.51922e-08 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00281    |
|    explained_variance   | 0.0149      |
|    learning_rate        | 0.001       |
|    loss                 | 3.45e+04    |
|    n_updates            | 7760        |
|    policy_gradient_loss | -3.66e-05   |
|    value_loss           | 9.34e+04    |
-----------------------------------------
Eval num_timesteps=252500, episode_reward=662.55 +/- 821.68
Episode length: 37.00 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 663      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 432      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 247      |
|    time_elapsed    | 903      |
|    total_timesteps | 252928   |
---------------------------------
Eval num_timesteps=253000, episode_reward=593.36 +/- 798.38
Episode length: 36.06 +/- 6.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 593           |
| time/                   |               |
|    total_timesteps      | 253000        |
| train/                  |               |
|    approx_kl            | 1.8568244e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00186      |
|    explained_variance   | 0.14          |
|    learning_rate        | 0.001         |
|    loss                 | 3.91e+04      |
|    n_updates            | 7770          |
|    policy_gradient_loss | -3.96e-05     |
|    value_loss           | 9.53e+04      |
-------------------------------------------
Eval num_timesteps=253500, episode_reward=510.61 +/- 736.52
Episode length: 36.84 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 511      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 492      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 248      |
|    time_elapsed    | 907      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=224.62 +/- 539.25
Episode length: 33.84 +/- 6.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 225           |
| time/                   |               |
|    total_timesteps      | 254000        |
| train/                  |               |
|    approx_kl            | 2.5262125e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00209      |
|    explained_variance   | -0.0488       |
|    learning_rate        | 0.001         |
|    loss                 | 4.26e+04      |
|    n_updates            | 7780          |
|    policy_gradient_loss | -2.54e-05     |
|    value_loss           | 1.12e+05      |
-------------------------------------------
Eval num_timesteps=254500, episode_reward=601.81 +/- 796.70
Episode length: 36.10 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 602      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 480      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 249      |
|    time_elapsed    | 910      |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=255000, episode_reward=259.49 +/- 559.36
Episode length: 35.04 +/- 5.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 259           |
| time/                   |               |
|    total_timesteps      | 255000        |
| train/                  |               |
|    approx_kl            | 3.4414697e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00236      |
|    explained_variance   | 0.0848        |
|    learning_rate        | 0.001         |
|    loss                 | 3.86e+04      |
|    n_updates            | 7790          |
|    policy_gradient_loss | -4.83e-05     |
|    value_loss           | 7.71e+04      |
-------------------------------------------
Eval num_timesteps=255500, episode_reward=454.88 +/- 690.13
Episode length: 36.10 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 455      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=412.47 +/- 683.06
Episode length: 34.52 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 412      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 250      |
|    time_elapsed    | 915      |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=376.73 +/- 731.84
Episode length: 34.40 +/- 6.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 377           |
| time/                   |               |
|    total_timesteps      | 256500        |
| train/                  |               |
|    approx_kl            | 1.8859282e-08 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00257      |
|    explained_variance   | -0.265        |
|    learning_rate        | 0.001         |
|    loss                 | 4.6e+04       |
|    n_updates            | 7800          |
|    policy_gradient_loss | -1.56e-05     |
|    value_loss           | 9.39e+04      |
-------------------------------------------
Eval num_timesteps=257000, episode_reward=379.74 +/- 669.45
Episode length: 34.72 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 318      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 251      |
|    time_elapsed    | 919      |
|    total_timesteps | 257024   |
---------------------------------
Eval num_timesteps=257500, episode_reward=549.46 +/- 747.90
Episode length: 36.14 +/- 6.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 549           |
| time/                   |               |
|    total_timesteps      | 257500        |
| train/                  |               |
|    approx_kl            | 2.2412394e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00232      |
|    explained_variance   | 0.109         |
|    learning_rate        | 0.001         |
|    loss                 | 5.55e+04      |
|    n_updates            | 7810          |
|    policy_gradient_loss | 0.00124       |
|    value_loss           | 9.49e+04      |
-------------------------------------------
Eval num_timesteps=258000, episode_reward=402.80 +/- 730.30
Episode length: 34.72 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 342      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 252      |
|    time_elapsed    | 922      |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=303.54 +/- 624.80
Episode length: 34.48 +/- 6.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.5         |
|    mean_reward          | 304          |
| time/                   |              |
|    total_timesteps      | 258500       |
| train/                  |              |
|    approx_kl            | 5.507609e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00206     |
|    explained_variance   | -0.089       |
|    learning_rate        | 0.001        |
|    loss                 | 3.56e+04     |
|    n_updates            | 7820         |
|    policy_gradient_loss | -4.23e-05    |
|    value_loss           | 1.02e+05     |
------------------------------------------
Eval num_timesteps=259000, episode_reward=502.98 +/- 772.90
Episode length: 35.88 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 503      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 360      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 253      |
|    time_elapsed    | 926      |
|    total_timesteps | 259072   |
---------------------------------
Eval num_timesteps=259500, episode_reward=389.31 +/- 658.93
Episode length: 35.42 +/- 6.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 389           |
| time/                   |               |
|    total_timesteps      | 259500        |
| train/                  |               |
|    approx_kl            | 1.5355181e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00271      |
|    explained_variance   | -0.0191       |
|    learning_rate        | 0.001         |
|    loss                 | 5.33e+04      |
|    n_updates            | 7830          |
|    policy_gradient_loss | -9.7e-06      |
|    value_loss           | 1.18e+05      |
-------------------------------------------
Eval num_timesteps=260000, episode_reward=277.86 +/- 701.61
Episode length: 33.08 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 383      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 254      |
|    time_elapsed    | 930      |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=334.60 +/- 685.67
Episode length: 34.24 +/- 6.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.2         |
|    mean_reward          | 335          |
| time/                   |              |
|    total_timesteps      | 260500       |
| train/                  |              |
|    approx_kl            | 5.122274e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00226     |
|    explained_variance   | 0.0119       |
|    learning_rate        | 0.001        |
|    loss                 | 2.43e+04     |
|    n_updates            | 7840         |
|    policy_gradient_loss | -1.31e-05    |
|    value_loss           | 7.95e+04     |
------------------------------------------
Eval num_timesteps=261000, episode_reward=371.62 +/- 628.35
Episode length: 35.44 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 255      |
|    time_elapsed    | 933      |
|    total_timesteps | 261120   |
---------------------------------
Eval num_timesteps=261500, episode_reward=417.91 +/- 648.54
Episode length: 35.98 +/- 5.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 418          |
| time/                   |              |
|    total_timesteps      | 261500       |
| train/                  |              |
|    approx_kl            | 4.440313e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00253     |
|    explained_variance   | -0.0884      |
|    learning_rate        | 0.001        |
|    loss                 | 3.63e+04     |
|    n_updates            | 7850         |
|    policy_gradient_loss | 2.79e-05     |
|    value_loss           | 8.46e+04     |
------------------------------------------
Eval num_timesteps=262000, episode_reward=456.87 +/- 809.54
Episode length: 34.12 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 301      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 256      |
|    time_elapsed    | 937      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=268.51 +/- 680.63
Episode length: 33.02 +/- 7.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 269           |
| time/                   |               |
|    total_timesteps      | 262500        |
| train/                  |               |
|    approx_kl            | 2.8594513e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00234      |
|    explained_variance   | -0.21         |
|    learning_rate        | 0.001         |
|    loss                 | 3.53e+04      |
|    n_updates            | 7860          |
|    policy_gradient_loss | -9.13e-05     |
|    value_loss           | 6.7e+04       |
-------------------------------------------
Eval num_timesteps=263000, episode_reward=429.54 +/- 691.31
Episode length: 35.90 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 430      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 287      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 257      |
|    time_elapsed    | 940      |
|    total_timesteps | 263168   |
---------------------------------
Eval num_timesteps=263500, episode_reward=490.82 +/- 692.71
Episode length: 35.44 +/- 6.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 491          |
| time/                   |              |
|    total_timesteps      | 263500       |
| train/                  |              |
|    approx_kl            | 7.056515e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00263     |
|    explained_variance   | 0.0347       |
|    learning_rate        | 0.001        |
|    loss                 | 3.56e+04     |
|    n_updates            | 7870         |
|    policy_gradient_loss | -2.9e-05     |
|    value_loss           | 7.63e+04     |
------------------------------------------
Eval num_timesteps=264000, episode_reward=646.37 +/- 764.27
Episode length: 37.38 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 646      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 271      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 258      |
|    time_elapsed    | 944      |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=295.66 +/- 643.38
Episode length: 34.34 +/- 5.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 296           |
| time/                   |               |
|    total_timesteps      | 264500        |
| train/                  |               |
|    approx_kl            | 1.2196484e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00245      |
|    explained_variance   | 0.0295        |
|    learning_rate        | 0.001         |
|    loss                 | 4.8e+04       |
|    n_updates            | 7880          |
|    policy_gradient_loss | -0.000167     |
|    value_loss           | 9.18e+04      |
-------------------------------------------
Eval num_timesteps=265000, episode_reward=427.05 +/- 773.04
Episode length: 34.48 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 271      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 259      |
|    time_elapsed    | 947      |
|    total_timesteps | 265216   |
---------------------------------
Eval num_timesteps=265500, episode_reward=500.67 +/- 735.56
Episode length: 35.48 +/- 6.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 501           |
| time/                   |               |
|    total_timesteps      | 265500        |
| train/                  |               |
|    approx_kl            | 2.1494809e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00246      |
|    explained_variance   | 0.0202        |
|    learning_rate        | 0.001         |
|    loss                 | 4.79e+04      |
|    n_updates            | 7890          |
|    policy_gradient_loss | -7.48e-05     |
|    value_loss           | 1.1e+05       |
-------------------------------------------
Eval num_timesteps=266000, episode_reward=359.81 +/- 673.62
Episode length: 34.58 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 266      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 260      |
|    time_elapsed    | 951      |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=480.42 +/- 775.44
Episode length: 34.80 +/- 6.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.8         |
|    mean_reward          | 480          |
| time/                   |              |
|    total_timesteps      | 266500       |
| train/                  |              |
|    approx_kl            | 0.0009830329 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00264     |
|    explained_variance   | -0.044       |
|    learning_rate        | 0.001        |
|    loss                 | 3.52e+04     |
|    n_updates            | 7900         |
|    policy_gradient_loss | -0.000161    |
|    value_loss           | 9.87e+04     |
------------------------------------------
Eval num_timesteps=267000, episode_reward=339.89 +/- 679.11
Episode length: 34.88 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 261      |
|    time_elapsed    | 955      |
|    total_timesteps | 267264   |
---------------------------------
Eval num_timesteps=267500, episode_reward=281.21 +/- 652.25
Episode length: 33.70 +/- 7.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.7          |
|    mean_reward          | 281           |
| time/                   |               |
|    total_timesteps      | 267500        |
| train/                  |               |
|    approx_kl            | 3.4621917e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00215      |
|    explained_variance   | 0.0552        |
|    learning_rate        | 0.001         |
|    loss                 | 4.17e+04      |
|    n_updates            | 7910          |
|    policy_gradient_loss | -3.51e-05     |
|    value_loss           | 1.14e+05      |
-------------------------------------------
Eval num_timesteps=268000, episode_reward=208.48 +/- 645.11
Episode length: 33.56 +/- 7.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 208      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 408      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 262      |
|    time_elapsed    | 958      |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=391.79 +/- 678.50
Episode length: 35.56 +/- 5.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 392           |
| time/                   |               |
|    total_timesteps      | 268500        |
| train/                  |               |
|    approx_kl            | 1.0896474e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00194      |
|    explained_variance   | 0.05          |
|    learning_rate        | 0.001         |
|    loss                 | 3.79e+04      |
|    n_updates            | 7920          |
|    policy_gradient_loss | -3.35e-05     |
|    value_loss           | 9.43e+04      |
-------------------------------------------
Eval num_timesteps=269000, episode_reward=630.65 +/- 808.09
Episode length: 35.36 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 631      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 433      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 263      |
|    time_elapsed    | 962      |
|    total_timesteps | 269312   |
---------------------------------
Eval num_timesteps=269500, episode_reward=607.33 +/- 797.77
Episode length: 35.76 +/- 6.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 607          |
| time/                   |              |
|    total_timesteps      | 269500       |
| train/                  |              |
|    approx_kl            | 7.905532e-06 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00186     |
|    explained_variance   | 0.0343       |
|    learning_rate        | 0.001        |
|    loss                 | 6.44e+04     |
|    n_updates            | 7930         |
|    policy_gradient_loss | 7.08e-05     |
|    value_loss           | 1.15e+05     |
------------------------------------------
Eval num_timesteps=270000, episode_reward=439.41 +/- 732.97
Episode length: 36.12 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 443      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 264      |
|    time_elapsed    | 965      |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=445.64 +/- 721.15
Episode length: 34.82 +/- 7.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 446           |
| time/                   |               |
|    total_timesteps      | 270500        |
| train/                  |               |
|    approx_kl            | 1.1455268e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00179      |
|    explained_variance   | 0.105         |
|    learning_rate        | 0.001         |
|    loss                 | 4.26e+04      |
|    n_updates            | 7940          |
|    policy_gradient_loss | -5.43e-05     |
|    value_loss           | 1.02e+05      |
-------------------------------------------
Eval num_timesteps=271000, episode_reward=511.91 +/- 740.12
Episode length: 35.36 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 449      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 265      |
|    time_elapsed    | 969      |
|    total_timesteps | 271360   |
---------------------------------
Eval num_timesteps=271500, episode_reward=397.80 +/- 709.04
Episode length: 35.08 +/- 7.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 398          |
| time/                   |              |
|    total_timesteps      | 271500       |
| train/                  |              |
|    approx_kl            | 9.895302e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00156     |
|    explained_variance   | -0.0762      |
|    learning_rate        | 0.001        |
|    loss                 | 4.71e+04     |
|    n_updates            | 7950         |
|    policy_gradient_loss | -2.87e-05    |
|    value_loss           | 1.15e+05     |
------------------------------------------
Eval num_timesteps=272000, episode_reward=560.68 +/- 774.73
Episode length: 36.30 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 561      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 398      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 266      |
|    time_elapsed    | 972      |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=434.03 +/- 788.21
Episode length: 34.98 +/- 6.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 434          |
| time/                   |              |
|    total_timesteps      | 272500       |
| train/                  |              |
|    approx_kl            | 8.841394e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00337     |
|    explained_variance   | -0.177       |
|    learning_rate        | 0.001        |
|    loss                 | 3.39e+04     |
|    n_updates            | 7960         |
|    policy_gradient_loss | -0.000152    |
|    value_loss           | 9.09e+04     |
------------------------------------------
Eval num_timesteps=273000, episode_reward=262.97 +/- 628.04
Episode length: 34.08 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 453      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 267      |
|    time_elapsed    | 976      |
|    total_timesteps | 273408   |
---------------------------------
Eval num_timesteps=273500, episode_reward=556.60 +/- 764.24
Episode length: 35.94 +/- 7.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 557           |
| time/                   |               |
|    total_timesteps      | 273500        |
| train/                  |               |
|    approx_kl            | 2.2397144e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00213      |
|    explained_variance   | 0.0656        |
|    learning_rate        | 0.001         |
|    loss                 | 3.26e+04      |
|    n_updates            | 7970          |
|    policy_gradient_loss | -6.69e-05     |
|    value_loss           | 8.51e+04      |
-------------------------------------------
Eval num_timesteps=274000, episode_reward=544.82 +/- 765.89
Episode length: 36.26 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 545      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 435      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 268      |
|    time_elapsed    | 979      |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=639.65 +/- 760.06
Episode length: 37.06 +/- 5.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.1         |
|    mean_reward          | 640          |
| time/                   |              |
|    total_timesteps      | 274500       |
| train/                  |              |
|    approx_kl            | 2.910383e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00163     |
|    explained_variance   | 0.0307       |
|    learning_rate        | 0.001        |
|    loss                 | 4.27e+04     |
|    n_updates            | 7980         |
|    policy_gradient_loss | -1.7e-05     |
|    value_loss           | 1.03e+05     |
------------------------------------------
Eval num_timesteps=275000, episode_reward=482.19 +/- 743.55
Episode length: 35.28 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 482      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 431      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 269      |
|    time_elapsed    | 983      |
|    total_timesteps | 275456   |
---------------------------------
Eval num_timesteps=275500, episode_reward=345.30 +/- 703.18
Episode length: 33.80 +/- 6.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 345           |
| time/                   |               |
|    total_timesteps      | 275500        |
| train/                  |               |
|    approx_kl            | 3.8475264e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00184      |
|    explained_variance   | -0.0469       |
|    learning_rate        | 0.001         |
|    loss                 | 4.24e+04      |
|    n_updates            | 7990          |
|    policy_gradient_loss | -2.47e-05     |
|    value_loss           | 1.05e+05      |
-------------------------------------------
Eval num_timesteps=276000, episode_reward=386.23 +/- 758.20
Episode length: 33.80 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 435      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 270      |
|    time_elapsed    | 987      |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=459.99 +/- 706.28
Episode length: 35.96 +/- 6.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 460           |
| time/                   |               |
|    total_timesteps      | 276500        |
| train/                  |               |
|    approx_kl            | 3.1850068e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00197      |
|    explained_variance   | 0.138         |
|    learning_rate        | 0.001         |
|    loss                 | 3.34e+04      |
|    n_updates            | 8000          |
|    policy_gradient_loss | -5.89e-05     |
|    value_loss           | 8.84e+04      |
-------------------------------------------
Eval num_timesteps=277000, episode_reward=501.66 +/- 697.11
Episode length: 36.68 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=345.14 +/- 693.35
Episode length: 34.84 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 345      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 397      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 271      |
|    time_elapsed    | 992      |
|    total_timesteps | 277504   |
---------------------------------
Eval num_timesteps=278000, episode_reward=493.26 +/- 674.58
Episode length: 37.18 +/- 5.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.2         |
|    mean_reward          | 493          |
| time/                   |              |
|    total_timesteps      | 278000       |
| train/                  |              |
|    approx_kl            | 7.921364e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00262     |
|    explained_variance   | -0.0867      |
|    learning_rate        | 0.001        |
|    loss                 | 3.02e+04     |
|    n_updates            | 8010         |
|    policy_gradient_loss | -3.52e-05    |
|    value_loss           | 8.12e+04     |
------------------------------------------
Eval num_timesteps=278500, episode_reward=379.63 +/- 672.04
Episode length: 34.98 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 345      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 272      |
|    time_elapsed    | 995      |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=404.12 +/- 708.36
Episode length: 36.20 +/- 6.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.2          |
|    mean_reward          | 404           |
| time/                   |               |
|    total_timesteps      | 279000        |
| train/                  |               |
|    approx_kl            | 5.2129617e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00257      |
|    explained_variance   | 0.117         |
|    learning_rate        | 0.001         |
|    loss                 | 4.03e+04      |
|    n_updates            | 8020          |
|    policy_gradient_loss | -0.00019      |
|    value_loss           | 9e+04         |
-------------------------------------------
Eval num_timesteps=279500, episode_reward=512.32 +/- 750.18
Episode length: 36.38 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 273      |
|    time_elapsed    | 999      |
|    total_timesteps | 279552   |
---------------------------------
Eval num_timesteps=280000, episode_reward=484.70 +/- 729.12
Episode length: 34.98 +/- 6.57
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 485           |
| time/                   |               |
|    total_timesteps      | 280000        |
| train/                  |               |
|    approx_kl            | 2.6950147e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00176      |
|    explained_variance   | -0.0429       |
|    learning_rate        | 0.001         |
|    loss                 | 3.58e+04      |
|    n_updates            | 8030          |
|    policy_gradient_loss | -1.8e-05      |
|    value_loss           | 1.17e+05      |
-------------------------------------------
Eval num_timesteps=280500, episode_reward=416.94 +/- 709.33
Episode length: 35.02 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 364      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 274      |
|    time_elapsed    | 1002     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=514.54 +/- 792.95
Episode length: 36.10 +/- 7.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 515           |
| time/                   |               |
|    total_timesteps      | 281000        |
| train/                  |               |
|    approx_kl            | 1.0069925e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00201      |
|    explained_variance   | -0.0966       |
|    learning_rate        | 0.001         |
|    loss                 | 4.53e+04      |
|    n_updates            | 8040          |
|    policy_gradient_loss | -2.14e-05     |
|    value_loss           | 8.96e+04      |
-------------------------------------------
Eval num_timesteps=281500, episode_reward=410.98 +/- 764.00
Episode length: 34.72 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 411      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 408      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 275      |
|    time_elapsed    | 1006     |
|    total_timesteps | 281600   |
---------------------------------
Eval num_timesteps=282000, episode_reward=558.44 +/- 776.69
Episode length: 36.42 +/- 6.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.4         |
|    mean_reward          | 558          |
| time/                   |              |
|    total_timesteps      | 282000       |
| train/                  |              |
|    approx_kl            | 3.259629e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00175     |
|    explained_variance   | 0.0744       |
|    learning_rate        | 0.001        |
|    loss                 | 3.45e+04     |
|    n_updates            | 8050         |
|    policy_gradient_loss | -1.61e-05    |
|    value_loss           | 1.02e+05     |
------------------------------------------
Eval num_timesteps=282500, episode_reward=377.33 +/- 745.74
Episode length: 33.96 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 415      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 276      |
|    time_elapsed    | 1009     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=505.37 +/- 717.12
Episode length: 35.76 +/- 6.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 505          |
| time/                   |              |
|    total_timesteps      | 283000       |
| train/                  |              |
|    approx_kl            | 4.598405e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00182     |
|    explained_variance   | -0.0163      |
|    learning_rate        | 0.001        |
|    loss                 | 4.79e+04     |
|    n_updates            | 8060         |
|    policy_gradient_loss | -1.57e-05    |
|    value_loss           | 1.08e+05     |
------------------------------------------
Eval num_timesteps=283500, episode_reward=455.72 +/- 763.21
Episode length: 35.18 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 509      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 277      |
|    time_elapsed    | 1013     |
|    total_timesteps | 283648   |
---------------------------------
Eval num_timesteps=284000, episode_reward=434.50 +/- 728.72
Episode length: 34.92 +/- 7.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.9        |
|    mean_reward          | 435         |
| time/                   |             |
|    total_timesteps      | 284000      |
| train/                  |             |
|    approx_kl            | 0.013086738 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00154    |
|    explained_variance   | 0.0892      |
|    learning_rate        | 0.001       |
|    loss                 | 3.76e+04    |
|    n_updates            | 8070        |
|    policy_gradient_loss | 0.00131     |
|    value_loss           | 8.81e+04    |
-----------------------------------------
Eval num_timesteps=284500, episode_reward=280.55 +/- 655.21
Episode length: 34.20 +/- 7.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 281      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 490      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 278      |
|    time_elapsed    | 1017     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=518.04 +/- 719.38
Episode length: 35.80 +/- 6.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 518           |
| time/                   |               |
|    total_timesteps      | 285000        |
| train/                  |               |
|    approx_kl            | 6.4028427e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00156      |
|    explained_variance   | -0.00885      |
|    learning_rate        | 0.001         |
|    loss                 | 5.15e+04      |
|    n_updates            | 8080          |
|    policy_gradient_loss | -2.03e-05     |
|    value_loss           | 9.89e+04      |
-------------------------------------------
Eval num_timesteps=285500, episode_reward=439.21 +/- 741.78
Episode length: 34.44 +/- 7.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 452      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 279      |
|    time_elapsed    | 1020     |
|    total_timesteps | 285696   |
---------------------------------
Eval num_timesteps=286000, episode_reward=654.80 +/- 744.90
Episode length: 36.94 +/- 6.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.9         |
|    mean_reward          | 655          |
| time/                   |              |
|    total_timesteps      | 286000       |
| train/                  |              |
|    approx_kl            | 4.773028e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00151     |
|    explained_variance   | -0.0478      |
|    learning_rate        | 0.001        |
|    loss                 | 5.19e+04     |
|    n_updates            | 8090         |
|    policy_gradient_loss | -1.27e-05    |
|    value_loss           | 1.02e+05     |
------------------------------------------
Eval num_timesteps=286500, episode_reward=382.09 +/- 667.57
Episode length: 35.38 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 382      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 493      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 280      |
|    time_elapsed    | 1024     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=436.49 +/- 748.22
Episode length: 35.22 +/- 6.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 436          |
| time/                   |              |
|    total_timesteps      | 287000       |
| train/                  |              |
|    approx_kl            | 5.355105e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00167     |
|    explained_variance   | 0.036        |
|    learning_rate        | 0.001        |
|    loss                 | 3.94e+04     |
|    n_updates            | 8100         |
|    policy_gradient_loss | -2.28e-05    |
|    value_loss           | 9.71e+04     |
------------------------------------------
Eval num_timesteps=287500, episode_reward=313.49 +/- 655.53
Episode length: 33.78 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 462      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 281      |
|    time_elapsed    | 1027     |
|    total_timesteps | 287744   |
---------------------------------
Eval num_timesteps=288000, episode_reward=336.08 +/- 654.18
Episode length: 34.72 +/- 5.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 336           |
| time/                   |               |
|    total_timesteps      | 288000        |
| train/                  |               |
|    approx_kl            | 1.5890691e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00158      |
|    explained_variance   | 0.107         |
|    learning_rate        | 0.001         |
|    loss                 | 4.57e+04      |
|    n_updates            | 8110          |
|    policy_gradient_loss | -1.46e-05     |
|    value_loss           | 1.13e+05      |
-------------------------------------------
Eval num_timesteps=288500, episode_reward=390.21 +/- 694.75
Episode length: 34.90 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 390      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 282      |
|    time_elapsed    | 1031     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=405.66 +/- 662.33
Episode length: 35.36 +/- 6.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 406           |
| time/                   |               |
|    total_timesteps      | 289000        |
| train/                  |               |
|    approx_kl            | 1.7416314e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0017       |
|    explained_variance   | 0.0744        |
|    learning_rate        | 0.001         |
|    loss                 | 3.78e+04      |
|    n_updates            | 8120          |
|    policy_gradient_loss | -6.58e-05     |
|    value_loss           | 9.9e+04       |
-------------------------------------------
Eval num_timesteps=289500, episode_reward=236.70 +/- 601.63
Episode length: 33.90 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 237      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 419      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 283      |
|    time_elapsed    | 1034     |
|    total_timesteps | 289792   |
---------------------------------
Eval num_timesteps=290000, episode_reward=664.97 +/- 767.94
Episode length: 37.02 +/- 5.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37            |
|    mean_reward          | 665           |
| time/                   |               |
|    total_timesteps      | 290000        |
| train/                  |               |
|    approx_kl            | 6.0535967e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00171      |
|    explained_variance   | 0.0396        |
|    learning_rate        | 0.001         |
|    loss                 | 5.01e+04      |
|    n_updates            | 8130          |
|    policy_gradient_loss | -6.31e-06     |
|    value_loss           | 1.07e+05      |
-------------------------------------------
Eval num_timesteps=290500, episode_reward=495.29 +/- 713.20
Episode length: 35.78 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 420      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 284      |
|    time_elapsed    | 1038     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=531.34 +/- 695.29
Episode length: 37.36 +/- 5.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.4         |
|    mean_reward          | 531          |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 5.676993e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00185     |
|    explained_variance   | 0.0706       |
|    learning_rate        | 0.001        |
|    loss                 | 3.45e+04     |
|    n_updates            | 8140         |
|    policy_gradient_loss | -3.12e-05    |
|    value_loss           | 8.7e+04      |
------------------------------------------
Eval num_timesteps=291500, episode_reward=477.74 +/- 743.30
Episode length: 35.04 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 428      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 285      |
|    time_elapsed    | 1042     |
|    total_timesteps | 291840   |
---------------------------------
Eval num_timesteps=292000, episode_reward=522.02 +/- 769.72
Episode length: 36.50 +/- 6.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.5          |
|    mean_reward          | 522           |
| time/                   |               |
|    total_timesteps      | 292000        |
| train/                  |               |
|    approx_kl            | 2.2358727e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00253      |
|    explained_variance   | -0.0758       |
|    learning_rate        | 0.001         |
|    loss                 | 3.6e+04       |
|    n_updates            | 8150          |
|    policy_gradient_loss | -4.44e-05     |
|    value_loss           | 9.81e+04      |
-------------------------------------------
Eval num_timesteps=292500, episode_reward=508.52 +/- 720.09
Episode length: 36.44 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 509      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 421      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 286      |
|    time_elapsed    | 1045     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=393.54 +/- 656.42
Episode length: 35.84 +/- 5.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 394           |
| time/                   |               |
|    total_timesteps      | 293000        |
| train/                  |               |
|    approx_kl            | 1.1417433e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00211      |
|    explained_variance   | 0.141         |
|    learning_rate        | 0.001         |
|    loss                 | 2.57e+04      |
|    n_updates            | 8160          |
|    policy_gradient_loss | -4.96e-05     |
|    value_loss           | 7.56e+04      |
-------------------------------------------
Eval num_timesteps=293500, episode_reward=414.63 +/- 674.50
Episode length: 35.70 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 423      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 287      |
|    time_elapsed    | 1049     |
|    total_timesteps | 293888   |
---------------------------------
Eval num_timesteps=294000, episode_reward=407.08 +/- 651.07
Episode length: 36.58 +/- 5.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 407           |
| time/                   |               |
|    total_timesteps      | 294000        |
| train/                  |               |
|    approx_kl            | 1.5748199e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00286      |
|    explained_variance   | 0.0974        |
|    learning_rate        | 0.001         |
|    loss                 | 3.33e+04      |
|    n_updates            | 8170          |
|    policy_gradient_loss | -0.000299     |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=294500, episode_reward=298.96 +/- 592.05
Episode length: 33.78 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 299      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 498      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 288      |
|    time_elapsed    | 1052     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=438.90 +/- 755.15
Episode length: 35.08 +/- 6.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 439           |
| time/                   |               |
|    total_timesteps      | 295000        |
| train/                  |               |
|    approx_kl            | 3.6030542e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00193      |
|    explained_variance   | -0.15         |
|    learning_rate        | 0.001         |
|    loss                 | 4.09e+04      |
|    n_updates            | 8180          |
|    policy_gradient_loss | -0.000294     |
|    value_loss           | 1.07e+05      |
-------------------------------------------
Eval num_timesteps=295500, episode_reward=308.58 +/- 712.37
Episode length: 33.92 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 515      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 289      |
|    time_elapsed    | 1056     |
|    total_timesteps | 295936   |
---------------------------------
Eval num_timesteps=296000, episode_reward=485.66 +/- 789.53
Episode length: 36.12 +/- 6.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 486          |
| time/                   |              |
|    total_timesteps      | 296000       |
| train/                  |              |
|    approx_kl            | 8.480274e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00133     |
|    explained_variance   | 0.128        |
|    learning_rate        | 0.001        |
|    loss                 | 4.31e+04     |
|    n_updates            | 8190         |
|    policy_gradient_loss | -5.11e-05    |
|    value_loss           | 1.04e+05     |
------------------------------------------
Eval num_timesteps=296500, episode_reward=368.21 +/- 747.99
Episode length: 34.18 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 527      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 290      |
|    time_elapsed    | 1060     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=524.10 +/- 753.28
Episode length: 35.58 +/- 6.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 524           |
| time/                   |               |
|    total_timesteps      | 297000        |
| train/                  |               |
|    approx_kl            | 1.0291638e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0031       |
|    explained_variance   | -0.0704       |
|    learning_rate        | 0.001         |
|    loss                 | 4.45e+04      |
|    n_updates            | 8200          |
|    policy_gradient_loss | -0.000218     |
|    value_loss           | 1.04e+05      |
-------------------------------------------
Eval num_timesteps=297500, episode_reward=456.15 +/- 674.53
Episode length: 36.50 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 455      |
| time/              |          |
|    fps             | 280      |
|    iterations      | 291      |
|    time_elapsed    | 1063     |
|    total_timesteps | 297984   |
---------------------------------
Eval num_timesteps=298000, episode_reward=407.31 +/- 712.29
Episode length: 34.86 +/- 7.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 407           |
| time/                   |               |
|    total_timesteps      | 298000        |
| train/                  |               |
|    approx_kl            | 2.8521754e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00175      |
|    explained_variance   | 0.0682        |
|    learning_rate        | 0.001         |
|    loss                 | 4.71e+04      |
|    n_updates            | 8210          |
|    policy_gradient_loss | -2.44e-05     |
|    value_loss           | 9.66e+04      |
-------------------------------------------
Eval num_timesteps=298500, episode_reward=284.99 +/- 678.12
Episode length: 34.28 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=433.05 +/- 724.31
Episode length: 35.54 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 440      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 292      |
|    time_elapsed    | 1068     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=283.74 +/- 574.83
Episode length: 34.56 +/- 6.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 284           |
| time/                   |               |
|    total_timesteps      | 299500        |
| train/                  |               |
|    approx_kl            | 1.6132253e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00264      |
|    explained_variance   | 0.0219        |
|    learning_rate        | 0.001         |
|    loss                 | 4.1e+04       |
|    n_updates            | 8220          |
|    policy_gradient_loss | -6.62e-05     |
|    value_loss           | 9.61e+04      |
-------------------------------------------
Eval num_timesteps=300000, episode_reward=456.94 +/- 744.05
Episode length: 35.24 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 437      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 293      |
|    time_elapsed    | 1071     |
|    total_timesteps | 300032   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-4/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 1024, 'batch_size': 64, 'learning_rate': 0.001, 'gamma': 0.9635, 'gae_lambda': 0.879}
Training steps: 300000
Frame skip: 4
