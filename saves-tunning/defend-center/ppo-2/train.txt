/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.8     |
|    ep_rew_mean     | 0.12     |
| time/              |          |
|    fps             | 1049     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 83.7        |
|    ep_rew_mean          | 0.354       |
| time/                   |             |
|    fps                  | 926         |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011355295 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.0577     |
|    learning_rate        | 0.00074     |
|    loss                 | -0.00667    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.0772      |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=2.10 +/- 0.83
Episode length: 97.60 +/- 20.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.6        |
|    mean_reward          | 2.1         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.013239533 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.308       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0152     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 0.0666      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.5     |
|    ep_rew_mean     | 0.549    |
| time/              |          |
|    fps             | 835      |
|    iterations      | 3        |
|    time_elapsed    | 7        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 90.2        |
|    ep_rew_mean          | 0.9         |
| time/                   |             |
|    fps                  | 850         |
|    iterations           | 4           |
|    time_elapsed         | 9           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.015885128 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.00074     |
|    loss                 | 0.00851     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 0.0757      |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=3.10 +/- 1.58
Episode length: 108.70 +/- 29.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 109         |
|    mean_reward          | 3.1         |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.023264222 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.407       |
|    learning_rate        | 0.00074     |
|    loss                 | 0.00834     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0306     |
|    value_loss           | 0.079       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.9     |
|    ep_rew_mean     | 1.25     |
| time/              |          |
|    fps             | 810      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 100        |
|    ep_rew_mean          | 2.01       |
| time/                   |            |
|    fps                  | 823        |
|    iterations           | 6          |
|    time_elapsed         | 14         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.03148838 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.426      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0424    |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0373    |
|    value_loss           | 0.0707     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 108        |
|    ep_rew_mean          | 2.74       |
| time/                   |            |
|    fps                  | 833        |
|    iterations           | 7          |
|    time_elapsed         | 17         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.02880368 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.18      |
|    explained_variance   | 0.455      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.054     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0358    |
|    value_loss           | 0.0832     |
----------------------------------------
Eval num_timesteps=15000, episode_reward=5.80 +/- 1.83
Episode length: 144.50 +/- 25.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 144         |
|    mean_reward          | 5.8         |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.042575404 |
|    clip_fraction        | 0.351       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0443     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0374     |
|    value_loss           | 0.0759      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 114      |
|    ep_rew_mean     | 3.36     |
| time/              |          |
|    fps             | 802      |
|    iterations      | 8        |
|    time_elapsed    | 20       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 121         |
|    ep_rew_mean          | 4.08        |
| time/                   |             |
|    fps                  | 812         |
|    iterations           | 9           |
|    time_elapsed         | 22          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.044050276 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0362     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0409     |
|    value_loss           | 0.087       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=8.40 +/- 1.36
Episode length: 176.80 +/- 38.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 177         |
|    mean_reward          | 8.4         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.044610552 |
|    clip_fraction        | 0.348       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.993      |
|    explained_variance   | 0.375       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0453     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.037      |
|    value_loss           | 0.0933      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 4.59     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 10       |
|    time_elapsed    | 26       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 131        |
|    ep_rew_mean          | 5.14       |
| time/                   |            |
|    fps                  | 792        |
|    iterations           | 11         |
|    time_elapsed         | 28         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.04698834 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.923     |
|    explained_variance   | 0.452      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0401    |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0358    |
|    value_loss           | 0.106      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 136        |
|    ep_rew_mean          | 5.59       |
| time/                   |            |
|    fps                  | 800        |
|    iterations           | 12         |
|    time_elapsed         | 30         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.06261037 |
|    clip_fraction        | 0.39       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.896     |
|    explained_variance   | 0.494      |
|    learning_rate        | 0.00074    |
|    loss                 | 0.0166     |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0362    |
|    value_loss           | 0.0872     |
----------------------------------------
Eval num_timesteps=25000, episode_reward=9.80 +/- 3.16
Episode length: 204.50 +/- 44.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 204         |
|    mean_reward          | 9.8         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.064698726 |
|    clip_fraction        | 0.36        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.909      |
|    explained_variance   | 0.49        |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0505     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0354     |
|    value_loss           | 0.0934      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 6.22     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 13       |
|    time_elapsed    | 34       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 149        |
|    ep_rew_mean          | 6.57       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 14         |
|    time_elapsed         | 36         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.06458963 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.846     |
|    explained_variance   | 0.336      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0195    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0457    |
|    value_loss           | 0.0923     |
----------------------------------------
Eval num_timesteps=30000, episode_reward=12.50 +/- 1.50
Episode length: 233.70 +/- 19.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 234        |
|    mean_reward          | 12.5       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.07552469 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.793     |
|    explained_variance   | 0.49       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.028     |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0323    |
|    value_loss           | 0.0916     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 154      |
|    ep_rew_mean     | 7.09     |
| time/              |          |
|    fps             | 760      |
|    iterations      | 15       |
|    time_elapsed    | 40       |
|    total_timesteps | 30720    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 160        |
|    ep_rew_mean          | 7.53       |
| time/                   |            |
|    fps                  | 768        |
|    iterations           | 16         |
|    time_elapsed         | 42         |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.08181897 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.789     |
|    explained_variance   | 0.568      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0332    |
|    n_updates            | 150        |
|    policy_gradient_loss | -0.037     |
|    value_loss           | 0.083      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 167        |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 774        |
|    iterations           | 17         |
|    time_elapsed         | 44         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.06989533 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.706     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0367    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0341    |
|    value_loss           | 0.0843     |
----------------------------------------
Eval num_timesteps=35000, episode_reward=14.40 +/- 2.01
Episode length: 251.10 +/- 44.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 251        |
|    mean_reward          | 14.4       |
| time/                   |            |
|    total_timesteps      | 35000      |
| train/                  |            |
|    approx_kl            | 0.06801513 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.672     |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0185    |
|    n_updates            | 170        |
|    policy_gradient_loss | -0.0361    |
|    value_loss           | 0.0883     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 8.36     |
| time/              |          |
|    fps             | 754      |
|    iterations      | 18       |
|    time_elapsed    | 48       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 181         |
|    ep_rew_mean          | 8.9         |
| time/                   |             |
|    fps                  | 761         |
|    iterations           | 19          |
|    time_elapsed         | 51          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.075368986 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.658      |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0236     |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0307     |
|    value_loss           | 0.0845      |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=16.90 +/- 1.45
Episode length: 295.10 +/- 24.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 295        |
|    mean_reward          | 16.9       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.07020577 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.642     |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0429    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0315    |
|    value_loss           | 0.0836     |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 9.53     |
| time/              |          |
|    fps             | 740      |
|    iterations      | 20       |
|    time_elapsed    | 55       |
|    total_timesteps | 40960    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 196       |
|    ep_rew_mean          | 10.1      |
| time/                   |           |
|    fps                  | 746       |
|    iterations           | 21        |
|    time_elapsed         | 57        |
|    total_timesteps      | 43008     |
| train/                  |           |
|    approx_kl            | 0.0818683 |
|    clip_fraction        | 0.279     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.539    |
|    explained_variance   | 0.58      |
|    learning_rate        | 0.00074   |
|    loss                 | -0.00762  |
|    n_updates            | 200       |
|    policy_gradient_loss | -0.0313   |
|    value_loss           | 0.0917    |
---------------------------------------
Eval num_timesteps=45000, episode_reward=18.00 +/- 1.48
Episode length: 310.40 +/- 26.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 310        |
|    mean_reward          | 18         |
| time/                   |            |
|    total_timesteps      | 45000      |
| train/                  |            |
|    approx_kl            | 0.06530305 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.562     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0251    |
|    n_updates            | 210        |
|    policy_gradient_loss | -0.0287    |
|    value_loss           | 0.08       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 727      |
|    iterations      | 22       |
|    time_elapsed    | 61       |
|    total_timesteps | 45056    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 208         |
|    ep_rew_mean          | 10.8        |
| time/                   |             |
|    fps                  | 733         |
|    iterations           | 23          |
|    time_elapsed         | 64          |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.073403575 |
|    clip_fraction        | 0.324       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.704      |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0186     |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.0379     |
|    value_loss           | 0.08        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 213        |
|    ep_rew_mean          | 11.3       |
| time/                   |            |
|    fps                  | 739        |
|    iterations           | 24         |
|    time_elapsed         | 66         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.07788591 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.713     |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0266    |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0386    |
|    value_loss           | 0.0692     |
----------------------------------------
Eval num_timesteps=50000, episode_reward=18.80 +/- 1.40
Episode length: 324.90 +/- 28.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 325         |
|    mean_reward          | 18.8        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.074472025 |
|    clip_fraction        | 0.319       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.656      |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0563     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0412     |
|    value_loss           | 0.0726      |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | 11.5     |
| time/              |          |
|    fps             | 722      |
|    iterations      | 25       |
|    time_elapsed    | 70       |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 222         |
|    ep_rew_mean          | 11.9        |
| time/                   |             |
|    fps                  | 728         |
|    iterations           | 26          |
|    time_elapsed         | 73          |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.068475574 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.596      |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0437     |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0365     |
|    value_loss           | 0.0726      |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=15.50 +/- 3.11
Episode length: 265.70 +/- 47.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 266        |
|    mean_reward          | 15.5       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.09141439 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.632     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.032     |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0292    |
|    value_loss           | 0.0825     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | 12.2     |
| time/              |          |
|    fps             | 717      |
|    iterations      | 27       |
|    time_elapsed    | 77       |
|    total_timesteps | 55296    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 223       |
|    ep_rew_mean          | 12.2      |
| time/                   |           |
|    fps                  | 722       |
|    iterations           | 28        |
|    time_elapsed         | 79        |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.0882291 |
|    clip_fraction        | 0.304     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.635    |
|    explained_variance   | 0.646     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0168   |
|    n_updates            | 270       |
|    policy_gradient_loss | -0.0306   |
|    value_loss           | 0.0828    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 229        |
|    ep_rew_mean          | 12.7       |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 29         |
|    time_elapsed         | 81         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.08248125 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.647     |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0333    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0389    |
|    value_loss           | 0.0758     |
----------------------------------------
Eval num_timesteps=60000, episode_reward=12.00 +/- 3.66
Episode length: 207.00 +/- 50.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 207        |
|    mean_reward          | 12         |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.10142116 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.595     |
|    explained_variance   | 0.687      |
|    learning_rate        | 0.00074    |
|    loss                 | 0.00968    |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0287    |
|    value_loss           | 0.0694     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 12.8     |
| time/              |          |
|    fps             | 721      |
|    iterations      | 30       |
|    time_elapsed    | 85       |
|    total_timesteps | 61440    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 224        |
|    ep_rew_mean          | 12.7       |
| time/                   |            |
|    fps                  | 726        |
|    iterations           | 31         |
|    time_elapsed         | 87         |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.10343896 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.629     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0582    |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0442    |
|    value_loss           | 0.068      |
----------------------------------------
Eval num_timesteps=65000, episode_reward=11.30 +/- 1.85
Episode length: 209.30 +/- 19.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 209        |
|    mean_reward          | 11.3       |
| time/                   |            |
|    total_timesteps      | 65000      |
| train/                  |            |
|    approx_kl            | 0.08897715 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.582     |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0248    |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.041     |
|    value_loss           | 0.0766     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 12.6     |
| time/              |          |
|    fps             | 719      |
|    iterations      | 32       |
|    time_elapsed    | 91       |
|    total_timesteps | 65536    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 220        |
|    ep_rew_mean          | 12.7       |
| time/                   |            |
|    fps                  | 724        |
|    iterations           | 33         |
|    time_elapsed         | 93         |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.09864731 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.634     |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0557    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0344    |
|    value_loss           | 0.0716     |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 222         |
|    ep_rew_mean          | 12.8        |
| time/                   |             |
|    fps                  | 728         |
|    iterations           | 34          |
|    time_elapsed         | 95          |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.122916475 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0551     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0338     |
|    value_loss           | 0.0671      |
-----------------------------------------
Eval num_timesteps=70000, episode_reward=15.30 +/- 2.57
Episode length: 237.40 +/- 47.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 237        |
|    mean_reward          | 15.3       |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.09834363 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.522     |
|    explained_variance   | 0.641      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0591    |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0321    |
|    value_loss           | 0.0781     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 721      |
|    iterations      | 35       |
|    time_elapsed    | 99       |
|    total_timesteps | 71680    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 215         |
|    ep_rew_mean          | 12.8        |
| time/                   |             |
|    fps                  | 725         |
|    iterations           | 36          |
|    time_elapsed         | 101         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.099642456 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.541      |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0359     |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0359     |
|    value_loss           | 0.0634      |
-----------------------------------------
Eval num_timesteps=75000, episode_reward=10.70 +/- 5.12
Episode length: 187.40 +/- 62.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 187        |
|    mean_reward          | 10.7       |
| time/                   |            |
|    total_timesteps      | 75000      |
| train/                  |            |
|    approx_kl            | 0.10999282 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.508     |
|    explained_variance   | 0.647      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0604    |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0332    |
|    value_loss           | 0.0667     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 12.6     |
| time/              |          |
|    fps             | 721      |
|    iterations      | 37       |
|    time_elapsed    | 105      |
|    total_timesteps | 75776    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 209        |
|    ep_rew_mean          | 12.5       |
| time/                   |            |
|    fps                  | 725        |
|    iterations           | 38         |
|    time_elapsed         | 107        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.10698271 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.549     |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0352    |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0355    |
|    value_loss           | 0.0696     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 205        |
|    ep_rew_mean          | 12.1       |
| time/                   |            |
|    fps                  | 729        |
|    iterations           | 39         |
|    time_elapsed         | 109        |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.12020963 |
|    clip_fraction        | 0.362      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.574     |
|    explained_variance   | 0.721      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0514    |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0365    |
|    value_loss           | 0.0613     |
----------------------------------------
Eval num_timesteps=80000, episode_reward=9.50 +/- 2.97
Episode length: 172.50 +/- 42.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 172         |
|    mean_reward          | 9.5         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.100401126 |
|    clip_fraction        | 0.349       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.61       |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0711     |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.0374     |
|    value_loss           | 0.0628      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 202      |
|    ep_rew_mean     | 12       |
| time/              |          |
|    fps             | 725      |
|    iterations      | 40       |
|    time_elapsed    | 112      |
|    total_timesteps | 81920    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 201        |
|    ep_rew_mean          | 11.9       |
| time/                   |            |
|    fps                  | 729        |
|    iterations           | 41         |
|    time_elapsed         | 115        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.11359264 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.567     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0474    |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0352    |
|    value_loss           | 0.0724     |
----------------------------------------
Eval num_timesteps=85000, episode_reward=11.90 +/- 3.08
Episode length: 194.70 +/- 34.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 195         |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 85000       |
| train/                  |             |
|    approx_kl            | 0.093232244 |
|    clip_fraction        | 0.357       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.638      |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.00074     |
|    loss                 | -0.0172     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0426     |
|    value_loss           | 0.0556      |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 12.1     |
| time/              |          |
|    fps             | 724      |
|    iterations      | 42       |
|    time_elapsed    | 118      |
|    total_timesteps | 86016    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 204        |
|    ep_rew_mean          | 12         |
| time/                   |            |
|    fps                  | 728        |
|    iterations           | 43         |
|    time_elapsed         | 120        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.10396618 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.673     |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0516    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.0649     |
----------------------------------------
Eval num_timesteps=90000, episode_reward=12.10 +/- 3.73
Episode length: 205.60 +/- 41.58
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 206      |
|    mean_reward          | 12.1     |
| time/                   |          |
|    total_timesteps      | 90000    |
| train/                  |          |
|    approx_kl            | 0.126417 |
|    clip_fraction        | 0.381    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.618   |
|    explained_variance   | 0.651    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0463  |
|    n_updates            | 430      |
|    policy_gradient_loss | -0.0428  |
|    value_loss           | 0.0635   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 11.6     |
| time/              |          |
|    fps             | 723      |
|    iterations      | 44       |
|    time_elapsed    | 124      |
|    total_timesteps | 90112    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 11.2       |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 45         |
|    time_elapsed         | 126        |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.11642164 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.672     |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0672    |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0477    |
|    value_loss           | 0.065      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 11.2       |
| time/                   |            |
|    fps                  | 730        |
|    iterations           | 46         |
|    time_elapsed         | 128        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.13870722 |
|    clip_fraction        | 0.374      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.59      |
|    explained_variance   | 0.585      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0368    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0431    |
|    value_loss           | 0.0667     |
----------------------------------------
Eval num_timesteps=95000, episode_reward=12.80 +/- 3.09
Episode length: 217.20 +/- 55.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 217        |
|    mean_reward          | 12.8       |
| time/                   |            |
|    total_timesteps      | 95000      |
| train/                  |            |
|    approx_kl            | 0.13573724 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.579     |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.069     |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.04      |
|    value_loss           | 0.0713     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 11.3     |
| time/              |          |
|    fps             | 725      |
|    iterations      | 47       |
|    time_elapsed    | 132      |
|    total_timesteps | 96256    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 198        |
|    ep_rew_mean          | 11.4       |
| time/                   |            |
|    fps                  | 728        |
|    iterations           | 48         |
|    time_elapsed         | 134        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.11077214 |
|    clip_fraction        | 0.371      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.598     |
|    explained_variance   | 0.693      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0362    |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.0498    |
|    value_loss           | 0.0658     |
----------------------------------------
Eval num_timesteps=100000, episode_reward=12.00 +/- 4.38
Episode length: 210.50 +/- 69.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 12         |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.13054697 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.56      |
|    explained_variance   | 0.678      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.026     |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0344    |
|    value_loss           | 0.0733     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 11.3     |
| time/              |          |
|    fps             | 724      |
|    iterations      | 49       |
|    time_elapsed    | 138      |
|    total_timesteps | 100352   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | 11.6       |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 50         |
|    time_elapsed         | 140        |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.12970626 |
|    clip_fraction        | 0.397      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.601     |
|    explained_variance   | 0.579      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0607    |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.0497    |
|    value_loss           | 0.071      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 198       |
|    ep_rew_mean          | 11.4      |
| time/                   |           |
|    fps                  | 730       |
|    iterations           | 51        |
|    time_elapsed         | 142       |
|    total_timesteps      | 104448    |
| train/                  |           |
|    approx_kl            | 0.1539604 |
|    clip_fraction        | 0.401     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.644    |
|    explained_variance   | 0.581     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.065    |
|    n_updates            | 500       |
|    policy_gradient_loss | -0.0468   |
|    value_loss           | 0.0736    |
---------------------------------------
Eval num_timesteps=105000, episode_reward=13.30 +/- 2.72
Episode length: 231.90 +/- 45.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 232        |
|    mean_reward          | 13.3       |
| time/                   |            |
|    total_timesteps      | 105000     |
| train/                  |            |
|    approx_kl            | 0.13860798 |
|    clip_fraction        | 0.417      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.651     |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0643    |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0482    |
|    value_loss           | 0.068      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 11.2     |
| time/              |          |
|    fps             | 725      |
|    iterations      | 52       |
|    time_elapsed    | 146      |
|    total_timesteps | 106496   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 193       |
|    ep_rew_mean          | 11.2      |
| time/                   |           |
|    fps                  | 728       |
|    iterations           | 53        |
|    time_elapsed         | 148       |
|    total_timesteps      | 108544    |
| train/                  |           |
|    approx_kl            | 0.1347456 |
|    clip_fraction        | 0.383     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.617    |
|    explained_variance   | 0.668     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0555   |
|    n_updates            | 520       |
|    policy_gradient_loss | -0.0465   |
|    value_loss           | 0.0714    |
---------------------------------------
Eval num_timesteps=110000, episode_reward=12.90 +/- 2.59
Episode length: 228.00 +/- 43.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 12.9       |
| time/                   |            |
|    total_timesteps      | 110000     |
| train/                  |            |
|    approx_kl            | 0.12912038 |
|    clip_fraction        | 0.4        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.652     |
|    explained_variance   | 0.681      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0586    |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0456    |
|    value_loss           | 0.0601     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 11.3     |
| time/              |          |
|    fps             | 724      |
|    iterations      | 54       |
|    time_elapsed    | 152      |
|    total_timesteps | 110592   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 194        |
|    ep_rew_mean          | 11.2       |
| time/                   |            |
|    fps                  | 726        |
|    iterations           | 55         |
|    time_elapsed         | 154        |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.13218236 |
|    clip_fraction        | 0.376      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.66      |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0654    |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.0481    |
|    value_loss           | 0.0633     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 11.1       |
| time/                   |            |
|    fps                  | 729        |
|    iterations           | 56         |
|    time_elapsed         | 157        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.13319111 |
|    clip_fraction        | 0.383      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.645     |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0839    |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 0.0596     |
----------------------------------------
Eval num_timesteps=115000, episode_reward=11.80 +/- 1.78
Episode length: 203.30 +/- 41.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 203        |
|    mean_reward          | 11.8       |
| time/                   |            |
|    total_timesteps      | 115000     |
| train/                  |            |
|    approx_kl            | 0.12705077 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.648     |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0592    |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.0604     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 726      |
|    iterations      | 57       |
|    time_elapsed    | 160      |
|    total_timesteps | 116736   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 190      |
|    ep_rew_mean          | 10.9     |
| time/                   |          |
|    fps                  | 728      |
|    iterations           | 58       |
|    time_elapsed         | 162      |
|    total_timesteps      | 118784   |
| train/                  |          |
|    approx_kl            | 0.286619 |
|    clip_fraction        | 0.413    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.668   |
|    explained_variance   | 0.656    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0696  |
|    n_updates            | 570      |
|    policy_gradient_loss | -0.05    |
|    value_loss           | 0.0643   |
--------------------------------------
Eval num_timesteps=120000, episode_reward=11.20 +/- 5.62
Episode length: 200.50 +/- 85.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 200        |
|    mean_reward          | 11.2       |
| time/                   |            |
|    total_timesteps      | 120000     |
| train/                  |            |
|    approx_kl            | 0.17743361 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.703     |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0767    |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.057     |
|    value_loss           | 0.0578     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 725      |
|    iterations      | 59       |
|    time_elapsed    | 166      |
|    total_timesteps | 120832   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 184        |
|    ep_rew_mean          | 10.2       |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 60         |
|    time_elapsed         | 168        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.16590649 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.769     |
|    explained_variance   | 0.61       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0564    |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.061      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 185        |
|    ep_rew_mean          | 10.2       |
| time/                   |            |
|    fps                  | 730        |
|    iterations           | 61         |
|    time_elapsed         | 171        |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.18320441 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.748     |
|    explained_variance   | 0.505      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.062     |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0572    |
|    value_loss           | 0.0748     |
----------------------------------------
Eval num_timesteps=125000, episode_reward=11.30 +/- 5.33
Episode length: 195.70 +/- 70.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 196       |
|    mean_reward          | 11.3      |
| time/                   |           |
|    total_timesteps      | 125000    |
| train/                  |           |
|    approx_kl            | 0.1902953 |
|    clip_fraction        | 0.464     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.731    |
|    explained_variance   | 0.579     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0806   |
|    n_updates            | 610       |
|    policy_gradient_loss | -0.0621   |
|    value_loss           | 0.0605    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 727      |
|    iterations      | 62       |
|    time_elapsed    | 174      |
|    total_timesteps | 126976   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 186        |
|    ep_rew_mean          | 10.3       |
| time/                   |            |
|    fps                  | 729        |
|    iterations           | 63         |
|    time_elapsed         | 176        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.16489226 |
|    clip_fraction        | 0.405      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.686     |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.064     |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.0602    |
|    value_loss           | 0.0591     |
----------------------------------------
Eval num_timesteps=130000, episode_reward=12.60 +/- 3.64
Episode length: 211.00 +/- 56.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 211        |
|    mean_reward          | 12.6       |
| time/                   |            |
|    total_timesteps      | 130000     |
| train/                  |            |
|    approx_kl            | 0.14808586 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.689     |
|    explained_variance   | 0.585      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0731    |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0606    |
|    value_loss           | 0.0652     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 726      |
|    iterations      | 64       |
|    time_elapsed    | 180      |
|    total_timesteps | 131072   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 185        |
|    ep_rew_mean          | 10.2       |
| time/                   |            |
|    fps                  | 728        |
|    iterations           | 65         |
|    time_elapsed         | 182        |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.16509214 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.718     |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0903    |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0628    |
|    value_loss           | 0.0653     |
----------------------------------------
Eval num_timesteps=135000, episode_reward=12.10 +/- 3.36
Episode length: 205.90 +/- 48.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 206       |
|    mean_reward          | 12.1      |
| time/                   |           |
|    total_timesteps      | 135000    |
| train/                  |           |
|    approx_kl            | 0.1679923 |
|    clip_fraction        | 0.434     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.705    |
|    explained_variance   | 0.528     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0654   |
|    n_updates            | 650       |
|    policy_gradient_loss | -0.0621   |
|    value_loss           | 0.0631    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 725      |
|    iterations      | 66       |
|    time_elapsed    | 186      |
|    total_timesteps | 135168   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 188        |
|    ep_rew_mean          | 10.4       |
| time/                   |            |
|    fps                  | 727        |
|    iterations           | 67         |
|    time_elapsed         | 188        |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.19238146 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.732     |
|    explained_variance   | 0.489      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0734    |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.063     |
|    value_loss           | 0.0688     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 10.6       |
| time/                   |            |
|    fps                  | 730        |
|    iterations           | 68         |
|    time_elapsed         | 190        |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.18831107 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.718     |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0631    |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.0583    |
|    value_loss           | 0.0651     |
----------------------------------------
Eval num_timesteps=140000, episode_reward=11.90 +/- 2.88
Episode length: 199.30 +/- 42.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 199        |
|    mean_reward          | 11.9       |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.25193724 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.682     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.101     |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0639    |
|    value_loss           | 0.0584     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 11.1     |
| time/              |          |
|    fps             | 727      |
|    iterations      | 69       |
|    time_elapsed    | 194      |
|    total_timesteps | 141312   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 197        |
|    ep_rew_mean          | 11.3       |
| time/                   |            |
|    fps                  | 729        |
|    iterations           | 70         |
|    time_elapsed         | 196        |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.17199457 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.663     |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0842    |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0571    |
|    value_loss           | 0.0752     |
----------------------------------------
Eval num_timesteps=145000, episode_reward=11.00 +/- 2.61
Episode length: 190.40 +/- 42.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 190        |
|    mean_reward          | 11         |
| time/                   |            |
|    total_timesteps      | 145000     |
| train/                  |            |
|    approx_kl            | 0.21095002 |
|    clip_fraction        | 0.458      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.668     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0676    |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0586    |
|    value_loss           | 0.0655     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 11       |
| time/              |          |
|    fps             | 727      |
|    iterations      | 71       |
|    time_elapsed    | 199      |
|    total_timesteps | 145408   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | 10.8       |
| time/                   |            |
|    fps                  | 729        |
|    iterations           | 72         |
|    time_elapsed         | 202        |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.19210054 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.723     |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0529    |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.0623    |
|    value_loss           | 0.0626     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 187        |
|    ep_rew_mean          | 10.8       |
| time/                   |            |
|    fps                  | 731        |
|    iterations           | 73         |
|    time_elapsed         | 204        |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.14749062 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.692     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0958    |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.061     |
|    value_loss           | 0.0628     |
----------------------------------------
Eval num_timesteps=150000, episode_reward=13.00 +/- 3.41
Episode length: 210.50 +/- 32.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 13         |
| time/                   |            |
|    total_timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.16859755 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.662     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0446    |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0595    |
|    value_loss           | 0.055      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 729      |
|    iterations      | 74       |
|    time_elapsed    | 207      |
|    total_timesteps | 151552   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 11.1       |
| time/                   |            |
|    fps                  | 731        |
|    iterations           | 75         |
|    time_elapsed         | 210        |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.16932592 |
|    clip_fraction        | 0.43       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.683     |
|    explained_variance   | 0.706      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0697    |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0585    |
|    value_loss           | 0.0556     |
----------------------------------------
Eval num_timesteps=155000, episode_reward=11.60 +/- 3.83
Episode length: 197.20 +/- 63.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 197        |
|    mean_reward          | 11.6       |
| time/                   |            |
|    total_timesteps      | 155000     |
| train/                  |            |
|    approx_kl            | 0.17262135 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.689     |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0583    |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.0591    |
|    value_loss           | 0.0579     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 728      |
|    iterations      | 76       |
|    time_elapsed    | 213      |
|    total_timesteps | 155648   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | 11.1       |
| time/                   |            |
|    fps                  | 730        |
|    iterations           | 77         |
|    time_elapsed         | 215        |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.19021183 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.687     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0728    |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.0656    |
|    value_loss           | 0.0566     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 10.9       |
| time/                   |            |
|    fps                  | 732        |
|    iterations           | 78         |
|    time_elapsed         | 218        |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.15652739 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.703     |
|    explained_variance   | 0.7        |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0731    |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.0627    |
|    value_loss           | 0.055      |
----------------------------------------
Eval num_timesteps=160000, episode_reward=10.40 +/- 3.38
Episode length: 177.00 +/- 46.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 177        |
|    mean_reward          | 10.4       |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.18218154 |
|    clip_fraction        | 0.487      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.744     |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0977    |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.0603    |
|    value_loss           | 0.069      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 730      |
|    iterations      | 79       |
|    time_elapsed    | 221      |
|    total_timesteps | 161792   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 10.9       |
| time/                   |            |
|    fps                  | 732        |
|    iterations           | 80         |
|    time_elapsed         | 223        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.21504651 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.665     |
|    explained_variance   | 0.64       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0895    |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0655    |
|    value_loss           | 0.0606     |
----------------------------------------
Eval num_timesteps=165000, episode_reward=11.20 +/- 3.31
Episode length: 193.20 +/- 50.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 193        |
|    mean_reward          | 11.2       |
| time/                   |            |
|    total_timesteps      | 165000     |
| train/                  |            |
|    approx_kl            | 0.16113096 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.688     |
|    explained_variance   | 0.616      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0747    |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.0582    |
|    value_loss           | 0.062      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 729      |
|    iterations      | 81       |
|    time_elapsed    | 227      |
|    total_timesteps | 165888   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 193        |
|    ep_rew_mean          | 11.1       |
| time/                   |            |
|    fps                  | 731        |
|    iterations           | 82         |
|    time_elapsed         | 229        |
|    total_timesteps      | 167936     |
| train/                  |            |
|    approx_kl            | 0.18110459 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.681     |
|    explained_variance   | 0.641      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0819    |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.0562     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 193        |
|    ep_rew_mean          | 11         |
| time/                   |            |
|    fps                  | 733        |
|    iterations           | 83         |
|    time_elapsed         | 231        |
|    total_timesteps      | 169984     |
| train/                  |            |
|    approx_kl            | 0.24344602 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.678     |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.104     |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0686    |
|    value_loss           | 0.0633     |
----------------------------------------
Eval num_timesteps=170000, episode_reward=10.40 +/- 2.24
Episode length: 177.00 +/- 31.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 177        |
|    mean_reward          | 10.4       |
| time/                   |            |
|    total_timesteps      | 170000     |
| train/                  |            |
|    approx_kl            | 0.23179114 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.679     |
|    explained_variance   | 0.586      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0644    |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.0612    |
|    value_loss           | 0.07       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 10.9     |
| time/              |          |
|    fps             | 731      |
|    iterations      | 84       |
|    time_elapsed    | 235      |
|    total_timesteps | 172032   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 187        |
|    ep_rew_mean          | 10.6       |
| time/                   |            |
|    fps                  | 733        |
|    iterations           | 85         |
|    time_elapsed         | 237        |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.22584812 |
|    clip_fraction        | 0.472      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.657     |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0858    |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0642    |
|    value_loss           | 0.0556     |
----------------------------------------
Eval num_timesteps=175000, episode_reward=10.00 +/- 2.65
Episode length: 193.10 +/- 35.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 193        |
|    mean_reward          | 10         |
| time/                   |            |
|    total_timesteps      | 175000     |
| train/                  |            |
|    approx_kl            | 0.17981714 |
|    clip_fraction        | 0.449      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.647     |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.084     |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.064     |
|    value_loss           | 0.0556     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    fps             | 730      |
|    iterations      | 86       |
|    time_elapsed    | 240      |
|    total_timesteps | 176128   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 182       |
|    ep_rew_mean          | 10.2      |
| time/                   |           |
|    fps                  | 732       |
|    iterations           | 87        |
|    time_elapsed         | 243       |
|    total_timesteps      | 178176    |
| train/                  |           |
|    approx_kl            | 0.1890926 |
|    clip_fraction        | 0.465     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.671    |
|    explained_variance   | 0.703     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.088    |
|    n_updates            | 860       |
|    policy_gradient_loss | -0.0607   |
|    value_loss           | 0.0565    |
---------------------------------------
Eval num_timesteps=180000, episode_reward=11.00 +/- 3.92
Episode length: 187.70 +/- 58.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 188        |
|    mean_reward          | 11         |
| time/                   |            |
|    total_timesteps      | 180000     |
| train/                  |            |
|    approx_kl            | 0.19765896 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.674     |
|    explained_variance   | 0.696      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.107     |
|    n_updates            | 870        |
|    policy_gradient_loss | -0.0612    |
|    value_loss           | 0.0621     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    fps             | 730      |
|    iterations      | 88       |
|    time_elapsed    | 246      |
|    total_timesteps | 180224   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 10.5       |
| time/                   |            |
|    fps                  | 732        |
|    iterations           | 89         |
|    time_elapsed         | 248        |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.21532394 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.645     |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0624    |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.0625    |
|    value_loss           | 0.052      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 10.4       |
| time/                   |            |
|    fps                  | 733        |
|    iterations           | 90         |
|    time_elapsed         | 251        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.21412137 |
|    clip_fraction        | 0.449      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.658     |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0833    |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0612    |
|    value_loss           | 0.0547     |
----------------------------------------
Eval num_timesteps=185000, episode_reward=11.40 +/- 3.04
Episode length: 183.20 +/- 45.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 183        |
|    mean_reward          | 11.4       |
| time/                   |            |
|    total_timesteps      | 185000     |
| train/                  |            |
|    approx_kl            | 0.20772463 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.664     |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0933    |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0583    |
|    value_loss           | 0.0555     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    fps             | 731      |
|    iterations      | 91       |
|    time_elapsed    | 254      |
|    total_timesteps | 186368   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 182        |
|    ep_rew_mean          | 10.4       |
| time/                   |            |
|    fps                  | 733        |
|    iterations           | 92         |
|    time_elapsed         | 256        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.21918426 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.639     |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0565    |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.0625    |
|    value_loss           | 0.0648     |
----------------------------------------
Eval num_timesteps=190000, episode_reward=8.80 +/- 2.82
Episode length: 165.30 +/- 56.99
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 165      |
|    mean_reward          | 8.8      |
| time/                   |          |
|    total_timesteps      | 190000   |
| train/                  |          |
|    approx_kl            | 0.21759  |
|    clip_fraction        | 0.457    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.628   |
|    explained_variance   | 0.674    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0885  |
|    n_updates            | 920      |
|    policy_gradient_loss | -0.0614  |
|    value_loss           | 0.0579   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 732      |
|    iterations      | 93       |
|    time_elapsed    | 260      |
|    total_timesteps | 190464   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 187       |
|    ep_rew_mean          | 10.8      |
| time/                   |           |
|    fps                  | 733       |
|    iterations           | 94        |
|    time_elapsed         | 262       |
|    total_timesteps      | 192512    |
| train/                  |           |
|    approx_kl            | 0.3321746 |
|    clip_fraction        | 0.435     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.643    |
|    explained_variance   | 0.672     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0686   |
|    n_updates            | 930       |
|    policy_gradient_loss | -0.0597   |
|    value_loss           | 0.0599    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 10.6       |
| time/                   |            |
|    fps                  | 735        |
|    iterations           | 95         |
|    time_elapsed         | 264        |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.24204648 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.63      |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0802    |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0639    |
|    value_loss           | 0.064      |
----------------------------------------
Eval num_timesteps=195000, episode_reward=9.90 +/- 2.47
Episode length: 183.00 +/- 39.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 183       |
|    mean_reward          | 9.9       |
| time/                   |           |
|    total_timesteps      | 195000    |
| train/                  |           |
|    approx_kl            | 0.2055816 |
|    clip_fraction        | 0.44      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.636    |
|    explained_variance   | 0.653     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0757   |
|    n_updates            | 950       |
|    policy_gradient_loss | -0.0554   |
|    value_loss           | 0.0594    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 733      |
|    iterations      | 96       |
|    time_elapsed    | 268      |
|    total_timesteps | 196608   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 181        |
|    ep_rew_mean          | 10.1       |
| time/                   |            |
|    fps                  | 734        |
|    iterations           | 97         |
|    time_elapsed         | 270        |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.29347286 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.635     |
|    explained_variance   | 0.534      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0765    |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0624    |
|    value_loss           | 0.0584     |
----------------------------------------
Eval num_timesteps=200000, episode_reward=7.50 +/- 2.87
Episode length: 154.30 +/- 46.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 154        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 200000     |
| train/                  |            |
|    approx_kl            | 0.23196718 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.681     |
|    explained_variance   | 0.627      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0532    |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0589    |
|    value_loss           | 0.0589     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 9.22     |
| time/              |          |
|    fps             | 733      |
|    iterations      | 98       |
|    time_elapsed    | 273      |
|    total_timesteps | 200704   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 173        |
|    ep_rew_mean          | 9.22       |
| time/                   |            |
|    fps                  | 734        |
|    iterations           | 99         |
|    time_elapsed         | 275        |
|    total_timesteps      | 202752     |
| train/                  |            |
|    approx_kl            | 0.25341883 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.555      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0923    |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.0615    |
|    value_loss           | 0.0624     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | 9.35       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 100        |
|    time_elapsed         | 278        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.28237575 |
|    clip_fraction        | 0.492      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.654     |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.101     |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.0619    |
|    value_loss           | 0.0614     |
----------------------------------------
Eval num_timesteps=205000, episode_reward=11.60 +/- 2.29
Episode length: 205.70 +/- 32.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 206       |
|    mean_reward          | 11.6      |
| time/                   |           |
|    total_timesteps      | 205000    |
| train/                  |           |
|    approx_kl            | 0.2422482 |
|    clip_fraction        | 0.479     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.669    |
|    explained_variance   | 0.667     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0714   |
|    n_updates            | 1000      |
|    policy_gradient_loss | -0.0606   |
|    value_loss           | 0.0575    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 9.15     |
| time/              |          |
|    fps             | 734      |
|    iterations      | 101      |
|    time_elapsed    | 281      |
|    total_timesteps | 206848   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | 9.08       |
| time/                   |            |
|    fps                  | 735        |
|    iterations           | 102        |
|    time_elapsed         | 283        |
|    total_timesteps      | 208896     |
| train/                  |            |
|    approx_kl            | 0.29894128 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.62      |
|    explained_variance   | 0.589      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0959    |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0658    |
|    value_loss           | 0.0525     |
----------------------------------------
Eval num_timesteps=210000, episode_reward=7.50 +/- 2.20
Episode length: 165.40 +/- 35.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 165        |
|    mean_reward          | 7.5        |
| time/                   |            |
|    total_timesteps      | 210000     |
| train/                  |            |
|    approx_kl            | 0.25669867 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.711     |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0924    |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.0692    |
|    value_loss           | 0.0543     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 9.13     |
| time/              |          |
|    fps             | 734      |
|    iterations      | 103      |
|    time_elapsed    | 287      |
|    total_timesteps | 210944   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 182        |
|    ep_rew_mean          | 9.38       |
| time/                   |            |
|    fps                  | 735        |
|    iterations           | 104        |
|    time_elapsed         | 289        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.25056207 |
|    clip_fraction        | 0.499      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.672     |
|    explained_variance   | 0.502      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.118     |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0653    |
|    value_loss           | 0.0628     |
----------------------------------------
Eval num_timesteps=215000, episode_reward=12.20 +/- 2.32
Episode length: 215.20 +/- 42.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | 12.2       |
| time/                   |            |
|    total_timesteps      | 215000     |
| train/                  |            |
|    approx_kl            | 0.32575247 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.649     |
|    explained_variance   | 0.474      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0682    |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.066     |
|    value_loss           | 0.0599     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 9.63     |
| time/              |          |
|    fps             | 733      |
|    iterations      | 105      |
|    time_elapsed    | 293      |
|    total_timesteps | 215040   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 186       |
|    ep_rew_mean          | 9.77      |
| time/                   |           |
|    fps                  | 734       |
|    iterations           | 106       |
|    time_elapsed         | 295       |
|    total_timesteps      | 217088    |
| train/                  |           |
|    approx_kl            | 0.2405958 |
|    clip_fraction        | 0.484     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.724    |
|    explained_variance   | 0.591     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.102    |
|    n_updates            | 1050      |
|    policy_gradient_loss | -0.0713   |
|    value_loss           | 0.0555    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 193        |
|    ep_rew_mean          | 10.3       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 107        |
|    time_elapsed         | 297        |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.25801456 |
|    clip_fraction        | 0.5        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.669     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0848    |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0712    |
|    value_loss           | 0.0549     |
----------------------------------------
Eval num_timesteps=220000, episode_reward=9.70 +/- 3.49
Episode length: 169.30 +/- 47.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 169        |
|    mean_reward          | 9.7        |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.22340125 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.642     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0694    |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.0627    |
|    value_loss           | 0.0609     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 734      |
|    iterations      | 108      |
|    time_elapsed    | 301      |
|    total_timesteps | 221184   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 199        |
|    ep_rew_mean          | 10.7       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 109        |
|    time_elapsed         | 303        |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.26750225 |
|    clip_fraction        | 0.478      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.642     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.106     |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.0661    |
|    value_loss           | 0.058      |
----------------------------------------
Eval num_timesteps=225000, episode_reward=12.10 +/- 3.62
Episode length: 202.40 +/- 55.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | 12.1       |
| time/                   |            |
|    total_timesteps      | 225000     |
| train/                  |            |
|    approx_kl            | 0.32379353 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.573     |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.106     |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0664    |
|    value_loss           | 0.0593     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 199      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 734      |
|    iterations      | 110      |
|    time_elapsed    | 306      |
|    total_timesteps | 225280   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 196       |
|    ep_rew_mean          | 10.7      |
| time/                   |           |
|    fps                  | 735       |
|    iterations           | 111       |
|    time_elapsed         | 309       |
|    total_timesteps      | 227328    |
| train/                  |           |
|    approx_kl            | 0.3345006 |
|    clip_fraction        | 0.488     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.594    |
|    explained_variance   | 0.581     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0847   |
|    n_updates            | 1100      |
|    policy_gradient_loss | -0.0605   |
|    value_loss           | 0.057     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 195       |
|    ep_rew_mean          | 10.6      |
| time/                   |           |
|    fps                  | 736       |
|    iterations           | 112       |
|    time_elapsed         | 311       |
|    total_timesteps      | 229376    |
| train/                  |           |
|    approx_kl            | 0.2944528 |
|    clip_fraction        | 0.45      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.576    |
|    explained_variance   | 0.694     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0632   |
|    n_updates            | 1110      |
|    policy_gradient_loss | -0.0622   |
|    value_loss           | 0.0548    |
---------------------------------------
Eval num_timesteps=230000, episode_reward=10.00 +/- 3.38
Episode length: 202.70 +/- 49.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 203        |
|    mean_reward          | 10         |
| time/                   |            |
|    total_timesteps      | 230000     |
| train/                  |            |
|    approx_kl            | 0.31891167 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.619     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0895    |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0594    |
|    value_loss           | 0.0593     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 734      |
|    iterations      | 113      |
|    time_elapsed    | 314      |
|    total_timesteps | 231424   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 10.5       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 114        |
|    time_elapsed         | 317        |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.33426917 |
|    clip_fraction        | 0.493      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.595     |
|    explained_variance   | 0.528      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0808    |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.067     |
|    value_loss           | 0.0676     |
----------------------------------------
Eval num_timesteps=235000, episode_reward=11.00 +/- 3.00
Episode length: 192.10 +/- 48.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 192        |
|    mean_reward          | 11         |
| time/                   |            |
|    total_timesteps      | 235000     |
| train/                  |            |
|    approx_kl            | 0.26485932 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.582     |
|    explained_variance   | 0.51       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.059     |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.0638    |
|    value_loss           | 0.0649     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    fps             | 734      |
|    iterations      | 115      |
|    time_elapsed    | 320      |
|    total_timesteps | 235520   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 196       |
|    ep_rew_mean          | 10.5      |
| time/                   |           |
|    fps                  | 735       |
|    iterations           | 116       |
|    time_elapsed         | 322       |
|    total_timesteps      | 237568    |
| train/                  |           |
|    approx_kl            | 0.3591084 |
|    clip_fraction        | 0.448     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.554    |
|    explained_variance   | 0.424     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0413   |
|    n_updates            | 1150      |
|    policy_gradient_loss | -0.0562   |
|    value_loss           | 0.0713    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 198        |
|    ep_rew_mean          | 10.7       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 117        |
|    time_elapsed         | 325        |
|    total_timesteps      | 239616     |
| train/                  |            |
|    approx_kl            | 0.39781645 |
|    clip_fraction        | 0.476      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.56      |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.099     |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.0668    |
|    value_loss           | 0.0596     |
----------------------------------------
Eval num_timesteps=240000, episode_reward=11.20 +/- 4.19
Episode length: 192.30 +/- 52.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 192       |
|    mean_reward          | 11.2      |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.3383208 |
|    clip_fraction        | 0.451     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.548    |
|    explained_variance   | 0.578     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0988   |
|    n_updates            | 1170      |
|    policy_gradient_loss | -0.0584   |
|    value_loss           | 0.0619    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 118      |
|    time_elapsed    | 328      |
|    total_timesteps | 241664   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 194       |
|    ep_rew_mean          | 10.6      |
| time/                   |           |
|    fps                  | 736       |
|    iterations           | 119       |
|    time_elapsed         | 330       |
|    total_timesteps      | 243712    |
| train/                  |           |
|    approx_kl            | 0.3048511 |
|    clip_fraction        | 0.442     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.565    |
|    explained_variance   | 0.613     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0789   |
|    n_updates            | 1180      |
|    policy_gradient_loss | -0.0607   |
|    value_loss           | 0.0587    |
---------------------------------------
Eval num_timesteps=245000, episode_reward=12.60 +/- 3.14
Episode length: 216.40 +/- 43.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 12.6       |
| time/                   |            |
|    total_timesteps      | 245000     |
| train/                  |            |
|    approx_kl            | 0.25750837 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.55      |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0991    |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.0555    |
|    value_loss           | 0.0629     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 734      |
|    iterations      | 120      |
|    time_elapsed    | 334      |
|    total_timesteps | 245760   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | 10.4       |
| time/                   |            |
|    fps                  | 735        |
|    iterations           | 121        |
|    time_elapsed         | 336        |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.28337258 |
|    clip_fraction        | 0.427      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.545     |
|    explained_variance   | 0.711      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.104     |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0611    |
|    value_loss           | 0.0529     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 10.4       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 122        |
|    time_elapsed         | 339        |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.33761692 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.606     |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0883    |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.0627    |
|    value_loss           | 0.0587     |
----------------------------------------
Eval num_timesteps=250000, episode_reward=9.40 +/- 1.43
Episode length: 186.50 +/- 21.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 186        |
|    mean_reward          | 9.4        |
| time/                   |            |
|    total_timesteps      | 250000     |
| train/                  |            |
|    approx_kl            | 0.30301696 |
|    clip_fraction        | 0.482      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.615     |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0714    |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0586    |
|    value_loss           | 0.0568     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 10.3     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 123      |
|    time_elapsed    | 342      |
|    total_timesteps | 251904   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 186       |
|    ep_rew_mean          | 10.1      |
| time/                   |           |
|    fps                  | 736       |
|    iterations           | 124       |
|    time_elapsed         | 344       |
|    total_timesteps      | 253952    |
| train/                  |           |
|    approx_kl            | 0.3114619 |
|    clip_fraction        | 0.483     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.601    |
|    explained_variance   | 0.702     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0885   |
|    n_updates            | 1230      |
|    policy_gradient_loss | -0.0613   |
|    value_loss           | 0.0532    |
---------------------------------------
Eval num_timesteps=255000, episode_reward=10.00 +/- 2.86
Episode length: 181.80 +/- 55.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 182       |
|    mean_reward          | 10        |
| time/                   |           |
|    total_timesteps      | 255000    |
| train/                  |           |
|    approx_kl            | 0.3286925 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.597    |
|    explained_variance   | 0.681     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.074    |
|    n_updates            | 1240      |
|    policy_gradient_loss | -0.0558   |
|    value_loss           | 0.0575    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 9.91     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 125      |
|    time_elapsed    | 348      |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 183       |
|    ep_rew_mean          | 9.81      |
| time/                   |           |
|    fps                  | 736       |
|    iterations           | 126       |
|    time_elapsed         | 350       |
|    total_timesteps      | 258048    |
| train/                  |           |
|    approx_kl            | 0.3393296 |
|    clip_fraction        | 0.495     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.621    |
|    explained_variance   | 0.726     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.059    |
|    n_updates            | 1250      |
|    policy_gradient_loss | -0.0652   |
|    value_loss           | 0.0503    |
---------------------------------------
Eval num_timesteps=260000, episode_reward=9.60 +/- 2.24
Episode length: 175.20 +/- 33.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 175       |
|    mean_reward          | 9.6       |
| time/                   |           |
|    total_timesteps      | 260000    |
| train/                  |           |
|    approx_kl            | 0.3041996 |
|    clip_fraction        | 0.465     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.601    |
|    explained_variance   | 0.73      |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0898   |
|    n_updates            | 1260      |
|    policy_gradient_loss | -0.0615   |
|    value_loss           | 0.0497    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 9.63     |
| time/              |          |
|    fps             | 734      |
|    iterations      | 127      |
|    time_elapsed    | 353      |
|    total_timesteps | 260096   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 177        |
|    ep_rew_mean          | 9.27       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 128        |
|    time_elapsed         | 356        |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.27825704 |
|    clip_fraction        | 0.453      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.602     |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0709    |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.0567    |
|    value_loss           | 0.0561     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 175        |
|    ep_rew_mean          | 9.05       |
| time/                   |            |
|    fps                  | 737        |
|    iterations           | 129        |
|    time_elapsed         | 358        |
|    total_timesteps      | 264192     |
| train/                  |            |
|    approx_kl            | 0.30024117 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.624     |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0852    |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.0626    |
|    value_loss           | 0.0495     |
----------------------------------------
Eval num_timesteps=265000, episode_reward=9.80 +/- 2.48
Episode length: 175.70 +/- 38.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 176        |
|    mean_reward          | 9.8        |
| time/                   |            |
|    total_timesteps      | 265000     |
| train/                  |            |
|    approx_kl            | 0.32138228 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.621     |
|    explained_variance   | 0.7        |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0831    |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.0621    |
|    value_loss           | 0.0526     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 9.07     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 130      |
|    time_elapsed    | 361      |
|    total_timesteps | 266240   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 9.15       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 131        |
|    time_elapsed         | 364        |
|    total_timesteps      | 268288     |
| train/                  |            |
|    approx_kl            | 0.37966922 |
|    clip_fraction        | 0.48       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.628     |
|    explained_variance   | 0.592      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0776    |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.0585     |
----------------------------------------
Eval num_timesteps=270000, episode_reward=11.70 +/- 1.73
Episode length: 230.80 +/- 39.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 231        |
|    mean_reward          | 11.7       |
| time/                   |            |
|    total_timesteps      | 270000     |
| train/                  |            |
|    approx_kl            | 0.31497324 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.636     |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.108     |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.0601    |
|    value_loss           | 0.0568     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 9.19     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 132      |
|    time_elapsed    | 367      |
|    total_timesteps | 270336   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 176        |
|    ep_rew_mean          | 9.38       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 133        |
|    time_elapsed         | 370        |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.28587955 |
|    clip_fraction        | 0.505      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.627     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0625    |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.064     |
|    value_loss           | 0.0666     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 176       |
|    ep_rew_mean          | 9.5       |
| time/                   |           |
|    fps                  | 737       |
|    iterations           | 134       |
|    time_elapsed         | 372       |
|    total_timesteps      | 274432    |
| train/                  |           |
|    approx_kl            | 0.3003198 |
|    clip_fraction        | 0.495     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.609    |
|    explained_variance   | 0.617     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0558   |
|    n_updates            | 1330      |
|    policy_gradient_loss | -0.062    |
|    value_loss           | 0.0659    |
---------------------------------------
Eval num_timesteps=275000, episode_reward=10.80 +/- 3.40
Episode length: 196.90 +/- 56.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 197       |
|    mean_reward          | 10.8      |
| time/                   |           |
|    total_timesteps      | 275000    |
| train/                  |           |
|    approx_kl            | 0.3551649 |
|    clip_fraction        | 0.469     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.59     |
|    explained_variance   | 0.669     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0722   |
|    n_updates            | 1340      |
|    policy_gradient_loss | -0.0682   |
|    value_loss           | 0.0572    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 9.5      |
| time/              |          |
|    fps             | 735      |
|    iterations      | 135      |
|    time_elapsed    | 375      |
|    total_timesteps | 276480   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 177       |
|    ep_rew_mean          | 9.6       |
| time/                   |           |
|    fps                  | 736       |
|    iterations           | 136       |
|    time_elapsed         | 378       |
|    total_timesteps      | 278528    |
| train/                  |           |
|    approx_kl            | 0.2802848 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.59     |
|    explained_variance   | 0.597     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0778   |
|    n_updates            | 1350      |
|    policy_gradient_loss | -0.0686   |
|    value_loss           | 0.0638    |
---------------------------------------
Eval num_timesteps=280000, episode_reward=10.80 +/- 2.48
Episode length: 196.00 +/- 51.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 196        |
|    mean_reward          | 10.8       |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.29877535 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.592     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0885    |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0591    |
|    value_loss           | 0.065      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 9.88     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 137      |
|    time_elapsed    | 381      |
|    total_timesteps | 280576   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 184       |
|    ep_rew_mean          | 10.1      |
| time/                   |           |
|    fps                  | 736       |
|    iterations           | 138       |
|    time_elapsed         | 383       |
|    total_timesteps      | 282624    |
| train/                  |           |
|    approx_kl            | 0.3461089 |
|    clip_fraction        | 0.471     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.579    |
|    explained_variance   | 0.651     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0717   |
|    n_updates            | 1370      |
|    policy_gradient_loss | -0.0578   |
|    value_loss           | 0.0606    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 189        |
|    ep_rew_mean          | 10.3       |
| time/                   |            |
|    fps                  | 737        |
|    iterations           | 139        |
|    time_elapsed         | 386        |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.29505897 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.545     |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0946    |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0624    |
|    value_loss           | 0.067      |
----------------------------------------
Eval num_timesteps=285000, episode_reward=13.60 +/- 3.17
Episode length: 227.80 +/- 51.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 13.6       |
| time/                   |            |
|    total_timesteps      | 285000     |
| train/                  |            |
|    approx_kl            | 0.34293658 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.55      |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0924    |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.0608    |
|    value_loss           | 0.0586     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 140      |
|    time_elapsed    | 389      |
|    total_timesteps | 286720   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 10.6       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 141        |
|    time_elapsed         | 392        |
|    total_timesteps      | 288768     |
| train/                  |            |
|    approx_kl            | 0.33289057 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.551     |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0518    |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.0565    |
|    value_loss           | 0.0643     |
----------------------------------------
Eval num_timesteps=290000, episode_reward=9.50 +/- 3.23
Episode length: 175.10 +/- 49.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 175        |
|    mean_reward          | 9.5        |
| time/                   |            |
|    total_timesteps      | 290000     |
| train/                  |            |
|    approx_kl            | 0.30532938 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.56      |
|    explained_variance   | 0.663      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0847    |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.0615    |
|    value_loss           | 0.0635     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 10.6     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 142      |
|    time_elapsed    | 395      |
|    total_timesteps | 290816   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 190       |
|    ep_rew_mean          | 10.7      |
| time/                   |           |
|    fps                  | 736       |
|    iterations           | 143       |
|    time_elapsed         | 397       |
|    total_timesteps      | 292864    |
| train/                  |           |
|    approx_kl            | 0.3618206 |
|    clip_fraction        | 0.453     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.507    |
|    explained_variance   | 0.595     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0693   |
|    n_updates            | 1420      |
|    policy_gradient_loss | -0.0588   |
|    value_loss           | 0.0682    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 10.6       |
| time/                   |            |
|    fps                  | 737        |
|    iterations           | 144        |
|    time_elapsed         | 399        |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.33997306 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.556     |
|    explained_variance   | 0.702      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.1       |
|    n_updates            | 1430       |
|    policy_gradient_loss | -0.0596    |
|    value_loss           | 0.0581     |
----------------------------------------
Eval num_timesteps=295000, episode_reward=10.70 +/- 2.76
Episode length: 181.20 +/- 31.23
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 181        |
|    mean_reward          | 10.7       |
| time/                   |            |
|    total_timesteps      | 295000     |
| train/                  |            |
|    approx_kl            | 0.29634282 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.601     |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0663    |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.058     |
|    value_loss           | 0.0588     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 189      |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    fps             | 736      |
|    iterations      | 145      |
|    time_elapsed    | 403      |
|    total_timesteps | 296960   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 185       |
|    ep_rew_mean          | 10.3      |
| time/                   |           |
|    fps                  | 737       |
|    iterations           | 146       |
|    time_elapsed         | 405       |
|    total_timesteps      | 299008    |
| train/                  |           |
|    approx_kl            | 0.3454288 |
|    clip_fraction        | 0.494     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.59     |
|    explained_variance   | 0.614     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0442   |
|    n_updates            | 1450      |
|    policy_gradient_loss | -0.0618   |
|    value_loss           | 0.0721    |
---------------------------------------
Eval num_timesteps=300000, episode_reward=12.50 +/- 2.80
Episode length: 215.80 +/- 37.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 12.5       |
| time/                   |            |
|    total_timesteps      | 300000     |
| train/                  |            |
|    approx_kl            | 0.42229816 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.561     |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0785    |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.0587    |
|    value_loss           | 0.0611     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 147      |
|    time_elapsed    | 409      |
|    total_timesteps | 301056   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 186        |
|    ep_rew_mean          | 10.5       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 148        |
|    time_elapsed         | 411        |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.43655747 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.526     |
|    explained_variance   | 0.663      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0715    |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.0619    |
|    value_loss           | 0.0533     |
----------------------------------------
Eval num_timesteps=305000, episode_reward=12.00 +/- 3.35
Episode length: 202.40 +/- 45.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 202       |
|    mean_reward          | 12        |
| time/                   |           |
|    total_timesteps      | 305000    |
| train/                  |           |
|    approx_kl            | 0.3784358 |
|    clip_fraction        | 0.458     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.524    |
|    explained_variance   | 0.722     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0869   |
|    n_updates            | 1480      |
|    policy_gradient_loss | -0.0641   |
|    value_loss           | 0.0525    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 149      |
|    time_elapsed    | 415      |
|    total_timesteps | 305152   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 191      |
|    ep_rew_mean          | 10.6     |
| time/                   |          |
|    fps                  | 736      |
|    iterations           | 150      |
|    time_elapsed         | 417      |
|    total_timesteps      | 307200   |
| train/                  |          |
|    approx_kl            | 0.350132 |
|    clip_fraction        | 0.459    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.512   |
|    explained_variance   | 0.664    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0982  |
|    n_updates            | 1490     |
|    policy_gradient_loss | -0.0612  |
|    value_loss           | 0.0594   |
--------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 196       |
|    ep_rew_mean          | 10.8      |
| time/                   |           |
|    fps                  | 736       |
|    iterations           | 151       |
|    time_elapsed         | 419       |
|    total_timesteps      | 309248    |
| train/                  |           |
|    approx_kl            | 0.3658192 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.544    |
|    explained_variance   | 0.585     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0576   |
|    n_updates            | 1500      |
|    policy_gradient_loss | -0.0422   |
|    value_loss           | 0.0641    |
---------------------------------------
Eval num_timesteps=310000, episode_reward=11.20 +/- 4.45
Episode length: 204.90 +/- 70.77
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 205      |
|    mean_reward          | 11.2     |
| time/                   |          |
|    total_timesteps      | 310000   |
| train/                  |          |
|    approx_kl            | 0.333921 |
|    clip_fraction        | 0.465    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.563   |
|    explained_variance   | 0.657    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0542  |
|    n_updates            | 1510     |
|    policy_gradient_loss | -0.0575  |
|    value_loss           | 0.06     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 10.7     |
| time/              |          |
|    fps             | 735      |
|    iterations      | 152      |
|    time_elapsed    | 423      |
|    total_timesteps | 311296   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 197        |
|    ep_rew_mean          | 10.6       |
| time/                   |            |
|    fps                  | 736        |
|    iterations           | 153        |
|    time_elapsed         | 425        |
|    total_timesteps      | 313344     |
| train/                  |            |
|    approx_kl            | 0.35942125 |
|    clip_fraction        | 0.483      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.599     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.065     |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0605    |
|    value_loss           | 0.0656     |
----------------------------------------
Eval num_timesteps=315000, episode_reward=2.60 +/- 1.91
Episode length: 91.40 +/- 23.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 91.4       |
|    mean_reward          | 2.6        |
| time/                   |            |
|    total_timesteps      | 315000     |
| train/                  |            |
|    approx_kl            | 0.36478692 |
|    clip_fraction        | 0.513      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.617     |
|    explained_variance   | 0.504      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0813    |
|    n_updates            | 1530       |
|    policy_gradient_loss | -0.0602    |
|    value_loss           | 0.0698     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | 10.6     |
| time/              |          |
|    fps             | 736      |
|    iterations      | 154      |
|    time_elapsed    | 428      |
|    total_timesteps | 315392   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 198       |
|    ep_rew_mean          | 10.6      |
| time/                   |           |
|    fps                  | 737       |
|    iterations           | 155       |
|    time_elapsed         | 430       |
|    total_timesteps      | 317440    |
| train/                  |           |
|    approx_kl            | 0.4419374 |
|    clip_fraction        | 0.5       |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.636    |
|    explained_variance   | 0.559     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0696   |
|    n_updates            | 1540      |
|    policy_gradient_loss | -0.0647   |
|    value_loss           | 0.0698    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 200        |
|    ep_rew_mean          | 10.7       |
| time/                   |            |
|    fps                  | 738        |
|    iterations           | 156        |
|    time_elapsed         | 432        |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.44300526 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.608     |
|    explained_variance   | 0.53       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0842    |
|    n_updates            | 1550       |
|    policy_gradient_loss | -0.0586    |
|    value_loss           | 0.0693     |
----------------------------------------
Eval num_timesteps=320000, episode_reward=8.40 +/- 1.74
Episode length: 162.30 +/- 23.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 162        |
|    mean_reward          | 8.4        |
| time/                   |            |
|    total_timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.34168756 |
|    clip_fraction        | 0.49       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.617     |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0872    |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.0646    |
|    value_loss           | 0.0701     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 10.5     |
| time/              |          |
|    fps             | 737      |
|    iterations      | 157      |
|    time_elapsed    | 436      |
|    total_timesteps | 321536   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 194        |
|    ep_rew_mean          | 10.4       |
| time/                   |            |
|    fps                  | 738        |
|    iterations           | 158        |
|    time_elapsed         | 438        |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.36997354 |
|    clip_fraction        | 0.507      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.595     |
|    explained_variance   | 0.546      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0876    |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.0663    |
|    value_loss           | 0.0657     |
----------------------------------------
Eval num_timesteps=325000, episode_reward=9.70 +/- 2.76
Episode length: 188.80 +/- 49.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 189        |
|    mean_reward          | 9.7        |
| time/                   |            |
|    total_timesteps      | 325000     |
| train/                  |            |
|    approx_kl            | 0.45630902 |
|    clip_fraction        | 0.469      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.561     |
|    explained_variance   | 0.45       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0786    |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0611    |
|    value_loss           | 0.068      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 736      |
|    iterations      | 159      |
|    time_elapsed    | 441      |
|    total_timesteps | 325632   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 184       |
|    ep_rew_mean          | 9.93      |
| time/                   |           |
|    fps                  | 737       |
|    iterations           | 160       |
|    time_elapsed         | 444       |
|    total_timesteps      | 327680    |
| train/                  |           |
|    approx_kl            | 0.3692017 |
|    clip_fraction        | 0.478     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.56     |
|    explained_variance   | 0.544     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0754   |
|    n_updates            | 1590      |
|    policy_gradient_loss | -0.0574   |
|    value_loss           | 0.0671    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 186        |
|    ep_rew_mean          | 10.1       |
| time/                   |            |
|    fps                  | 738        |
|    iterations           | 161        |
|    time_elapsed         | 446        |
|    total_timesteps      | 329728     |
| train/                  |            |
|    approx_kl            | 0.38929832 |
|    clip_fraction        | 0.484      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.598     |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0752    |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.0593    |
|    value_loss           | 0.0646     |
----------------------------------------
Eval num_timesteps=330000, episode_reward=9.80 +/- 2.79
Episode length: 179.90 +/- 39.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 180        |
|    mean_reward          | 9.8        |
| time/                   |            |
|    total_timesteps      | 330000     |
| train/                  |            |
|    approx_kl            | 0.37591374 |
|    clip_fraction        | 0.457      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.549     |
|    explained_variance   | 0.6        |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0877    |
|    n_updates            | 1610       |
|    policy_gradient_loss | -0.0618    |
|    value_loss           | 0.0593     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 9.92     |
| time/              |          |
|    fps             | 737      |
|    iterations      | 162      |
|    time_elapsed    | 449      |
|    total_timesteps | 331776   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 181       |
|    ep_rew_mean          | 9.87      |
| time/                   |           |
|    fps                  | 738       |
|    iterations           | 163       |
|    time_elapsed         | 452       |
|    total_timesteps      | 333824    |
| train/                  |           |
|    approx_kl            | 0.3511148 |
|    clip_fraction        | 0.463     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.537    |
|    explained_variance   | 0.517     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0533   |
|    n_updates            | 1620      |
|    policy_gradient_loss | -0.0536   |
|    value_loss           | 0.0696    |
---------------------------------------
Eval num_timesteps=335000, episode_reward=8.90 +/- 3.51
Episode length: 169.40 +/- 49.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 169        |
|    mean_reward          | 8.9        |
| time/                   |            |
|    total_timesteps      | 335000     |
| train/                  |            |
|    approx_kl            | 0.36170447 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.542     |
|    explained_variance   | 0.696      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0711    |
|    n_updates            | 1630       |
|    policy_gradient_loss | -0.0563    |
|    value_loss           | 0.0606     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 9.85     |
| time/              |          |
|    fps             | 737      |
|    iterations      | 164      |
|    time_elapsed    | 455      |
|    total_timesteps | 335872   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 179       |
|    ep_rew_mean          | 9.81      |
| time/                   |           |
|    fps                  | 738       |
|    iterations           | 165       |
|    time_elapsed         | 457       |
|    total_timesteps      | 337920    |
| train/                  |           |
|    approx_kl            | 0.3955875 |
|    clip_fraction        | 0.464     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.527    |
|    explained_variance   | 0.548     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0959   |
|    n_updates            | 1640      |
|    policy_gradient_loss | -0.0623   |
|    value_loss           | 0.0668    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 177        |
|    ep_rew_mean          | 9.74       |
| time/                   |            |
|    fps                  | 739        |
|    iterations           | 166        |
|    time_elapsed         | 459        |
|    total_timesteps      | 339968     |
| train/                  |            |
|    approx_kl            | 0.39365405 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.498     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0736    |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.0571    |
|    value_loss           | 0.0598     |
----------------------------------------
Eval num_timesteps=340000, episode_reward=8.80 +/- 2.44
Episode length: 169.10 +/- 39.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 169       |
|    mean_reward          | 8.8       |
| time/                   |           |
|    total_timesteps      | 340000    |
| train/                  |           |
|    approx_kl            | 0.3282886 |
|    clip_fraction        | 0.428     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.495    |
|    explained_variance   | 0.676     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0762   |
|    n_updates            | 1660      |
|    policy_gradient_loss | -0.0577   |
|    value_loss           | 0.0518    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 9.64     |
| time/              |          |
|    fps             | 738      |
|    iterations      | 167      |
|    time_elapsed    | 463      |
|    total_timesteps | 342016   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 9.45       |
| time/                   |            |
|    fps                  | 739        |
|    iterations           | 168        |
|    time_elapsed         | 465        |
|    total_timesteps      | 344064     |
| train/                  |            |
|    approx_kl            | 0.35180038 |
|    clip_fraction        | 0.431      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.5       |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0639    |
|    n_updates            | 1670       |
|    policy_gradient_loss | -0.0494    |
|    value_loss           | 0.0565     |
----------------------------------------
Eval num_timesteps=345000, episode_reward=8.00 +/- 2.90
Episode length: 164.70 +/- 38.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 165        |
|    mean_reward          | 8          |
| time/                   |            |
|    total_timesteps      | 345000     |
| train/                  |            |
|    approx_kl            | 0.43424428 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.549     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0911    |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.0633    |
|    value_loss           | 0.0586     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 9.33     |
| time/              |          |
|    fps             | 738      |
|    iterations      | 169      |
|    time_elapsed    | 468      |
|    total_timesteps | 346112   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 171       |
|    ep_rew_mean          | 9.47      |
| time/                   |           |
|    fps                  | 738       |
|    iterations           | 170       |
|    time_elapsed         | 471       |
|    total_timesteps      | 348160    |
| train/                  |           |
|    approx_kl            | 0.3799071 |
|    clip_fraction        | 0.443     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.514    |
|    explained_variance   | 0.659     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0714   |
|    n_updates            | 1690      |
|    policy_gradient_loss | -0.0582   |
|    value_loss           | 0.0616    |
---------------------------------------
Eval num_timesteps=350000, episode_reward=5.80 +/- 1.08
Episode length: 134.10 +/- 22.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 134        |
|    mean_reward          | 5.8        |
| time/                   |            |
|    total_timesteps      | 350000     |
| train/                  |            |
|    approx_kl            | 0.35868132 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.539     |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0768    |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0606    |
|    value_loss           | 0.0569     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 168      |
|    ep_rew_mean     | 9.38     |
| time/              |          |
|    fps             | 738      |
|    iterations      | 171      |
|    time_elapsed    | 474      |
|    total_timesteps | 350208   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 165       |
|    ep_rew_mean          | 9.19      |
| time/                   |           |
|    fps                  | 739       |
|    iterations           | 172       |
|    time_elapsed         | 476       |
|    total_timesteps      | 352256    |
| train/                  |           |
|    approx_kl            | 0.4259936 |
|    clip_fraction        | 0.472     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.531    |
|    explained_variance   | 0.603     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.101    |
|    n_updates            | 1710      |
|    policy_gradient_loss | -0.0636   |
|    value_loss           | 0.0609    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 164        |
|    ep_rew_mean          | 9.04       |
| time/                   |            |
|    fps                  | 739        |
|    iterations           | 173        |
|    time_elapsed         | 478        |
|    total_timesteps      | 354304     |
| train/                  |            |
|    approx_kl            | 0.41367573 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.546     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0692    |
|    n_updates            | 1720       |
|    policy_gradient_loss | -0.0594    |
|    value_loss           | 0.0679     |
----------------------------------------
Eval num_timesteps=355000, episode_reward=8.10 +/- 2.17
Episode length: 161.50 +/- 42.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 162        |
|    mean_reward          | 8.1        |
| time/                   |            |
|    total_timesteps      | 355000     |
| train/                  |            |
|    approx_kl            | 0.39245647 |
|    clip_fraction        | 0.515      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.589     |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0984    |
|    n_updates            | 1730       |
|    policy_gradient_loss | -0.0592    |
|    value_loss           | 0.0666     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 8.66     |
| time/              |          |
|    fps             | 739      |
|    iterations      | 174      |
|    time_elapsed    | 482      |
|    total_timesteps | 356352   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 160        |
|    ep_rew_mean          | 8.33       |
| time/                   |            |
|    fps                  | 739        |
|    iterations           | 175        |
|    time_elapsed         | 484        |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.34823132 |
|    clip_fraction        | 0.51       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.613     |
|    explained_variance   | 0.559      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0809    |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0613    |
|    value_loss           | 0.0676     |
----------------------------------------
Eval num_timesteps=360000, episode_reward=9.50 +/- 4.15
Episode length: 172.60 +/- 48.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 173       |
|    mean_reward          | 9.5       |
| time/                   |           |
|    total_timesteps      | 360000    |
| train/                  |           |
|    approx_kl            | 0.3967741 |
|    clip_fraction        | 0.491     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.592    |
|    explained_variance   | 0.581     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0803   |
|    n_updates            | 1750      |
|    policy_gradient_loss | -0.0597   |
|    value_loss           | 0.0645    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 8.12     |
| time/              |          |
|    fps             | 739      |
|    iterations      | 176      |
|    time_elapsed    | 487      |
|    total_timesteps | 360448   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 156        |
|    ep_rew_mean          | 7.84       |
| time/                   |            |
|    fps                  | 739        |
|    iterations           | 177        |
|    time_elapsed         | 489        |
|    total_timesteps      | 362496     |
| train/                  |            |
|    approx_kl            | 0.50471413 |
|    clip_fraction        | 0.489      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.57      |
|    explained_variance   | 0.504      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0988    |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.059     |
|    value_loss           | 0.0735     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 158        |
|    ep_rew_mean          | 7.85       |
| time/                   |            |
|    fps                  | 740        |
|    iterations           | 178        |
|    time_elapsed         | 492        |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.47439253 |
|    clip_fraction        | 0.497      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.552     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.078     |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.0637    |
|    value_loss           | 0.0696     |
----------------------------------------
Eval num_timesteps=365000, episode_reward=10.40 +/- 3.56
Episode length: 195.40 +/- 56.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 195        |
|    mean_reward          | 10.4       |
| time/                   |            |
|    total_timesteps      | 365000     |
| train/                  |            |
|    approx_kl            | 0.39788476 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.527     |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0722    |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0534    |
|    value_loss           | 0.0656     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 8.02     |
| time/              |          |
|    fps             | 739      |
|    iterations      | 179      |
|    time_elapsed    | 495      |
|    total_timesteps | 366592   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 162        |
|    ep_rew_mean          | 8.04       |
| time/                   |            |
|    fps                  | 740        |
|    iterations           | 180        |
|    time_elapsed         | 497        |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.39263064 |
|    clip_fraction        | 0.479      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.577     |
|    explained_variance   | 0.53       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0763    |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.0645    |
|    value_loss           | 0.0746     |
----------------------------------------
Eval num_timesteps=370000, episode_reward=9.40 +/- 2.80
Episode length: 163.20 +/- 36.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 163        |
|    mean_reward          | 9.4        |
| time/                   |            |
|    total_timesteps      | 370000     |
| train/                  |            |
|    approx_kl            | 0.39082983 |
|    clip_fraction        | 0.485      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.565     |
|    explained_variance   | 0.545      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0685    |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.0622    |
|    value_loss           | 0.0673     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 163      |
|    ep_rew_mean     | 8.13     |
| time/              |          |
|    fps             | 739      |
|    iterations      | 181      |
|    time_elapsed    | 501      |
|    total_timesteps | 370688   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 168        |
|    ep_rew_mean          | 8.52       |
| time/                   |            |
|    fps                  | 740        |
|    iterations           | 182        |
|    time_elapsed         | 503        |
|    total_timesteps      | 372736     |
| train/                  |            |
|    approx_kl            | 0.35535565 |
|    clip_fraction        | 0.479      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.564     |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0812    |
|    n_updates            | 1810       |
|    policy_gradient_loss | -0.0606    |
|    value_loss           | 0.0629     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 171       |
|    ep_rew_mean          | 8.96      |
| time/                   |           |
|    fps                  | 740       |
|    iterations           | 183       |
|    time_elapsed         | 505       |
|    total_timesteps      | 374784    |
| train/                  |           |
|    approx_kl            | 0.4134739 |
|    clip_fraction        | 0.458     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.539    |
|    explained_variance   | 0.575     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0741   |
|    n_updates            | 1820      |
|    policy_gradient_loss | -0.0556   |
|    value_loss           | 0.066     |
---------------------------------------
Eval num_timesteps=375000, episode_reward=11.20 +/- 2.64
Episode length: 204.70 +/- 43.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 205        |
|    mean_reward          | 11.2       |
| time/                   |            |
|    total_timesteps      | 375000     |
| train/                  |            |
|    approx_kl            | 0.46957058 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.491     |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0293    |
|    n_updates            | 1830       |
|    policy_gradient_loss | -0.0471    |
|    value_loss           | 0.057      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 9.25     |
| time/              |          |
|    fps             | 739      |
|    iterations      | 184      |
|    time_elapsed    | 509      |
|    total_timesteps | 376832   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 177        |
|    ep_rew_mean          | 9.68       |
| time/                   |            |
|    fps                  | 740        |
|    iterations           | 185        |
|    time_elapsed         | 511        |
|    total_timesteps      | 378880     |
| train/                  |            |
|    approx_kl            | 0.48989862 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.482     |
|    explained_variance   | 0.678      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0304    |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.0673     |
----------------------------------------
Eval num_timesteps=380000, episode_reward=10.20 +/- 3.74
Episode length: 181.30 +/- 40.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 181        |
|    mean_reward          | 10.2       |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.38933912 |
|    clip_fraction        | 0.452      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.501     |
|    explained_variance   | 0.573      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.082     |
|    n_updates            | 1850       |
|    policy_gradient_loss | -0.0522    |
|    value_loss           | 0.0689     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 177      |
|    ep_rew_mean     | 9.8      |
| time/              |          |
|    fps             | 739      |
|    iterations      | 186      |
|    time_elapsed    | 515      |
|    total_timesteps | 380928   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 10.3       |
| time/                   |            |
|    fps                  | 740        |
|    iterations           | 187        |
|    time_elapsed         | 517        |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.40397894 |
|    clip_fraction        | 0.439      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.488     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0734    |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0577    |
|    value_loss           | 0.0645     |
----------------------------------------
Eval num_timesteps=385000, episode_reward=13.40 +/- 4.43
Episode length: 221.40 +/- 64.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 221        |
|    mean_reward          | 13.4       |
| time/                   |            |
|    total_timesteps      | 385000     |
| train/                  |            |
|    approx_kl            | 0.33260053 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.527     |
|    explained_variance   | 0.678      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0448    |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.0523    |
|    value_loss           | 0.0666     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 10.4     |
| time/              |          |
|    fps             | 739      |
|    iterations      | 188      |
|    time_elapsed    | 520      |
|    total_timesteps | 385024   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 10.8       |
| time/                   |            |
|    fps                  | 739        |
|    iterations           | 189        |
|    time_elapsed         | 523        |
|    total_timesteps      | 387072     |
| train/                  |            |
|    approx_kl            | 0.45353007 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.486     |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0926    |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.0611    |
|    value_loss           | 0.0549     |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 193       |
|    ep_rew_mean          | 10.9      |
| time/                   |           |
|    fps                  | 740       |
|    iterations           | 190       |
|    time_elapsed         | 525       |
|    total_timesteps      | 389120    |
| train/                  |           |
|    approx_kl            | 0.3895731 |
|    clip_fraction        | 0.413     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.497    |
|    explained_variance   | 0.667     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.059    |
|    n_updates            | 1890      |
|    policy_gradient_loss | -0.0571   |
|    value_loss           | 0.0578    |
---------------------------------------
Eval num_timesteps=390000, episode_reward=8.40 +/- 3.90
Episode length: 164.80 +/- 56.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 165       |
|    mean_reward          | 8.4       |
| time/                   |           |
|    total_timesteps      | 390000    |
| train/                  |           |
|    approx_kl            | 0.3535712 |
|    clip_fraction        | 0.395     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.489    |
|    explained_variance   | 0.752     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0655   |
|    n_updates            | 1900      |
|    policy_gradient_loss | -0.0481   |
|    value_loss           | 0.0534    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 11       |
| time/              |          |
|    fps             | 739      |
|    iterations      | 191      |
|    time_elapsed    | 528      |
|    total_timesteps | 391168   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 192       |
|    ep_rew_mean          | 10.8      |
| time/                   |           |
|    fps                  | 740       |
|    iterations           | 192       |
|    time_elapsed         | 530       |
|    total_timesteps      | 393216    |
| train/                  |           |
|    approx_kl            | 0.4503171 |
|    clip_fraction        | 0.445     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.512    |
|    explained_variance   | 0.633     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0898   |
|    n_updates            | 1910      |
|    policy_gradient_loss | -0.0525   |
|    value_loss           | 0.0623    |
---------------------------------------
Eval num_timesteps=395000, episode_reward=9.90 +/- 2.88
Episode length: 182.20 +/- 34.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 182        |
|    mean_reward          | 9.9        |
| time/                   |            |
|    total_timesteps      | 395000     |
| train/                  |            |
|    approx_kl            | 0.33231935 |
|    clip_fraction        | 0.426      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.494     |
|    explained_variance   | 0.627      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0699    |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.0604    |
|    value_loss           | 0.0634     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 739      |
|    iterations      | 193      |
|    time_elapsed    | 534      |
|    total_timesteps | 395264   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 192        |
|    ep_rew_mean          | 10.7       |
| time/                   |            |
|    fps                  | 740        |
|    iterations           | 194        |
|    time_elapsed         | 536        |
|    total_timesteps      | 397312     |
| train/                  |            |
|    approx_kl            | 0.35570323 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.502     |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0863    |
|    n_updates            | 1930       |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.0657     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 10.7       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 195        |
|    time_elapsed         | 538        |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.40462953 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.514     |
|    explained_variance   | 0.711      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.058     |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.0595    |
|    value_loss           | 0.0531     |
----------------------------------------
Eval num_timesteps=400000, episode_reward=8.10 +/- 2.47
Episode length: 157.60 +/- 34.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 158        |
|    mean_reward          | 8.1        |
| time/                   |            |
|    total_timesteps      | 400000     |
| train/                  |            |
|    approx_kl            | 0.36434346 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.523     |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0904    |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.0581    |
|    value_loss           | 0.06       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 10.6     |
| time/              |          |
|    fps             | 740      |
|    iterations      | 196      |
|    time_elapsed    | 542      |
|    total_timesteps | 401408   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 186        |
|    ep_rew_mean          | 10.3       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 197        |
|    time_elapsed         | 544        |
|    total_timesteps      | 403456     |
| train/                  |            |
|    approx_kl            | 0.46036804 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.493     |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0667    |
|    n_updates            | 1960       |
|    policy_gradient_loss | -0.0604    |
|    value_loss           | 0.0644     |
----------------------------------------
Eval num_timesteps=405000, episode_reward=10.20 +/- 2.99
Episode length: 183.50 +/- 43.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 184        |
|    mean_reward          | 10.2       |
| time/                   |            |
|    total_timesteps      | 405000     |
| train/                  |            |
|    approx_kl            | 0.43118533 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.475     |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0915    |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.0479    |
|    value_loss           | 0.0615     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 9.78     |
| time/              |          |
|    fps             | 740      |
|    iterations      | 198      |
|    time_elapsed    | 547      |
|    total_timesteps | 405504   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 179        |
|    ep_rew_mean          | 9.74       |
| time/                   |            |
|    fps                  | 740        |
|    iterations           | 199        |
|    time_elapsed         | 550        |
|    total_timesteps      | 407552     |
| train/                  |            |
|    approx_kl            | 0.39911276 |
|    clip_fraction        | 0.468      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.512     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0631    |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.058     |
|    value_loss           | 0.0601     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 179        |
|    ep_rew_mean          | 9.87       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 200        |
|    time_elapsed         | 552        |
|    total_timesteps      | 409600     |
| train/                  |            |
|    approx_kl            | 0.43482023 |
|    clip_fraction        | 0.479      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.515     |
|    explained_variance   | 0.64       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0789    |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.066     |
|    value_loss           | 0.0546     |
----------------------------------------
Eval num_timesteps=410000, episode_reward=9.60 +/- 4.67
Episode length: 178.20 +/- 74.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 178        |
|    mean_reward          | 9.6        |
| time/                   |            |
|    total_timesteps      | 410000     |
| train/                  |            |
|    approx_kl            | 0.34911317 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.522     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0681    |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.0565    |
|    value_loss           | 0.0754     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 740      |
|    iterations      | 201      |
|    time_elapsed    | 555      |
|    total_timesteps | 411648   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 186        |
|    ep_rew_mean          | 10.3       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 202        |
|    time_elapsed         | 557        |
|    total_timesteps      | 413696     |
| train/                  |            |
|    approx_kl            | 0.42810827 |
|    clip_fraction        | 0.448      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.483     |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.058     |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.0584    |
|    value_loss           | 0.0639     |
----------------------------------------
Eval num_timesteps=415000, episode_reward=11.90 +/- 2.91
Episode length: 206.10 +/- 42.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 206        |
|    mean_reward          | 11.9       |
| time/                   |            |
|    total_timesteps      | 415000     |
| train/                  |            |
|    approx_kl            | 0.36183363 |
|    clip_fraction        | 0.442      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.497     |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0845    |
|    n_updates            | 2020       |
|    policy_gradient_loss | -0.0503    |
|    value_loss           | 0.0759     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    fps             | 740      |
|    iterations      | 203      |
|    time_elapsed    | 561      |
|    total_timesteps | 415744   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 194       |
|    ep_rew_mean          | 11        |
| time/                   |           |
|    fps                  | 741       |
|    iterations           | 204       |
|    time_elapsed         | 563       |
|    total_timesteps      | 417792    |
| train/                  |           |
|    approx_kl            | 0.2987532 |
|    clip_fraction        | 0.438     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.496    |
|    explained_variance   | 0.617     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0588   |
|    n_updates            | 2030      |
|    policy_gradient_loss | -0.0433   |
|    value_loss           | 0.0689    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 196        |
|    ep_rew_mean          | 11.1       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 205        |
|    time_elapsed         | 565        |
|    total_timesteps      | 419840     |
| train/                  |            |
|    approx_kl            | 0.36484903 |
|    clip_fraction        | 0.44       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.499     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0748    |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.0576    |
|    value_loss           | 0.0684     |
----------------------------------------
Eval num_timesteps=420000, episode_reward=11.80 +/- 4.51
Episode length: 206.10 +/- 55.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 206        |
|    mean_reward          | 11.8       |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.40727937 |
|    clip_fraction        | 0.447      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.473     |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0993    |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.0597    |
|    value_loss           | 0.0633     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 11.2     |
| time/              |          |
|    fps             | 740      |
|    iterations      | 206      |
|    time_elapsed    | 569      |
|    total_timesteps | 421888   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 201        |
|    ep_rew_mean          | 11.6       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 207        |
|    time_elapsed         | 571        |
|    total_timesteps      | 423936     |
| train/                  |            |
|    approx_kl            | 0.37891304 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.458     |
|    explained_variance   | 0.699      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0319    |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.052     |
|    value_loss           | 0.0629     |
----------------------------------------
Eval num_timesteps=425000, episode_reward=11.90 +/- 3.45
Episode length: 207.50 +/- 38.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 208        |
|    mean_reward          | 11.9       |
| time/                   |            |
|    total_timesteps      | 425000     |
| train/                  |            |
|    approx_kl            | 0.35715312 |
|    clip_fraction        | 0.403      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.464     |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0826    |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.0533    |
|    value_loss           | 0.0697     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 208      |
|    ep_rew_mean     | 12       |
| time/              |          |
|    fps             | 740      |
|    iterations      | 208      |
|    time_elapsed    | 575      |
|    total_timesteps | 425984   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 210        |
|    ep_rew_mean          | 12.2       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 209        |
|    time_elapsed         | 577        |
|    total_timesteps      | 428032     |
| train/                  |            |
|    approx_kl            | 0.40479067 |
|    clip_fraction        | 0.406      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.444     |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0549    |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.0499    |
|    value_loss           | 0.0519     |
----------------------------------------
Eval num_timesteps=430000, episode_reward=12.70 +/- 4.24
Episode length: 215.70 +/- 63.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 216       |
|    mean_reward          | 12.7      |
| time/                   |           |
|    total_timesteps      | 430000    |
| train/                  |           |
|    approx_kl            | 0.4437967 |
|    clip_fraction        | 0.38      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.414    |
|    explained_variance   | 0.615     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.07     |
|    n_updates            | 2090      |
|    policy_gradient_loss | -0.0536   |
|    value_loss           | 0.0668    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 12.5     |
| time/              |          |
|    fps             | 740      |
|    iterations      | 210      |
|    time_elapsed    | 581      |
|    total_timesteps | 430080   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 212        |
|    ep_rew_mean          | 12.5       |
| time/                   |            |
|    fps                  | 740        |
|    iterations           | 211        |
|    time_elapsed         | 583        |
|    total_timesteps      | 432128     |
| train/                  |            |
|    approx_kl            | 0.35291255 |
|    clip_fraction        | 0.411      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.452     |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0609    |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.0531    |
|    value_loss           | 0.0549     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 214        |
|    ep_rew_mean          | 12.5       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 212        |
|    time_elapsed         | 585        |
|    total_timesteps      | 434176     |
| train/                  |            |
|    approx_kl            | 0.49337363 |
|    clip_fraction        | 0.432      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.471     |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0731    |
|    n_updates            | 2110       |
|    policy_gradient_loss | -0.0551    |
|    value_loss           | 0.0593     |
----------------------------------------
Eval num_timesteps=435000, episode_reward=7.30 +/- 2.61
Episode length: 148.20 +/- 32.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 148        |
|    mean_reward          | 7.3        |
| time/                   |            |
|    total_timesteps      | 435000     |
| train/                  |            |
|    approx_kl            | 0.42348334 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.519     |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.092     |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0519    |
|    value_loss           | 0.0615     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 12.3     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 213      |
|    time_elapsed    | 588      |
|    total_timesteps | 436224   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 207        |
|    ep_rew_mean          | 12         |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 214        |
|    time_elapsed         | 590        |
|    total_timesteps      | 438272     |
| train/                  |            |
|    approx_kl            | 0.38795596 |
|    clip_fraction        | 0.446      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.487     |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0658    |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.051     |
|    value_loss           | 0.0597     |
----------------------------------------
Eval num_timesteps=440000, episode_reward=10.80 +/- 2.96
Episode length: 206.90 +/- 49.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 207        |
|    mean_reward          | 10.8       |
| time/                   |            |
|    total_timesteps      | 440000     |
| train/                  |            |
|    approx_kl            | 0.43442354 |
|    clip_fraction        | 0.455      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.496     |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0906    |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0574    |
|    value_loss           | 0.0575     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 209      |
|    ep_rew_mean     | 12       |
| time/              |          |
|    fps             | 740      |
|    iterations      | 215      |
|    time_elapsed    | 594      |
|    total_timesteps | 440320   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 208        |
|    ep_rew_mean          | 11.9       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 216        |
|    time_elapsed         | 596        |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.41684058 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.537     |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0772    |
|    n_updates            | 2150       |
|    policy_gradient_loss | -0.062     |
|    value_loss           | 0.0628     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 202        |
|    ep_rew_mean          | 11.4       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 217        |
|    time_elapsed         | 598        |
|    total_timesteps      | 444416     |
| train/                  |            |
|    approx_kl            | 0.37113988 |
|    clip_fraction        | 0.439      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.527     |
|    explained_variance   | 0.657      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0675    |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.0546    |
|    value_loss           | 0.0626     |
----------------------------------------
Eval num_timesteps=445000, episode_reward=7.60 +/- 3.56
Episode length: 150.40 +/- 45.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 150        |
|    mean_reward          | 7.6        |
| time/                   |            |
|    total_timesteps      | 445000     |
| train/                  |            |
|    approx_kl            | 0.37733835 |
|    clip_fraction        | 0.473      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.54      |
|    explained_variance   | 0.688      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.048     |
|    n_updates            | 2170       |
|    policy_gradient_loss | -0.061     |
|    value_loss           | 0.0635     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 198      |
|    ep_rew_mean     | 11.2     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 218      |
|    time_elapsed    | 602      |
|    total_timesteps | 446464   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 196        |
|    ep_rew_mean          | 10.9       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 219        |
|    time_elapsed         | 604        |
|    total_timesteps      | 448512     |
| train/                  |            |
|    approx_kl            | 0.31979597 |
|    clip_fraction        | 0.454      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.543     |
|    explained_variance   | 0.734      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0737    |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.0635    |
|    value_loss           | 0.0603     |
----------------------------------------
Eval num_timesteps=450000, episode_reward=9.30 +/- 3.00
Episode length: 179.30 +/- 39.02
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 179      |
|    mean_reward          | 9.3      |
| time/                   |          |
|    total_timesteps      | 450000   |
| train/                  |          |
|    approx_kl            | 0.365953 |
|    clip_fraction        | 0.463    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.561   |
|    explained_variance   | 0.661    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0671  |
|    n_updates            | 2190     |
|    policy_gradient_loss | -0.0642  |
|    value_loss           | 0.0647   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 10.6     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 220      |
|    time_elapsed    | 607      |
|    total_timesteps | 450560   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 188       |
|    ep_rew_mean          | 10.1      |
| time/                   |           |
|    fps                  | 742       |
|    iterations           | 221       |
|    time_elapsed         | 609       |
|    total_timesteps      | 452608    |
| train/                  |           |
|    approx_kl            | 0.4202943 |
|    clip_fraction        | 0.475     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.536    |
|    explained_variance   | 0.731     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0558   |
|    n_updates            | 2200      |
|    policy_gradient_loss | -0.0633   |
|    value_loss           | 0.0553    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 185        |
|    ep_rew_mean          | 9.92       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 222        |
|    time_elapsed         | 612        |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.47875354 |
|    clip_fraction        | 0.496      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.544     |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.074     |
|    n_updates            | 2210       |
|    policy_gradient_loss | -0.0689    |
|    value_loss           | 0.0624     |
----------------------------------------
Eval num_timesteps=455000, episode_reward=10.50 +/- 3.38
Episode length: 186.00 +/- 45.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 186        |
|    mean_reward          | 10.5       |
| time/                   |            |
|    total_timesteps      | 455000     |
| train/                  |            |
|    approx_kl            | 0.46607578 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.506     |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0997    |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0631    |
|    value_loss           | 0.0634     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 185      |
|    ep_rew_mean     | 9.93     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 223      |
|    time_elapsed    | 615      |
|    total_timesteps | 456704   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 184        |
|    ep_rew_mean          | 9.85       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 224        |
|    time_elapsed         | 617        |
|    total_timesteps      | 458752     |
| train/                  |            |
|    approx_kl            | 0.63415873 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.497     |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0697    |
|    n_updates            | 2230       |
|    policy_gradient_loss | -0.0603    |
|    value_loss           | 0.0613     |
----------------------------------------
Eval num_timesteps=460000, episode_reward=10.60 +/- 2.91
Episode length: 183.60 +/- 39.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 184        |
|    mean_reward          | 10.6       |
| time/                   |            |
|    total_timesteps      | 460000     |
| train/                  |            |
|    approx_kl            | 0.46305668 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.468     |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.064     |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.0537    |
|    value_loss           | 0.0644     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 9.92     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 225      |
|    time_elapsed    | 621      |
|    total_timesteps | 460800   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 185       |
|    ep_rew_mean          | 10.1      |
| time/                   |           |
|    fps                  | 742       |
|    iterations           | 226       |
|    time_elapsed         | 623       |
|    total_timesteps      | 462848    |
| train/                  |           |
|    approx_kl            | 0.4092775 |
|    clip_fraction        | 0.443     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.483    |
|    explained_variance   | 0.638     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0595   |
|    n_updates            | 2250      |
|    policy_gradient_loss | -0.0553   |
|    value_loss           | 0.0679    |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 187       |
|    ep_rew_mean          | 10.2      |
| time/                   |           |
|    fps                  | 742       |
|    iterations           | 227       |
|    time_elapsed         | 625       |
|    total_timesteps      | 464896    |
| train/                  |           |
|    approx_kl            | 0.5191374 |
|    clip_fraction        | 0.422     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.459    |
|    explained_variance   | 0.626     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0701   |
|    n_updates            | 2260      |
|    policy_gradient_loss | -0.0511   |
|    value_loss           | 0.0656    |
---------------------------------------
Eval num_timesteps=465000, episode_reward=9.80 +/- 2.60
Episode length: 185.80 +/- 42.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 186       |
|    mean_reward          | 9.8       |
| time/                   |           |
|    total_timesteps      | 465000    |
| train/                  |           |
|    approx_kl            | 0.5778008 |
|    clip_fraction        | 0.454     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.464    |
|    explained_variance   | 0.659     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0608   |
|    n_updates            | 2270      |
|    policy_gradient_loss | -0.0558   |
|    value_loss           | 0.0615    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 10       |
| time/              |          |
|    fps             | 741      |
|    iterations      | 228      |
|    time_elapsed    | 629      |
|    total_timesteps | 466944   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 185       |
|    ep_rew_mean          | 9.9       |
| time/                   |           |
|    fps                  | 742       |
|    iterations           | 229       |
|    time_elapsed         | 631       |
|    total_timesteps      | 468992    |
| train/                  |           |
|    approx_kl            | 0.4358793 |
|    clip_fraction        | 0.498     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.52     |
|    explained_variance   | 0.611     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0633   |
|    n_updates            | 2280      |
|    policy_gradient_loss | -0.0546   |
|    value_loss           | 0.0734    |
---------------------------------------
Eval num_timesteps=470000, episode_reward=11.10 +/- 2.88
Episode length: 200.10 +/- 54.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 200        |
|    mean_reward          | 11.1       |
| time/                   |            |
|    total_timesteps      | 470000     |
| train/                  |            |
|    approx_kl            | 0.48484123 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.53      |
|    explained_variance   | 0.594      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0517    |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.0571    |
|    value_loss           | 0.07       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 10       |
| time/              |          |
|    fps             | 741      |
|    iterations      | 230      |
|    time_elapsed    | 635      |
|    total_timesteps | 471040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 188        |
|    ep_rew_mean          | 10         |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 231        |
|    time_elapsed         | 637        |
|    total_timesteps      | 473088     |
| train/                  |            |
|    approx_kl            | 0.38139772 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.565     |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0776    |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.0502    |
|    value_loss           | 0.0674     |
----------------------------------------
Eval num_timesteps=475000, episode_reward=10.80 +/- 2.52
Episode length: 200.60 +/- 43.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 201       |
|    mean_reward          | 10.8      |
| time/                   |           |
|    total_timesteps      | 475000    |
| train/                  |           |
|    approx_kl            | 0.5381325 |
|    clip_fraction        | 0.489     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.53     |
|    explained_variance   | 0.663     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0727   |
|    n_updates            | 2310      |
|    policy_gradient_loss | -0.0602   |
|    value_loss           | 0.0714    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 187      |
|    ep_rew_mean     | 9.99     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 232      |
|    time_elapsed    | 640      |
|    total_timesteps | 475136   |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 186      |
|    ep_rew_mean          | 9.9      |
| time/                   |          |
|    fps                  | 741      |
|    iterations           | 233      |
|    time_elapsed         | 643      |
|    total_timesteps      | 477184   |
| train/                  |          |
|    approx_kl            | 0.473675 |
|    clip_fraction        | 0.489    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.541   |
|    explained_variance   | 0.685    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0804  |
|    n_updates            | 2320     |
|    policy_gradient_loss | -0.0588  |
|    value_loss           | 0.065    |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 187        |
|    ep_rew_mean          | 9.86       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 234        |
|    time_elapsed         | 645        |
|    total_timesteps      | 479232     |
| train/                  |            |
|    approx_kl            | 0.50059617 |
|    clip_fraction        | 0.467      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.534     |
|    explained_variance   | 0.7        |
|    learning_rate        | 0.00074    |
|    loss                 | -0.05      |
|    n_updates            | 2330       |
|    policy_gradient_loss | -0.0582    |
|    value_loss           | 0.0703     |
----------------------------------------
Eval num_timesteps=480000, episode_reward=11.30 +/- 2.19
Episode length: 211.60 +/- 28.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 212        |
|    mean_reward          | 11.3       |
| time/                   |            |
|    total_timesteps      | 480000     |
| train/                  |            |
|    approx_kl            | 0.37607336 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.518     |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0535    |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.0523    |
|    value_loss           | 0.065      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 9.87     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 235      |
|    time_elapsed    | 649      |
|    total_timesteps | 481280   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 188        |
|    ep_rew_mean          | 9.8        |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 236        |
|    time_elapsed         | 651        |
|    total_timesteps      | 483328     |
| train/                  |            |
|    approx_kl            | 0.44281334 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.491     |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0389    |
|    n_updates            | 2350       |
|    policy_gradient_loss | -0.057     |
|    value_loss           | 0.0598     |
----------------------------------------
Eval num_timesteps=485000, episode_reward=13.00 +/- 2.57
Episode length: 218.60 +/- 49.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 219        |
|    mean_reward          | 13         |
| time/                   |            |
|    total_timesteps      | 485000     |
| train/                  |            |
|    approx_kl            | 0.44734263 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.488     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0686    |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.0533    |
|    value_loss           | 0.0688     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | 10       |
| time/              |          |
|    fps             | 741      |
|    iterations      | 237      |
|    time_elapsed    | 654      |
|    total_timesteps | 485376   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 188        |
|    ep_rew_mean          | 10         |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 238        |
|    time_elapsed         | 657        |
|    total_timesteps      | 487424     |
| train/                  |            |
|    approx_kl            | 0.36861792 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.514     |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0395    |
|    n_updates            | 2370       |
|    policy_gradient_loss | -0.0555    |
|    value_loss           | 0.0706     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 10.2       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 239        |
|    time_elapsed         | 659        |
|    total_timesteps      | 489472     |
| train/                  |            |
|    approx_kl            | 0.45600182 |
|    clip_fraction        | 0.451      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.513     |
|    explained_variance   | 0.61       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0657    |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.0605    |
|    value_loss           | 0.0684     |
----------------------------------------
Eval num_timesteps=490000, episode_reward=10.90 +/- 2.12
Episode length: 198.00 +/- 32.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 198       |
|    mean_reward          | 10.9      |
| time/                   |           |
|    total_timesteps      | 490000    |
| train/                  |           |
|    approx_kl            | 0.4044215 |
|    clip_fraction        | 0.442     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.495    |
|    explained_variance   | 0.71      |
|    learning_rate        | 0.00074   |
|    loss                 | -0.074    |
|    n_updates            | 2390      |
|    policy_gradient_loss | -0.0522   |
|    value_loss           | 0.0543    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 191      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 240      |
|    time_elapsed    | 662      |
|    total_timesteps | 491520   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 191        |
|    ep_rew_mean          | 10.3       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 241        |
|    time_elapsed         | 665        |
|    total_timesteps      | 493568     |
| train/                  |            |
|    approx_kl            | 0.40747494 |
|    clip_fraction        | 0.423      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.487     |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0782    |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.0544    |
|    value_loss           | 0.0669     |
----------------------------------------
Eval num_timesteps=495000, episode_reward=8.70 +/- 2.19
Episode length: 169.90 +/- 46.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 170        |
|    mean_reward          | 8.7        |
| time/                   |            |
|    total_timesteps      | 495000     |
| train/                  |            |
|    approx_kl            | 0.46381012 |
|    clip_fraction        | 0.439      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.505     |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0528    |
|    n_updates            | 2410       |
|    policy_gradient_loss | -0.0524    |
|    value_loss           | 0.0667     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 10.2     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 242      |
|    time_elapsed    | 668      |
|    total_timesteps | 495616   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 192       |
|    ep_rew_mean          | 10.3      |
| time/                   |           |
|    fps                  | 741       |
|    iterations           | 243       |
|    time_elapsed         | 670       |
|    total_timesteps      | 497664    |
| train/                  |           |
|    approx_kl            | 0.3834807 |
|    clip_fraction        | 0.468     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.543    |
|    explained_variance   | 0.717     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0811   |
|    n_updates            | 2420      |
|    policy_gradient_loss | -0.0593   |
|    value_loss           | 0.0553    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 190        |
|    ep_rew_mean          | 10.1       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 244        |
|    time_elapsed         | 673        |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.43712598 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.522     |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0722    |
|    n_updates            | 2430       |
|    policy_gradient_loss | -0.0544    |
|    value_loss           | 0.0588     |
----------------------------------------
Eval num_timesteps=500000, episode_reward=9.00 +/- 3.38
Episode length: 176.00 +/- 55.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 176        |
|    mean_reward          | 9          |
| time/                   |            |
|    total_timesteps      | 500000     |
| train/                  |            |
|    approx_kl            | 0.46654502 |
|    clip_fraction        | 0.47       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.535     |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0747    |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0545    |
|    value_loss           | 0.0646     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 188      |
|    ep_rew_mean     | 9.98     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 245      |
|    time_elapsed    | 676      |
|    total_timesteps | 501760   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 184        |
|    ep_rew_mean          | 9.69       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 246        |
|    time_elapsed         | 678        |
|    total_timesteps      | 503808     |
| train/                  |            |
|    approx_kl            | 0.41629094 |
|    clip_fraction        | 0.465      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.538     |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0902    |
|    n_updates            | 2450       |
|    policy_gradient_loss | -0.0607    |
|    value_loss           | 0.0626     |
----------------------------------------
Eval num_timesteps=505000, episode_reward=11.30 +/- 3.03
Episode length: 186.30 +/- 50.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 186        |
|    mean_reward          | 11.3       |
| time/                   |            |
|    total_timesteps      | 505000     |
| train/                  |            |
|    approx_kl            | 0.56074035 |
|    clip_fraction        | 0.488      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.531     |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0734    |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.058     |
|    value_loss           | 0.0767     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 9.6      |
| time/              |          |
|    fps             | 741      |
|    iterations      | 247      |
|    time_elapsed    | 682      |
|    total_timesteps | 505856   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 183       |
|    ep_rew_mean          | 9.75      |
| time/                   |           |
|    fps                  | 742       |
|    iterations           | 248       |
|    time_elapsed         | 684       |
|    total_timesteps      | 507904    |
| train/                  |           |
|    approx_kl            | 0.5122584 |
|    clip_fraction        | 0.426     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.476    |
|    explained_variance   | 0.689     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0477   |
|    n_updates            | 2470      |
|    policy_gradient_loss | -0.0551   |
|    value_loss           | 0.0643    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 184        |
|    ep_rew_mean          | 9.87       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 249        |
|    time_elapsed         | 686        |
|    total_timesteps      | 509952     |
| train/                  |            |
|    approx_kl            | 0.39150304 |
|    clip_fraction        | 0.429      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.49      |
|    explained_variance   | 0.687      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0641    |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.0582    |
|    value_loss           | 0.0665     |
----------------------------------------
Eval num_timesteps=510000, episode_reward=12.90 +/- 1.81
Episode length: 217.30 +/- 30.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 217        |
|    mean_reward          | 12.9       |
| time/                   |            |
|    total_timesteps      | 510000     |
| train/                  |            |
|    approx_kl            | 0.38110796 |
|    clip_fraction        | 0.434      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.492     |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0468    |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.0605    |
|    value_loss           | 0.065      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 9.85     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 250      |
|    time_elapsed    | 690      |
|    total_timesteps | 512000   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 179        |
|    ep_rew_mean          | 9.7        |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 251        |
|    time_elapsed         | 692        |
|    total_timesteps      | 514048     |
| train/                  |            |
|    approx_kl            | 0.45300674 |
|    clip_fraction        | 0.45       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.504     |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0817    |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.0587    |
|    value_loss           | 0.0715     |
----------------------------------------
Eval num_timesteps=515000, episode_reward=10.70 +/- 2.41
Episode length: 191.70 +/- 36.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 192       |
|    mean_reward          | 10.7      |
| time/                   |           |
|    total_timesteps      | 515000    |
| train/                  |           |
|    approx_kl            | 0.4632631 |
|    clip_fraction        | 0.49      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.545    |
|    explained_variance   | 0.667     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0668   |
|    n_updates            | 2510      |
|    policy_gradient_loss | -0.0585   |
|    value_loss           | 0.0654    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 9.77     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 252      |
|    time_elapsed    | 696      |
|    total_timesteps | 516096   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 181        |
|    ep_rew_mean          | 9.82       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 253        |
|    time_elapsed         | 698        |
|    total_timesteps      | 518144     |
| train/                  |            |
|    approx_kl            | 0.46427712 |
|    clip_fraction        | 0.461      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.504     |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0799    |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.0585    |
|    value_loss           | 0.0662     |
----------------------------------------
Eval num_timesteps=520000, episode_reward=11.50 +/- 3.58
Episode length: 196.80 +/- 42.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 197        |
|    mean_reward          | 11.5       |
| time/                   |            |
|    total_timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.50568664 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.538     |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0869    |
|    n_updates            | 2530       |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 0.0658     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 181      |
|    ep_rew_mean     | 9.85     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 254      |
|    time_elapsed    | 701      |
|    total_timesteps | 520192   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 182        |
|    ep_rew_mean          | 10         |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 255        |
|    time_elapsed         | 704        |
|    total_timesteps      | 522240     |
| train/                  |            |
|    approx_kl            | 0.51200604 |
|    clip_fraction        | 0.424      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.458     |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0586    |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.0561    |
|    value_loss           | 0.0715     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | 9.85       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 256        |
|    time_elapsed         | 706        |
|    total_timesteps      | 524288     |
| train/                  |            |
|    approx_kl            | 0.43594852 |
|    clip_fraction        | 0.417      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.466     |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0654    |
|    n_updates            | 2550       |
|    policy_gradient_loss | -0.0504    |
|    value_loss           | 0.0607     |
----------------------------------------
Eval num_timesteps=525000, episode_reward=9.60 +/- 2.76
Episode length: 168.30 +/- 35.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 168        |
|    mean_reward          | 9.6        |
| time/                   |            |
|    total_timesteps      | 525000     |
| train/                  |            |
|    approx_kl            | 0.45813978 |
|    clip_fraction        | 0.438      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.494     |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0569    |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.0571    |
|    value_loss           | 0.0583     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 178      |
|    ep_rew_mean     | 9.67     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 257      |
|    time_elapsed    | 709      |
|    total_timesteps | 526336   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 175        |
|    ep_rew_mean          | 9.56       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 258        |
|    time_elapsed         | 711        |
|    total_timesteps      | 528384     |
| train/                  |            |
|    approx_kl            | 0.40906715 |
|    clip_fraction        | 0.408      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.453     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0705    |
|    n_updates            | 2570       |
|    policy_gradient_loss | -0.0506    |
|    value_loss           | 0.0638     |
----------------------------------------
Eval num_timesteps=530000, episode_reward=9.80 +/- 2.44
Episode length: 177.60 +/- 30.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 178        |
|    mean_reward          | 9.8        |
| time/                   |            |
|    total_timesteps      | 530000     |
| train/                  |            |
|    approx_kl            | 0.40399072 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.519     |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0701    |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.0504    |
|    value_loss           | 0.0694     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 175      |
|    ep_rew_mean     | 9.51     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 259      |
|    time_elapsed    | 715      |
|    total_timesteps | 530432   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 9.53       |
| time/                   |            |
|    fps                  | 741        |
|    iterations           | 260        |
|    time_elapsed         | 717        |
|    total_timesteps      | 532480     |
| train/                  |            |
|    approx_kl            | 0.39888126 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.506     |
|    explained_variance   | 0.681      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0735    |
|    n_updates            | 2590       |
|    policy_gradient_loss | -0.0583    |
|    value_loss           | 0.068      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 170        |
|    ep_rew_mean          | 9.36       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 261        |
|    time_elapsed         | 719        |
|    total_timesteps      | 534528     |
| train/                  |            |
|    approx_kl            | 0.52473503 |
|    clip_fraction        | 0.462      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.538     |
|    explained_variance   | 0.695      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0935    |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.0592    |
|    value_loss           | 0.0644     |
----------------------------------------
Eval num_timesteps=535000, episode_reward=9.20 +/- 3.09
Episode length: 187.90 +/- 51.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 188        |
|    mean_reward          | 9.2        |
| time/                   |            |
|    total_timesteps      | 535000     |
| train/                  |            |
|    approx_kl            | 0.35927877 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.537     |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0487    |
|    n_updates            | 2610       |
|    policy_gradient_loss | -0.0554    |
|    value_loss           | 0.0704     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 9.26     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 262      |
|    time_elapsed    | 723      |
|    total_timesteps | 536576   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 169       |
|    ep_rew_mean          | 9.18      |
| time/                   |           |
|    fps                  | 742       |
|    iterations           | 263       |
|    time_elapsed         | 725       |
|    total_timesteps      | 538624    |
| train/                  |           |
|    approx_kl            | 0.4771188 |
|    clip_fraction        | 0.457     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.508    |
|    explained_variance   | 0.663     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0886   |
|    n_updates            | 2620      |
|    policy_gradient_loss | -0.0572   |
|    value_loss           | 0.0631    |
---------------------------------------
Eval num_timesteps=540000, episode_reward=10.20 +/- 3.19
Episode length: 173.10 +/- 38.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 173        |
|    mean_reward          | 10.2       |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.44088268 |
|    clip_fraction        | 0.46       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.511     |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0761    |
|    n_updates            | 2630       |
|    policy_gradient_loss | -0.0549    |
|    value_loss           | 0.0688     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 9.23     |
| time/              |          |
|    fps             | 741      |
|    iterations      | 264      |
|    time_elapsed    | 728      |
|    total_timesteps | 540672   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 9.49       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 265        |
|    time_elapsed         | 731        |
|    total_timesteps      | 542720     |
| train/                  |            |
|    approx_kl            | 0.38725466 |
|    clip_fraction        | 0.435      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.505     |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0942    |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.0622    |
|    value_loss           | 0.0775     |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 173        |
|    ep_rew_mean          | 9.32       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 266        |
|    time_elapsed         | 733        |
|    total_timesteps      | 544768     |
| train/                  |            |
|    approx_kl            | 0.39294612 |
|    clip_fraction        | 0.439      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.494     |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0599    |
|    n_updates            | 2650       |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.0746     |
----------------------------------------
Eval num_timesteps=545000, episode_reward=5.70 +/- 2.37
Episode length: 118.00 +/- 43.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 118       |
|    mean_reward          | 5.7       |
| time/                   |           |
|    total_timesteps      | 545000    |
| train/                  |           |
|    approx_kl            | 0.4387385 |
|    clip_fraction        | 0.474     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.554    |
|    explained_variance   | 0.578     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0551   |
|    n_updates            | 2660      |
|    policy_gradient_loss | -0.0556   |
|    value_loss           | 0.0749    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 171      |
|    ep_rew_mean     | 9.15     |
| time/              |          |
|    fps             | 742      |
|    iterations      | 267      |
|    time_elapsed    | 736      |
|    total_timesteps | 546816   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 171        |
|    ep_rew_mean          | 9.05       |
| time/                   |            |
|    fps                  | 742        |
|    iterations           | 268        |
|    time_elapsed         | 738        |
|    total_timesteps      | 548864     |
| train/                  |            |
|    approx_kl            | 0.48581666 |
|    clip_fraction        | 0.475      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.508     |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.065     |
|    n_updates            | 2670       |
|    policy_gradient_loss | -0.0629    |
|    value_loss           | 0.0629     |
----------------------------------------
Eval num_timesteps=550000, episode_reward=6.70 +/- 2.45
Episode length: 145.90 +/- 27.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 146        |
|    mean_reward          | 6.7        |
| time/                   |            |
|    total_timesteps      | 550000     |
| train/                  |            |
|    approx_kl            | 0.42542157 |
|    clip_fraction        | 0.466      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.532     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0705    |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.0495    |
|    value_loss           | 0.0686     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 8.62     |
| time/              |          |
|    fps             | 742      |
|    iterations      | 269      |
|    time_elapsed    | 741      |
|    total_timesteps | 550912   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 166        |
|    ep_rew_mean          | 8.47       |
| time/                   |            |
|    fps                  | 743        |
|    iterations           | 270        |
|    time_elapsed         | 744        |
|    total_timesteps      | 552960     |
| train/                  |            |
|    approx_kl            | 0.41582295 |
|    clip_fraction        | 0.501      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.624     |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0443    |
|    n_updates            | 2690       |
|    policy_gradient_loss | -0.0562    |
|    value_loss           | 0.0701     |
----------------------------------------
Eval num_timesteps=555000, episode_reward=10.20 +/- 3.19
Episode length: 194.10 +/- 41.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 194        |
|    mean_reward          | 10.2       |
| time/                   |            |
|    total_timesteps      | 555000     |
| train/                  |            |
|    approx_kl            | 0.47092617 |
|    clip_fraction        | 0.474      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.53      |
|    explained_variance   | 0.571      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0714    |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 0.0729     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 166      |
|    ep_rew_mean     | 8.31     |
| time/              |          |
|    fps             | 742      |
|    iterations      | 271      |
|    time_elapsed    | 747      |
|    total_timesteps | 555008   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 166       |
|    ep_rew_mean          | 8.32      |
| time/                   |           |
|    fps                  | 742       |
|    iterations           | 272       |
|    time_elapsed         | 749       |
|    total_timesteps      | 557056    |
| train/                  |           |
|    approx_kl            | 0.4331997 |
|    clip_fraction        | 0.47      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.535    |
|    explained_variance   | 0.597     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0982   |
|    n_updates            | 2710      |
|    policy_gradient_loss | -0.0567   |
|    value_loss           | 0.0683    |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 167      |
|    ep_rew_mean          | 8.27     |
| time/                   |          |
|    fps                  | 743      |
|    iterations           | 273      |
|    time_elapsed         | 752      |
|    total_timesteps      | 559104   |
| train/                  |          |
|    approx_kl            | 0.435028 |
|    clip_fraction        | 0.434    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.485   |
|    explained_variance   | 0.599    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.074   |
|    n_updates            | 2720     |
|    policy_gradient_loss | -0.0546  |
|    value_loss           | 0.0767   |
--------------------------------------
Eval num_timesteps=560000, episode_reward=10.70 +/- 2.10
Episode length: 198.50 +/- 37.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 198       |
|    mean_reward          | 10.7      |
| time/                   |           |
|    total_timesteps      | 560000    |
| train/                  |           |
|    approx_kl            | 0.4912752 |
|    clip_fraction        | 0.434     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.495    |
|    explained_variance   | 0.696     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0774   |
|    n_updates            | 2730      |
|    policy_gradient_loss | -0.0508   |
|    value_loss           | 0.062     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 8.4      |
| time/              |          |
|    fps             | 742      |
|    iterations      | 274      |
|    time_elapsed    | 755      |
|    total_timesteps | 561152   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 174        |
|    ep_rew_mean          | 8.79       |
| time/                   |            |
|    fps                  | 743        |
|    iterations           | 275        |
|    time_elapsed         | 757        |
|    total_timesteps      | 563200     |
| train/                  |            |
|    approx_kl            | 0.40201128 |
|    clip_fraction        | 0.422      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.494     |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0663    |
|    n_updates            | 2740       |
|    policy_gradient_loss | -0.0562    |
|    value_loss           | 0.0687     |
----------------------------------------
Eval num_timesteps=565000, episode_reward=8.80 +/- 2.79
Episode length: 175.50 +/- 42.84
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 176      |
|    mean_reward          | 8.8      |
| time/                   |          |
|    total_timesteps      | 565000   |
| train/                  |          |
|    approx_kl            | 0.371779 |
|    clip_fraction        | 0.421    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.505   |
|    explained_variance   | 0.731    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0793  |
|    n_updates            | 2750     |
|    policy_gradient_loss | -0.0502  |
|    value_loss           | 0.0652   |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 173      |
|    ep_rew_mean     | 8.88     |
| time/              |          |
|    fps             | 742      |
|    iterations      | 276      |
|    time_elapsed    | 761      |
|    total_timesteps | 565248   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 178       |
|    ep_rew_mean          | 9.21      |
| time/                   |           |
|    fps                  | 742       |
|    iterations           | 277       |
|    time_elapsed         | 763       |
|    total_timesteps      | 567296    |
| train/                  |           |
|    approx_kl            | 0.6043469 |
|    clip_fraction        | 0.444     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.472    |
|    explained_variance   | 0.653     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0716   |
|    n_updates            | 2760      |
|    policy_gradient_loss | -0.0509   |
|    value_loss           | 0.0765    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 182        |
|    ep_rew_mean          | 9.68       |
| time/                   |            |
|    fps                  | 743        |
|    iterations           | 278        |
|    time_elapsed         | 765        |
|    total_timesteps      | 569344     |
| train/                  |            |
|    approx_kl            | 0.55632865 |
|    clip_fraction        | 0.444      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.448     |
|    explained_variance   | 0.678      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0614    |
|    n_updates            | 2770       |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.0654     |
----------------------------------------
Eval num_timesteps=570000, episode_reward=9.20 +/- 2.79
Episode length: 170.60 +/- 37.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 171       |
|    mean_reward          | 9.2       |
| time/                   |           |
|    total_timesteps      | 570000    |
| train/                  |           |
|    approx_kl            | 0.6432924 |
|    clip_fraction        | 0.391     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.418    |
|    explained_variance   | 0.587     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0774   |
|    n_updates            | 2780      |
|    policy_gradient_loss | -0.0523   |
|    value_loss           | 0.0762    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 180      |
|    ep_rew_mean     | 9.73     |
| time/              |          |
|    fps             | 742      |
|    iterations      | 279      |
|    time_elapsed    | 769      |
|    total_timesteps | 571392   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 10.1       |
| time/                   |            |
|    fps                  | 743        |
|    iterations           | 280        |
|    time_elapsed         | 771        |
|    total_timesteps      | 573440     |
| train/                  |            |
|    approx_kl            | 0.42876667 |
|    clip_fraction        | 0.419      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.426     |
|    explained_variance   | 0.61       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0643    |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.049     |
|    value_loss           | 0.0738     |
----------------------------------------
Eval num_timesteps=575000, episode_reward=9.90 +/- 3.21
Episode length: 183.40 +/- 36.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 183        |
|    mean_reward          | 9.9        |
| time/                   |            |
|    total_timesteps      | 575000     |
| train/                  |            |
|    approx_kl            | 0.48662642 |
|    clip_fraction        | 0.437      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.459     |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0558    |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.0529    |
|    value_loss           | 0.0706     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 184      |
|    ep_rew_mean     | 10.1     |
| time/              |          |
|    fps             | 742      |
|    iterations      | 281      |
|    time_elapsed    | 774      |
|    total_timesteps | 575488   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 181       |
|    ep_rew_mean          | 10.1      |
| time/                   |           |
|    fps                  | 743       |
|    iterations           | 282       |
|    time_elapsed         | 777       |
|    total_timesteps      | 577536    |
| train/                  |           |
|    approx_kl            | 0.5665219 |
|    clip_fraction        | 0.453     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.465    |
|    explained_variance   | 0.566     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0394   |
|    n_updates            | 2810      |
|    policy_gradient_loss | -0.0522   |
|    value_loss           | 0.0672    |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 179        |
|    ep_rew_mean          | 9.92       |
| time/                   |            |
|    fps                  | 743        |
|    iterations           | 283        |
|    time_elapsed         | 779        |
|    total_timesteps      | 579584     |
| train/                  |            |
|    approx_kl            | 0.56924605 |
|    clip_fraction        | 0.471      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.469     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0352    |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.055     |
|    value_loss           | 0.0729     |
----------------------------------------
Eval num_timesteps=580000, episode_reward=8.70 +/- 2.76
Episode length: 172.50 +/- 23.81
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 172        |
|    mean_reward          | 8.7        |
| time/                   |            |
|    total_timesteps      | 580000     |
| train/                  |            |
|    approx_kl            | 0.48990303 |
|    clip_fraction        | 0.459      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.503     |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0331    |
|    n_updates            | 2830       |
|    policy_gradient_loss | -0.0503    |
|    value_loss           | 0.069      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 9.74     |
| time/              |          |
|    fps             | 743      |
|    iterations      | 284      |
|    time_elapsed    | 782      |
|    total_timesteps | 581632   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 176       |
|    ep_rew_mean          | 9.79      |
| time/                   |           |
|    fps                  | 743       |
|    iterations           | 285       |
|    time_elapsed         | 785       |
|    total_timesteps      | 583680    |
| train/                  |           |
|    approx_kl            | 0.5222284 |
|    clip_fraction        | 0.428     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.453    |
|    explained_variance   | 0.555     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0573   |
|    n_updates            | 2840      |
|    policy_gradient_loss | -0.0473   |
|    value_loss           | 0.0798    |
---------------------------------------
Eval num_timesteps=585000, episode_reward=6.90 +/- 4.23
Episode length: 159.50 +/- 51.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 160       |
|    mean_reward          | 6.9       |
| time/                   |           |
|    total_timesteps      | 585000    |
| train/                  |           |
|    approx_kl            | 0.5881891 |
|    clip_fraction        | 0.455     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.448    |
|    explained_variance   | 0.47      |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0503   |
|    n_updates            | 2850      |
|    policy_gradient_loss | -0.0544   |
|    value_loss           | 0.0843    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 176      |
|    ep_rew_mean     | 9.68     |
| time/              |          |
|    fps             | 742      |
|    iterations      | 286      |
|    time_elapsed    | 788      |
|    total_timesteps | 585728   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 175       |
|    ep_rew_mean          | 9.6       |
| time/                   |           |
|    fps                  | 743       |
|    iterations           | 287       |
|    time_elapsed         | 790       |
|    total_timesteps      | 587776    |
| train/                  |           |
|    approx_kl            | 0.5658133 |
|    clip_fraction        | 0.437     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.445    |
|    explained_variance   | 0.502     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0397   |
|    n_updates            | 2860      |
|    policy_gradient_loss | -0.0519   |
|    value_loss           | 0.0795    |
---------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 175      |
|    ep_rew_mean          | 9.57     |
| time/                   |          |
|    fps                  | 743      |
|    iterations           | 288      |
|    time_elapsed         | 792      |
|    total_timesteps      | 589824   |
| train/                  |          |
|    approx_kl            | 0.523502 |
|    clip_fraction        | 0.43     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.448   |
|    explained_variance   | 0.537    |
|    learning_rate        | 0.00074  |
|    loss                 | -0.0436  |
|    n_updates            | 2870     |
|    policy_gradient_loss | -0.0476  |
|    value_loss           | 0.0852   |
--------------------------------------
Eval num_timesteps=590000, episode_reward=7.30 +/- 2.10
Episode length: 158.80 +/- 27.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 159        |
|    mean_reward          | 7.3        |
| time/                   |            |
|    total_timesteps      | 590000     |
| train/                  |            |
|    approx_kl            | 0.55383927 |
|    clip_fraction        | 0.421      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.478     |
|    explained_variance   | 0.556      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0733    |
|    n_updates            | 2880       |
|    policy_gradient_loss | -0.0536    |
|    value_loss           | 0.0774     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 172      |
|    ep_rew_mean     | 9.29     |
| time/              |          |
|    fps             | 743      |
|    iterations      | 289      |
|    time_elapsed    | 796      |
|    total_timesteps | 591872   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 173       |
|    ep_rew_mean          | 9.23      |
| time/                   |           |
|    fps                  | 743       |
|    iterations           | 290       |
|    time_elapsed         | 798       |
|    total_timesteps      | 593920    |
| train/                  |           |
|    approx_kl            | 0.4702176 |
|    clip_fraction        | 0.441     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.492    |
|    explained_variance   | 0.645     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0619   |
|    n_updates            | 2890      |
|    policy_gradient_loss | -0.0564   |
|    value_loss           | 0.0686    |
---------------------------------------
Eval num_timesteps=595000, episode_reward=5.80 +/- 2.32
Episode length: 139.40 +/- 28.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 139       |
|    mean_reward          | 5.8       |
| time/                   |           |
|    total_timesteps      | 595000    |
| train/                  |           |
|    approx_kl            | 0.4506719 |
|    clip_fraction        | 0.441     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.482    |
|    explained_variance   | 0.663     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.0805   |
|    n_updates            | 2900      |
|    policy_gradient_loss | -0.0547   |
|    value_loss           | 0.0649    |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 9        |
| time/              |          |
|    fps             | 743      |
|    iterations      | 291      |
|    time_elapsed    | 801      |
|    total_timesteps | 595968   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 167       |
|    ep_rew_mean          | 8.9       |
| time/                   |           |
|    fps                  | 743       |
|    iterations           | 292       |
|    time_elapsed         | 803       |
|    total_timesteps      | 598016    |
| train/                  |           |
|    approx_kl            | 0.4938997 |
|    clip_fraction        | 0.462     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.51     |
|    explained_variance   | 0.647     |
|    learning_rate        | 0.00074   |
|    loss                 | -0.083    |
|    n_updates            | 2910      |
|    policy_gradient_loss | -0.0621   |
|    value_loss           | 0.0739    |
---------------------------------------
Eval num_timesteps=600000, episode_reward=7.20 +/- 2.52
Episode length: 140.90 +/- 27.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 141        |
|    mean_reward          | 7.2        |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.39476395 |
|    clip_fraction        | 0.449      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.527     |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.00074    |
|    loss                 | -0.0369    |
|    n_updates            | 2920       |
|    policy_gradient_loss | -0.0574    |
|    value_loss           | 0.0613     |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 8.76     |
| time/              |          |
|    fps             | 743      |
|    iterations      | 293      |
|    time_elapsed    | 806      |
|    total_timesteps | 600064   |
---------------------------------
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'saves-tunning/defend-center/ppo-2/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
