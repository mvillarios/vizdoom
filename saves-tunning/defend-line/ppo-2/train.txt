/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 6        |
| time/              |          |
|    fps             | 1085     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 131         |
|    ep_rew_mean          | 7.29        |
| time/                   |             |
|    fps                  | 999         |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009571804 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0545     |
|    learning_rate        | 0.00038     |
|    loss                 | 0.0215      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.16        |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=6.80 +/- 1.94
Episode length: 128.10 +/- 33.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 128         |
|    mean_reward          | 6.8         |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.012730926 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | 0.211       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.00215    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 0.182       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 8.28     |
| time/              |          |
|    fps             | 881      |
|    iterations      | 3        |
|    time_elapsed    | 6        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 147         |
|    ep_rew_mean          | 9           |
| time/                   |             |
|    fps                  | 911         |
|    iterations           | 4           |
|    time_elapsed         | 8           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.014925333 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.413       |
|    learning_rate        | 0.00038     |
|    loss                 | 0.00624     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0322     |
|    value_loss           | 0.158       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=11.60 +/- 5.68
Episode length: 144.90 +/- 61.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 145         |
|    mean_reward          | 11.6        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.018068839 |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.347       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.00986    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0426     |
|    value_loss           | 0.167       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 9.84     |
| time/              |          |
|    fps             | 853      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 153         |
|    ep_rew_mean          | 10.5        |
| time/                   |             |
|    fps                  | 876         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.024840709 |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.27       |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0267     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0506     |
|    value_loss           | 0.174       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | 11.3        |
| time/                   |             |
|    fps                  | 893         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.025543813 |
|    clip_fraction        | 0.284       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0261     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0498     |
|    value_loss           | 0.171       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=16.40 +/- 4.36
Episode length: 175.60 +/- 54.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 176         |
|    mean_reward          | 16.4        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.027335469 |
|    clip_fraction        | 0.282       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.505       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0225     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0473     |
|    value_loss           | 0.162       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    fps             | 850      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 171         |
|    ep_rew_mean          | 13.3        |
| time/                   |             |
|    fps                  | 866         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.031184591 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.14       |
|    explained_variance   | 0.416       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.011      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0528     |
|    value_loss           | 0.174       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=19.90 +/- 3.81
Episode length: 186.50 +/- 45.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 186         |
|    mean_reward          | 19.9        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.032756418 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.12       |
|    explained_variance   | 0.56        |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0217     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0533     |
|    value_loss           | 0.164       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | 14.4     |
| time/              |          |
|    fps             | 833      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 179         |
|    ep_rew_mean          | 15.2        |
| time/                   |             |
|    fps                  | 847         |
|    iterations           | 11          |
|    time_elapsed         | 26          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.040357936 |
|    clip_fraction        | 0.338       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.49        |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0493     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0595     |
|    value_loss           | 0.154       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 180         |
|    ep_rew_mean          | 16          |
| time/                   |             |
|    fps                  | 859         |
|    iterations           | 12          |
|    time_elapsed         | 28          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.042326797 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | 0.509       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0342     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0586     |
|    value_loss           | 0.165       |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=19.20 +/- 5.60
Episode length: 170.00 +/- 54.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 170         |
|    mean_reward          | 19.2        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.037143797 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.529       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0347     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0526     |
|    value_loss           | 0.167       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 182      |
|    ep_rew_mean     | 16.6     |
| time/              |          |
|    fps             | 837      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 183        |
|    ep_rew_mean          | 17         |
| time/                   |            |
|    fps                  | 848        |
|    iterations           | 14         |
|    time_elapsed         | 33         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.03938526 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.04      |
|    explained_variance   | 0.525      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0293    |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0548    |
|    value_loss           | 0.17       |
----------------------------------------
Eval num_timesteps=30000, episode_reward=22.80 +/- 3.31
Episode length: 217.60 +/- 45.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 218         |
|    mean_reward          | 22.8        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.040631495 |
|    clip_fraction        | 0.337       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.992      |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0207     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.057      |
|    value_loss           | 0.194       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | 17.6     |
| time/              |          |
|    fps             | 823      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 188         |
|    ep_rew_mean          | 18          |
| time/                   |             |
|    fps                  | 833         |
|    iterations           | 16          |
|    time_elapsed         | 39          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.046547815 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.953      |
|    explained_variance   | 0.47        |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0196     |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0533     |
|    value_loss           | 0.174       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 193        |
|    ep_rew_mean          | 18.5       |
| time/                   |            |
|    fps                  | 842        |
|    iterations           | 17         |
|    time_elapsed         | 41         |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.04108035 |
|    clip_fraction        | 0.351      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.953     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0355    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0533    |
|    value_loss           | 0.157      |
----------------------------------------
Eval num_timesteps=35000, episode_reward=22.40 +/- 4.84
Episode length: 202.10 +/- 52.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | 22.4        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.049119957 |
|    clip_fraction        | 0.356       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.933      |
|    explained_variance   | 0.538       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0296     |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0556     |
|    value_loss           | 0.157       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    fps             | 824      |
|    iterations      | 18       |
|    time_elapsed    | 44       |
|    total_timesteps | 36864    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 195        |
|    ep_rew_mean          | 19.2       |
| time/                   |            |
|    fps                  | 832        |
|    iterations           | 19         |
|    time_elapsed         | 46         |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.04631085 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.852     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00868   |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0467    |
|    value_loss           | 0.16       |
----------------------------------------
Eval num_timesteps=40000, episode_reward=21.10 +/- 6.59
Episode length: 186.80 +/- 61.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 187        |
|    mean_reward          | 21.1       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.04979568 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.86      |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.052     |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.054     |
|    value_loss           | 0.169      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 197      |
|    ep_rew_mean     | 19.6     |
| time/              |          |
|    fps             | 818      |
|    iterations      | 20       |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 203         |
|    ep_rew_mean          | 20.3        |
| time/                   |             |
|    fps                  | 826         |
|    iterations           | 21          |
|    time_elapsed         | 52          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.060421184 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.783      |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.00709    |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0505     |
|    value_loss           | 0.169       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=22.60 +/- 2.84
Episode length: 190.50 +/- 28.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 190         |
|    mean_reward          | 22.6        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.060755953 |
|    clip_fraction        | 0.33        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.739      |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.00038     |
|    loss                 | -0.000967   |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0424     |
|    value_loss           | 0.153       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 206      |
|    ep_rew_mean     | 20.8     |
| time/              |          |
|    fps             | 814      |
|    iterations      | 22       |
|    time_elapsed    | 55       |
|    total_timesteps | 45056    |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 209       |
|    ep_rew_mean          | 21.3      |
| time/                   |           |
|    fps                  | 821       |
|    iterations           | 23        |
|    time_elapsed         | 57        |
|    total_timesteps      | 47104     |
| train/                  |           |
|    approx_kl            | 0.0601791 |
|    clip_fraction        | 0.326     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.723    |
|    explained_variance   | 0.609     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0213   |
|    n_updates            | 220       |
|    policy_gradient_loss | -0.0479   |
|    value_loss           | 0.168     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 216        |
|    ep_rew_mean          | 22.2       |
| time/                   |            |
|    fps                  | 827        |
|    iterations           | 24         |
|    time_elapsed         | 59         |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.05362181 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.71      |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.026     |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0433    |
|    value_loss           | 0.173      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=24.40 +/- 4.48
Episode length: 204.10 +/- 39.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 204        |
|    mean_reward          | 24.4       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.05433548 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.709     |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0198    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0412    |
|    value_loss           | 0.171      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 22.6     |
| time/              |          |
|    fps             | 815      |
|    iterations      | 25       |
|    time_elapsed    | 62       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 217        |
|    ep_rew_mean          | 22.6       |
| time/                   |            |
|    fps                  | 821        |
|    iterations           | 26         |
|    time_elapsed         | 64         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.07268287 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.702     |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0141    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.157      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=23.60 +/- 3.67
Episode length: 214.50 +/- 39.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 214       |
|    mean_reward          | 23.6      |
| time/                   |           |
|    total_timesteps      | 55000     |
| train/                  |           |
|    approx_kl            | 0.0661085 |
|    clip_fraction        | 0.321     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.675    |
|    explained_variance   | 0.646     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0194   |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.047    |
|    value_loss           | 0.168     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 810      |
|    iterations      | 27       |
|    time_elapsed    | 68       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 218        |
|    ep_rew_mean          | 23         |
| time/                   |            |
|    fps                  | 815        |
|    iterations           | 28         |
|    time_elapsed         | 70         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.08248423 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.655     |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0174    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0455    |
|    value_loss           | 0.179      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 215         |
|    ep_rew_mean          | 22.7        |
| time/                   |             |
|    fps                  | 821         |
|    iterations           | 29          |
|    time_elapsed         | 72          |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.085797675 |
|    clip_fraction        | 0.322       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.632      |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.00165    |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0466     |
|    value_loss           | 0.176       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=24.00 +/- 3.79
Episode length: 208.10 +/- 38.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 208        |
|    mean_reward          | 24         |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.06815012 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.64      |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00663   |
|    n_updates            | 290        |
|    policy_gradient_loss | -0.0365    |
|    value_loss           | 0.183      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 22.8     |
| time/              |          |
|    fps             | 811      |
|    iterations      | 30       |
|    time_elapsed    | 75       |
|    total_timesteps | 61440    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 215        |
|    ep_rew_mean          | 23         |
| time/                   |            |
|    fps                  | 816        |
|    iterations           | 31         |
|    time_elapsed         | 77         |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.06457058 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.619     |
|    explained_variance   | 0.611      |
|    learning_rate        | 0.00038    |
|    loss                 | 0.0029     |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0421    |
|    value_loss           | 0.172      |
----------------------------------------
Eval num_timesteps=65000, episode_reward=22.00 +/- 7.96
Episode length: 195.00 +/- 71.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 195       |
|    mean_reward          | 22        |
| time/                   |           |
|    total_timesteps      | 65000     |
| train/                  |           |
|    approx_kl            | 0.0826697 |
|    clip_fraction        | 0.328     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.596    |
|    explained_variance   | 0.699     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0347   |
|    n_updates            | 310       |
|    policy_gradient_loss | -0.0495   |
|    value_loss           | 0.142     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 23.1     |
| time/              |          |
|    fps             | 808      |
|    iterations      | 32       |
|    time_elapsed    | 81       |
|    total_timesteps | 65536    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 213        |
|    ep_rew_mean          | 22.9       |
| time/                   |            |
|    fps                  | 813        |
|    iterations           | 33         |
|    time_elapsed         | 83         |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.08958248 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.577     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0124    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0481    |
|    value_loss           | 0.174      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 214        |
|    ep_rew_mean          | 23         |
| time/                   |            |
|    fps                  | 818        |
|    iterations           | 34         |
|    time_elapsed         | 85         |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.08918221 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.582     |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.016     |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.157      |
----------------------------------------
Eval num_timesteps=70000, episode_reward=24.50 +/- 6.34
Episode length: 236.00 +/- 58.36
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 236        |
|    mean_reward          | 24.5       |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.08401935 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.582     |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00872   |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0408    |
|    value_loss           | 0.179      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 211      |
|    ep_rew_mean     | 22.6     |
| time/              |          |
|    fps             | 808      |
|    iterations      | 35       |
|    time_elapsed    | 88       |
|    total_timesteps | 71680    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 211        |
|    ep_rew_mean          | 22.6       |
| time/                   |            |
|    fps                  | 812        |
|    iterations           | 36         |
|    time_elapsed         | 90         |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.09757989 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.591     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.00038    |
|    loss                 | 0.00838    |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.175      |
----------------------------------------
Eval num_timesteps=75000, episode_reward=20.90 +/- 5.80
Episode length: 187.80 +/- 52.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 188         |
|    mean_reward          | 20.9        |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.094923824 |
|    clip_fraction        | 0.316       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.52       |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0132     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0453     |
|    value_loss           | 0.167       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 805      |
|    iterations      | 37       |
|    time_elapsed    | 94       |
|    total_timesteps | 75776    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 214        |
|    ep_rew_mean          | 23         |
| time/                   |            |
|    fps                  | 810        |
|    iterations           | 38         |
|    time_elapsed         | 96         |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.09444897 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.521     |
|    explained_variance   | 0.64       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.042     |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0438    |
|    value_loss           | 0.149      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 216        |
|    ep_rew_mean          | 23.3       |
| time/                   |            |
|    fps                  | 814        |
|    iterations           | 39         |
|    time_elapsed         | 98         |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.10529028 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.509     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00161   |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0359    |
|    value_loss           | 0.176      |
----------------------------------------
Eval num_timesteps=80000, episode_reward=22.60 +/- 5.28
Episode length: 196.90 +/- 49.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 197        |
|    mean_reward          | 22.6       |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.10062728 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.481     |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00206   |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0414    |
|    value_loss           | 0.178      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 23.6     |
| time/              |          |
|    fps             | 807      |
|    iterations      | 40       |
|    time_elapsed    | 101      |
|    total_timesteps | 81920    |
---------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 217      |
|    ep_rew_mean          | 23.5     |
| time/                   |          |
|    fps                  | 811      |
|    iterations           | 41       |
|    time_elapsed         | 103      |
|    total_timesteps      | 83968    |
| train/                  |          |
|    approx_kl            | 0.084885 |
|    clip_fraction        | 0.284    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.483   |
|    explained_variance   | 0.593    |
|    learning_rate        | 0.00038  |
|    loss                 | -0.0183  |
|    n_updates            | 400      |
|    policy_gradient_loss | -0.0379  |
|    value_loss           | 0.163    |
--------------------------------------
Eval num_timesteps=85000, episode_reward=26.10 +/- 6.43
Episode length: 227.10 +/- 63.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 227         |
|    mean_reward          | 26.1        |
| time/                   |             |
|    total_timesteps      | 85000       |
| train/                  |             |
|    approx_kl            | 0.105671376 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.499      |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0374     |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.042      |
|    value_loss           | 0.137       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | 23.5     |
| time/              |          |
|    fps             | 804      |
|    iterations      | 42       |
|    time_elapsed    | 106      |
|    total_timesteps | 86016    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 23.7       |
| time/                   |            |
|    fps                  | 807        |
|    iterations           | 43         |
|    time_elapsed         | 108        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.10693474 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.496     |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0193    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0413    |
|    value_loss           | 0.18       |
----------------------------------------
Eval num_timesteps=90000, episode_reward=26.40 +/- 4.63
Episode length: 228.40 +/- 45.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 228       |
|    mean_reward          | 26.4      |
| time/                   |           |
|    total_timesteps      | 90000     |
| train/                  |           |
|    approx_kl            | 0.1034611 |
|    clip_fraction        | 0.295     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.485    |
|    explained_variance   | 0.592     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.00181  |
|    n_updates            | 430       |
|    policy_gradient_loss | -0.0386   |
|    value_loss           | 0.162     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 23.8     |
| time/              |          |
|    fps             | 800      |
|    iterations      | 44       |
|    time_elapsed    | 112      |
|    total_timesteps | 90112    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 220        |
|    ep_rew_mean          | 23.9       |
| time/                   |            |
|    fps                  | 804        |
|    iterations           | 45         |
|    time_elapsed         | 114        |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.10459423 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.488     |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.033     |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0415    |
|    value_loss           | 0.164      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 222       |
|    ep_rew_mean          | 24.2      |
| time/                   |           |
|    fps                  | 807       |
|    iterations           | 46        |
|    time_elapsed         | 116       |
|    total_timesteps      | 94208     |
| train/                  |           |
|    approx_kl            | 0.1372635 |
|    clip_fraction        | 0.318     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.498    |
|    explained_variance   | 0.644     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0211   |
|    n_updates            | 450       |
|    policy_gradient_loss | -0.0475   |
|    value_loss           | 0.145     |
---------------------------------------
Eval num_timesteps=95000, episode_reward=23.60 +/- 3.75
Episode length: 202.40 +/- 35.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | 23.6       |
| time/                   |            |
|    total_timesteps      | 95000      |
| train/                  |            |
|    approx_kl            | 0.10878645 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.459     |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0171    |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0367    |
|    value_loss           | 0.189      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 24.4     |
| time/              |          |
|    fps             | 802      |
|    iterations      | 47       |
|    time_elapsed    | 119      |
|    total_timesteps | 96256    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 221        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 805        |
|    iterations           | 48         |
|    time_elapsed         | 121        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.09451927 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.474     |
|    explained_variance   | 0.592      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0293    |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.163      |
----------------------------------------
Eval num_timesteps=100000, episode_reward=25.40 +/- 4.10
Episode length: 216.20 +/- 42.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 25.4       |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.16344322 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.48      |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00913   |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.04      |
|    value_loss           | 0.165      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 24.1     |
| time/              |          |
|    fps             | 799      |
|    iterations      | 49       |
|    time_elapsed    | 125      |
|    total_timesteps | 100352   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 803        |
|    iterations           | 50         |
|    time_elapsed         | 127        |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.10972561 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.462     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0149    |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.039     |
|    value_loss           | 0.172      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 221         |
|    ep_rew_mean          | 24.3        |
| time/                   |             |
|    fps                  | 806         |
|    iterations           | 51          |
|    time_elapsed         | 129         |
|    total_timesteps      | 104448      |
| train/                  |             |
|    approx_kl            | 0.100956276 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.476      |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0255     |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.043      |
|    value_loss           | 0.146       |
-----------------------------------------
Eval num_timesteps=105000, episode_reward=25.90 +/- 4.53
Episode length: 230.90 +/- 48.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 231        |
|    mean_reward          | 25.9       |
| time/                   |            |
|    total_timesteps      | 105000     |
| train/                  |            |
|    approx_kl            | 0.11874306 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.488     |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0182    |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0392    |
|    value_loss           | 0.169      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 24.6     |
| time/              |          |
|    fps             | 800      |
|    iterations      | 52       |
|    time_elapsed    | 133      |
|    total_timesteps | 106496   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 224         |
|    ep_rew_mean          | 24.6        |
| time/                   |             |
|    fps                  | 803         |
|    iterations           | 53          |
|    time_elapsed         | 135         |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.107422754 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.519      |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0257     |
|    n_updates            | 520         |
|    policy_gradient_loss | -0.0448     |
|    value_loss           | 0.158       |
-----------------------------------------
Eval num_timesteps=110000, episode_reward=27.50 +/- 5.92
Episode length: 243.30 +/- 58.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 243        |
|    mean_reward          | 27.5       |
| time/                   |            |
|    total_timesteps      | 110000     |
| train/                  |            |
|    approx_kl            | 0.17762277 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.504     |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0112    |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.156      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | 24.5     |
| time/              |          |
|    fps             | 796      |
|    iterations      | 54       |
|    time_elapsed    | 138      |
|    total_timesteps | 110592   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 224        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 800        |
|    iterations           | 55         |
|    time_elapsed         | 140        |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.11955501 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.486     |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0125    |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.0387    |
|    value_loss           | 0.166      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 224        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 803        |
|    iterations           | 56         |
|    time_elapsed         | 142        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.11774534 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.485     |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0128    |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.153      |
----------------------------------------
Eval num_timesteps=115000, episode_reward=24.30 +/- 5.25
Episode length: 208.50 +/- 51.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 208        |
|    mean_reward          | 24.3       |
| time/                   |            |
|    total_timesteps      | 115000     |
| train/                  |            |
|    approx_kl            | 0.10911344 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.45      |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0161    |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0366    |
|    value_loss           | 0.152      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 24.9     |
| time/              |          |
|    fps             | 798      |
|    iterations      | 57       |
|    time_elapsed    | 146      |
|    total_timesteps | 116736   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 226        |
|    ep_rew_mean          | 25         |
| time/                   |            |
|    fps                  | 801        |
|    iterations           | 58         |
|    time_elapsed         | 148        |
|    total_timesteps      | 118784     |
| train/                  |            |
|    approx_kl            | 0.10769817 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.433     |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.028     |
|    n_updates            | 570        |
|    policy_gradient_loss | -0.0401    |
|    value_loss           | 0.156      |
----------------------------------------
Eval num_timesteps=120000, episode_reward=24.70 +/- 4.63
Episode length: 219.80 +/- 35.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 220        |
|    mean_reward          | 24.7       |
| time/                   |            |
|    total_timesteps      | 120000     |
| train/                  |            |
|    approx_kl            | 0.13225822 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.424     |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.000265  |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0364    |
|    value_loss           | 0.149      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 24.6     |
| time/              |          |
|    fps             | 796      |
|    iterations      | 59       |
|    time_elapsed    | 151      |
|    total_timesteps | 120832   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 225        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 799        |
|    iterations           | 60         |
|    time_elapsed         | 153        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.12675387 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.458     |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0102    |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0407    |
|    value_loss           | 0.146      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 225         |
|    ep_rew_mean          | 24.7        |
| time/                   |             |
|    fps                  | 801         |
|    iterations           | 61          |
|    time_elapsed         | 155         |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.119855255 |
|    clip_fraction        | 0.305       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.439      |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0282     |
|    n_updates            | 600         |
|    policy_gradient_loss | -0.0396     |
|    value_loss           | 0.154       |
-----------------------------------------
Eval num_timesteps=125000, episode_reward=29.10 +/- 3.08
Episode length: 266.20 +/- 30.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 266        |
|    mean_reward          | 29.1       |
| time/                   |            |
|    total_timesteps      | 125000     |
| train/                  |            |
|    approx_kl            | 0.11610334 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.429     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00757   |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0413    |
|    value_loss           | 0.148      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 24.7     |
| time/              |          |
|    fps             | 795      |
|    iterations      | 62       |
|    time_elapsed    | 159      |
|    total_timesteps | 126976   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 221        |
|    ep_rew_mean          | 24.5       |
| time/                   |            |
|    fps                  | 798        |
|    iterations           | 63         |
|    time_elapsed         | 161        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.12384114 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.401     |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0403    |
|    n_updates            | 620        |
|    policy_gradient_loss | -0.036     |
|    value_loss           | 0.139      |
----------------------------------------
Eval num_timesteps=130000, episode_reward=25.70 +/- 3.74
Episode length: 226.50 +/- 42.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 226        |
|    mean_reward          | 25.7       |
| time/                   |            |
|    total_timesteps      | 130000     |
| train/                  |            |
|    approx_kl            | 0.15482116 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.433     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0425    |
|    n_updates            | 630        |
|    policy_gradient_loss | -0.0438    |
|    value_loss           | 0.143      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 24.3     |
| time/              |          |
|    fps             | 793      |
|    iterations      | 64       |
|    time_elapsed    | 165      |
|    total_timesteps | 131072   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 218       |
|    ep_rew_mean          | 24.3      |
| time/                   |           |
|    fps                  | 796       |
|    iterations           | 65        |
|    time_elapsed         | 167       |
|    total_timesteps      | 133120    |
| train/                  |           |
|    approx_kl            | 0.1442353 |
|    clip_fraction        | 0.306     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.44     |
|    explained_variance   | 0.584     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0327   |
|    n_updates            | 640       |
|    policy_gradient_loss | -0.0444   |
|    value_loss           | 0.168     |
---------------------------------------
Eval num_timesteps=135000, episode_reward=29.10 +/- 4.53
Episode length: 265.00 +/- 58.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 265        |
|    mean_reward          | 29.1       |
| time/                   |            |
|    total_timesteps      | 135000     |
| train/                  |            |
|    approx_kl            | 0.14465725 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.423     |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0205    |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.0431    |
|    value_loss           | 0.155      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 24.4     |
| time/              |          |
|    fps             | 790      |
|    iterations      | 66       |
|    time_elapsed    | 170      |
|    total_timesteps | 135168   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 215        |
|    ep_rew_mean          | 24         |
| time/                   |            |
|    fps                  | 793        |
|    iterations           | 67         |
|    time_elapsed         | 173        |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.14445113 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.43      |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0344    |
|    n_updates            | 660        |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.15       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 215       |
|    ep_rew_mean          | 23.9      |
| time/                   |           |
|    fps                  | 795       |
|    iterations           | 68        |
|    time_elapsed         | 175       |
|    total_timesteps      | 139264    |
| train/                  |           |
|    approx_kl            | 0.1290207 |
|    clip_fraction        | 0.3       |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.416    |
|    explained_variance   | 0.596     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0149   |
|    n_updates            | 670       |
|    policy_gradient_loss | -0.0386   |
|    value_loss           | 0.159     |
---------------------------------------
Eval num_timesteps=140000, episode_reward=28.20 +/- 5.67
Episode length: 258.20 +/- 61.20
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 258       |
|    mean_reward          | 28.2      |
| time/                   |           |
|    total_timesteps      | 140000    |
| train/                  |           |
|    approx_kl            | 0.1323668 |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.422    |
|    explained_variance   | 0.622     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0197   |
|    n_updates            | 680       |
|    policy_gradient_loss | -0.0406   |
|    value_loss           | 0.153     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 24       |
| time/              |          |
|    fps             | 790      |
|    iterations      | 69       |
|    time_elapsed    | 178      |
|    total_timesteps | 141312   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 216        |
|    ep_rew_mean          | 24.1       |
| time/                   |            |
|    fps                  | 792        |
|    iterations           | 70         |
|    time_elapsed         | 180        |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.13338459 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.411     |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0363    |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0391    |
|    value_loss           | 0.152      |
----------------------------------------
Eval num_timesteps=145000, episode_reward=25.20 +/- 6.93
Episode length: 232.80 +/- 70.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 233        |
|    mean_reward          | 25.2       |
| time/                   |            |
|    total_timesteps      | 145000     |
| train/                  |            |
|    approx_kl            | 0.13353647 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.393     |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00354   |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0372    |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 24.1     |
| time/              |          |
|    fps             | 788      |
|    iterations      | 71       |
|    time_elapsed    | 184      |
|    total_timesteps | 145408   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 218        |
|    ep_rew_mean          | 24.3       |
| time/                   |            |
|    fps                  | 790        |
|    iterations           | 72         |
|    time_elapsed         | 186        |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.13226259 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.432     |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0239    |
|    n_updates            | 710        |
|    policy_gradient_loss | -0.0424    |
|    value_loss           | 0.149      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 214        |
|    ep_rew_mean          | 23.9       |
| time/                   |            |
|    fps                  | 793        |
|    iterations           | 73         |
|    time_elapsed         | 188        |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.14064237 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.439     |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0221    |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.156      |
----------------------------------------
Eval num_timesteps=150000, episode_reward=23.30 +/- 3.63
Episode length: 197.70 +/- 39.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 198         |
|    mean_reward          | 23.3        |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.120700486 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.454      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0373     |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.0443     |
|    value_loss           | 0.153       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 23.9     |
| time/              |          |
|    fps             | 790      |
|    iterations      | 74       |
|    time_elapsed    | 191      |
|    total_timesteps | 151552   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 792        |
|    iterations           | 75         |
|    time_elapsed         | 193        |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.12523872 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.458     |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0197    |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0411    |
|    value_loss           | 0.145      |
----------------------------------------
Eval num_timesteps=155000, episode_reward=23.10 +/- 6.92
Episode length: 219.40 +/- 70.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 219        |
|    mean_reward          | 23.1       |
| time/                   |            |
|    total_timesteps      | 155000     |
| train/                  |            |
|    approx_kl            | 0.13424514 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.464     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0237    |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.041     |
|    value_loss           | 0.158      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 24       |
| time/              |          |
|    fps             | 788      |
|    iterations      | 76       |
|    time_elapsed    | 197      |
|    total_timesteps | 155648   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 215        |
|    ep_rew_mean          | 23.7       |
| time/                   |            |
|    fps                  | 791        |
|    iterations           | 77         |
|    time_elapsed         | 199        |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.15418777 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.439     |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0241    |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.0473    |
|    value_loss           | 0.148      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 216        |
|    ep_rew_mean          | 23.8       |
| time/                   |            |
|    fps                  | 793        |
|    iterations           | 78         |
|    time_elapsed         | 201        |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.11812493 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.429     |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.028     |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.0458    |
|    value_loss           | 0.138      |
----------------------------------------
Eval num_timesteps=160000, episode_reward=23.10 +/- 5.03
Episode length: 196.60 +/- 45.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 197        |
|    mean_reward          | 23.1       |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.12531008 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.413     |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0226    |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.0422    |
|    value_loss           | 0.135      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 23.8     |
| time/              |          |
|    fps             | 790      |
|    iterations      | 79       |
|    time_elapsed    | 204      |
|    total_timesteps | 161792   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 215        |
|    ep_rew_mean          | 23.6       |
| time/                   |            |
|    fps                  | 792        |
|    iterations           | 80         |
|    time_elapsed         | 206        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.14667486 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.424     |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.019     |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0427    |
|    value_loss           | 0.144      |
----------------------------------------
Eval num_timesteps=165000, episode_reward=24.10 +/- 8.48
Episode length: 224.30 +/- 69.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 224         |
|    mean_reward          | 24.1        |
| time/                   |             |
|    total_timesteps      | 165000      |
| train/                  |             |
|    approx_kl            | 0.123163685 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.42       |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0268     |
|    n_updates            | 800         |
|    policy_gradient_loss | -0.0391     |
|    value_loss           | 0.143       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | 23.8     |
| time/              |          |
|    fps             | 788      |
|    iterations      | 81       |
|    time_elapsed    | 210      |
|    total_timesteps | 165888   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 217        |
|    ep_rew_mean          | 23.8       |
| time/                   |            |
|    fps                  | 791        |
|    iterations           | 82         |
|    time_elapsed         | 212        |
|    total_timesteps      | 167936     |
| train/                  |            |
|    approx_kl            | 0.16288272 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.395     |
|    explained_variance   | 0.585      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0252    |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.0434    |
|    value_loss           | 0.153      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 217        |
|    ep_rew_mean          | 23.9       |
| time/                   |            |
|    fps                  | 793        |
|    iterations           | 83         |
|    time_elapsed         | 214        |
|    total_timesteps      | 169984     |
| train/                  |            |
|    approx_kl            | 0.12253686 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0258    |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0368    |
|    value_loss           | 0.149      |
----------------------------------------
Eval num_timesteps=170000, episode_reward=24.10 +/- 6.20
Episode length: 215.00 +/- 60.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | 24.1       |
| time/                   |            |
|    total_timesteps      | 170000     |
| train/                  |            |
|    approx_kl            | 0.14726451 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.428     |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0328    |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.0424    |
|    value_loss           | 0.158      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 24       |
| time/              |          |
|    fps             | 789      |
|    iterations      | 84       |
|    time_elapsed    | 217      |
|    total_timesteps | 172032   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 220        |
|    ep_rew_mean          | 24.3       |
| time/                   |            |
|    fps                  | 792        |
|    iterations           | 85         |
|    time_elapsed         | 219        |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.17153579 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.427     |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0215    |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0422    |
|    value_loss           | 0.155      |
----------------------------------------
Eval num_timesteps=175000, episode_reward=27.20 +/- 3.79
Episode length: 239.70 +/- 47.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 240        |
|    mean_reward          | 27.2       |
| time/                   |            |
|    total_timesteps      | 175000     |
| train/                  |            |
|    approx_kl            | 0.12882194 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.405     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0265    |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.0413    |
|    value_loss           | 0.154      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 24.4     |
| time/              |          |
|    fps             | 788      |
|    iterations      | 86       |
|    time_elapsed    | 223      |
|    total_timesteps | 176128   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 221        |
|    ep_rew_mean          | 24.5       |
| time/                   |            |
|    fps                  | 790        |
|    iterations           | 87         |
|    time_elapsed         | 225        |
|    total_timesteps      | 178176     |
| train/                  |            |
|    approx_kl            | 0.19912499 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.601      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0195    |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 0.146      |
----------------------------------------
Eval num_timesteps=180000, episode_reward=22.20 +/- 6.84
Episode length: 210.30 +/- 77.88
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 22.2       |
| time/                   |            |
|    total_timesteps      | 180000     |
| train/                  |            |
|    approx_kl            | 0.18036282 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.347     |
|    explained_variance   | 0.616      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0107    |
|    n_updates            | 870        |
|    policy_gradient_loss | -0.0363    |
|    value_loss           | 0.15       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 24.5     |
| time/              |          |
|    fps             | 787      |
|    iterations      | 88       |
|    time_elapsed    | 228      |
|    total_timesteps | 180224   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 789        |
|    iterations           | 89         |
|    time_elapsed         | 230        |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.14659959 |
|    clip_fraction        | 0.283      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.376     |
|    explained_variance   | 0.659      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0145    |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.0341    |
|    value_loss           | 0.143      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 221        |
|    ep_rew_mean          | 24.4       |
| time/                   |            |
|    fps                  | 791        |
|    iterations           | 90         |
|    time_elapsed         | 232        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.16438346 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.376     |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0396    |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0376    |
|    value_loss           | 0.145      |
----------------------------------------
Eval num_timesteps=185000, episode_reward=22.50 +/- 5.02
Episode length: 201.30 +/- 47.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 201        |
|    mean_reward          | 22.5       |
| time/                   |            |
|    total_timesteps      | 185000     |
| train/                  |            |
|    approx_kl            | 0.14376113 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.394     |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0324    |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0405    |
|    value_loss           | 0.137      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 222      |
|    ep_rew_mean     | 24.6     |
| time/              |          |
|    fps             | 788      |
|    iterations      | 91       |
|    time_elapsed    | 236      |
|    total_timesteps | 186368   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 218        |
|    ep_rew_mean          | 24.1       |
| time/                   |            |
|    fps                  | 790        |
|    iterations           | 92         |
|    time_elapsed         | 238        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.16237774 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.396     |
|    explained_variance   | 0.688      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0377    |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.0403    |
|    value_loss           | 0.131      |
----------------------------------------
Eval num_timesteps=190000, episode_reward=21.50 +/- 3.50
Episode length: 189.10 +/- 34.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 189        |
|    mean_reward          | 21.5       |
| time/                   |            |
|    total_timesteps      | 190000     |
| train/                  |            |
|    approx_kl            | 0.19588229 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.41      |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0458    |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0417    |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 24.1     |
| time/              |          |
|    fps             | 788      |
|    iterations      | 93       |
|    time_elapsed    | 241      |
|    total_timesteps | 190464   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 223        |
|    ep_rew_mean          | 24.5       |
| time/                   |            |
|    fps                  | 790        |
|    iterations           | 94         |
|    time_elapsed         | 243        |
|    total_timesteps      | 192512     |
| train/                  |            |
|    approx_kl            | 0.14458394 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.411     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0397    |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.0412    |
|    value_loss           | 0.138      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 24.1       |
| time/                   |            |
|    fps                  | 792        |
|    iterations           | 95         |
|    time_elapsed         | 245        |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.17337754 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.405     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0467    |
|    n_updates            | 940        |
|    policy_gradient_loss | -0.0422    |
|    value_loss           | 0.134      |
----------------------------------------
Eval num_timesteps=195000, episode_reward=24.20 +/- 5.08
Episode length: 219.40 +/- 57.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 219        |
|    mean_reward          | 24.2       |
| time/                   |            |
|    total_timesteps      | 195000     |
| train/                  |            |
|    approx_kl            | 0.16920687 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.386     |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0243    |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.0408    |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 24       |
| time/              |          |
|    fps             | 789      |
|    iterations      | 96       |
|    time_elapsed    | 249      |
|    total_timesteps | 196608   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 216       |
|    ep_rew_mean          | 23.7      |
| time/                   |           |
|    fps                  | 790       |
|    iterations           | 97        |
|    time_elapsed         | 251       |
|    total_timesteps      | 198656    |
| train/                  |           |
|    approx_kl            | 0.2015844 |
|    clip_fraction        | 0.307     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.39     |
|    explained_variance   | 0.625     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0302   |
|    n_updates            | 960       |
|    policy_gradient_loss | -0.0371   |
|    value_loss           | 0.141     |
---------------------------------------
Eval num_timesteps=200000, episode_reward=27.00 +/- 4.69
Episode length: 243.90 +/- 49.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 244        |
|    mean_reward          | 27         |
| time/                   |            |
|    total_timesteps      | 200000     |
| train/                  |            |
|    approx_kl            | 0.16154677 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.405     |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.023     |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0392    |
|    value_loss           | 0.129      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | 23.8     |
| time/              |          |
|    fps             | 787      |
|    iterations      | 98       |
|    time_elapsed    | 254      |
|    total_timesteps | 200704   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 23.9       |
| time/                   |            |
|    fps                  | 789        |
|    iterations           | 99         |
|    time_elapsed         | 256        |
|    total_timesteps      | 202752     |
| train/                  |            |
|    approx_kl            | 0.19580582 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.371     |
|    explained_variance   | 0.552      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00073   |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.035     |
|    value_loss           | 0.165      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | 24.3       |
| time/                   |            |
|    fps                  | 791        |
|    iterations           | 100        |
|    time_elapsed         | 258        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.16316165 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.369     |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0154    |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.0396    |
|    value_loss           | 0.137      |
----------------------------------------
Eval num_timesteps=205000, episode_reward=23.90 +/- 5.30
Episode length: 223.60 +/- 54.59
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 224        |
|    mean_reward          | 23.9       |
| time/                   |            |
|    total_timesteps      | 205000     |
| train/                  |            |
|    approx_kl            | 0.17016326 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.39      |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0337    |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0428    |
|    value_loss           | 0.138      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 24.5     |
| time/              |          |
|    fps             | 788      |
|    iterations      | 101      |
|    time_elapsed    | 262      |
|    total_timesteps | 206848   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 224        |
|    ep_rew_mean          | 24.5       |
| time/                   |            |
|    fps                  | 790        |
|    iterations           | 102        |
|    time_elapsed         | 264        |
|    total_timesteps      | 208896     |
| train/                  |            |
|    approx_kl            | 0.20488335 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.387     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0235    |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.132      |
----------------------------------------
Eval num_timesteps=210000, episode_reward=24.70 +/- 5.62
Episode length: 217.90 +/- 54.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 218       |
|    mean_reward          | 24.7      |
| time/                   |           |
|    total_timesteps      | 210000    |
| train/                  |           |
|    approx_kl            | 0.1693281 |
|    clip_fraction        | 0.295     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.387    |
|    explained_variance   | 0.644     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0496   |
|    n_updates            | 1020      |
|    policy_gradient_loss | -0.0417   |
|    value_loss           | 0.142     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 230      |
|    ep_rew_mean     | 25.2     |
| time/              |          |
|    fps             | 787      |
|    iterations      | 103      |
|    time_elapsed    | 267      |
|    total_timesteps | 210944   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 25.3       |
| time/                   |            |
|    fps                  | 789        |
|    iterations           | 104        |
|    time_elapsed         | 269        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.20272425 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.389     |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0361    |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0412    |
|    value_loss           | 0.132      |
----------------------------------------
Eval num_timesteps=215000, episode_reward=24.30 +/- 5.80
Episode length: 219.30 +/- 54.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 219        |
|    mean_reward          | 24.3       |
| time/                   |            |
|    total_timesteps      | 215000     |
| train/                  |            |
|    approx_kl            | 0.18071167 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.385     |
|    explained_variance   | 0.601      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0254    |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.0436    |
|    value_loss           | 0.148      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 226      |
|    ep_rew_mean     | 24.9     |
| time/              |          |
|    fps             | 786      |
|    iterations      | 105      |
|    time_elapsed    | 273      |
|    total_timesteps | 215040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 228        |
|    ep_rew_mean          | 25         |
| time/                   |            |
|    fps                  | 788        |
|    iterations           | 106        |
|    time_elapsed         | 275        |
|    total_timesteps      | 217088     |
| train/                  |            |
|    approx_kl            | 0.17124936 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.389     |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0297    |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.0427    |
|    value_loss           | 0.153      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 226        |
|    ep_rew_mean          | 24.8       |
| time/                   |            |
|    fps                  | 789        |
|    iterations           | 107        |
|    time_elapsed         | 277        |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.16022326 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.409     |
|    explained_variance   | 0.64       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0394    |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.15       |
----------------------------------------
Eval num_timesteps=220000, episode_reward=25.70 +/- 4.34
Episode length: 238.20 +/- 49.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 238        |
|    mean_reward          | 25.7       |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.19059426 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.395     |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0527    |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.136      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 24.9     |
| time/              |          |
|    fps             | 786      |
|    iterations      | 108      |
|    time_elapsed    | 281      |
|    total_timesteps | 221184   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 225        |
|    ep_rew_mean          | 24.6       |
| time/                   |            |
|    fps                  | 788        |
|    iterations           | 109        |
|    time_elapsed         | 283        |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.17577824 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.377     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0229    |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.0395    |
|    value_loss           | 0.129      |
----------------------------------------
Eval num_timesteps=225000, episode_reward=24.40 +/- 4.94
Episode length: 217.90 +/- 49.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 218        |
|    mean_reward          | 24.4       |
| time/                   |            |
|    total_timesteps      | 225000     |
| train/                  |            |
|    approx_kl            | 0.15803564 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.389     |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0293    |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0407    |
|    value_loss           | 0.137      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 226      |
|    ep_rew_mean     | 24.7     |
| time/              |          |
|    fps             | 786      |
|    iterations      | 110      |
|    time_elapsed    | 286      |
|    total_timesteps | 225280   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 225        |
|    ep_rew_mean          | 24.6       |
| time/                   |            |
|    fps                  | 787        |
|    iterations           | 111        |
|    time_elapsed         | 288        |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.16814183 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.366     |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0355    |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0387    |
|    value_loss           | 0.146      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 225        |
|    ep_rew_mean          | 24.5       |
| time/                   |            |
|    fps                  | 789        |
|    iterations           | 112        |
|    time_elapsed         | 290        |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.22088891 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0172    |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.14       |
----------------------------------------
Eval num_timesteps=230000, episode_reward=23.20 +/- 6.48
Episode length: 212.50 +/- 65.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 212        |
|    mean_reward          | 23.2       |
| time/                   |            |
|    total_timesteps      | 230000     |
| train/                  |            |
|    approx_kl            | 0.19161037 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.397     |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0369    |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0439    |
|    value_loss           | 0.139      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 24.3     |
| time/              |          |
|    fps             | 786      |
|    iterations      | 113      |
|    time_elapsed    | 294      |
|    total_timesteps | 231424   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 23.7       |
| time/                   |            |
|    fps                  | 788        |
|    iterations           | 114        |
|    time_elapsed         | 296        |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.17002925 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.382     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0362    |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.0417    |
|    value_loss           | 0.129      |
----------------------------------------
Eval num_timesteps=235000, episode_reward=28.80 +/- 5.91
Episode length: 262.70 +/- 68.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 263        |
|    mean_reward          | 28.8       |
| time/                   |            |
|    total_timesteps      | 235000     |
| train/                  |            |
|    approx_kl            | 0.18697627 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.402     |
|    explained_variance   | 0.536      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0141    |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.0439    |
|    value_loss           | 0.147      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 23.7     |
| time/              |          |
|    fps             | 785      |
|    iterations      | 115      |
|    time_elapsed    | 299      |
|    total_timesteps | 235520   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 23.6       |
| time/                   |            |
|    fps                  | 786        |
|    iterations           | 116        |
|    time_elapsed         | 301        |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.16163325 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.431     |
|    explained_variance   | 0.586      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0208    |
|    n_updates            | 1150       |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.132      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 220        |
|    ep_rew_mean          | 23.8       |
| time/                   |            |
|    fps                  | 788        |
|    iterations           | 117        |
|    time_elapsed         | 303        |
|    total_timesteps      | 239616     |
| train/                  |            |
|    approx_kl            | 0.17144723 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.424     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0497    |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.0478    |
|    value_loss           | 0.138      |
----------------------------------------
Eval num_timesteps=240000, episode_reward=27.60 +/- 4.90
Episode length: 249.40 +/- 45.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 249       |
|    mean_reward          | 27.6      |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.1642245 |
|    clip_fraction        | 0.312     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.414    |
|    explained_variance   | 0.598     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0503   |
|    n_updates            | 1170      |
|    policy_gradient_loss | -0.0487   |
|    value_loss           | 0.135     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | 24.2     |
| time/              |          |
|    fps             | 785      |
|    iterations      | 118      |
|    time_elapsed    | 307      |
|    total_timesteps | 241664   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 224        |
|    ep_rew_mean          | 24.3       |
| time/                   |            |
|    fps                  | 787        |
|    iterations           | 119        |
|    time_elapsed         | 309        |
|    total_timesteps      | 243712     |
| train/                  |            |
|    approx_kl            | 0.16255343 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.411     |
|    explained_variance   | 0.573      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0585    |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.142      |
----------------------------------------
Eval num_timesteps=245000, episode_reward=26.90 +/- 3.39
Episode length: 237.90 +/- 33.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 238        |
|    mean_reward          | 26.9       |
| time/                   |            |
|    total_timesteps      | 245000     |
| train/                  |            |
|    approx_kl            | 0.17996234 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.403     |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0376    |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.0444    |
|    value_loss           | 0.123      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 24.6     |
| time/              |          |
|    fps             | 784      |
|    iterations      | 120      |
|    time_elapsed    | 313      |
|    total_timesteps | 245760   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 226        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 121        |
|    time_elapsed         | 315        |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.16389291 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.375     |
|    explained_variance   | 0.6        |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0294    |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0453    |
|    value_loss           | 0.134      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 228        |
|    ep_rew_mean          | 24.8       |
| time/                   |            |
|    fps                  | 787        |
|    iterations           | 122        |
|    time_elapsed         | 317        |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.20391747 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.375     |
|    explained_variance   | 0.558      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0202    |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.0425    |
|    value_loss           | 0.159      |
----------------------------------------
Eval num_timesteps=250000, episode_reward=27.50 +/- 5.57
Episode length: 254.50 +/- 58.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 254        |
|    mean_reward          | 27.5       |
| time/                   |            |
|    total_timesteps      | 250000     |
| train/                  |            |
|    approx_kl            | 0.23150377 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.356     |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0289    |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.155      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 24.4     |
| time/              |          |
|    fps             | 784      |
|    iterations      | 123      |
|    time_elapsed    | 321      |
|    total_timesteps | 251904   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 786        |
|    iterations           | 124        |
|    time_elapsed         | 323        |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.20678875 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.365     |
|    explained_variance   | 0.579      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0496    |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0478    |
|    value_loss           | 0.144      |
----------------------------------------
Eval num_timesteps=255000, episode_reward=23.10 +/- 7.62
Episode length: 210.10 +/- 76.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 210        |
|    mean_reward          | 23.1       |
| time/                   |            |
|    total_timesteps      | 255000     |
| train/                  |            |
|    approx_kl            | 0.25485057 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0358    |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.0423    |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 24       |
| time/              |          |
|    fps             | 784      |
|    iterations      | 125      |
|    time_elapsed    | 326      |
|    total_timesteps | 256000   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | 24.1       |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 126        |
|    time_elapsed         | 328        |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.24456061 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.293     |
|    explained_variance   | 0.571      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0478    |
|    n_updates            | 1250       |
|    policy_gradient_loss | -0.039     |
|    value_loss           | 0.147      |
----------------------------------------
Eval num_timesteps=260000, episode_reward=27.10 +/- 5.34
Episode length: 242.30 +/- 52.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 242        |
|    mean_reward          | 27.1       |
| time/                   |            |
|    total_timesteps      | 260000     |
| train/                  |            |
|    approx_kl            | 0.19113997 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.335     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0312    |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.0372    |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | 24.2     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 127      |
|    time_elapsed    | 332      |
|    total_timesteps | 260096   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 223        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 784        |
|    iterations           | 128        |
|    time_elapsed         | 334        |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.21104465 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.339     |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0325    |
|    n_updates            | 1270       |
|    policy_gradient_loss | -0.0358    |
|    value_loss           | 0.134      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 226        |
|    ep_rew_mean          | 24.5       |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 129        |
|    time_elapsed         | 336        |
|    total_timesteps      | 264192     |
| train/                  |            |
|    approx_kl            | 0.22394368 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0419    |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.0415    |
|    value_loss           | 0.135      |
----------------------------------------
Eval num_timesteps=265000, episode_reward=22.50 +/- 6.09
Episode length: 188.80 +/- 55.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 189        |
|    mean_reward          | 22.5       |
| time/                   |            |
|    total_timesteps      | 265000     |
| train/                  |            |
|    approx_kl            | 0.17254958 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.337     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0375    |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.0366    |
|    value_loss           | 0.129      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 226      |
|    ep_rew_mean     | 24.5     |
| time/              |          |
|    fps             | 784      |
|    iterations      | 130      |
|    time_elapsed    | 339      |
|    total_timesteps | 266240   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 224       |
|    ep_rew_mean          | 24.4      |
| time/                   |           |
|    fps                  | 785       |
|    iterations           | 131       |
|    time_elapsed         | 341       |
|    total_timesteps      | 268288    |
| train/                  |           |
|    approx_kl            | 0.1426021 |
|    clip_fraction        | 0.258     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.325    |
|    explained_variance   | 0.584     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0426   |
|    n_updates            | 1300      |
|    policy_gradient_loss | -0.0404   |
|    value_loss           | 0.142     |
---------------------------------------
Eval num_timesteps=270000, episode_reward=21.80 +/- 5.15
Episode length: 193.20 +/- 48.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 193        |
|    mean_reward          | 21.8       |
| time/                   |            |
|    total_timesteps      | 270000     |
| train/                  |            |
|    approx_kl            | 0.18831852 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.336     |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0322    |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.134      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 24.4     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 132      |
|    time_elapsed    | 344      |
|    total_timesteps | 270336   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 223       |
|    ep_rew_mean          | 24.2      |
| time/                   |           |
|    fps                  | 785       |
|    iterations           | 133       |
|    time_elapsed         | 346       |
|    total_timesteps      | 272384    |
| train/                  |           |
|    approx_kl            | 0.2293174 |
|    clip_fraction        | 0.291     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.341    |
|    explained_variance   | 0.618     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0212   |
|    n_updates            | 1320      |
|    policy_gradient_loss | -0.0414   |
|    value_loss           | 0.139     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 227        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 786        |
|    iterations           | 134        |
|    time_elapsed         | 348        |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.20584537 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.343     |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0386    |
|    n_updates            | 1330       |
|    policy_gradient_loss | -0.0439    |
|    value_loss           | 0.153      |
----------------------------------------
Eval num_timesteps=275000, episode_reward=24.70 +/- 4.43
Episode length: 219.60 +/- 41.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 220        |
|    mean_reward          | 24.7       |
| time/                   |            |
|    total_timesteps      | 275000     |
| train/                  |            |
|    approx_kl            | 0.20190276 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.334     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.017     |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0396    |
|    value_loss           | 0.142      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 25.4     |
| time/              |          |
|    fps             | 784      |
|    iterations      | 135      |
|    time_elapsed    | 352      |
|    total_timesteps | 276480   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 237        |
|    ep_rew_mean          | 26         |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 136        |
|    time_elapsed         | 354        |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.18835226 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0425    |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.0442    |
|    value_loss           | 0.15       |
----------------------------------------
Eval num_timesteps=280000, episode_reward=24.80 +/- 6.00
Episode length: 216.50 +/- 54.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 24.8       |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.19866922 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.367     |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0532    |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0392    |
|    value_loss           | 0.131      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 236      |
|    ep_rew_mean     | 25.9     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 137      |
|    time_elapsed    | 357      |
|    total_timesteps | 280576   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 236        |
|    ep_rew_mean          | 26         |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 138        |
|    time_elapsed         | 359        |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.18894462 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.379     |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0672    |
|    n_updates            | 1370       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.135      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 235        |
|    ep_rew_mean          | 25.8       |
| time/                   |            |
|    fps                  | 786        |
|    iterations           | 139        |
|    time_elapsed         | 361        |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.20583645 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.398     |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0523    |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0485    |
|    value_loss           | 0.128      |
----------------------------------------
Eval num_timesteps=285000, episode_reward=23.30 +/- 6.68
Episode length: 214.70 +/- 71.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | 23.3       |
| time/                   |            |
|    total_timesteps      | 285000     |
| train/                  |            |
|    approx_kl            | 0.22226302 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.394     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0314    |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.0474    |
|    value_loss           | 0.142      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 25.9     |
| time/              |          |
|    fps             | 784      |
|    iterations      | 140      |
|    time_elapsed    | 365      |
|    total_timesteps | 286720   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 25.4       |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 141        |
|    time_elapsed         | 367        |
|    total_timesteps      | 288768     |
| train/                  |            |
|    approx_kl            | 0.19229108 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.389     |
|    explained_variance   | 0.555      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.03      |
|    n_updates            | 1400       |
|    policy_gradient_loss | -0.0444    |
|    value_loss           | 0.156      |
----------------------------------------
Eval num_timesteps=290000, episode_reward=27.80 +/- 7.40
Episode length: 259.50 +/- 78.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 260        |
|    mean_reward          | 27.8       |
| time/                   |            |
|    total_timesteps      | 290000     |
| train/                  |            |
|    approx_kl            | 0.20609441 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.398     |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.049     |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.0467    |
|    value_loss           | 0.126      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 25.4     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 142      |
|    time_elapsed    | 371      |
|    total_timesteps | 290816   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 25.2       |
| time/                   |            |
|    fps                  | 784        |
|    iterations           | 143        |
|    time_elapsed         | 373        |
|    total_timesteps      | 292864     |
| train/                  |            |
|    approx_kl            | 0.21573052 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.406     |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0252    |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0493    |
|    value_loss           | 0.123      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 230        |
|    ep_rew_mean          | 24.9       |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 144        |
|    time_elapsed         | 375        |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.20047548 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.407     |
|    explained_variance   | 0.592      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0602    |
|    n_updates            | 1430       |
|    policy_gradient_loss | -0.0462    |
|    value_loss           | 0.141      |
----------------------------------------
Eval num_timesteps=295000, episode_reward=27.50 +/- 2.84
Episode length: 246.80 +/- 26.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 247        |
|    mean_reward          | 27.5       |
| time/                   |            |
|    total_timesteps      | 295000     |
| train/                  |            |
|    approx_kl            | 0.21445395 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.399     |
|    explained_variance   | 0.506      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0311    |
|    n_updates            | 1440       |
|    policy_gradient_loss | -0.048     |
|    value_loss           | 0.128      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 24.5     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 145      |
|    time_elapsed    | 378      |
|    total_timesteps | 296960   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 226        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 784        |
|    iterations           | 146        |
|    time_elapsed         | 380        |
|    total_timesteps      | 299008     |
| train/                  |            |
|    approx_kl            | 0.20947671 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.421     |
|    explained_variance   | 0.528      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0439    |
|    n_updates            | 1450       |
|    policy_gradient_loss | -0.0509    |
|    value_loss           | 0.14       |
----------------------------------------
Eval num_timesteps=300000, episode_reward=23.10 +/- 6.28
Episode length: 208.50 +/- 66.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 208        |
|    mean_reward          | 23.1       |
| time/                   |            |
|    total_timesteps      | 300000     |
| train/                  |            |
|    approx_kl            | 0.56270754 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.399     |
|    explained_variance   | 0.529      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0311    |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.0488    |
|    value_loss           | 0.133      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 23.9     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 147      |
|    time_elapsed    | 384      |
|    total_timesteps | 301056   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | 23.6       |
| time/                   |            |
|    fps                  | 784        |
|    iterations           | 148        |
|    time_elapsed         | 386        |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.19973278 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0504    |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.0481    |
|    value_loss           | 0.135      |
----------------------------------------
Eval num_timesteps=305000, episode_reward=25.40 +/- 6.18
Episode length: 239.20 +/- 65.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 239        |
|    mean_reward          | 25.4       |
| time/                   |            |
|    total_timesteps      | 305000     |
| train/                  |            |
|    approx_kl            | 0.20523505 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.411     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0746    |
|    n_updates            | 1480       |
|    policy_gradient_loss | -0.043     |
|    value_loss           | 0.134      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 23.4     |
| time/              |          |
|    fps             | 782      |
|    iterations      | 149      |
|    time_elapsed    | 390      |
|    total_timesteps | 305152   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | 23.8       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 150        |
|    time_elapsed         | 392        |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.17845808 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.395     |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0546    |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0478    |
|    value_loss           | 0.13       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 224        |
|    ep_rew_mean          | 24         |
| time/                   |            |
|    fps                  | 784        |
|    iterations           | 151        |
|    time_elapsed         | 394        |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.18706506 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.405     |
|    explained_variance   | 0.61       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0325    |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.047     |
|    value_loss           | 0.141      |
----------------------------------------
Eval num_timesteps=310000, episode_reward=28.40 +/- 4.65
Episode length: 255.40 +/- 56.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 255        |
|    mean_reward          | 28.4       |
| time/                   |            |
|    total_timesteps      | 310000     |
| train/                  |            |
|    approx_kl            | 0.18295883 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.411     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0444    |
|    n_updates            | 1510       |
|    policy_gradient_loss | -0.0481    |
|    value_loss           | 0.136      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 24.1     |
| time/              |          |
|    fps             | 782      |
|    iterations      | 152      |
|    time_elapsed    | 397      |
|    total_timesteps | 311296   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 224        |
|    ep_rew_mean          | 24.1       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 153        |
|    time_elapsed         | 399        |
|    total_timesteps      | 313344     |
| train/                  |            |
|    approx_kl            | 0.19770063 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.409     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0393    |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0442    |
|    value_loss           | 0.122      |
----------------------------------------
Eval num_timesteps=315000, episode_reward=27.50 +/- 6.42
Episode length: 256.10 +/- 67.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 256        |
|    mean_reward          | 27.5       |
| time/                   |            |
|    total_timesteps      | 315000     |
| train/                  |            |
|    approx_kl            | 0.30201605 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.403     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0363    |
|    n_updates            | 1530       |
|    policy_gradient_loss | -0.0487    |
|    value_loss           | 0.137      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 226      |
|    ep_rew_mean     | 24.2     |
| time/              |          |
|    fps             | 781      |
|    iterations      | 154      |
|    time_elapsed    | 403      |
|    total_timesteps | 315392   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 226        |
|    ep_rew_mean          | 24.4       |
| time/                   |            |
|    fps                  | 782        |
|    iterations           | 155        |
|    time_elapsed         | 405        |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.24368292 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.389     |
|    explained_variance   | 0.567      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0445    |
|    n_updates            | 1540       |
|    policy_gradient_loss | -0.0473    |
|    value_loss           | 0.147      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 228        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 156        |
|    time_elapsed         | 407        |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.19142358 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.366     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0622    |
|    n_updates            | 1550       |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.141      |
----------------------------------------
Eval num_timesteps=320000, episode_reward=26.30 +/- 5.51
Episode length: 236.80 +/- 50.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 237        |
|    mean_reward          | 26.3       |
| time/                   |            |
|    total_timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.15957817 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.385     |
|    explained_variance   | 0.532      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0551    |
|    n_updates            | 1560       |
|    policy_gradient_loss | -0.0439    |
|    value_loss           | 0.137      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 230      |
|    ep_rew_mean     | 25       |
| time/              |          |
|    fps             | 781      |
|    iterations      | 157      |
|    time_elapsed    | 411      |
|    total_timesteps | 321536   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 228        |
|    ep_rew_mean          | 24.8       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 158        |
|    time_elapsed         | 413        |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.17999846 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.347     |
|    explained_variance   | 0.584      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0296    |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.0431    |
|    value_loss           | 0.127      |
----------------------------------------
Eval num_timesteps=325000, episode_reward=27.40 +/- 4.18
Episode length: 250.80 +/- 41.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 251        |
|    mean_reward          | 27.4       |
| time/                   |            |
|    total_timesteps      | 325000     |
| train/                  |            |
|    approx_kl            | 0.20431897 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.37      |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0528    |
|    n_updates            | 1580       |
|    policy_gradient_loss | -0.0431    |
|    value_loss           | 0.15       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 25       |
| time/              |          |
|    fps             | 781      |
|    iterations      | 159      |
|    time_elapsed    | 416      |
|    total_timesteps | 325632   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 228       |
|    ep_rew_mean          | 25        |
| time/                   |           |
|    fps                  | 782       |
|    iterations           | 160       |
|    time_elapsed         | 418       |
|    total_timesteps      | 327680    |
| train/                  |           |
|    approx_kl            | 0.2567932 |
|    clip_fraction        | 0.329     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.384    |
|    explained_variance   | 0.583     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0543   |
|    n_updates            | 1590      |
|    policy_gradient_loss | -0.0422   |
|    value_loss           | 0.142     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 25.4       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 161        |
|    time_elapsed         | 420        |
|    total_timesteps      | 329728     |
| train/                  |            |
|    approx_kl            | 0.19704583 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.375     |
|    explained_variance   | 0.61       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0269    |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.0407    |
|    value_loss           | 0.155      |
----------------------------------------
Eval num_timesteps=330000, episode_reward=26.70 +/- 4.71
Episode length: 237.50 +/- 55.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 238        |
|    mean_reward          | 26.7       |
| time/                   |            |
|    total_timesteps      | 330000     |
| train/                  |            |
|    approx_kl            | 0.20456122 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.376     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0457    |
|    n_updates            | 1610       |
|    policy_gradient_loss | -0.0482    |
|    value_loss           | 0.14       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 25.4     |
| time/              |          |
|    fps             | 781      |
|    iterations      | 162      |
|    time_elapsed    | 424      |
|    total_timesteps | 331776   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 236        |
|    ep_rew_mean          | 25.7       |
| time/                   |            |
|    fps                  | 782        |
|    iterations           | 163        |
|    time_elapsed         | 426        |
|    total_timesteps      | 333824     |
| train/                  |            |
|    approx_kl            | 0.19248402 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.647      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0337    |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.0462    |
|    value_loss           | 0.125      |
----------------------------------------
Eval num_timesteps=335000, episode_reward=27.70 +/- 2.93
Episode length: 243.20 +/- 32.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 243        |
|    mean_reward          | 27.7       |
| time/                   |            |
|    total_timesteps      | 335000     |
| train/                  |            |
|    approx_kl            | 0.20425044 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0453    |
|    n_updates            | 1630       |
|    policy_gradient_loss | -0.0463    |
|    value_loss           | 0.127      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 26.1     |
| time/              |          |
|    fps             | 780      |
|    iterations      | 164      |
|    time_elapsed    | 430      |
|    total_timesteps | 335872   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 237        |
|    ep_rew_mean          | 26.1       |
| time/                   |            |
|    fps                  | 781        |
|    iterations           | 165        |
|    time_elapsed         | 432        |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.18932846 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.406     |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0471    |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0502    |
|    value_loss           | 0.142      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 238        |
|    ep_rew_mean          | 26.3       |
| time/                   |            |
|    fps                  | 782        |
|    iterations           | 166        |
|    time_elapsed         | 434        |
|    total_timesteps      | 339968     |
| train/                  |            |
|    approx_kl            | 0.24050374 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.365     |
|    explained_variance   | 0.539      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0427    |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.0486    |
|    value_loss           | 0.14       |
----------------------------------------
Eval num_timesteps=340000, episode_reward=25.70 +/- 6.69
Episode length: 228.40 +/- 72.81
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 25.7       |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.18631437 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.341     |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0284    |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.0373    |
|    value_loss           | 0.139      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 26.1     |
| time/              |          |
|    fps             | 781      |
|    iterations      | 167      |
|    time_elapsed    | 437      |
|    total_timesteps | 342016   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 235        |
|    ep_rew_mean          | 26.1       |
| time/                   |            |
|    fps                  | 782        |
|    iterations           | 168        |
|    time_elapsed         | 439        |
|    total_timesteps      | 344064     |
| train/                  |            |
|    approx_kl            | 0.21229218 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0391    |
|    n_updates            | 1670       |
|    policy_gradient_loss | -0.0447    |
|    value_loss           | 0.131      |
----------------------------------------
Eval num_timesteps=345000, episode_reward=26.50 +/- 4.25
Episode length: 228.20 +/- 42.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 26.5       |
| time/                   |            |
|    total_timesteps      | 345000     |
| train/                  |            |
|    approx_kl            | 0.22281288 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.379     |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0302    |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.044     |
|    value_loss           | 0.132      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 26.4     |
| time/              |          |
|    fps             | 780      |
|    iterations      | 169      |
|    time_elapsed    | 443      |
|    total_timesteps | 346112   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 240        |
|    ep_rew_mean          | 26.6       |
| time/                   |            |
|    fps                  | 781        |
|    iterations           | 170        |
|    time_elapsed         | 445        |
|    total_timesteps      | 348160     |
| train/                  |            |
|    approx_kl            | 0.16693059 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.382     |
|    explained_variance   | 0.571      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.033     |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0462    |
|    value_loss           | 0.133      |
----------------------------------------
Eval num_timesteps=350000, episode_reward=27.20 +/- 4.09
Episode length: 243.10 +/- 39.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 243        |
|    mean_reward          | 27.2       |
| time/                   |            |
|    total_timesteps      | 350000     |
| train/                  |            |
|    approx_kl            | 0.20093915 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.579      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0465    |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0431    |
|    value_loss           | 0.139      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 236      |
|    ep_rew_mean     | 26.2     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 171      |
|    time_elapsed    | 449      |
|    total_timesteps | 350208   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 240        |
|    ep_rew_mean          | 26.6       |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 172        |
|    time_elapsed         | 451        |
|    total_timesteps      | 352256     |
| train/                  |            |
|    approx_kl            | 0.18686685 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.385     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0161    |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.047     |
|    value_loss           | 0.129      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 240       |
|    ep_rew_mean          | 26.4      |
| time/                   |           |
|    fps                  | 781       |
|    iterations           | 173       |
|    time_elapsed         | 453       |
|    total_timesteps      | 354304    |
| train/                  |           |
|    approx_kl            | 0.2596702 |
|    clip_fraction        | 0.323     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.378    |
|    explained_variance   | 0.631     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0637   |
|    n_updates            | 1720      |
|    policy_gradient_loss | -0.0531   |
|    value_loss           | 0.127     |
---------------------------------------
Eval num_timesteps=355000, episode_reward=30.00 +/- 4.27
Episode length: 273.70 +/- 45.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 274        |
|    mean_reward          | 30         |
| time/                   |            |
|    total_timesteps      | 355000     |
| train/                  |            |
|    approx_kl            | 0.20573218 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.38      |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0458    |
|    n_updates            | 1730       |
|    policy_gradient_loss | -0.0484    |
|    value_loss           | 0.13       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 236      |
|    ep_rew_mean     | 26       |
| time/              |          |
|    fps             | 779      |
|    iterations      | 174      |
|    time_elapsed    | 456      |
|    total_timesteps | 356352   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 235        |
|    ep_rew_mean          | 25.8       |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 175        |
|    time_elapsed         | 458        |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.18863821 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.376     |
|    explained_variance   | 0.681      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0519    |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0485    |
|    value_loss           | 0.127      |
----------------------------------------
Eval num_timesteps=360000, episode_reward=28.20 +/- 6.38
Episode length: 250.10 +/- 59.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 250        |
|    mean_reward          | 28.2       |
| time/                   |            |
|    total_timesteps      | 360000     |
| train/                  |            |
|    approx_kl            | 0.19935384 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.378     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0509    |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.049     |
|    value_loss           | 0.138      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 236      |
|    ep_rew_mean     | 25.8     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 176      |
|    time_elapsed    | 462      |
|    total_timesteps | 360448   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 235        |
|    ep_rew_mean          | 25.6       |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 177        |
|    time_elapsed         | 464        |
|    total_timesteps      | 362496     |
| train/                  |            |
|    approx_kl            | 0.22127432 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.37      |
|    explained_variance   | 0.592      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0502    |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0498    |
|    value_loss           | 0.126      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 236        |
|    ep_rew_mean          | 25.7       |
| time/                   |            |
|    fps                  | 781        |
|    iterations           | 178        |
|    time_elapsed         | 466        |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.18196458 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.365     |
|    explained_variance   | 0.538      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0369    |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.047     |
|    value_loss           | 0.142      |
----------------------------------------
Eval num_timesteps=365000, episode_reward=23.90 +/- 4.30
Episode length: 207.40 +/- 46.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 207        |
|    mean_reward          | 23.9       |
| time/                   |            |
|    total_timesteps      | 365000     |
| train/                  |            |
|    approx_kl            | 0.18456899 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0444    |
|    n_updates            | 1780       |
|    policy_gradient_loss | -0.0441    |
|    value_loss           | 0.14       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 179      |
|    time_elapsed    | 470      |
|    total_timesteps | 366592   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 25.1       |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 180        |
|    time_elapsed         | 472        |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.21717371 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.367     |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0495    |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.0479    |
|    value_loss           | 0.136      |
----------------------------------------
Eval num_timesteps=370000, episode_reward=24.30 +/- 5.53
Episode length: 215.10 +/- 61.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 215        |
|    mean_reward          | 24.3       |
| time/                   |            |
|    total_timesteps      | 370000     |
| train/                  |            |
|    approx_kl            | 0.22195733 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.355     |
|    explained_variance   | 0.511      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0565    |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.0507    |
|    value_loss           | 0.131      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 25.2     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 181      |
|    time_elapsed    | 475      |
|    total_timesteps | 370688   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 25.2       |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 182        |
|    time_elapsed         | 477        |
|    total_timesteps      | 372736     |
| train/                  |            |
|    approx_kl            | 0.19833168 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.375     |
|    explained_variance   | 0.557      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.035     |
|    n_updates            | 1810       |
|    policy_gradient_loss | -0.0434    |
|    value_loss           | 0.134      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 235        |
|    ep_rew_mean          | 25.3       |
| time/                   |            |
|    fps                  | 781        |
|    iterations           | 183        |
|    time_elapsed         | 479        |
|    total_timesteps      | 374784     |
| train/                  |            |
|    approx_kl            | 0.17506136 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.378     |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0676    |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.0469    |
|    value_loss           | 0.124      |
----------------------------------------
Eval num_timesteps=375000, episode_reward=25.40 +/- 4.80
Episode length: 222.80 +/- 46.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 223        |
|    mean_reward          | 25.4       |
| time/                   |            |
|    total_timesteps      | 375000     |
| train/                  |            |
|    approx_kl            | 0.20725386 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.368     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0365    |
|    n_updates            | 1830       |
|    policy_gradient_loss | -0.0453    |
|    value_loss           | 0.114      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 25.3     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 184      |
|    time_elapsed    | 483      |
|    total_timesteps | 376832   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 25.1       |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 185        |
|    time_elapsed         | 485        |
|    total_timesteps      | 378880     |
| train/                  |            |
|    approx_kl            | 0.21782234 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0437    |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.113      |
----------------------------------------
Eval num_timesteps=380000, episode_reward=24.10 +/- 6.19
Episode length: 216.10 +/- 58.24
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 216      |
|    mean_reward          | 24.1     |
| time/                   |          |
|    total_timesteps      | 380000   |
| train/                  |          |
|    approx_kl            | 0.179906 |
|    clip_fraction        | 0.314    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.384   |
|    explained_variance   | 0.616    |
|    learning_rate        | 0.00038  |
|    loss                 | -0.0546  |
|    n_updates            | 1850     |
|    policy_gradient_loss | -0.0504  |
|    value_loss           | 0.132    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 25.3     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 186      |
|    time_elapsed    | 488      |
|    total_timesteps | 380928   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 230        |
|    ep_rew_mean          | 25         |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 187        |
|    time_elapsed         | 490        |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.18098897 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.357     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0267    |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.0435    |
|    value_loss           | 0.128      |
----------------------------------------
Eval num_timesteps=385000, episode_reward=25.70 +/- 3.93
Episode length: 234.60 +/- 33.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 235        |
|    mean_reward          | 25.7       |
| time/                   |            |
|    total_timesteps      | 385000     |
| train/                  |            |
|    approx_kl            | 0.24335814 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.376     |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.041     |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.0461    |
|    value_loss           | 0.134      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 25.4     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 188      |
|    time_elapsed    | 494      |
|    total_timesteps | 385024   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 25.2       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 189        |
|    time_elapsed         | 496        |
|    total_timesteps      | 387072     |
| train/                  |            |
|    approx_kl            | 0.19357528 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.054     |
|    n_updates            | 1880       |
|    policy_gradient_loss | -0.0455    |
|    value_loss           | 0.122      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 230        |
|    ep_rew_mean          | 25         |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 190        |
|    time_elapsed         | 498        |
|    total_timesteps      | 389120     |
| train/                  |            |
|    approx_kl            | 0.19406322 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.385     |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0463    |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.0477    |
|    value_loss           | 0.129      |
----------------------------------------
Eval num_timesteps=390000, episode_reward=25.70 +/- 4.65
Episode length: 232.00 +/- 52.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 232        |
|    mean_reward          | 25.7       |
| time/                   |            |
|    total_timesteps      | 390000     |
| train/                  |            |
|    approx_kl            | 0.17451711 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.374     |
|    explained_variance   | 0.609      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0235    |
|    n_updates            | 1900       |
|    policy_gradient_loss | -0.0447    |
|    value_loss           | 0.133      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 24.8     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 191      |
|    time_elapsed    | 501      |
|    total_timesteps | 391168   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | 24.4       |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 192        |
|    time_elapsed         | 503        |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.19311607 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.389     |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0461    |
|    n_updates            | 1910       |
|    policy_gradient_loss | -0.0504    |
|    value_loss           | 0.138      |
----------------------------------------
Eval num_timesteps=395000, episode_reward=27.00 +/- 6.13
Episode length: 255.10 +/- 64.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 255        |
|    mean_reward          | 27         |
| time/                   |            |
|    total_timesteps      | 395000     |
| train/                  |            |
|    approx_kl            | 0.23267102 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.401     |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0536    |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.0446    |
|    value_loss           | 0.132      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 222      |
|    ep_rew_mean     | 24.5     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 193      |
|    time_elapsed    | 507      |
|    total_timesteps | 395264   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 224        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 194        |
|    time_elapsed         | 509        |
|    total_timesteps      | 397312     |
| train/                  |            |
|    approx_kl            | 0.20538422 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.399     |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0468    |
|    n_updates            | 1930       |
|    policy_gradient_loss | -0.052     |
|    value_loss           | 0.14       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 221        |
|    ep_rew_mean          | 24.4       |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 195        |
|    time_elapsed         | 511        |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.24738935 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.392     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0581    |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.049     |
|    value_loss           | 0.134      |
----------------------------------------
Eval num_timesteps=400000, episode_reward=25.10 +/- 5.65
Episode length: 234.60 +/- 62.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 235        |
|    mean_reward          | 25.1       |
| time/                   |            |
|    total_timesteps      | 400000     |
| train/                  |            |
|    approx_kl            | 0.20439142 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.383     |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0321    |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.0481    |
|    value_loss           | 0.138      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | 24.7     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 196      |
|    time_elapsed    | 515      |
|    total_timesteps | 401408   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 222       |
|    ep_rew_mean          | 24.6      |
| time/                   |           |
|    fps                  | 779       |
|    iterations           | 197       |
|    time_elapsed         | 517       |
|    total_timesteps      | 403456    |
| train/                  |           |
|    approx_kl            | 0.2310692 |
|    clip_fraction        | 0.323     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.365    |
|    explained_variance   | 0.672     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0523   |
|    n_updates            | 1960      |
|    policy_gradient_loss | -0.0398   |
|    value_loss           | 0.118     |
---------------------------------------
Eval num_timesteps=405000, episode_reward=26.50 +/- 2.66
Episode length: 244.90 +/- 32.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 245        |
|    mean_reward          | 26.5       |
| time/                   |            |
|    total_timesteps      | 405000     |
| train/                  |            |
|    approx_kl            | 0.24520907 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.399     |
|    explained_variance   | 0.533      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0625    |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.052     |
|    value_loss           | 0.14       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 24.8     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 198      |
|    time_elapsed    | 520      |
|    total_timesteps | 405504   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 221        |
|    ep_rew_mean          | 24.4       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 199        |
|    time_elapsed         | 522        |
|    total_timesteps      | 407552     |
| train/                  |            |
|    approx_kl            | 0.21565512 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.395     |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0493    |
|    n_updates            | 1980       |
|    policy_gradient_loss | -0.049     |
|    value_loss           | 0.123      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | 24.5       |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 200        |
|    time_elapsed         | 524        |
|    total_timesteps      | 409600     |
| train/                  |            |
|    approx_kl            | 0.21454917 |
|    clip_fraction        | 0.35       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.414     |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0543    |
|    n_updates            | 1990       |
|    policy_gradient_loss | -0.049     |
|    value_loss           | 0.151      |
----------------------------------------
Eval num_timesteps=410000, episode_reward=24.90 +/- 5.22
Episode length: 233.10 +/- 46.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 233        |
|    mean_reward          | 24.9       |
| time/                   |            |
|    total_timesteps      | 410000     |
| train/                  |            |
|    approx_kl            | 0.22579038 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.379     |
|    explained_variance   | 0.533      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0466    |
|    n_updates            | 2000       |
|    policy_gradient_loss | -0.0471    |
|    value_loss           | 0.137      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 24.8     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 201      |
|    time_elapsed    | 528      |
|    total_timesteps | 411648   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 228        |
|    ep_rew_mean          | 24.9       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 202        |
|    time_elapsed         | 530        |
|    total_timesteps      | 413696     |
| train/                  |            |
|    approx_kl            | 0.25059605 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0312    |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.0503    |
|    value_loss           | 0.134      |
----------------------------------------
Eval num_timesteps=415000, episode_reward=23.60 +/- 6.31
Episode length: 218.80 +/- 66.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 219       |
|    mean_reward          | 23.6      |
| time/                   |           |
|    total_timesteps      | 415000    |
| train/                  |           |
|    approx_kl            | 0.2507565 |
|    clip_fraction        | 0.327     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.377    |
|    explained_variance   | 0.608     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0536   |
|    n_updates            | 2020      |
|    policy_gradient_loss | -0.0487   |
|    value_loss           | 0.128     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 25       |
| time/              |          |
|    fps             | 778      |
|    iterations      | 203      |
|    time_elapsed    | 534      |
|    total_timesteps | 415744   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 25.1       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 204        |
|    time_elapsed         | 536        |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.21143463 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.363     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0294    |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.0437    |
|    value_loss           | 0.134      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 229        |
|    ep_rew_mean          | 24.9       |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 205        |
|    time_elapsed         | 538        |
|    total_timesteps      | 419840     |
| train/                  |            |
|    approx_kl            | 0.22802156 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.358     |
|    explained_variance   | 0.548      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0397    |
|    n_updates            | 2040       |
|    policy_gradient_loss | -0.0501    |
|    value_loss           | 0.14       |
----------------------------------------
Eval num_timesteps=420000, episode_reward=23.90 +/- 6.06
Episode length: 222.50 +/- 63.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 222        |
|    mean_reward          | 23.9       |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.23094484 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.367     |
|    explained_variance   | 0.589      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0463    |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.0503    |
|    value_loss           | 0.134      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 231      |
|    ep_rew_mean     | 24.8     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 206      |
|    time_elapsed    | 541      |
|    total_timesteps | 421888   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 229        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 207        |
|    time_elapsed         | 543        |
|    total_timesteps      | 423936     |
| train/                  |            |
|    approx_kl            | 0.26983586 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.368     |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0596    |
|    n_updates            | 2060       |
|    policy_gradient_loss | -0.0479    |
|    value_loss           | 0.144      |
----------------------------------------
Eval num_timesteps=425000, episode_reward=26.30 +/- 5.73
Episode length: 241.60 +/- 61.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 242       |
|    mean_reward          | 26.3      |
| time/                   |           |
|    total_timesteps      | 425000    |
| train/                  |           |
|    approx_kl            | 0.2215901 |
|    clip_fraction        | 0.317     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.369    |
|    explained_variance   | 0.614     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.033    |
|    n_updates            | 2070      |
|    policy_gradient_loss | -0.0454   |
|    value_loss           | 0.149     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 24.6     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 208      |
|    time_elapsed    | 547      |
|    total_timesteps | 425984   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 228        |
|    ep_rew_mean          | 24.5       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 209        |
|    time_elapsed         | 549        |
|    total_timesteps      | 428032     |
| train/                  |            |
|    approx_kl            | 0.22348306 |
|    clip_fraction        | 0.341      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.371     |
|    explained_variance   | 0.568      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0581    |
|    n_updates            | 2080       |
|    policy_gradient_loss | -0.0541    |
|    value_loss           | 0.136      |
----------------------------------------
Eval num_timesteps=430000, episode_reward=29.20 +/- 4.07
Episode length: 259.80 +/- 44.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 260        |
|    mean_reward          | 29.2       |
| time/                   |            |
|    total_timesteps      | 430000     |
| train/                  |            |
|    approx_kl            | 0.28244382 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.363     |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0678    |
|    n_updates            | 2090       |
|    policy_gradient_loss | -0.0499    |
|    value_loss           | 0.133      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 25.1     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 210      |
|    time_elapsed    | 552      |
|    total_timesteps | 430080   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 233        |
|    ep_rew_mean          | 24.9       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 211        |
|    time_elapsed         | 554        |
|    total_timesteps      | 432128     |
| train/                  |            |
|    approx_kl            | 0.22926968 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.343     |
|    explained_variance   | 0.589      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0354    |
|    n_updates            | 2100       |
|    policy_gradient_loss | -0.0464    |
|    value_loss           | 0.128      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 226       |
|    ep_rew_mean          | 24.4      |
| time/                   |           |
|    fps                  | 779       |
|    iterations           | 212       |
|    time_elapsed         | 556       |
|    total_timesteps      | 434176    |
| train/                  |           |
|    approx_kl            | 0.2668549 |
|    clip_fraction        | 0.331     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.365    |
|    explained_variance   | 0.561     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0371   |
|    n_updates            | 2110      |
|    policy_gradient_loss | -0.0469   |
|    value_loss           | 0.142     |
---------------------------------------
Eval num_timesteps=435000, episode_reward=21.80 +/- 5.58
Episode length: 193.90 +/- 61.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 194       |
|    mean_reward          | 21.8      |
| time/                   |           |
|    total_timesteps      | 435000    |
| train/                  |           |
|    approx_kl            | 0.2904514 |
|    clip_fraction        | 0.328     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.371    |
|    explained_variance   | 0.608     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0482   |
|    n_updates            | 2120      |
|    policy_gradient_loss | -0.0496   |
|    value_loss           | 0.145     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | 24.4     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 213      |
|    time_elapsed    | 560      |
|    total_timesteps | 436224   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 228        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 214        |
|    time_elapsed         | 562        |
|    total_timesteps      | 438272     |
| train/                  |            |
|    approx_kl            | 0.21354857 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.386     |
|    explained_variance   | 0.562      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0362    |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.0511    |
|    value_loss           | 0.139      |
----------------------------------------
Eval num_timesteps=440000, episode_reward=25.30 +/- 3.03
Episode length: 230.10 +/- 35.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 230        |
|    mean_reward          | 25.3       |
| time/                   |            |
|    total_timesteps      | 440000     |
| train/                  |            |
|    approx_kl            | 0.20426735 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | 0.559      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.046     |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0469    |
|    value_loss           | 0.139      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 24.7     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 215      |
|    time_elapsed    | 565      |
|    total_timesteps | 440320   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 225        |
|    ep_rew_mean          | 24.6       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 216        |
|    time_elapsed         | 567        |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.22294539 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.374     |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0703    |
|    n_updates            | 2150       |
|    policy_gradient_loss | -0.0521    |
|    value_loss           | 0.124      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 228        |
|    ep_rew_mean          | 24.9       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 217        |
|    time_elapsed         | 569        |
|    total_timesteps      | 444416     |
| train/                  |            |
|    approx_kl            | 0.20778282 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.383     |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0444    |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.0508    |
|    value_loss           | 0.117      |
----------------------------------------
Eval num_timesteps=445000, episode_reward=24.30 +/- 5.88
Episode length: 228.40 +/- 58.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 24.3       |
| time/                   |            |
|    total_timesteps      | 445000     |
| train/                  |            |
|    approx_kl            | 0.25164694 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.397     |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0335    |
|    n_updates            | 2170       |
|    policy_gradient_loss | -0.0509    |
|    value_loss           | 0.117      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 226      |
|    ep_rew_mean     | 24.6     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 218      |
|    time_elapsed    | 573      |
|    total_timesteps | 446464   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 229        |
|    ep_rew_mean          | 24.9       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 219        |
|    time_elapsed         | 575        |
|    total_timesteps      | 448512     |
| train/                  |            |
|    approx_kl            | 0.21553811 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.402     |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0267    |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.0516    |
|    value_loss           | 0.144      |
----------------------------------------
Eval num_timesteps=450000, episode_reward=26.10 +/- 6.47
Episode length: 248.80 +/- 65.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 249        |
|    mean_reward          | 26.1       |
| time/                   |            |
|    total_timesteps      | 450000     |
| train/                  |            |
|    approx_kl            | 0.21417755 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.389     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0286    |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.048     |
|    value_loss           | 0.142      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 226      |
|    ep_rew_mean     | 24.6     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 220      |
|    time_elapsed    | 579      |
|    total_timesteps | 450560   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 223        |
|    ep_rew_mean          | 24.3       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 221        |
|    time_elapsed         | 581        |
|    total_timesteps      | 452608     |
| train/                  |            |
|    approx_kl            | 0.31923485 |
|    clip_fraction        | 0.348      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.397     |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0286    |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.0512    |
|    value_loss           | 0.13       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 220        |
|    ep_rew_mean          | 24.1       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 222        |
|    time_elapsed         | 583        |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.20132469 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.4       |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0474    |
|    n_updates            | 2210       |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.13       |
----------------------------------------
Eval num_timesteps=455000, episode_reward=25.20 +/- 3.49
Episode length: 225.90 +/- 39.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 226        |
|    mean_reward          | 25.2       |
| time/                   |            |
|    total_timesteps      | 455000     |
| train/                  |            |
|    approx_kl            | 0.23251078 |
|    clip_fraction        | 0.347      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.406     |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.066     |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0529    |
|    value_loss           | 0.137      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 222      |
|    ep_rew_mean     | 24       |
| time/              |          |
|    fps             | 778      |
|    iterations      | 223      |
|    time_elapsed    | 586      |
|    total_timesteps | 456704   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 223        |
|    ep_rew_mean          | 24.1       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 224        |
|    time_elapsed         | 588        |
|    total_timesteps      | 458752     |
| train/                  |            |
|    approx_kl            | 0.21625517 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.396     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0675    |
|    n_updates            | 2230       |
|    policy_gradient_loss | -0.0531    |
|    value_loss           | 0.148      |
----------------------------------------
Eval num_timesteps=460000, episode_reward=23.70 +/- 6.39
Episode length: 220.20 +/- 70.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 220        |
|    mean_reward          | 23.7       |
| time/                   |            |
|    total_timesteps      | 460000     |
| train/                  |            |
|    approx_kl            | 0.21296749 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.36      |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0409    |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.132      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 23.6     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 225      |
|    time_elapsed    | 592      |
|    total_timesteps | 460800   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 23.5       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 226        |
|    time_elapsed         | 594        |
|    total_timesteps      | 462848     |
| train/                  |            |
|    approx_kl            | 0.23441477 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.504      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.04      |
|    n_updates            | 2250       |
|    policy_gradient_loss | -0.0476    |
|    value_loss           | 0.157      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 23.4       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 227        |
|    time_elapsed         | 596        |
|    total_timesteps      | 464896     |
| train/                  |            |
|    approx_kl            | 0.20690957 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.336     |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0574    |
|    n_updates            | 2260       |
|    policy_gradient_loss | -0.0489    |
|    value_loss           | 0.133      |
----------------------------------------
Eval num_timesteps=465000, episode_reward=27.90 +/- 5.07
Episode length: 262.60 +/- 59.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 263        |
|    mean_reward          | 27.9       |
| time/                   |            |
|    total_timesteps      | 465000     |
| train/                  |            |
|    approx_kl            | 0.24304216 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.341     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0501    |
|    n_updates            | 2270       |
|    policy_gradient_loss | -0.0503    |
|    value_loss           | 0.143      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 23.5     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 228      |
|    time_elapsed    | 600      |
|    total_timesteps | 466944   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | 23.8       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 229        |
|    time_elapsed         | 602        |
|    total_timesteps      | 468992     |
| train/                  |            |
|    approx_kl            | 0.22900802 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.346     |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0473    |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0507    |
|    value_loss           | 0.142      |
----------------------------------------
Eval num_timesteps=470000, episode_reward=23.90 +/- 5.32
Episode length: 211.50 +/- 56.04
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 212        |
|    mean_reward          | 23.9       |
| time/                   |            |
|    total_timesteps      | 470000     |
| train/                  |            |
|    approx_kl            | 0.22458759 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.382     |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0564    |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.0488    |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 23.8     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 230      |
|    time_elapsed    | 605      |
|    total_timesteps | 471040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 220        |
|    ep_rew_mean          | 23.6       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 231        |
|    time_elapsed         | 607        |
|    total_timesteps      | 473088     |
| train/                  |            |
|    approx_kl            | 0.19658285 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.355     |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0508    |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.0492    |
|    value_loss           | 0.138      |
----------------------------------------
Eval num_timesteps=475000, episode_reward=25.80 +/- 4.60
Episode length: 232.50 +/- 47.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 232        |
|    mean_reward          | 25.8       |
| time/                   |            |
|    total_timesteps      | 475000     |
| train/                  |            |
|    approx_kl            | 0.19522515 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.394     |
|    explained_variance   | 0.678      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0587    |
|    n_updates            | 2310       |
|    policy_gradient_loss | -0.0488    |
|    value_loss           | 0.127      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | 23.3     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 232      |
|    time_elapsed    | 611      |
|    total_timesteps | 475136   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 23.6       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 233        |
|    time_elapsed         | 613        |
|    total_timesteps      | 477184     |
| train/                  |            |
|    approx_kl            | 0.22570351 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0585    |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.0534    |
|    value_loss           | 0.146      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | 24         |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 234        |
|    time_elapsed         | 615        |
|    total_timesteps      | 479232     |
| train/                  |            |
|    approx_kl            | 0.20049727 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.37      |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0578    |
|    n_updates            | 2330       |
|    policy_gradient_loss | -0.0497    |
|    value_loss           | 0.134      |
----------------------------------------
Eval num_timesteps=480000, episode_reward=24.40 +/- 5.77
Episode length: 222.20 +/- 59.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 222        |
|    mean_reward          | 24.4       |
| time/                   |            |
|    total_timesteps      | 480000     |
| train/                  |            |
|    approx_kl            | 0.22235146 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.056     |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.0561    |
|    value_loss           | 0.126      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 220      |
|    ep_rew_mean     | 23.9     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 235      |
|    time_elapsed    | 618      |
|    total_timesteps | 481280   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | 24.3       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 236        |
|    time_elapsed         | 620        |
|    total_timesteps      | 483328     |
| train/                  |            |
|    approx_kl            | 0.21916929 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.384     |
|    explained_variance   | 0.53       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0502    |
|    n_updates            | 2350       |
|    policy_gradient_loss | -0.0503    |
|    value_loss           | 0.152      |
----------------------------------------
Eval num_timesteps=485000, episode_reward=24.50 +/- 4.59
Episode length: 228.10 +/- 47.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 24.5       |
| time/                   |            |
|    total_timesteps      | 485000     |
| train/                  |            |
|    approx_kl            | 0.24289134 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.372     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0272    |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.0485    |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 226      |
|    ep_rew_mean     | 24.5     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 237      |
|    time_elapsed    | 624      |
|    total_timesteps | 485376   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 227        |
|    ep_rew_mean          | 24.5       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 238        |
|    time_elapsed         | 626        |
|    total_timesteps      | 487424     |
| train/                  |            |
|    approx_kl            | 0.22178459 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.399     |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0712    |
|    n_updates            | 2370       |
|    policy_gradient_loss | -0.0498    |
|    value_loss           | 0.135      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 24.8       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 239        |
|    time_elapsed         | 628        |
|    total_timesteps      | 489472     |
| train/                  |            |
|    approx_kl            | 0.24594355 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.388     |
|    explained_variance   | 0.647      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0582    |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.05      |
|    value_loss           | 0.132      |
----------------------------------------
Eval num_timesteps=490000, episode_reward=27.10 +/- 4.89
Episode length: 246.40 +/- 39.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 246        |
|    mean_reward          | 27.1       |
| time/                   |            |
|    total_timesteps      | 490000     |
| train/                  |            |
|    approx_kl            | 0.20231487 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0507    |
|    n_updates            | 2390       |
|    policy_gradient_loss | -0.0441    |
|    value_loss           | 0.137      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 24.4     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 240      |
|    time_elapsed    | 631      |
|    total_timesteps | 491520   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 228        |
|    ep_rew_mean          | 24.4       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 241        |
|    time_elapsed         | 633        |
|    total_timesteps      | 493568     |
| train/                  |            |
|    approx_kl            | 0.24995574 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.352     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0426    |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.0473    |
|    value_loss           | 0.133      |
----------------------------------------
Eval num_timesteps=495000, episode_reward=29.70 +/- 6.54
Episode length: 289.80 +/- 62.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 290        |
|    mean_reward          | 29.7       |
| time/                   |            |
|    total_timesteps      | 495000     |
| train/                  |            |
|    approx_kl            | 0.24807452 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.337     |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0426    |
|    n_updates            | 2410       |
|    policy_gradient_loss | -0.0511    |
|    value_loss           | 0.125      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 24.5     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 242      |
|    time_elapsed    | 637      |
|    total_timesteps | 495616   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 230       |
|    ep_rew_mean          | 24.8      |
| time/                   |           |
|    fps                  | 777       |
|    iterations           | 243       |
|    time_elapsed         | 639       |
|    total_timesteps      | 497664    |
| train/                  |           |
|    approx_kl            | 0.2224842 |
|    clip_fraction        | 0.284     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.303    |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0387   |
|    n_updates            | 2420      |
|    policy_gradient_loss | -0.0433   |
|    value_loss           | 0.14      |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 235       |
|    ep_rew_mean          | 25.2      |
| time/                   |           |
|    fps                  | 778       |
|    iterations           | 244       |
|    time_elapsed         | 641       |
|    total_timesteps      | 499712    |
| train/                  |           |
|    approx_kl            | 0.2670738 |
|    clip_fraction        | 0.301     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.309    |
|    explained_variance   | 0.563     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.031    |
|    n_updates            | 2430      |
|    policy_gradient_loss | -0.0465   |
|    value_loss           | 0.133     |
---------------------------------------
Eval num_timesteps=500000, episode_reward=24.10 +/- 4.01
Episode length: 216.10 +/- 46.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 216       |
|    mean_reward          | 24.1      |
| time/                   |           |
|    total_timesteps      | 500000    |
| train/                  |           |
|    approx_kl            | 0.2655362 |
|    clip_fraction        | 0.304     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.335    |
|    explained_variance   | 0.576     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0369   |
|    n_updates            | 2440      |
|    policy_gradient_loss | -0.0471   |
|    value_loss           | 0.149     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 237      |
|    ep_rew_mean     | 25.4     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 245      |
|    time_elapsed    | 645      |
|    total_timesteps | 501760   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 237        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 246        |
|    time_elapsed         | 647        |
|    total_timesteps      | 503808     |
| train/                  |            |
|    approx_kl            | 0.27403927 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.305     |
|    explained_variance   | 0.542      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0521    |
|    n_updates            | 2450       |
|    policy_gradient_loss | -0.0496    |
|    value_loss           | 0.14       |
----------------------------------------
Eval num_timesteps=505000, episode_reward=25.30 +/- 6.12
Episode length: 226.80 +/- 47.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 227        |
|    mean_reward          | 25.3       |
| time/                   |            |
|    total_timesteps      | 505000     |
| train/                  |            |
|    approx_kl            | 0.22439124 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.328     |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0483    |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.0461    |
|    value_loss           | 0.122      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 247      |
|    time_elapsed    | 650      |
|    total_timesteps | 505856   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 236        |
|    ep_rew_mean          | 25.3       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 248        |
|    time_elapsed         | 652        |
|    total_timesteps      | 507904     |
| train/                  |            |
|    approx_kl            | 0.24835357 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.346     |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0322    |
|    n_updates            | 2470       |
|    policy_gradient_loss | -0.0442    |
|    value_loss           | 0.151      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 239        |
|    ep_rew_mean          | 25.6       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 249        |
|    time_elapsed         | 654        |
|    total_timesteps      | 509952     |
| train/                  |            |
|    approx_kl            | 0.27170232 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.322     |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0305    |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.047     |
|    value_loss           | 0.143      |
----------------------------------------
Eval num_timesteps=510000, episode_reward=23.50 +/- 6.28
Episode length: 215.90 +/- 51.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 23.5       |
| time/                   |            |
|    total_timesteps      | 510000     |
| train/                  |            |
|    approx_kl            | 0.24949783 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.335     |
|    explained_variance   | 0.511      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0419    |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.136      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 25.4     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 250      |
|    time_elapsed    | 658      |
|    total_timesteps | 512000   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 233        |
|    ep_rew_mean          | 25.2       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 251        |
|    time_elapsed         | 660        |
|    total_timesteps      | 514048     |
| train/                  |            |
|    approx_kl            | 0.25797302 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.338     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0445    |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.0525    |
|    value_loss           | 0.129      |
----------------------------------------
Eval num_timesteps=515000, episode_reward=27.80 +/- 5.76
Episode length: 260.20 +/- 68.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 260       |
|    mean_reward          | 27.8      |
| time/                   |           |
|    total_timesteps      | 515000    |
| train/                  |           |
|    approx_kl            | 0.2563387 |
|    clip_fraction        | 0.301     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.336    |
|    explained_variance   | 0.643     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0578   |
|    n_updates            | 2510      |
|    policy_gradient_loss | -0.0459   |
|    value_loss           | 0.133     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 236      |
|    ep_rew_mean     | 25.5     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 252      |
|    time_elapsed    | 664      |
|    total_timesteps | 516096   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 236       |
|    ep_rew_mean          | 25.5      |
| time/                   |           |
|    fps                  | 777       |
|    iterations           | 253       |
|    time_elapsed         | 666       |
|    total_timesteps      | 518144    |
| train/                  |           |
|    approx_kl            | 0.2930331 |
|    clip_fraction        | 0.315     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.351    |
|    explained_variance   | 0.631     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0396   |
|    n_updates            | 2520      |
|    policy_gradient_loss | -0.0509   |
|    value_loss           | 0.129     |
---------------------------------------
Eval num_timesteps=520000, episode_reward=27.20 +/- 7.08
Episode length: 246.20 +/- 74.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 246        |
|    mean_reward          | 27.2       |
| time/                   |            |
|    total_timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.23107392 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.347     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.011     |
|    n_updates            | 2530       |
|    policy_gradient_loss | -0.0498    |
|    value_loss           | 0.156      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 240      |
|    ep_rew_mean     | 25.8     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 254      |
|    time_elapsed    | 669      |
|    total_timesteps | 520192   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 243        |
|    ep_rew_mean          | 26.1       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 255        |
|    time_elapsed         | 671        |
|    total_timesteps      | 522240     |
| train/                  |            |
|    approx_kl            | 0.22797057 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0419    |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.0438    |
|    value_loss           | 0.136      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 236        |
|    ep_rew_mean          | 25.4       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 256        |
|    time_elapsed         | 673        |
|    total_timesteps      | 524288     |
| train/                  |            |
|    approx_kl            | 0.25154728 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.371     |
|    explained_variance   | 0.702      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0526    |
|    n_updates            | 2550       |
|    policy_gradient_loss | -0.0484    |
|    value_loss           | 0.128      |
----------------------------------------
Eval num_timesteps=525000, episode_reward=24.20 +/- 2.60
Episode length: 220.60 +/- 27.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 221        |
|    mean_reward          | 24.2       |
| time/                   |            |
|    total_timesteps      | 525000     |
| train/                  |            |
|    approx_kl            | 0.28017825 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.378     |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0566    |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.0524    |
|    value_loss           | 0.138      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 257      |
|    time_elapsed    | 677      |
|    total_timesteps | 526336   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 241        |
|    ep_rew_mean          | 25.9       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 258        |
|    time_elapsed         | 679        |
|    total_timesteps      | 528384     |
| train/                  |            |
|    approx_kl            | 0.23604062 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0503    |
|    n_updates            | 2570       |
|    policy_gradient_loss | -0.0493    |
|    value_loss           | 0.15       |
----------------------------------------
Eval num_timesteps=530000, episode_reward=20.60 +/- 7.84
Episode length: 206.50 +/- 62.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 206        |
|    mean_reward          | 20.6       |
| time/                   |            |
|    total_timesteps      | 530000     |
| train/                  |            |
|    approx_kl            | 0.24747518 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.356     |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0578    |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.0488    |
|    value_loss           | 0.147      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 240      |
|    ep_rew_mean     | 25.9     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 259      |
|    time_elapsed    | 682      |
|    total_timesteps | 530432   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 237        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 260        |
|    time_elapsed         | 684        |
|    total_timesteps      | 532480     |
| train/                  |            |
|    approx_kl            | 0.23177132 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.378     |
|    explained_variance   | 0.507      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0396    |
|    n_updates            | 2590       |
|    policy_gradient_loss | -0.0489    |
|    value_loss           | 0.158      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 239        |
|    ep_rew_mean          | 25.7       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 261        |
|    time_elapsed         | 686        |
|    total_timesteps      | 534528     |
| train/                  |            |
|    approx_kl            | 0.23962623 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.367     |
|    explained_variance   | 0.611      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.051     |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.0501    |
|    value_loss           | 0.148      |
----------------------------------------
Eval num_timesteps=535000, episode_reward=25.70 +/- 6.63
Episode length: 228.30 +/- 71.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 228       |
|    mean_reward          | 25.7      |
| time/                   |           |
|    total_timesteps      | 535000    |
| train/                  |           |
|    approx_kl            | 0.2368826 |
|    clip_fraction        | 0.345     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.4      |
|    explained_variance   | 0.603     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0454   |
|    n_updates            | 2610      |
|    policy_gradient_loss | -0.0546   |
|    value_loss           | 0.133     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 244      |
|    ep_rew_mean     | 26.3     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 262      |
|    time_elapsed    | 690      |
|    total_timesteps | 536576   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 244        |
|    ep_rew_mean          | 26.3       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 263        |
|    time_elapsed         | 692        |
|    total_timesteps      | 538624     |
| train/                  |            |
|    approx_kl            | 0.26574308 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.334     |
|    explained_variance   | 0.456      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.041     |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.0504    |
|    value_loss           | 0.169      |
----------------------------------------
Eval num_timesteps=540000, episode_reward=25.60 +/- 6.09
Episode length: 233.20 +/- 57.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 233        |
|    mean_reward          | 25.6       |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.20539731 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0467    |
|    n_updates            | 2630       |
|    policy_gradient_loss | -0.0476    |
|    value_loss           | 0.15       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 25.8     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 264      |
|    time_elapsed    | 695      |
|    total_timesteps | 540672   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 240        |
|    ep_rew_mean          | 25.7       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 265        |
|    time_elapsed         | 697        |
|    total_timesteps      | 542720     |
| train/                  |            |
|    approx_kl            | 0.23677579 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.386     |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0551    |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.0531    |
|    value_loss           | 0.146      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 239        |
|    ep_rew_mean          | 25.7       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 266        |
|    time_elapsed         | 699        |
|    total_timesteps      | 544768     |
| train/                  |            |
|    approx_kl            | 0.22415933 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.386     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0621    |
|    n_updates            | 2650       |
|    policy_gradient_loss | -0.0538    |
|    value_loss           | 0.151      |
----------------------------------------
Eval num_timesteps=545000, episode_reward=30.50 +/- 7.54
Episode length: 287.00 +/- 76.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 287        |
|    mean_reward          | 30.5       |
| time/                   |            |
|    total_timesteps      | 545000     |
| train/                  |            |
|    approx_kl            | 0.19813037 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.377     |
|    explained_variance   | 0.702      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0367    |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.046     |
|    value_loss           | 0.141      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 240      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 267      |
|    time_elapsed    | 703      |
|    total_timesteps | 546816   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 240       |
|    ep_rew_mean          | 25.7      |
| time/                   |           |
|    fps                  | 777       |
|    iterations           | 268       |
|    time_elapsed         | 705       |
|    total_timesteps      | 548864    |
| train/                  |           |
|    approx_kl            | 0.2143767 |
|    clip_fraction        | 0.342     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.399    |
|    explained_variance   | 0.671     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.033    |
|    n_updates            | 2670      |
|    policy_gradient_loss | -0.0591   |
|    value_loss           | 0.132     |
---------------------------------------
Eval num_timesteps=550000, episode_reward=24.50 +/- 4.54
Episode length: 228.70 +/- 42.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 229        |
|    mean_reward          | 24.5       |
| time/                   |            |
|    total_timesteps      | 550000     |
| train/                  |            |
|    approx_kl            | 0.30545402 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.374     |
|    explained_variance   | 0.571      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.047     |
|    n_updates            | 2680       |
|    policy_gradient_loss | -0.0541    |
|    value_loss           | 0.136      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 269      |
|    time_elapsed    | 709      |
|    total_timesteps | 550912   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 238        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 270        |
|    time_elapsed         | 711        |
|    total_timesteps      | 552960     |
| train/                  |            |
|    approx_kl            | 0.26817644 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.352     |
|    explained_variance   | 0.471      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0524    |
|    n_updates            | 2690       |
|    policy_gradient_loss | -0.0499    |
|    value_loss           | 0.154      |
----------------------------------------
Eval num_timesteps=555000, episode_reward=25.60 +/- 4.80
Episode length: 233.70 +/- 61.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 234        |
|    mean_reward          | 25.6       |
| time/                   |            |
|    total_timesteps      | 555000     |
| train/                  |            |
|    approx_kl            | 0.29249972 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.352     |
|    explained_variance   | 0.585      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0559    |
|    n_updates            | 2700       |
|    policy_gradient_loss | -0.0475    |
|    value_loss           | 0.151      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 241      |
|    ep_rew_mean     | 25.8     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 271      |
|    time_elapsed    | 715      |
|    total_timesteps | 555008   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 240        |
|    ep_rew_mean          | 25.6       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 272        |
|    time_elapsed         | 717        |
|    total_timesteps      | 557056     |
| train/                  |            |
|    approx_kl            | 0.25930268 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | 0.601      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0582    |
|    n_updates            | 2710       |
|    policy_gradient_loss | -0.0476    |
|    value_loss           | 0.147      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 239        |
|    ep_rew_mean          | 25.4       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 273        |
|    time_elapsed         | 719        |
|    total_timesteps      | 559104     |
| train/                  |            |
|    approx_kl            | 0.29764754 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.38      |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0369    |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.135      |
----------------------------------------
Eval num_timesteps=560000, episode_reward=23.20 +/- 6.37
Episode length: 228.00 +/- 71.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 23.2       |
| time/                   |            |
|    total_timesteps      | 560000     |
| train/                  |            |
|    approx_kl            | 0.27903318 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.353     |
|    explained_variance   | 0.578      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0348    |
|    n_updates            | 2730       |
|    policy_gradient_loss | -0.0457    |
|    value_loss           | 0.148      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 237      |
|    ep_rew_mean     | 25.1     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 274      |
|    time_elapsed    | 722      |
|    total_timesteps | 561152   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 238        |
|    ep_rew_mean          | 25.2       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 275        |
|    time_elapsed         | 724        |
|    total_timesteps      | 563200     |
| train/                  |            |
|    approx_kl            | 0.24437183 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.325     |
|    explained_variance   | 0.531      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0371    |
|    n_updates            | 2740       |
|    policy_gradient_loss | -0.0441    |
|    value_loss           | 0.161      |
----------------------------------------
Eval num_timesteps=565000, episode_reward=25.50 +/- 4.46
Episode length: 239.70 +/- 42.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 240       |
|    mean_reward          | 25.5      |
| time/                   |           |
|    total_timesteps      | 565000    |
| train/                  |           |
|    approx_kl            | 0.2917685 |
|    clip_fraction        | 0.313     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.317    |
|    explained_variance   | 0.604     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0404   |
|    n_updates            | 2750      |
|    policy_gradient_loss | -0.0466   |
|    value_loss           | 0.137     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 25.2     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 276      |
|    time_elapsed    | 728      |
|    total_timesteps | 565248   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 241       |
|    ep_rew_mean          | 25.4      |
| time/                   |           |
|    fps                  | 776       |
|    iterations           | 277       |
|    time_elapsed         | 730       |
|    total_timesteps      | 567296    |
| train/                  |           |
|    approx_kl            | 0.3018371 |
|    clip_fraction        | 0.323     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.331    |
|    explained_variance   | 0.59      |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0531   |
|    n_updates            | 2760      |
|    policy_gradient_loss | -0.0494   |
|    value_loss           | 0.136     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 237       |
|    ep_rew_mean          | 25        |
| time/                   |           |
|    fps                  | 777       |
|    iterations           | 278       |
|    time_elapsed         | 732       |
|    total_timesteps      | 569344    |
| train/                  |           |
|    approx_kl            | 0.3093216 |
|    clip_fraction        | 0.317     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.333    |
|    explained_variance   | 0.656     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0364   |
|    n_updates            | 2770      |
|    policy_gradient_loss | -0.0502   |
|    value_loss           | 0.145     |
---------------------------------------
Eval num_timesteps=570000, episode_reward=27.70 +/- 4.47
Episode length: 254.80 +/- 41.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 255        |
|    mean_reward          | 27.7       |
| time/                   |            |
|    total_timesteps      | 570000     |
| train/                  |            |
|    approx_kl            | 0.24275851 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.304     |
|    explained_variance   | 0.513      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0382    |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.043     |
|    value_loss           | 0.176      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 25.2     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 279      |
|    time_elapsed    | 736      |
|    total_timesteps | 571392   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 238       |
|    ep_rew_mean          | 25.2      |
| time/                   |           |
|    fps                  | 776       |
|    iterations           | 280       |
|    time_elapsed         | 738       |
|    total_timesteps      | 573440    |
| train/                  |           |
|    approx_kl            | 0.2504521 |
|    clip_fraction        | 0.284     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.292    |
|    explained_variance   | 0.616     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0169   |
|    n_updates            | 2790      |
|    policy_gradient_loss | -0.0405   |
|    value_loss           | 0.152     |
---------------------------------------
Eval num_timesteps=575000, episode_reward=25.20 +/- 4.31
Episode length: 226.50 +/- 43.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 226        |
|    mean_reward          | 25.2       |
| time/                   |            |
|    total_timesteps      | 575000     |
| train/                  |            |
|    approx_kl            | 0.28267357 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.308     |
|    explained_variance   | 0.589      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0406    |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.0441    |
|    value_loss           | 0.152      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 24.8     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 281      |
|    time_elapsed    | 741      |
|    total_timesteps | 575488   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 231       |
|    ep_rew_mean          | 24.5      |
| time/                   |           |
|    fps                  | 776       |
|    iterations           | 282       |
|    time_elapsed         | 743       |
|    total_timesteps      | 577536    |
| train/                  |           |
|    approx_kl            | 0.2713801 |
|    clip_fraction        | 0.326     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.345    |
|    explained_variance   | 0.576     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0182   |
|    n_updates            | 2810      |
|    policy_gradient_loss | -0.0484   |
|    value_loss           | 0.161     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 228        |
|    ep_rew_mean          | 24.4       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 283        |
|    time_elapsed         | 745        |
|    total_timesteps      | 579584     |
| train/                  |            |
|    approx_kl            | 0.24726786 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.33      |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0554    |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.0432    |
|    value_loss           | 0.146      |
----------------------------------------
Eval num_timesteps=580000, episode_reward=27.50 +/- 5.84
Episode length: 249.80 +/- 63.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 250       |
|    mean_reward          | 27.5      |
| time/                   |           |
|    total_timesteps      | 580000    |
| train/                  |           |
|    approx_kl            | 0.2430827 |
|    clip_fraction        | 0.296     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.309    |
|    explained_variance   | 0.64      |
|    learning_rate        | 0.00038   |
|    loss                 | -0.018    |
|    n_updates            | 2830      |
|    policy_gradient_loss | -0.0399   |
|    value_loss           | 0.149     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 24.9     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 284      |
|    time_elapsed    | 749      |
|    total_timesteps | 581632   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 235        |
|    ep_rew_mean          | 25.1       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 285        |
|    time_elapsed         | 751        |
|    total_timesteps      | 583680     |
| train/                  |            |
|    approx_kl            | 0.21774104 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.338     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0512    |
|    n_updates            | 2840       |
|    policy_gradient_loss | -0.0494    |
|    value_loss           | 0.138      |
----------------------------------------
Eval num_timesteps=585000, episode_reward=28.50 +/- 5.43
Episode length: 255.20 +/- 60.07
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 255        |
|    mean_reward          | 28.5       |
| time/                   |            |
|    total_timesteps      | 585000     |
| train/                  |            |
|    approx_kl            | 0.23610845 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.349     |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0305    |
|    n_updates            | 2850       |
|    policy_gradient_loss | -0.0473    |
|    value_loss           | 0.146      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 25.1     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 286      |
|    time_elapsed    | 755      |
|    total_timesteps | 585728   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 237        |
|    ep_rew_mean          | 25.4       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 287        |
|    time_elapsed         | 757        |
|    total_timesteps      | 587776     |
| train/                  |            |
|    approx_kl            | 0.26733983 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.356     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0269    |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.157      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 237        |
|    ep_rew_mean          | 25.6       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 288        |
|    time_elapsed         | 759        |
|    total_timesteps      | 589824     |
| train/                  |            |
|    approx_kl            | 0.19724436 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.335     |
|    explained_variance   | 0.573      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.034     |
|    n_updates            | 2870       |
|    policy_gradient_loss | -0.0429    |
|    value_loss           | 0.158      |
----------------------------------------
Eval num_timesteps=590000, episode_reward=26.70 +/- 6.15
Episode length: 250.40 +/- 62.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 250        |
|    mean_reward          | 26.7       |
| time/                   |            |
|    total_timesteps      | 590000     |
| train/                  |            |
|    approx_kl            | 0.22627258 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0516    |
|    n_updates            | 2880       |
|    policy_gradient_loss | -0.0487    |
|    value_loss           | 0.147      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 240      |
|    ep_rew_mean     | 25.9     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 289      |
|    time_elapsed    | 762      |
|    total_timesteps | 591872   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 238        |
|    ep_rew_mean          | 25.6       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 290        |
|    time_elapsed         | 764        |
|    total_timesteps      | 593920     |
| train/                  |            |
|    approx_kl            | 0.34886175 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.566      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0527    |
|    n_updates            | 2890       |
|    policy_gradient_loss | -0.0507    |
|    value_loss           | 0.152      |
----------------------------------------
Eval num_timesteps=595000, episode_reward=24.20 +/- 5.42
Episode length: 216.70 +/- 52.69
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 217        |
|    mean_reward          | 24.2       |
| time/                   |            |
|    total_timesteps      | 595000     |
| train/                  |            |
|    approx_kl            | 0.31422746 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.326     |
|    explained_variance   | 0.528      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0405    |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.0441    |
|    value_loss           | 0.164      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 236      |
|    ep_rew_mean     | 25.4     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 291      |
|    time_elapsed    | 768      |
|    total_timesteps | 595968   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 237        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 292        |
|    time_elapsed         | 770        |
|    total_timesteps      | 598016     |
| train/                  |            |
|    approx_kl            | 0.24540603 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0371    |
|    n_updates            | 2910       |
|    policy_gradient_loss | -0.0463    |
|    value_loss           | 0.154      |
----------------------------------------
Eval num_timesteps=600000, episode_reward=27.50 +/- 4.52
Episode length: 254.80 +/- 59.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 255        |
|    mean_reward          | 27.5       |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.22328773 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.358     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0287    |
|    n_updates            | 2920       |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.15       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 25.4     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 293      |
|    time_elapsed    | 774      |
|    total_timesteps | 600064   |
---------------------------------
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'saves-tunning/defend-line/ppo-2/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
