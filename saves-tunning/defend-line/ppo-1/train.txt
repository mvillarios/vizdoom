/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 9.54     |
| time/              |          |
|    fps             | 1063     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 158         |
|    ep_rew_mean          | 9.48        |
| time/                   |             |
|    fps                  | 990         |
|    iterations           | 2           |
|    time_elapsed         | 4           |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.009127958 |
|    clip_fraction        | 0.0957      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.0505     |
|    learning_rate        | 0.00038     |
|    loss                 | 0.0442      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.19        |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=12.30 +/- 2.57
Episode length: 158.00 +/- 39.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 158         |
|    mean_reward          | 12.3        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.010627138 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.35       |
|    explained_variance   | 0.184       |
|    learning_rate        | 0.00038     |
|    loss                 | 0.0122      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.169       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | 10       |
| time/              |          |
|    fps             | 851      |
|    iterations      | 3        |
|    time_elapsed    | 7        |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 160         |
|    ep_rew_mean          | 10.9        |
| time/                   |             |
|    fps                  | 886         |
|    iterations           | 4           |
|    time_elapsed         | 9           |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.012422228 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.00038     |
|    loss                 | 0.00487     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0285     |
|    value_loss           | 0.179       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=16.70 +/- 6.60
Episode length: 193.60 +/- 74.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 194        |
|    mean_reward          | 16.7       |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01642459 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.27      |
|    explained_variance   | 0.16       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0062    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0391    |
|    value_loss           | 0.18       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 160      |
|    ep_rew_mean     | 11.6     |
| time/              |          |
|    fps             | 815      |
|    iterations      | 5        |
|    time_elapsed    | 12       |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 162         |
|    ep_rew_mean          | 12.1        |
| time/                   |             |
|    fps                  | 843         |
|    iterations           | 6           |
|    time_elapsed         | 14          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.020347683 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.288       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.00699    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0439     |
|    value_loss           | 0.201       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 168         |
|    ep_rew_mean          | 12.9        |
| time/                   |             |
|    fps                  | 864         |
|    iterations           | 7           |
|    time_elapsed         | 16          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.023062544 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.19       |
|    explained_variance   | 0.381       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0156     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0481     |
|    value_loss           | 0.183       |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=16.10 +/- 5.47
Episode length: 170.80 +/- 63.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 171       |
|    mean_reward          | 16.1      |
| time/                   |           |
|    total_timesteps      | 15000     |
| train/                  |           |
|    approx_kl            | 0.0251849 |
|    clip_fraction        | 0.242     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.16     |
|    explained_variance   | 0.388     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0244   |
|    n_updates            | 70        |
|    policy_gradient_loss | -0.047    |
|    value_loss           | 0.163     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 167      |
|    ep_rew_mean     | 13.3     |
| time/              |          |
|    fps             | 830      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 171         |
|    ep_rew_mean          | 14.2        |
| time/                   |             |
|    fps                  | 848         |
|    iterations           | 9           |
|    time_elapsed         | 21          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.028441094 |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0332     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0483     |
|    value_loss           | 0.161       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=18.40 +/- 6.04
Episode length: 172.40 +/- 66.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 172         |
|    mean_reward          | 18.4        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.028970513 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0139     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0511     |
|    value_loss           | 0.183       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 174      |
|    ep_rew_mean     | 15.1     |
| time/              |          |
|    fps             | 822      |
|    iterations      | 10       |
|    time_elapsed    | 24       |
|    total_timesteps | 20480    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 180        |
|    ep_rew_mean          | 16.1       |
| time/                   |            |
|    fps                  | 837        |
|    iterations           | 11         |
|    time_elapsed         | 26         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.03643778 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.05      |
|    explained_variance   | 0.551      |
|    learning_rate        | 0.00038    |
|    loss                 | 0.0031     |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0502    |
|    value_loss           | 0.175      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 181        |
|    ep_rew_mean          | 16.9       |
| time/                   |            |
|    fps                  | 849        |
|    iterations           | 12         |
|    time_elapsed         | 28         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.03437481 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0269    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0521    |
|    value_loss           | 0.172      |
----------------------------------------
Eval num_timesteps=25000, episode_reward=21.30 +/- 4.73
Episode length: 195.10 +/- 55.95
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 195        |
|    mean_reward          | 21.3       |
| time/                   |            |
|    total_timesteps      | 25000      |
| train/                  |            |
|    approx_kl            | 0.04211382 |
|    clip_fraction        | 0.308      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.982     |
|    explained_variance   | 0.486      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0227    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0546    |
|    value_loss           | 0.164      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 183      |
|    ep_rew_mean     | 17.5     |
| time/              |          |
|    fps             | 825      |
|    iterations      | 13       |
|    time_elapsed    | 32       |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 185        |
|    ep_rew_mean          | 18         |
| time/                   |            |
|    fps                  | 836        |
|    iterations           | 14         |
|    time_elapsed         | 34         |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.03723798 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.959     |
|    explained_variance   | 0.544      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.018     |
|    n_updates            | 130        |
|    policy_gradient_loss | -0.0502    |
|    value_loss           | 0.17       |
----------------------------------------
Eval num_timesteps=30000, episode_reward=27.00 +/- 7.75
Episode length: 248.20 +/- 70.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 248        |
|    mean_reward          | 27         |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.04581663 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.96      |
|    explained_variance   | 0.529      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0411    |
|    n_updates            | 140        |
|    policy_gradient_loss | -0.0502    |
|    value_loss           | 0.177      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | 18.8     |
| time/              |          |
|    fps             | 809      |
|    iterations      | 15       |
|    time_elapsed    | 37       |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 189         |
|    ep_rew_mean          | 18.9        |
| time/                   |             |
|    fps                  | 820         |
|    iterations           | 16          |
|    time_elapsed         | 39          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.038917564 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.913      |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.00038     |
|    loss                 | -0.00571    |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0495     |
|    value_loss           | 0.169       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 190         |
|    ep_rew_mean          | 19.2        |
| time/                   |             |
|    fps                  | 829         |
|    iterations           | 17          |
|    time_elapsed         | 41          |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.049978506 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.853      |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.00038     |
|    loss                 | 0.00953     |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0515     |
|    value_loss           | 0.179       |
-----------------------------------------
Eval num_timesteps=35000, episode_reward=25.40 +/- 4.27
Episode length: 233.80 +/- 49.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 234         |
|    mean_reward          | 25.4        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.045759764 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.819      |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.017      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0447     |
|    value_loss           | 0.162       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 193      |
|    ep_rew_mean     | 19.7     |
| time/              |          |
|    fps             | 809      |
|    iterations      | 18       |
|    time_elapsed    | 45       |
|    total_timesteps | 36864    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 194         |
|    ep_rew_mean          | 19.9        |
| time/                   |             |
|    fps                  | 818         |
|    iterations           | 19          |
|    time_elapsed         | 47          |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.056066927 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.763      |
|    explained_variance   | 0.551       |
|    learning_rate        | 0.00038     |
|    loss                 | 0.000391    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.0456     |
|    value_loss           | 0.174       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=21.80 +/- 4.24
Episode length: 201.90 +/- 49.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | 21.8       |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.05410269 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.764     |
|    explained_variance   | 0.6        |
|    learning_rate        | 0.00038    |
|    loss                 | 0.00177    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.0447    |
|    value_loss           | 0.171      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 195      |
|    ep_rew_mean     | 20.1     |
| time/              |          |
|    fps             | 804      |
|    iterations      | 20       |
|    time_elapsed    | 50       |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 197         |
|    ep_rew_mean          | 20.3        |
| time/                   |             |
|    fps                  | 813         |
|    iterations           | 21          |
|    time_elapsed         | 52          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.050693452 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.74       |
|    explained_variance   | 0.602       |
|    learning_rate        | 0.00038     |
|    loss                 | 0.0129      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0436     |
|    value_loss           | 0.172       |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=23.80 +/- 4.83
Episode length: 225.30 +/- 46.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 225         |
|    mean_reward          | 23.8        |
| time/                   |             |
|    total_timesteps      | 45000       |
| train/                  |             |
|    approx_kl            | 0.058367305 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.685      |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.00136    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0455     |
|    value_loss           | 0.179       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 201      |
|    ep_rew_mean     | 20.9     |
| time/              |          |
|    fps             | 798      |
|    iterations      | 22       |
|    time_elapsed    | 56       |
|    total_timesteps | 45056    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 204        |
|    ep_rew_mean          | 21.2       |
| time/                   |            |
|    fps                  | 806        |
|    iterations           | 23         |
|    time_elapsed         | 58         |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.06016125 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.657     |
|    explained_variance   | 0.539      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0183    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0429    |
|    value_loss           | 0.181      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 206         |
|    ep_rew_mean          | 21.5        |
| time/                   |             |
|    fps                  | 813         |
|    iterations           | 24          |
|    time_elapsed         | 60          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.059750505 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.65       |
|    explained_variance   | 0.57        |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0116     |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0406     |
|    value_loss           | 0.162       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=26.80 +/- 4.45
Episode length: 246.20 +/- 46.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 246        |
|    mean_reward          | 26.8       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.07192102 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.635     |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0242    |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0436    |
|    value_loss           | 0.157      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 204      |
|    ep_rew_mean     | 21.4     |
| time/              |          |
|    fps             | 798      |
|    iterations      | 25       |
|    time_elapsed    | 64       |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 208        |
|    ep_rew_mean          | 21.8       |
| time/                   |            |
|    fps                  | 805        |
|    iterations           | 26         |
|    time_elapsed         | 66         |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.06552817 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.647     |
|    explained_variance   | 0.584      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0112    |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.0422    |
|    value_loss           | 0.177      |
----------------------------------------
Eval num_timesteps=55000, episode_reward=26.40 +/- 5.43
Episode length: 232.60 +/- 56.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 233        |
|    mean_reward          | 26.4       |
| time/                   |            |
|    total_timesteps      | 55000      |
| train/                  |            |
|    approx_kl            | 0.06124265 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.653     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00838   |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.175      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 209      |
|    ep_rew_mean     | 22       |
| time/              |          |
|    fps             | 793      |
|    iterations      | 27       |
|    time_elapsed    | 69       |
|    total_timesteps | 55296    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 210        |
|    ep_rew_mean          | 22.2       |
| time/                   |            |
|    fps                  | 800        |
|    iterations           | 28         |
|    time_elapsed         | 71         |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.06998407 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.644     |
|    explained_variance   | 0.545      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0193    |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.178      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 214        |
|    ep_rew_mean          | 22.7       |
| time/                   |            |
|    fps                  | 806        |
|    iterations           | 29         |
|    time_elapsed         | 73         |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.06962025 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.598     |
|    explained_variance   | 0.573      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0134    |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0391    |
|    value_loss           | 0.173      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=22.70 +/- 3.49
Episode length: 201.80 +/- 37.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 202         |
|    mean_reward          | 22.7        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.061139554 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.615      |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.00038     |
|    loss                 | 0.0115      |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0436     |
|    value_loss           | 0.167       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 213      |
|    ep_rew_mean     | 22.8     |
| time/              |          |
|    fps             | 797      |
|    iterations      | 30       |
|    time_elapsed    | 77       |
|    total_timesteps | 61440    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 213        |
|    ep_rew_mean          | 22.9       |
| time/                   |            |
|    fps                  | 803        |
|    iterations           | 31         |
|    time_elapsed         | 79         |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.05753153 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.608     |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.00038    |
|    loss                 | 0.000373   |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.041     |
|    value_loss           | 0.177      |
----------------------------------------
Eval num_timesteps=65000, episode_reward=26.80 +/- 4.42
Episode length: 242.30 +/- 38.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 242         |
|    mean_reward          | 26.8        |
| time/                   |             |
|    total_timesteps      | 65000       |
| train/                  |             |
|    approx_kl            | 0.074992605 |
|    clip_fraction        | 0.278       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.538      |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.00363    |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0386     |
|    value_loss           | 0.184       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 23.2     |
| time/              |          |
|    fps             | 792      |
|    iterations      | 32       |
|    time_elapsed    | 82       |
|    total_timesteps | 65536    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 216        |
|    ep_rew_mean          | 23.5       |
| time/                   |            |
|    fps                  | 798        |
|    iterations           | 33         |
|    time_elapsed         | 84         |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.07767218 |
|    clip_fraction        | 0.286      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.552     |
|    explained_variance   | 0.542      |
|    learning_rate        | 0.00038    |
|    loss                 | 0.00927    |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0419    |
|    value_loss           | 0.163      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 218        |
|    ep_rew_mean          | 23.6       |
| time/                   |            |
|    fps                  | 803        |
|    iterations           | 34         |
|    time_elapsed         | 86         |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.08822769 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.539     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0103    |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.0391    |
|    value_loss           | 0.172      |
----------------------------------------
Eval num_timesteps=70000, episode_reward=24.10 +/- 5.13
Episode length: 224.30 +/- 49.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 224        |
|    mean_reward          | 24.1       |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.08538335 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.542     |
|    explained_variance   | 0.695      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.012     |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.0396    |
|    value_loss           | 0.146      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 23.6     |
| time/              |          |
|    fps             | 794      |
|    iterations      | 35       |
|    time_elapsed    | 90       |
|    total_timesteps | 71680    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 219        |
|    ep_rew_mean          | 23.9       |
| time/                   |            |
|    fps                  | 799        |
|    iterations           | 36         |
|    time_elapsed         | 92         |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.08970958 |
|    clip_fraction        | 0.29       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.528     |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.00038    |
|    loss                 | 0.0165     |
|    n_updates            | 350        |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.175      |
----------------------------------------
Eval num_timesteps=75000, episode_reward=24.40 +/- 4.86
Episode length: 221.30 +/- 38.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 221         |
|    mean_reward          | 24.4        |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.090974934 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.568       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0294     |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.0372     |
|    value_loss           | 0.163       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 23.9     |
| time/              |          |
|    fps             | 791      |
|    iterations      | 37       |
|    time_elapsed    | 95       |
|    total_timesteps | 75776    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 220        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 796        |
|    iterations           | 38         |
|    time_elapsed         | 97         |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.08531448 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.526     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0109    |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0426    |
|    value_loss           | 0.169      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 217        |
|    ep_rew_mean          | 24         |
| time/                   |            |
|    fps                  | 800        |
|    iterations           | 39         |
|    time_elapsed         | 99         |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.08874727 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.526     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0132    |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0459    |
|    value_loss           | 0.145      |
----------------------------------------
Eval num_timesteps=80000, episode_reward=23.60 +/- 5.75
Episode length: 194.00 +/- 57.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 194       |
|    mean_reward          | 23.6      |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.1098346 |
|    clip_fraction        | 0.294     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.477    |
|    explained_variance   | 0.66      |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0104   |
|    n_updates            | 390       |
|    policy_gradient_loss | -0.0431   |
|    value_loss           | 0.161     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 222      |
|    ep_rew_mean     | 24.5     |
| time/              |          |
|    fps             | 795      |
|    iterations      | 40       |
|    time_elapsed    | 103      |
|    total_timesteps | 81920    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 225        |
|    ep_rew_mean          | 24.8       |
| time/                   |            |
|    fps                  | 799        |
|    iterations           | 41         |
|    time_elapsed         | 105        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.08757893 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.488     |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0062    |
|    n_updates            | 400        |
|    policy_gradient_loss | -0.0376    |
|    value_loss           | 0.155      |
----------------------------------------
Eval num_timesteps=85000, episode_reward=25.80 +/- 4.96
Episode length: 239.20 +/- 52.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 239        |
|    mean_reward          | 25.8       |
| time/                   |            |
|    total_timesteps      | 85000      |
| train/                  |            |
|    approx_kl            | 0.09819423 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.5       |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0257    |
|    n_updates            | 410        |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 25.1     |
| time/              |          |
|    fps             | 791      |
|    iterations      | 42       |
|    time_elapsed    | 108      |
|    total_timesteps | 86016    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 226        |
|    ep_rew_mean          | 24.9       |
| time/                   |            |
|    fps                  | 795        |
|    iterations           | 43         |
|    time_elapsed         | 110        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.10048182 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.491     |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.024     |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.143      |
----------------------------------------
Eval num_timesteps=90000, episode_reward=24.70 +/- 4.65
Episode length: 219.90 +/- 41.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 220         |
|    mean_reward          | 24.7        |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.091908686 |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.522      |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0462     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0445     |
|    value_loss           | 0.149       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | 24.6     |
| time/              |          |
|    fps             | 789      |
|    iterations      | 44       |
|    time_elapsed    | 114      |
|    total_timesteps | 90112    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | 24.6       |
| time/                   |            |
|    fps                  | 793        |
|    iterations           | 45         |
|    time_elapsed         | 116        |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.19389495 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.473     |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00554   |
|    n_updates            | 440        |
|    policy_gradient_loss | -0.0387    |
|    value_loss           | 0.166      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 222        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 797        |
|    iterations           | 46         |
|    time_elapsed         | 118        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.08406432 |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.473     |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0416    |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0385    |
|    value_loss           | 0.152      |
----------------------------------------
Eval num_timesteps=95000, episode_reward=25.70 +/- 2.76
Episode length: 228.10 +/- 27.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 228         |
|    mean_reward          | 25.7        |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.123342425 |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.435      |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0203     |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0399     |
|    value_loss           | 0.162       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 222      |
|    ep_rew_mean     | 24.8     |
| time/              |          |
|    fps             | 791      |
|    iterations      | 47       |
|    time_elapsed    | 121      |
|    total_timesteps | 96256    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 221        |
|    ep_rew_mean          | 24.8       |
| time/                   |            |
|    fps                  | 794        |
|    iterations           | 48         |
|    time_elapsed         | 123        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.11927929 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.451     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.03      |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.0465    |
|    value_loss           | 0.156      |
----------------------------------------
Eval num_timesteps=100000, episode_reward=25.80 +/- 6.05
Episode length: 243.10 +/- 68.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 243        |
|    mean_reward          | 25.8       |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.12160663 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.422     |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00293   |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0393    |
|    value_loss           | 0.159      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | 24.8     |
| time/              |          |
|    fps             | 788      |
|    iterations      | 49       |
|    time_elapsed    | 127      |
|    total_timesteps | 100352   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 225        |
|    ep_rew_mean          | 25         |
| time/                   |            |
|    fps                  | 791        |
|    iterations           | 50         |
|    time_elapsed         | 129        |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.13366827 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.427     |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0324    |
|    n_updates            | 490        |
|    policy_gradient_loss | -0.0379    |
|    value_loss           | 0.16       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 224         |
|    ep_rew_mean          | 24.8        |
| time/                   |             |
|    fps                  | 795         |
|    iterations           | 51          |
|    time_elapsed         | 131         |
|    total_timesteps      | 104448      |
| train/                  |             |
|    approx_kl            | 0.116846666 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.419      |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0304     |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.0341     |
|    value_loss           | 0.145       |
-----------------------------------------
Eval num_timesteps=105000, episode_reward=27.60 +/- 4.20
Episode length: 260.90 +/- 49.78
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 261        |
|    mean_reward          | 27.6       |
| time/                   |            |
|    total_timesteps      | 105000     |
| train/                  |            |
|    approx_kl            | 0.14841548 |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.405     |
|    explained_variance   | 0.589      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.004     |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.0395    |
|    value_loss           | 0.157      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 222      |
|    ep_rew_mean     | 24.6     |
| time/              |          |
|    fps             | 788      |
|    iterations      | 52       |
|    time_elapsed    | 135      |
|    total_timesteps | 106496   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 223       |
|    ep_rew_mean          | 24.6      |
| time/                   |           |
|    fps                  | 791       |
|    iterations           | 53        |
|    time_elapsed         | 137       |
|    total_timesteps      | 108544    |
| train/                  |           |
|    approx_kl            | 0.1410998 |
|    clip_fraction        | 0.296     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.417    |
|    explained_variance   | 0.659     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.00905  |
|    n_updates            | 520       |
|    policy_gradient_loss | -0.0386   |
|    value_loss           | 0.153     |
---------------------------------------
Eval num_timesteps=110000, episode_reward=25.50 +/- 4.72
Episode length: 236.80 +/- 44.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 237        |
|    mean_reward          | 25.5       |
| time/                   |            |
|    total_timesteps      | 110000     |
| train/                  |            |
|    approx_kl            | 0.14845793 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.375     |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0203    |
|    n_updates            | 530        |
|    policy_gradient_loss | -0.0353    |
|    value_loss           | 0.151      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | 24.9     |
| time/              |          |
|    fps             | 785      |
|    iterations      | 54       |
|    time_elapsed    | 140      |
|    total_timesteps | 110592   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 227        |
|    ep_rew_mean          | 25.1       |
| time/                   |            |
|    fps                  | 789        |
|    iterations           | 55         |
|    time_elapsed         | 142        |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.12223782 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.39      |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00774   |
|    n_updates            | 540        |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 0.153      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 226        |
|    ep_rew_mean          | 25         |
| time/                   |            |
|    fps                  | 792        |
|    iterations           | 56         |
|    time_elapsed         | 144        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.11215999 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.417     |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0141    |
|    n_updates            | 550        |
|    policy_gradient_loss | -0.0375    |
|    value_loss           | 0.154      |
----------------------------------------
Eval num_timesteps=115000, episode_reward=25.00 +/- 6.29
Episode length: 220.70 +/- 52.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 221        |
|    mean_reward          | 25         |
| time/                   |            |
|    total_timesteps      | 115000     |
| train/                  |            |
|    approx_kl            | 0.11661727 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.383     |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0332    |
|    n_updates            | 560        |
|    policy_gradient_loss | -0.0393    |
|    value_loss           | 0.157      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 25.3     |
| time/              |          |
|    fps             | 787      |
|    iterations      | 57       |
|    time_elapsed    | 148      |
|    total_timesteps | 116736   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 232         |
|    ep_rew_mean          | 25.6        |
| time/                   |             |
|    fps                  | 790         |
|    iterations           | 58          |
|    time_elapsed         | 150         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.110869214 |
|    clip_fraction        | 0.279       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.386      |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.016      |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.0396     |
|    value_loss           | 0.15        |
-----------------------------------------
Eval num_timesteps=120000, episode_reward=25.90 +/- 3.65
Episode length: 228.70 +/- 37.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 229        |
|    mean_reward          | 25.9       |
| time/                   |            |
|    total_timesteps      | 120000     |
| train/                  |            |
|    approx_kl            | 0.14717616 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.406     |
|    explained_variance   | 0.641      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0247    |
|    n_updates            | 580        |
|    policy_gradient_loss | -0.0367    |
|    value_loss           | 0.151      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 25.3     |
| time/              |          |
|    fps             | 785      |
|    iterations      | 59       |
|    time_elapsed    | 153      |
|    total_timesteps | 120832   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 230        |
|    ep_rew_mean          | 25.4       |
| time/                   |            |
|    fps                  | 788        |
|    iterations           | 60         |
|    time_elapsed         | 155        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.10913963 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.398     |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0124    |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0378    |
|    value_loss           | 0.158      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 25.4       |
| time/                   |            |
|    fps                  | 791        |
|    iterations           | 61         |
|    time_elapsed         | 157        |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.12476638 |
|    clip_fraction        | 0.289      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.406     |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0153    |
|    n_updates            | 600        |
|    policy_gradient_loss | -0.0363    |
|    value_loss           | 0.152      |
----------------------------------------
Eval num_timesteps=125000, episode_reward=27.30 +/- 4.31
Episode length: 241.70 +/- 38.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 242        |
|    mean_reward          | 27.3       |
| time/                   |            |
|    total_timesteps      | 125000     |
| train/                  |            |
|    approx_kl            | 0.11502762 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.423     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0407    |
|    n_updates            | 610        |
|    policy_gradient_loss | -0.0346    |
|    value_loss           | 0.161      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 230      |
|    ep_rew_mean     | 25.5     |
| time/              |          |
|    fps             | 786      |
|    iterations      | 62       |
|    time_elapsed    | 161      |
|    total_timesteps | 126976   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 229         |
|    ep_rew_mean          | 25.4        |
| time/                   |             |
|    fps                  | 789         |
|    iterations           | 63          |
|    time_elapsed         | 163         |
|    total_timesteps      | 129024      |
| train/                  |             |
|    approx_kl            | 0.123018794 |
|    clip_fraction        | 0.291       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.426      |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0209     |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.0423     |
|    value_loss           | 0.152       |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=25.70 +/- 6.53
Episode length: 226.70 +/- 70.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 227         |
|    mean_reward          | 25.7        |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.118512325 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.422      |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.0339     |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.0365     |
|    value_loss           | 0.16        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 25.8     |
| time/              |          |
|    fps             | 785      |
|    iterations      | 64       |
|    time_elapsed    | 166      |
|    total_timesteps | 131072   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 233        |
|    ep_rew_mean          | 25.8       |
| time/                   |            |
|    fps                  | 787        |
|    iterations           | 65         |
|    time_elapsed         | 168        |
|    total_timesteps      | 133120     |
| train/                  |            |
|    approx_kl            | 0.16853678 |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.407     |
|    explained_variance   | 0.523      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0175    |
|    n_updates            | 640        |
|    policy_gradient_loss | -0.0327    |
|    value_loss           | 0.164      |
----------------------------------------
Eval num_timesteps=135000, episode_reward=26.40 +/- 3.98
Episode length: 232.20 +/- 36.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 232        |
|    mean_reward          | 26.4       |
| time/                   |            |
|    total_timesteps      | 135000     |
| train/                  |            |
|    approx_kl            | 0.16046591 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.386     |
|    explained_variance   | 0.593      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0247    |
|    n_updates            | 650        |
|    policy_gradient_loss | -0.0383    |
|    value_loss           | 0.13       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 25.3     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 66       |
|    time_elapsed    | 172      |
|    total_timesteps | 135168   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 231       |
|    ep_rew_mean          | 25.6      |
| time/                   |           |
|    fps                  | 786       |
|    iterations           | 67        |
|    time_elapsed         | 174       |
|    total_timesteps      | 137216    |
| train/                  |           |
|    approx_kl            | 0.1318953 |
|    clip_fraction        | 0.29      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.395    |
|    explained_variance   | 0.634     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.00662  |
|    n_updates            | 660       |
|    policy_gradient_loss | -0.0395   |
|    value_loss           | 0.148     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 25.4       |
| time/                   |            |
|    fps                  | 788        |
|    iterations           | 68         |
|    time_elapsed         | 176        |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.12948401 |
|    clip_fraction        | 0.291      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.397     |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0371    |
|    n_updates            | 670        |
|    policy_gradient_loss | -0.0381    |
|    value_loss           | 0.133      |
----------------------------------------
Eval num_timesteps=140000, episode_reward=25.70 +/- 4.73
Episode length: 223.50 +/- 52.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 224        |
|    mean_reward          | 25.7       |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.15175664 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.42      |
|    explained_variance   | 0.516      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0296    |
|    n_updates            | 680        |
|    policy_gradient_loss | -0.0406    |
|    value_loss           | 0.151      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 231      |
|    ep_rew_mean     | 25.5     |
| time/              |          |
|    fps             | 785      |
|    iterations      | 69       |
|    time_elapsed    | 180      |
|    total_timesteps | 141312   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 787        |
|    iterations           | 70         |
|    time_elapsed         | 182        |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.19651976 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.412     |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0452    |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.0398    |
|    value_loss           | 0.146      |
----------------------------------------
Eval num_timesteps=145000, episode_reward=26.90 +/- 5.32
Episode length: 229.90 +/- 55.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 230        |
|    mean_reward          | 26.9       |
| time/                   |            |
|    total_timesteps      | 145000     |
| train/                  |            |
|    approx_kl            | 0.14027357 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.434     |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0378    |
|    n_updates            | 700        |
|    policy_gradient_loss | -0.0387    |
|    value_loss           | 0.148      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 25.9     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 71       |
|    time_elapsed    | 185      |
|    total_timesteps | 145408   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 234         |
|    ep_rew_mean          | 25.9        |
| time/                   |             |
|    fps                  | 786         |
|    iterations           | 72          |
|    time_elapsed         | 187         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.122332156 |
|    clip_fraction        | 0.299       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.447      |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.00038     |
|    loss                 | -0.042      |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.0442     |
|    value_loss           | 0.137       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 26         |
| time/                   |            |
|    fps                  | 788        |
|    iterations           | 73         |
|    time_elapsed         | 189        |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.12119286 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.44      |
|    explained_variance   | 0.578      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0278    |
|    n_updates            | 720        |
|    policy_gradient_loss | -0.0413    |
|    value_loss           | 0.161      |
----------------------------------------
Eval num_timesteps=150000, episode_reward=26.10 +/- 4.37
Episode length: 237.60 +/- 48.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 238        |
|    mean_reward          | 26.1       |
| time/                   |            |
|    total_timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.14929259 |
|    clip_fraction        | 0.343      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.491     |
|    explained_variance   | 0.549      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0354    |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.0455    |
|    value_loss           | 0.148      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 26.1     |
| time/              |          |
|    fps             | 784      |
|    iterations      | 74       |
|    time_elapsed    | 193      |
|    total_timesteps | 151552   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 230        |
|    ep_rew_mean          | 25.7       |
| time/                   |            |
|    fps                  | 786        |
|    iterations           | 75         |
|    time_elapsed         | 195        |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.10637914 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.472     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0337    |
|    n_updates            | 740        |
|    policy_gradient_loss | -0.0441    |
|    value_loss           | 0.156      |
----------------------------------------
Eval num_timesteps=155000, episode_reward=26.00 +/- 2.97
Episode length: 223.00 +/- 36.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 223        |
|    mean_reward          | 26         |
| time/                   |            |
|    total_timesteps      | 155000     |
| train/                  |            |
|    approx_kl            | 0.12599148 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.473     |
|    explained_variance   | 0.565      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0325    |
|    n_updates            | 750        |
|    policy_gradient_loss | -0.0437    |
|    value_loss           | 0.139      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 76       |
|    time_elapsed    | 198      |
|    total_timesteps | 155648   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 236        |
|    ep_rew_mean          | 26.2       |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 77         |
|    time_elapsed         | 200        |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.14593902 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.474     |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0366    |
|    n_updates            | 760        |
|    policy_gradient_loss | -0.047     |
|    value_loss           | 0.138      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 240        |
|    ep_rew_mean          | 26.5       |
| time/                   |            |
|    fps                  | 788        |
|    iterations           | 78         |
|    time_elapsed         | 202        |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.13508968 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.469     |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00363   |
|    n_updates            | 770        |
|    policy_gradient_loss | -0.0446    |
|    value_loss           | 0.163      |
----------------------------------------
Eval num_timesteps=160000, episode_reward=27.80 +/- 3.97
Episode length: 253.00 +/- 41.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 253        |
|    mean_reward          | 27.8       |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.14829166 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.48      |
|    explained_variance   | 0.542      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0196    |
|    n_updates            | 780        |
|    policy_gradient_loss | -0.0497    |
|    value_loss           | 0.144      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 26.3     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 79       |
|    time_elapsed    | 206      |
|    total_timesteps | 161792   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 238        |
|    ep_rew_mean          | 26.4       |
| time/                   |            |
|    fps                  | 786        |
|    iterations           | 80         |
|    time_elapsed         | 208        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.13571903 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.433     |
|    explained_variance   | 0.56       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0423    |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.0458    |
|    value_loss           | 0.147      |
----------------------------------------
Eval num_timesteps=165000, episode_reward=20.90 +/- 4.21
Episode length: 185.20 +/- 35.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 185       |
|    mean_reward          | 20.9      |
| time/                   |           |
|    total_timesteps      | 165000    |
| train/                  |           |
|    approx_kl            | 0.1730394 |
|    clip_fraction        | 0.324     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.437    |
|    explained_variance   | 0.533     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0308   |
|    n_updates            | 800       |
|    policy_gradient_loss | -0.0443   |
|    value_loss           | 0.152     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 26.1     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 81       |
|    time_elapsed    | 211      |
|    total_timesteps | 165888   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 238        |
|    ep_rew_mean          | 25.9       |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 82         |
|    time_elapsed         | 213        |
|    total_timesteps      | 167936     |
| train/                  |            |
|    approx_kl            | 0.28187206 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.433     |
|    explained_variance   | 0.519      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0165    |
|    n_updates            | 810        |
|    policy_gradient_loss | -0.0447    |
|    value_loss           | 0.146      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 25.3       |
| time/                   |            |
|    fps                  | 788        |
|    iterations           | 83         |
|    time_elapsed         | 215        |
|    total_timesteps      | 169984     |
| train/                  |            |
|    approx_kl            | 0.13440318 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.419     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0243    |
|    n_updates            | 820        |
|    policy_gradient_loss | -0.0423    |
|    value_loss           | 0.15       |
----------------------------------------
Eval num_timesteps=170000, episode_reward=22.80 +/- 3.99
Episode length: 206.10 +/- 35.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 206        |
|    mean_reward          | 22.8       |
| time/                   |            |
|    total_timesteps      | 170000     |
| train/                  |            |
|    approx_kl            | 0.23178804 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.414     |
|    explained_variance   | 0.596      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0335    |
|    n_updates            | 830        |
|    policy_gradient_loss | -0.0445    |
|    value_loss           | 0.144      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 236      |
|    ep_rew_mean     | 25.4     |
| time/              |          |
|    fps             | 785      |
|    iterations      | 84       |
|    time_elapsed    | 219      |
|    total_timesteps | 172032   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 25.1       |
| time/                   |            |
|    fps                  | 787        |
|    iterations           | 85         |
|    time_elapsed         | 221        |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.14175984 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.429     |
|    explained_variance   | 0.511      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0564    |
|    n_updates            | 840        |
|    policy_gradient_loss | -0.0404    |
|    value_loss           | 0.142      |
----------------------------------------
Eval num_timesteps=175000, episode_reward=27.20 +/- 8.45
Episode length: 245.20 +/- 80.18
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 245        |
|    mean_reward          | 27.2       |
| time/                   |            |
|    total_timesteps      | 175000     |
| train/                  |            |
|    approx_kl            | 0.16073135 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.452     |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.033     |
|    n_updates            | 850        |
|    policy_gradient_loss | -0.0492    |
|    value_loss           | 0.14       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 25       |
| time/              |          |
|    fps             | 783      |
|    iterations      | 86       |
|    time_elapsed    | 224      |
|    total_timesteps | 176128   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 24.9       |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 87         |
|    time_elapsed         | 226        |
|    total_timesteps      | 178176     |
| train/                  |            |
|    approx_kl            | 0.14223552 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.432     |
|    explained_variance   | 0.557      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0465    |
|    n_updates            | 860        |
|    policy_gradient_loss | -0.0477    |
|    value_loss           | 0.159      |
----------------------------------------
Eval num_timesteps=180000, episode_reward=26.60 +/- 4.00
Episode length: 246.50 +/- 48.92
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 246       |
|    mean_reward          | 26.6      |
| time/                   |           |
|    total_timesteps      | 180000    |
| train/                  |           |
|    approx_kl            | 0.1375083 |
|    clip_fraction        | 0.335     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.444    |
|    explained_variance   | 0.638     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0293   |
|    n_updates            | 870       |
|    policy_gradient_loss | -0.0454   |
|    value_loss           | 0.135     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 25       |
| time/              |          |
|    fps             | 782      |
|    iterations      | 88       |
|    time_elapsed    | 230      |
|    total_timesteps | 180224   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 24.6       |
| time/                   |            |
|    fps                  | 784        |
|    iterations           | 89         |
|    time_elapsed         | 232        |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.22512798 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.435     |
|    explained_variance   | 0.514      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0376    |
|    n_updates            | 880        |
|    policy_gradient_loss | -0.0442    |
|    value_loss           | 0.149      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 227        |
|    ep_rew_mean          | 24.3       |
| time/                   |            |
|    fps                  | 786        |
|    iterations           | 90         |
|    time_elapsed         | 234        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.15243995 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.438     |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0508    |
|    n_updates            | 890        |
|    policy_gradient_loss | -0.0465    |
|    value_loss           | 0.154      |
----------------------------------------
Eval num_timesteps=185000, episode_reward=23.90 +/- 5.28
Episode length: 212.20 +/- 52.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 212        |
|    mean_reward          | 23.9       |
| time/                   |            |
|    total_timesteps      | 185000     |
| train/                  |            |
|    approx_kl            | 0.15278289 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.431     |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0247    |
|    n_updates            | 900        |
|    policy_gradient_loss | -0.0433    |
|    value_loss           | 0.15       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 24.5     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 91       |
|    time_elapsed    | 237      |
|    total_timesteps | 186368   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 230        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 92         |
|    time_elapsed         | 239        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.15094836 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.404     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0326    |
|    n_updates            | 910        |
|    policy_gradient_loss | -0.0456    |
|    value_loss           | 0.141      |
----------------------------------------
Eval num_timesteps=190000, episode_reward=26.40 +/- 6.02
Episode length: 227.40 +/- 65.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 227        |
|    mean_reward          | 26.4       |
| time/                   |            |
|    total_timesteps      | 190000     |
| train/                  |            |
|    approx_kl            | 0.15081903 |
|    clip_fraction        | 0.294      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.386     |
|    explained_variance   | 0.586      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0342    |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.0403    |
|    value_loss           | 0.145      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 230      |
|    ep_rew_mean     | 24.7     |
| time/              |          |
|    fps             | 782      |
|    iterations      | 93       |
|    time_elapsed    | 243      |
|    total_timesteps | 190464   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 235        |
|    ep_rew_mean          | 25.2       |
| time/                   |            |
|    fps                  | 784        |
|    iterations           | 94         |
|    time_elapsed         | 245        |
|    total_timesteps      | 192512     |
| train/                  |            |
|    approx_kl            | 0.14746836 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.401     |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.028     |
|    n_updates            | 930        |
|    policy_gradient_loss | -0.046     |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 241       |
|    ep_rew_mean          | 25.8      |
| time/                   |           |
|    fps                  | 786       |
|    iterations           | 95        |
|    time_elapsed         | 247       |
|    total_timesteps      | 194560    |
| train/                  |           |
|    approx_kl            | 0.1629402 |
|    clip_fraction        | 0.316     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.418    |
|    explained_variance   | 0.62      |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0221   |
|    n_updates            | 940       |
|    policy_gradient_loss | -0.0444   |
|    value_loss           | 0.127     |
---------------------------------------
Eval num_timesteps=195000, episode_reward=28.70 +/- 4.90
Episode length: 249.50 +/- 46.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 250        |
|    mean_reward          | 28.7       |
| time/                   |            |
|    total_timesteps      | 195000     |
| train/                  |            |
|    approx_kl            | 0.18168908 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.428     |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0435    |
|    n_updates            | 950        |
|    policy_gradient_loss | -0.046     |
|    value_loss           | 0.128      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | 25.8     |
| time/              |          |
|    fps             | 783      |
|    iterations      | 96       |
|    time_elapsed    | 251      |
|    total_timesteps | 196608   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 239        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 784        |
|    iterations           | 97         |
|    time_elapsed         | 253        |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.19050609 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.448     |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0381    |
|    n_updates            | 960        |
|    policy_gradient_loss | -0.0463    |
|    value_loss           | 0.134      |
----------------------------------------
Eval num_timesteps=200000, episode_reward=25.40 +/- 4.54
Episode length: 226.80 +/- 43.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 227        |
|    mean_reward          | 25.4       |
| time/                   |            |
|    total_timesteps      | 200000     |
| train/                  |            |
|    approx_kl            | 0.23932022 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.434     |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0413    |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.135      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 241      |
|    ep_rew_mean     | 25.7     |
| time/              |          |
|    fps             | 782      |
|    iterations      | 98       |
|    time_elapsed    | 256      |
|    total_timesteps | 200704   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 240        |
|    ep_rew_mean          | 25.9       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 99         |
|    time_elapsed         | 258        |
|    total_timesteps      | 202752     |
| train/                  |            |
|    approx_kl            | 0.17745864 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.446     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0611    |
|    n_updates            | 980        |
|    policy_gradient_loss | -0.0493    |
|    value_loss           | 0.13       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 239        |
|    ep_rew_mean          | 25.8       |
| time/                   |            |
|    fps                  | 785        |
|    iterations           | 100        |
|    time_elapsed         | 260        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.18838838 |
|    clip_fraction        | 0.364      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.457     |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0479    |
|    n_updates            | 990        |
|    policy_gradient_loss | -0.0499    |
|    value_loss           | 0.137      |
----------------------------------------
Eval num_timesteps=205000, episode_reward=27.90 +/- 5.61
Episode length: 255.30 +/- 56.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 255        |
|    mean_reward          | 27.9       |
| time/                   |            |
|    total_timesteps      | 205000     |
| train/                  |            |
|    approx_kl            | 0.17308182 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.445     |
|    explained_variance   | 0.562      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0409    |
|    n_updates            | 1000       |
|    policy_gradient_loss | -0.0478    |
|    value_loss           | 0.154      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 241      |
|    ep_rew_mean     | 26       |
| time/              |          |
|    fps             | 782      |
|    iterations      | 101      |
|    time_elapsed    | 264      |
|    total_timesteps | 206848   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 240        |
|    ep_rew_mean          | 26.1       |
| time/                   |            |
|    fps                  | 784        |
|    iterations           | 102        |
|    time_elapsed         | 266        |
|    total_timesteps      | 208896     |
| train/                  |            |
|    approx_kl            | 0.18754375 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.436     |
|    explained_variance   | 0.549      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0458    |
|    n_updates            | 1010       |
|    policy_gradient_loss | -0.0471    |
|    value_loss           | 0.146      |
----------------------------------------
Eval num_timesteps=210000, episode_reward=25.50 +/- 3.29
Episode length: 228.10 +/- 34.54
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 25.5       |
| time/                   |            |
|    total_timesteps      | 210000     |
| train/                  |            |
|    approx_kl            | 0.13687953 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.436     |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0381    |
|    n_updates            | 1020       |
|    policy_gradient_loss | -0.0483    |
|    value_loss           | 0.157      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 25.9     |
| time/              |          |
|    fps             | 781      |
|    iterations      | 103      |
|    time_elapsed    | 269      |
|    total_timesteps | 210944   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 239        |
|    ep_rew_mean          | 25.9       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 104        |
|    time_elapsed         | 271        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.16286701 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.416     |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0323    |
|    n_updates            | 1030       |
|    policy_gradient_loss | -0.0434    |
|    value_loss           | 0.152      |
----------------------------------------
Eval num_timesteps=215000, episode_reward=28.10 +/- 5.87
Episode length: 254.30 +/- 64.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 254        |
|    mean_reward          | 28.1       |
| time/                   |            |
|    total_timesteps      | 215000     |
| train/                  |            |
|    approx_kl            | 0.20220347 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.414     |
|    explained_variance   | 0.546      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0501    |
|    n_updates            | 1040       |
|    policy_gradient_loss | -0.0401    |
|    value_loss           | 0.154      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 25.5     |
| time/              |          |
|    fps             | 780      |
|    iterations      | 105      |
|    time_elapsed    | 275      |
|    total_timesteps | 215040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 233        |
|    ep_rew_mean          | 25.4       |
| time/                   |            |
|    fps                  | 781        |
|    iterations           | 106        |
|    time_elapsed         | 277        |
|    total_timesteps      | 217088     |
| train/                  |            |
|    approx_kl            | 0.19617626 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.402     |
|    explained_variance   | 0.536      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0139    |
|    n_updates            | 1050       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.167      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 25.3       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 107        |
|    time_elapsed         | 279        |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.22471225 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.415     |
|    explained_variance   | 0.531      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0365    |
|    n_updates            | 1060       |
|    policy_gradient_loss | -0.0501    |
|    value_loss           | 0.146      |
----------------------------------------
Eval num_timesteps=220000, episode_reward=25.10 +/- 6.61
Episode length: 227.80 +/- 69.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 228        |
|    mean_reward          | 25.1       |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.25765085 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.401     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0392    |
|    n_updates            | 1070       |
|    policy_gradient_loss | -0.0403    |
|    value_loss           | 0.128      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 25.5     |
| time/              |          |
|    fps             | 781      |
|    iterations      | 108      |
|    time_elapsed    | 283      |
|    total_timesteps | 221184   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 233        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 782        |
|    iterations           | 109        |
|    time_elapsed         | 285        |
|    total_timesteps      | 223232     |
| train/                  |            |
|    approx_kl            | 0.21824327 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.417     |
|    explained_variance   | 0.531      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0364    |
|    n_updates            | 1080       |
|    policy_gradient_loss | -0.0465    |
|    value_loss           | 0.162      |
----------------------------------------
Eval num_timesteps=225000, episode_reward=26.80 +/- 3.79
Episode length: 249.50 +/- 38.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 250        |
|    mean_reward          | 26.8       |
| time/                   |            |
|    total_timesteps      | 225000     |
| train/                  |            |
|    approx_kl            | 0.21292728 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.396     |
|    explained_variance   | 0.534      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.043     |
|    n_updates            | 1090       |
|    policy_gradient_loss | -0.0473    |
|    value_loss           | 0.148      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 780      |
|    iterations      | 110      |
|    time_elapsed    | 288      |
|    total_timesteps | 225280   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 240        |
|    ep_rew_mean          | 25.8       |
| time/                   |            |
|    fps                  | 781        |
|    iterations           | 111        |
|    time_elapsed         | 290        |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.16004005 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.379     |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0412    |
|    n_updates            | 1100       |
|    policy_gradient_loss | -0.0414    |
|    value_loss           | 0.157      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 239        |
|    ep_rew_mean          | 25.7       |
| time/                   |            |
|    fps                  | 783        |
|    iterations           | 112        |
|    time_elapsed         | 292        |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.19911301 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.399     |
|    explained_variance   | 0.64       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0202    |
|    n_updates            | 1110       |
|    policy_gradient_loss | -0.0416    |
|    value_loss           | 0.138      |
----------------------------------------
Eval num_timesteps=230000, episode_reward=31.50 +/- 6.53
Episode length: 314.30 +/- 88.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 314        |
|    mean_reward          | 31.5       |
| time/                   |            |
|    total_timesteps      | 230000     |
| train/                  |            |
|    approx_kl            | 0.16739368 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.402     |
|    explained_variance   | 0.523      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0431    |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.0442    |
|    value_loss           | 0.145      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 113      |
|    time_elapsed    | 296      |
|    total_timesteps | 231424   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 239        |
|    ep_rew_mean          | 25.6       |
| time/                   |            |
|    fps                  | 781        |
|    iterations           | 114        |
|    time_elapsed         | 298        |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.22012429 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.426     |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0257    |
|    n_updates            | 1130       |
|    policy_gradient_loss | -0.0486    |
|    value_loss           | 0.145      |
----------------------------------------
Eval num_timesteps=235000, episode_reward=27.80 +/- 6.29
Episode length: 251.20 +/- 60.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 251        |
|    mean_reward          | 27.8       |
| time/                   |            |
|    total_timesteps      | 235000     |
| train/                  |            |
|    approx_kl            | 0.21816532 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.397     |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0298    |
|    n_updates            | 1140       |
|    policy_gradient_loss | -0.0432    |
|    value_loss           | 0.176      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 236      |
|    ep_rew_mean     | 25.3     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 115      |
|    time_elapsed    | 302      |
|    total_timesteps | 235520   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 237       |
|    ep_rew_mean          | 25.3      |
| time/                   |           |
|    fps                  | 779       |
|    iterations           | 116       |
|    time_elapsed         | 304       |
|    total_timesteps      | 237568    |
| train/                  |           |
|    approx_kl            | 0.2117515 |
|    clip_fraction        | 0.296     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.365    |
|    explained_variance   | 0.563     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0266   |
|    n_updates            | 1150      |
|    policy_gradient_loss | -0.0416   |
|    value_loss           | 0.168     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 240        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 781        |
|    iterations           | 117        |
|    time_elapsed         | 306        |
|    total_timesteps      | 239616     |
| train/                  |            |
|    approx_kl            | 0.21199195 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0629    |
|    n_updates            | 1160       |
|    policy_gradient_loss | -0.0453    |
|    value_loss           | 0.137      |
----------------------------------------
Eval num_timesteps=240000, episode_reward=23.40 +/- 5.85
Episode length: 211.80 +/- 56.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 212       |
|    mean_reward          | 23.4      |
| time/                   |           |
|    total_timesteps      | 240000    |
| train/                  |           |
|    approx_kl            | 0.2106235 |
|    clip_fraction        | 0.312     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.371    |
|    explained_variance   | 0.652     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.028    |
|    n_updates            | 1170      |
|    policy_gradient_loss | -0.0425   |
|    value_loss           | 0.138     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 241      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 779      |
|    iterations      | 118      |
|    time_elapsed    | 310      |
|    total_timesteps | 241664   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 244        |
|    ep_rew_mean          | 25.8       |
| time/                   |            |
|    fps                  | 781        |
|    iterations           | 119        |
|    time_elapsed         | 312        |
|    total_timesteps      | 243712     |
| train/                  |            |
|    approx_kl            | 0.22125588 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.377     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0386    |
|    n_updates            | 1180       |
|    policy_gradient_loss | -0.0487    |
|    value_loss           | 0.14       |
----------------------------------------
Eval num_timesteps=245000, episode_reward=27.50 +/- 7.46
Episode length: 261.00 +/- 86.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 261        |
|    mean_reward          | 27.5       |
| time/                   |            |
|    total_timesteps      | 245000     |
| train/                  |            |
|    approx_kl            | 0.19683152 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.37      |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0466    |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.042     |
|    value_loss           | 0.149      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 245      |
|    ep_rew_mean     | 25.9     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 120      |
|    time_elapsed    | 315      |
|    total_timesteps | 245760   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 244        |
|    ep_rew_mean          | 25.8       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 121        |
|    time_elapsed         | 317        |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.20944744 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.396     |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0252    |
|    n_updates            | 1200       |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.151      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 244        |
|    ep_rew_mean          | 25.8       |
| time/                   |            |
|    fps                  | 781        |
|    iterations           | 122        |
|    time_elapsed         | 319        |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.21211107 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.4       |
|    explained_variance   | 0.594      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0302    |
|    n_updates            | 1210       |
|    policy_gradient_loss | -0.0477    |
|    value_loss           | 0.138      |
----------------------------------------
Eval num_timesteps=250000, episode_reward=27.20 +/- 2.71
Episode length: 251.50 +/- 34.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 252        |
|    mean_reward          | 27.2       |
| time/                   |            |
|    total_timesteps      | 250000     |
| train/                  |            |
|    approx_kl            | 0.24592182 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0447    |
|    n_updates            | 1220       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.143      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 123      |
|    time_elapsed    | 323      |
|    total_timesteps | 251904   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 242        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 124        |
|    time_elapsed         | 325        |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.25675046 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.376     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0465    |
|    n_updates            | 1230       |
|    policy_gradient_loss | -0.0475    |
|    value_loss           | 0.148      |
----------------------------------------
Eval num_timesteps=255000, episode_reward=27.10 +/- 6.61
Episode length: 241.50 +/- 61.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 242        |
|    mean_reward          | 27.1       |
| time/                   |            |
|    total_timesteps      | 255000     |
| train/                  |            |
|    approx_kl            | 0.24397284 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0573    |
|    n_updates            | 1240       |
|    policy_gradient_loss | -0.0431    |
|    value_loss           | 0.135      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 240      |
|    ep_rew_mean     | 25.5     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 125      |
|    time_elapsed    | 329      |
|    total_timesteps | 256000   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 241        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 126        |
|    time_elapsed         | 331        |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.18219844 |
|    clip_fraction        | 0.3        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.343     |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0372    |
|    n_updates            | 1250       |
|    policy_gradient_loss | -0.0398    |
|    value_loss           | 0.152      |
----------------------------------------
Eval num_timesteps=260000, episode_reward=25.90 +/- 7.27
Episode length: 246.60 +/- 75.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 247       |
|    mean_reward          | 25.9      |
| time/                   |           |
|    total_timesteps      | 260000    |
| train/                  |           |
|    approx_kl            | 0.2706192 |
|    clip_fraction        | 0.31      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.346    |
|    explained_variance   | 0.626     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0347   |
|    n_updates            | 1260      |
|    policy_gradient_loss | -0.0415   |
|    value_loss           | 0.142     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 246      |
|    ep_rew_mean     | 25.9     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 127      |
|    time_elapsed    | 334      |
|    total_timesteps | 260096   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 248       |
|    ep_rew_mean          | 26.1      |
| time/                   |           |
|    fps                  | 778       |
|    iterations           | 128       |
|    time_elapsed         | 336       |
|    total_timesteps      | 262144    |
| train/                  |           |
|    approx_kl            | 0.1949847 |
|    clip_fraction        | 0.319     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.362    |
|    explained_variance   | 0.602     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0362   |
|    n_updates            | 1270      |
|    policy_gradient_loss | -0.043    |
|    value_loss           | 0.139     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 245        |
|    ep_rew_mean          | 25.8       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 129        |
|    time_elapsed         | 338        |
|    total_timesteps      | 264192     |
| train/                  |            |
|    approx_kl            | 0.24011496 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.358     |
|    explained_variance   | 0.611      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0448    |
|    n_updates            | 1280       |
|    policy_gradient_loss | -0.0438    |
|    value_loss           | 0.135      |
----------------------------------------
Eval num_timesteps=265000, episode_reward=26.40 +/- 5.04
Episode length: 239.90 +/- 47.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 240        |
|    mean_reward          | 26.4       |
| time/                   |            |
|    total_timesteps      | 265000     |
| train/                  |            |
|    approx_kl            | 0.19888695 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.357     |
|    explained_variance   | 0.681      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0306    |
|    n_updates            | 1290       |
|    policy_gradient_loss | -0.0371    |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 130      |
|    time_elapsed    | 342      |
|    total_timesteps | 266240   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 25.1       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 131        |
|    time_elapsed         | 344        |
|    total_timesteps      | 268288     |
| train/                  |            |
|    approx_kl            | 0.24107778 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.346     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0345    |
|    n_updates            | 1300       |
|    policy_gradient_loss | -0.0421    |
|    value_loss           | 0.135      |
----------------------------------------
Eval num_timesteps=270000, episode_reward=28.30 +/- 5.71
Episode length: 262.00 +/- 57.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 262        |
|    mean_reward          | 28.3       |
| time/                   |            |
|    total_timesteps      | 270000     |
| train/                  |            |
|    approx_kl            | 0.20393044 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.342     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0444    |
|    n_updates            | 1310       |
|    policy_gradient_loss | -0.0401    |
|    value_loss           | 0.145      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 24.9     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 132      |
|    time_elapsed    | 348      |
|    total_timesteps | 270336   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 233        |
|    ep_rew_mean          | 25         |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 133        |
|    time_elapsed         | 350        |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.24821791 |
|    clip_fraction        | 0.297      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.342     |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.048     |
|    n_updates            | 1320       |
|    policy_gradient_loss | -0.0421    |
|    value_loss           | 0.118      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 234       |
|    ep_rew_mean          | 25.2      |
| time/                   |           |
|    fps                  | 779       |
|    iterations           | 134       |
|    time_elapsed         | 352       |
|    total_timesteps      | 274432    |
| train/                  |           |
|    approx_kl            | 0.2528293 |
|    clip_fraction        | 0.302     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.33     |
|    explained_variance   | 0.681     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.049    |
|    n_updates            | 1330      |
|    policy_gradient_loss | -0.0444   |
|    value_loss           | 0.134     |
---------------------------------------
Eval num_timesteps=275000, episode_reward=26.50 +/- 4.67
Episode length: 249.00 +/- 56.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 249        |
|    mean_reward          | 26.5       |
| time/                   |            |
|    total_timesteps      | 275000     |
| train/                  |            |
|    approx_kl            | 0.24849239 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.34      |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0597    |
|    n_updates            | 1340       |
|    policy_gradient_loss | -0.0456    |
|    value_loss           | 0.145      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 25.1     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 135      |
|    time_elapsed    | 355      |
|    total_timesteps | 276480   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 25.2       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 136        |
|    time_elapsed         | 357        |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.20882004 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.356     |
|    explained_variance   | 0.631      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0224    |
|    n_updates            | 1350       |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.151      |
----------------------------------------
Eval num_timesteps=280000, episode_reward=27.00 +/- 4.43
Episode length: 242.10 +/- 37.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 242        |
|    mean_reward          | 27         |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.22926125 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.349     |
|    explained_variance   | 0.533      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0678    |
|    n_updates            | 1360       |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.127      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 25.5     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 137      |
|    time_elapsed    | 361      |
|    total_timesteps | 280576   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 237       |
|    ep_rew_mean          | 25.4      |
| time/                   |           |
|    fps                  | 777       |
|    iterations           | 138       |
|    time_elapsed         | 363       |
|    total_timesteps      | 282624    |
| train/                  |           |
|    approx_kl            | 0.2832938 |
|    clip_fraction        | 0.321     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.353    |
|    explained_variance   | 0.571     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0298   |
|    n_updates            | 1370      |
|    policy_gradient_loss | -0.0404   |
|    value_loss           | 0.138     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 25.1       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 139        |
|    time_elapsed         | 365        |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.22714074 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0568    |
|    n_updates            | 1380       |
|    policy_gradient_loss | -0.0465    |
|    value_loss           | 0.127      |
----------------------------------------
Eval num_timesteps=285000, episode_reward=24.50 +/- 4.27
Episode length: 223.90 +/- 51.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 224        |
|    mean_reward          | 24.5       |
| time/                   |            |
|    total_timesteps      | 285000     |
| train/                  |            |
|    approx_kl            | 0.22432037 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.346     |
|    explained_variance   | 0.645      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0431    |
|    n_updates            | 1390       |
|    policy_gradient_loss | -0.0367    |
|    value_loss           | 0.146      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 25.3     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 140      |
|    time_elapsed    | 368      |
|    total_timesteps | 286720   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 234       |
|    ep_rew_mean          | 25.2      |
| time/                   |           |
|    fps                  | 778       |
|    iterations           | 141       |
|    time_elapsed         | 370       |
|    total_timesteps      | 288768    |
| train/                  |           |
|    approx_kl            | 0.2108683 |
|    clip_fraction        | 0.314     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.343    |
|    explained_variance   | 0.531     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0396   |
|    n_updates            | 1400      |
|    policy_gradient_loss | -0.0411   |
|    value_loss           | 0.15      |
---------------------------------------
Eval num_timesteps=290000, episode_reward=23.70 +/- 5.85
Episode length: 204.20 +/- 55.32
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 204        |
|    mean_reward          | 23.7       |
| time/                   |            |
|    total_timesteps      | 290000     |
| train/                  |            |
|    approx_kl            | 0.23458362 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.327     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0386    |
|    n_updates            | 1410       |
|    policy_gradient_loss | -0.0415    |
|    value_loss           | 0.142      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 25.3     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 142      |
|    time_elapsed    | 374      |
|    total_timesteps | 290816   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 236        |
|    ep_rew_mean          | 25.6       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 143        |
|    time_elapsed         | 376        |
|    total_timesteps      | 292864     |
| train/                  |            |
|    approx_kl            | 0.27896678 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.33      |
|    explained_variance   | 0.6        |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0307    |
|    n_updates            | 1420       |
|    policy_gradient_loss | -0.0382    |
|    value_loss           | 0.132      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 25.6       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 144        |
|    time_elapsed         | 378        |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.23503786 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.326     |
|    explained_variance   | 0.506      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0365    |
|    n_updates            | 1430       |
|    policy_gradient_loss | -0.0409    |
|    value_loss           | 0.156      |
----------------------------------------
Eval num_timesteps=295000, episode_reward=25.00 +/- 4.15
Episode length: 233.90 +/- 50.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 234       |
|    mean_reward          | 25        |
| time/                   |           |
|    total_timesteps      | 295000    |
| train/                  |           |
|    approx_kl            | 0.2645377 |
|    clip_fraction        | 0.318     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.363    |
|    explained_variance   | 0.56      |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0333   |
|    n_updates            | 1440      |
|    policy_gradient_loss | -0.0413   |
|    value_loss           | 0.156     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 25.7     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 145      |
|    time_elapsed    | 381      |
|    total_timesteps | 296960   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 236       |
|    ep_rew_mean          | 25.7      |
| time/                   |           |
|    fps                  | 778       |
|    iterations           | 146       |
|    time_elapsed         | 383       |
|    total_timesteps      | 299008    |
| train/                  |           |
|    approx_kl            | 0.2292186 |
|    clip_fraction        | 0.29      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.327    |
|    explained_variance   | 0.59      |
|    learning_rate        | 0.00038   |
|    loss                 | -0.029    |
|    n_updates            | 1450      |
|    policy_gradient_loss | -0.0418   |
|    value_loss           | 0.152     |
---------------------------------------
Eval num_timesteps=300000, episode_reward=24.30 +/- 4.50
Episode length: 213.10 +/- 46.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 213        |
|    mean_reward          | 24.3       |
| time/                   |            |
|    total_timesteps      | 300000     |
| train/                  |            |
|    approx_kl            | 0.23154359 |
|    clip_fraction        | 0.292      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.328     |
|    explained_variance   | 0.572      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0372    |
|    n_updates            | 1460       |
|    policy_gradient_loss | -0.0397    |
|    value_loss           | 0.157      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 25.7     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 147      |
|    time_elapsed    | 387      |
|    total_timesteps | 301056   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 237        |
|    ep_rew_mean          | 26         |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 148        |
|    time_elapsed         | 389        |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.24338642 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.341     |
|    explained_variance   | 0.659      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0393    |
|    n_updates            | 1470       |
|    policy_gradient_loss | -0.0425    |
|    value_loss           | 0.14       |
----------------------------------------
Eval num_timesteps=305000, episode_reward=26.10 +/- 4.13
Episode length: 221.30 +/- 42.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 221       |
|    mean_reward          | 26.1      |
| time/                   |           |
|    total_timesteps      | 305000    |
| train/                  |           |
|    approx_kl            | 0.2476666 |
|    clip_fraction        | 0.292     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.338    |
|    explained_variance   | 0.52      |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0225   |
|    n_updates            | 1480      |
|    policy_gradient_loss | -0.0395   |
|    value_loss           | 0.147     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 26.1     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 149      |
|    time_elapsed    | 392      |
|    total_timesteps | 305152   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 235        |
|    ep_rew_mean          | 25.9       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 150        |
|    time_elapsed         | 394        |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.25863218 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.348     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.048     |
|    n_updates            | 1490       |
|    policy_gradient_loss | -0.0432    |
|    value_loss           | 0.133      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 233        |
|    ep_rew_mean          | 25.9       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 151        |
|    time_elapsed         | 396        |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.24213076 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.343     |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0356    |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.0437    |
|    value_loss           | 0.135      |
----------------------------------------
Eval num_timesteps=310000, episode_reward=25.40 +/- 6.90
Episode length: 240.90 +/- 71.06
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 241        |
|    mean_reward          | 25.4       |
| time/                   |            |
|    total_timesteps      | 310000     |
| train/                  |            |
|    approx_kl            | 0.23230404 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.33      |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0293    |
|    n_updates            | 1510       |
|    policy_gradient_loss | -0.0448    |
|    value_loss           | 0.151      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 231      |
|    ep_rew_mean     | 25.7     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 152      |
|    time_elapsed    | 400      |
|    total_timesteps | 311296   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 26         |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 153        |
|    time_elapsed         | 402        |
|    total_timesteps      | 313344     |
| train/                  |            |
|    approx_kl            | 0.24149236 |
|    clip_fraction        | 0.301      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.311     |
|    explained_variance   | 0.551      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0341    |
|    n_updates            | 1520       |
|    policy_gradient_loss | -0.0403    |
|    value_loss           | 0.157      |
----------------------------------------
Eval num_timesteps=315000, episode_reward=22.30 +/- 4.73
Episode length: 199.60 +/- 43.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 200        |
|    mean_reward          | 22.3       |
| time/                   |            |
|    total_timesteps      | 315000     |
| train/                  |            |
|    approx_kl            | 0.28201187 |
|    clip_fraction        | 0.293      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.326     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0382    |
|    n_updates            | 1530       |
|    policy_gradient_loss | -0.0422    |
|    value_loss           | 0.134      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 237      |
|    ep_rew_mean     | 26.2     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 154      |
|    time_elapsed    | 405      |
|    total_timesteps | 315392   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 238        |
|    ep_rew_mean          | 26.1       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 155        |
|    time_elapsed         | 407        |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.21046102 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.35      |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0472    |
|    n_updates            | 1540       |
|    policy_gradient_loss | -0.0418    |
|    value_loss           | 0.138      |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 238      |
|    ep_rew_mean          | 26       |
| time/                   |          |
|    fps                  | 779      |
|    iterations           | 156      |
|    time_elapsed         | 409      |
|    total_timesteps      | 319488   |
| train/                  |          |
|    approx_kl            | 0.286983 |
|    clip_fraction        | 0.318    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.36    |
|    explained_variance   | 0.635    |
|    learning_rate        | 0.00038  |
|    loss                 | -0.0415  |
|    n_updates            | 1550     |
|    policy_gradient_loss | -0.0424  |
|    value_loss           | 0.135    |
--------------------------------------
Eval num_timesteps=320000, episode_reward=24.60 +/- 4.74
Episode length: 215.10 +/- 38.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 215       |
|    mean_reward          | 24.6      |
| time/                   |           |
|    total_timesteps      | 320000    |
| train/                  |           |
|    approx_kl            | 0.1915016 |
|    clip_fraction        | 0.309     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.349    |
|    explained_variance   | 0.611     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0477   |
|    n_updates            | 1560      |
|    policy_gradient_loss | -0.0459   |
|    value_loss           | 0.13      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 26.1     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 157      |
|    time_elapsed    | 413      |
|    total_timesteps | 321536   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 237        |
|    ep_rew_mean          | 25.9       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 158        |
|    time_elapsed         | 415        |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.22584295 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.647      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0361    |
|    n_updates            | 1570       |
|    policy_gradient_loss | -0.0457    |
|    value_loss           | 0.134      |
----------------------------------------
Eval num_timesteps=325000, episode_reward=20.40 +/- 7.83
Episode length: 191.40 +/- 76.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 191       |
|    mean_reward          | 20.4      |
| time/                   |           |
|    total_timesteps      | 325000    |
| train/                  |           |
|    approx_kl            | 0.2635712 |
|    clip_fraction        | 0.315     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.338    |
|    explained_variance   | 0.596     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0405   |
|    n_updates            | 1580      |
|    policy_gradient_loss | -0.0422   |
|    value_loss           | 0.148     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 230      |
|    ep_rew_mean     | 25.1     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 159      |
|    time_elapsed    | 418      |
|    total_timesteps | 325632   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 227       |
|    ep_rew_mean          | 24.7      |
| time/                   |           |
|    fps                  | 778       |
|    iterations           | 160       |
|    time_elapsed         | 420       |
|    total_timesteps      | 327680    |
| train/                  |           |
|    approx_kl            | 0.2697939 |
|    clip_fraction        | 0.306     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.344    |
|    explained_variance   | 0.6       |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0654   |
|    n_updates            | 1590      |
|    policy_gradient_loss | -0.0421   |
|    value_loss           | 0.153     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 230        |
|    ep_rew_mean          | 24.9       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 161        |
|    time_elapsed         | 422        |
|    total_timesteps      | 329728     |
| train/                  |            |
|    approx_kl            | 0.25930473 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.371     |
|    explained_variance   | 0.585      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0528    |
|    n_updates            | 1600       |
|    policy_gradient_loss | -0.0478    |
|    value_loss           | 0.145      |
----------------------------------------
Eval num_timesteps=330000, episode_reward=25.00 +/- 5.55
Episode length: 225.20 +/- 45.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 225       |
|    mean_reward          | 25        |
| time/                   |           |
|    total_timesteps      | 330000    |
| train/                  |           |
|    approx_kl            | 0.2182289 |
|    clip_fraction        | 0.311     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.354    |
|    explained_variance   | 0.608     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0402   |
|    n_updates            | 1610      |
|    policy_gradient_loss | -0.0449   |
|    value_loss           | 0.141     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 230      |
|    ep_rew_mean     | 24.9     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 162      |
|    time_elapsed    | 426      |
|    total_timesteps | 331776   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 25         |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 163        |
|    time_elapsed         | 428        |
|    total_timesteps      | 333824     |
| train/                  |            |
|    approx_kl            | 0.33718434 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.333     |
|    explained_variance   | 0.572      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0303    |
|    n_updates            | 1620       |
|    policy_gradient_loss | -0.0438    |
|    value_loss           | 0.139      |
----------------------------------------
Eval num_timesteps=335000, episode_reward=25.10 +/- 5.52
Episode length: 223.00 +/- 54.09
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 223        |
|    mean_reward          | 25.1       |
| time/                   |            |
|    total_timesteps      | 335000     |
| train/                  |            |
|    approx_kl            | 0.23494433 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.353     |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0396    |
|    n_updates            | 1630       |
|    policy_gradient_loss | -0.0422    |
|    value_loss           | 0.144      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 25.3     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 164      |
|    time_elapsed    | 431      |
|    total_timesteps | 335872   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 229        |
|    ep_rew_mean          | 24.8       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 165        |
|    time_elapsed         | 433        |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.20916243 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.354     |
|    explained_variance   | 0.627      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0434    |
|    n_updates            | 1640       |
|    policy_gradient_loss | -0.0442    |
|    value_loss           | 0.128      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 227        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 780        |
|    iterations           | 166        |
|    time_elapsed         | 435        |
|    total_timesteps      | 339968     |
| train/                  |            |
|    approx_kl            | 0.20439088 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.351     |
|    explained_variance   | 0.56       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0455    |
|    n_updates            | 1650       |
|    policy_gradient_loss | -0.0398    |
|    value_loss           | 0.169      |
----------------------------------------
Eval num_timesteps=340000, episode_reward=22.30 +/- 6.66
Episode length: 202.00 +/- 57.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 202        |
|    mean_reward          | 22.3       |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.24129707 |
|    clip_fraction        | 0.295      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.345     |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0485    |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.0443    |
|    value_loss           | 0.136      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 25       |
| time/              |          |
|    fps             | 778      |
|    iterations      | 167      |
|    time_elapsed    | 439      |
|    total_timesteps | 342016   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 230        |
|    ep_rew_mean          | 25.1       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 168        |
|    time_elapsed         | 441        |
|    total_timesteps      | 344064     |
| train/                  |            |
|    approx_kl            | 0.19305839 |
|    clip_fraction        | 0.288      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.34      |
|    explained_variance   | 0.57       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0494    |
|    n_updates            | 1670       |
|    policy_gradient_loss | -0.0414    |
|    value_loss           | 0.139      |
----------------------------------------
Eval num_timesteps=345000, episode_reward=25.10 +/- 3.67
Episode length: 230.30 +/- 36.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 230        |
|    mean_reward          | 25.1       |
| time/                   |            |
|    total_timesteps      | 345000     |
| train/                  |            |
|    approx_kl            | 0.28752485 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.355     |
|    explained_variance   | 0.613      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0359    |
|    n_updates            | 1680       |
|    policy_gradient_loss | -0.0472    |
|    value_loss           | 0.128      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 25.2     |
| time/              |          |
|    fps             | 778      |
|    iterations      | 169      |
|    time_elapsed    | 444      |
|    total_timesteps | 346112   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 236        |
|    ep_rew_mean          | 25.6       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 170        |
|    time_elapsed         | 446        |
|    total_timesteps      | 348160     |
| train/                  |            |
|    approx_kl            | 0.18971659 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.369     |
|    explained_variance   | 0.49       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0496    |
|    n_updates            | 1690       |
|    policy_gradient_loss | -0.0432    |
|    value_loss           | 0.133      |
----------------------------------------
Eval num_timesteps=350000, episode_reward=28.00 +/- 6.05
Episode length: 269.90 +/- 60.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 270        |
|    mean_reward          | 28         |
| time/                   |            |
|    total_timesteps      | 350000     |
| train/                  |            |
|    approx_kl            | 0.20874262 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.39      |
|    explained_variance   | 0.558      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0398    |
|    n_updates            | 1700       |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.149      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 237      |
|    ep_rew_mean     | 25.8     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 171      |
|    time_elapsed    | 450      |
|    total_timesteps | 350208   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 238        |
|    ep_rew_mean          | 26         |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 172        |
|    time_elapsed         | 452        |
|    total_timesteps      | 352256     |
| train/                  |            |
|    approx_kl            | 0.21703771 |
|    clip_fraction        | 0.338      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.403     |
|    explained_variance   | 0.508      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0385    |
|    n_updates            | 1710       |
|    policy_gradient_loss | -0.0463    |
|    value_loss           | 0.151      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 240       |
|    ep_rew_mean          | 26.1      |
| time/                   |           |
|    fps                  | 779       |
|    iterations           | 173       |
|    time_elapsed         | 454       |
|    total_timesteps      | 354304    |
| train/                  |           |
|    approx_kl            | 0.2137123 |
|    clip_fraction        | 0.33      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.41     |
|    explained_variance   | 0.58      |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0458   |
|    n_updates            | 1720      |
|    policy_gradient_loss | -0.0512   |
|    value_loss           | 0.137     |
---------------------------------------
Eval num_timesteps=355000, episode_reward=26.00 +/- 5.78
Episode length: 243.10 +/- 42.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 243        |
|    mean_reward          | 26         |
| time/                   |            |
|    total_timesteps      | 355000     |
| train/                  |            |
|    approx_kl            | 0.24436167 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.422     |
|    explained_variance   | 0.592      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0569    |
|    n_updates            | 1730       |
|    policy_gradient_loss | -0.0523    |
|    value_loss           | 0.135      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 174      |
|    time_elapsed    | 458      |
|    total_timesteps | 356352   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 240        |
|    ep_rew_mean          | 25.6       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 175        |
|    time_elapsed         | 460        |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.21354622 |
|    clip_fraction        | 0.344      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.423     |
|    explained_variance   | 0.565      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0671    |
|    n_updates            | 1740       |
|    policy_gradient_loss | -0.0556    |
|    value_loss           | 0.132      |
----------------------------------------
Eval num_timesteps=360000, episode_reward=24.70 +/- 6.96
Episode length: 238.50 +/- 78.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 238        |
|    mean_reward          | 24.7       |
| time/                   |            |
|    total_timesteps      | 360000     |
| train/                  |            |
|    approx_kl            | 0.20649193 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.421     |
|    explained_variance   | 0.56       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.066     |
|    n_updates            | 1750       |
|    policy_gradient_loss | -0.053     |
|    value_loss           | 0.137      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 240      |
|    ep_rew_mean     | 25.4     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 176      |
|    time_elapsed    | 463      |
|    total_timesteps | 360448   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 242        |
|    ep_rew_mean          | 25.4       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 177        |
|    time_elapsed         | 465        |
|    total_timesteps      | 362496     |
| train/                  |            |
|    approx_kl            | 0.21883139 |
|    clip_fraction        | 0.353      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.44      |
|    explained_variance   | 0.507      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.067     |
|    n_updates            | 1760       |
|    policy_gradient_loss | -0.0545    |
|    value_loss           | 0.125      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 243        |
|    ep_rew_mean          | 25.4       |
| time/                   |            |
|    fps                  | 779        |
|    iterations           | 178        |
|    time_elapsed         | 467        |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.20400882 |
|    clip_fraction        | 0.357      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.435     |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0585    |
|    n_updates            | 1770       |
|    policy_gradient_loss | -0.0502    |
|    value_loss           | 0.124      |
----------------------------------------
Eval num_timesteps=365000, episode_reward=27.90 +/- 6.99
Episode length: 271.80 +/- 81.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 272       |
|    mean_reward          | 27.9      |
| time/                   |           |
|    total_timesteps      | 365000    |
| train/                  |           |
|    approx_kl            | 0.2564001 |
|    clip_fraction        | 0.349     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.404    |
|    explained_variance   | 0.557     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0497   |
|    n_updates            | 1780      |
|    policy_gradient_loss | -0.0493   |
|    value_loss           | 0.141     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 242      |
|    ep_rew_mean     | 25.1     |
| time/              |          |
|    fps             | 777      |
|    iterations      | 179      |
|    time_elapsed    | 471      |
|    total_timesteps | 366592   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 237        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 180        |
|    time_elapsed         | 473        |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.20683402 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.416     |
|    explained_variance   | 0.561      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0428    |
|    n_updates            | 1790       |
|    policy_gradient_loss | -0.0494    |
|    value_loss           | 0.13       |
----------------------------------------
Eval num_timesteps=370000, episode_reward=26.90 +/- 6.33
Episode length: 250.50 +/- 76.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 250        |
|    mean_reward          | 26.9       |
| time/                   |            |
|    total_timesteps      | 370000     |
| train/                  |            |
|    approx_kl            | 0.18917641 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.413     |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.046     |
|    n_updates            | 1800       |
|    policy_gradient_loss | -0.0476    |
|    value_loss           | 0.14       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 24.4     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 181      |
|    time_elapsed    | 477      |
|    total_timesteps | 370688   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 182        |
|    time_elapsed         | 479        |
|    total_timesteps      | 372736     |
| train/                  |            |
|    approx_kl            | 0.19423571 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.405     |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0511    |
|    n_updates            | 1810       |
|    policy_gradient_loss | -0.0525    |
|    value_loss           | 0.124      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 778        |
|    iterations           | 183        |
|    time_elapsed         | 481        |
|    total_timesteps      | 374784     |
| train/                  |            |
|    approx_kl            | 0.20402771 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.39      |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0591    |
|    n_updates            | 1820       |
|    policy_gradient_loss | -0.0507    |
|    value_loss           | 0.132      |
----------------------------------------
Eval num_timesteps=375000, episode_reward=24.10 +/- 5.03
Episode length: 234.00 +/- 54.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 234        |
|    mean_reward          | 24.1       |
| time/                   |            |
|    total_timesteps      | 375000     |
| train/                  |            |
|    approx_kl            | 0.21368018 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.388     |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0563    |
|    n_updates            | 1830       |
|    policy_gradient_loss | -0.0456    |
|    value_loss           | 0.13       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 23.9     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 184      |
|    time_elapsed    | 484      |
|    total_timesteps | 376832   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 24.4       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 185        |
|    time_elapsed         | 487        |
|    total_timesteps      | 378880     |
| train/                  |            |
|    approx_kl            | 0.23989448 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0528    |
|    n_updates            | 1840       |
|    policy_gradient_loss | -0.0473    |
|    value_loss           | 0.137      |
----------------------------------------
Eval num_timesteps=380000, episode_reward=25.70 +/- 7.43
Episode length: 252.70 +/- 74.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 253        |
|    mean_reward          | 25.7       |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.21892522 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | 0.556      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0627    |
|    n_updates            | 1850       |
|    policy_gradient_loss | -0.0439    |
|    value_loss           | 0.134      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 24.6     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 186      |
|    time_elapsed    | 490      |
|    total_timesteps | 380928   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 230        |
|    ep_rew_mean          | 24.5       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 187        |
|    time_elapsed         | 492        |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.22248507 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.366     |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0491    |
|    n_updates            | 1860       |
|    policy_gradient_loss | -0.052     |
|    value_loss           | 0.129      |
----------------------------------------
Eval num_timesteps=385000, episode_reward=25.20 +/- 7.05
Episode length: 231.50 +/- 78.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 232        |
|    mean_reward          | 25.2       |
| time/                   |            |
|    total_timesteps      | 385000     |
| train/                  |            |
|    approx_kl            | 0.32821804 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.379     |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0734    |
|    n_updates            | 1870       |
|    policy_gradient_loss | -0.0477    |
|    value_loss           | 0.123      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 25       |
| time/              |          |
|    fps             | 775      |
|    iterations      | 188      |
|    time_elapsed    | 496      |
|    total_timesteps | 385024   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 233       |
|    ep_rew_mean          | 25        |
| time/                   |           |
|    fps                  | 776       |
|    iterations           | 189       |
|    time_elapsed         | 498       |
|    total_timesteps      | 387072    |
| train/                  |           |
|    approx_kl            | 0.3322711 |
|    clip_fraction        | 0.32      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.368    |
|    explained_variance   | 0.59      |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0669   |
|    n_updates            | 1880      |
|    policy_gradient_loss | -0.0538   |
|    value_loss           | 0.119     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 231        |
|    ep_rew_mean          | 24.8       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 190        |
|    time_elapsed         | 500        |
|    total_timesteps      | 389120     |
| train/                  |            |
|    approx_kl            | 0.32381478 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.341     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0503    |
|    n_updates            | 1890       |
|    policy_gradient_loss | -0.0491    |
|    value_loss           | 0.126      |
----------------------------------------
Eval num_timesteps=390000, episode_reward=27.60 +/- 4.25
Episode length: 260.60 +/- 37.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 261       |
|    mean_reward          | 27.6      |
| time/                   |           |
|    total_timesteps      | 390000    |
| train/                  |           |
|    approx_kl            | 0.2447076 |
|    clip_fraction        | 0.329     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.363    |
|    explained_variance   | 0.637     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.05     |
|    n_updates            | 1900      |
|    policy_gradient_loss | -0.0495   |
|    value_loss           | 0.148     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 24.9     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 191      |
|    time_elapsed    | 504      |
|    total_timesteps | 391168   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 25         |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 192        |
|    time_elapsed         | 506        |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.22443438 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.631      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0378    |
|    n_updates            | 1910       |
|    policy_gradient_loss | -0.0463    |
|    value_loss           | 0.138      |
----------------------------------------
Eval num_timesteps=395000, episode_reward=27.00 +/- 4.84
Episode length: 255.80 +/- 65.28
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 256        |
|    mean_reward          | 27         |
| time/                   |            |
|    total_timesteps      | 395000     |
| train/                  |            |
|    approx_kl            | 0.21820843 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.356     |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0652    |
|    n_updates            | 1920       |
|    policy_gradient_loss | -0.0424    |
|    value_loss           | 0.13       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 25.3     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 193      |
|    time_elapsed    | 509      |
|    total_timesteps | 395264   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 239        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 194        |
|    time_elapsed         | 511        |
|    total_timesteps      | 397312     |
| train/                  |            |
|    approx_kl            | 0.24298038 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0484    |
|    n_updates            | 1930       |
|    policy_gradient_loss | -0.05      |
|    value_loss           | 0.137      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 243        |
|    ep_rew_mean          | 25.8       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 195        |
|    time_elapsed         | 513        |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.20456332 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.379     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.043     |
|    n_updates            | 1940       |
|    policy_gradient_loss | -0.048     |
|    value_loss           | 0.131      |
----------------------------------------
Eval num_timesteps=400000, episode_reward=24.90 +/- 5.70
Episode length: 226.20 +/- 56.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 226        |
|    mean_reward          | 24.9       |
| time/                   |            |
|    total_timesteps      | 400000     |
| train/                  |            |
|    approx_kl            | 0.25556374 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.392     |
|    explained_variance   | 0.53       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0236    |
|    n_updates            | 1950       |
|    policy_gradient_loss | -0.0504    |
|    value_loss           | 0.135      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 245      |
|    ep_rew_mean     | 26       |
| time/              |          |
|    fps             | 775      |
|    iterations      | 196      |
|    time_elapsed    | 517      |
|    total_timesteps | 401408   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 240        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 197        |
|    time_elapsed         | 519        |
|    total_timesteps      | 403456     |
| train/                  |            |
|    approx_kl            | 0.25258976 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.396     |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.055     |
|    n_updates            | 1960       |
|    policy_gradient_loss | -0.0498    |
|    value_loss           | 0.128      |
----------------------------------------
Eval num_timesteps=405000, episode_reward=24.00 +/- 5.90
Episode length: 218.60 +/- 66.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 219        |
|    mean_reward          | 24         |
| time/                   |            |
|    total_timesteps      | 405000     |
| train/                  |            |
|    approx_kl            | 0.21280178 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.393     |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0467    |
|    n_updates            | 1970       |
|    policy_gradient_loss | -0.0494    |
|    value_loss           | 0.144      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 240      |
|    ep_rew_mean     | 25.5     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 198      |
|    time_elapsed    | 522      |
|    total_timesteps | 405504   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 242       |
|    ep_rew_mean          | 25.5      |
| time/                   |           |
|    fps                  | 776       |
|    iterations           | 199       |
|    time_elapsed         | 524       |
|    total_timesteps      | 407552    |
| train/                  |           |
|    approx_kl            | 0.2898163 |
|    clip_fraction        | 0.338     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.393    |
|    explained_variance   | 0.566     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0434   |
|    n_updates            | 1980      |
|    policy_gradient_loss | -0.0482   |
|    value_loss           | 0.141     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 240       |
|    ep_rew_mean          | 25.2      |
| time/                   |           |
|    fps                  | 777       |
|    iterations           | 200       |
|    time_elapsed         | 526       |
|    total_timesteps      | 409600    |
| train/                  |           |
|    approx_kl            | 0.2624839 |
|    clip_fraction        | 0.352     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.396    |
|    explained_variance   | 0.607     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.053    |
|    n_updates            | 1990      |
|    policy_gradient_loss | -0.0505   |
|    value_loss           | 0.13      |
---------------------------------------
Eval num_timesteps=410000, episode_reward=21.20 +/- 4.77
Episode length: 194.00 +/- 39.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 194       |
|    mean_reward          | 21.2      |
| time/                   |           |
|    total_timesteps      | 410000    |
| train/                  |           |
|    approx_kl            | 0.2874655 |
|    clip_fraction        | 0.341     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.401    |
|    explained_variance   | 0.62      |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0528   |
|    n_updates            | 2000      |
|    policy_gradient_loss | -0.0514   |
|    value_loss           | 0.125     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 239      |
|    ep_rew_mean     | 25.1     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 201      |
|    time_elapsed    | 530      |
|    total_timesteps | 411648   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 241        |
|    ep_rew_mean          | 25.5       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 202        |
|    time_elapsed         | 532        |
|    total_timesteps      | 413696     |
| train/                  |            |
|    approx_kl            | 0.27437526 |
|    clip_fraction        | 0.327      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.367     |
|    explained_variance   | 0.641      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0518    |
|    n_updates            | 2010       |
|    policy_gradient_loss | -0.049     |
|    value_loss           | 0.149      |
----------------------------------------
Eval num_timesteps=415000, episode_reward=27.70 +/- 1.95
Episode length: 248.30 +/- 18.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 248        |
|    mean_reward          | 27.7       |
| time/                   |            |
|    total_timesteps      | 415000     |
| train/                  |            |
|    approx_kl            | 0.29559237 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.37      |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0141    |
|    n_updates            | 2020       |
|    policy_gradient_loss | -0.0493    |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 25       |
| time/              |          |
|    fps             | 775      |
|    iterations      | 203      |
|    time_elapsed    | 535      |
|    total_timesteps | 415744   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 24.5       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 204        |
|    time_elapsed         | 537        |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.25162458 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.357     |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0493    |
|    n_updates            | 2030       |
|    policy_gradient_loss | -0.0482    |
|    value_loss           | 0.149      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 234       |
|    ep_rew_mean          | 24.6      |
| time/                   |           |
|    fps                  | 777       |
|    iterations           | 205       |
|    time_elapsed         | 539       |
|    total_timesteps      | 419840    |
| train/                  |           |
|    approx_kl            | 0.2318626 |
|    clip_fraction        | 0.307     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.358    |
|    explained_variance   | 0.619     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0354   |
|    n_updates            | 2040      |
|    policy_gradient_loss | -0.0429   |
|    value_loss           | 0.144     |
---------------------------------------
Eval num_timesteps=420000, episode_reward=23.50 +/- 7.05
Episode length: 232.00 +/- 84.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 232        |
|    mean_reward          | 23.5       |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.18952116 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.365     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0514    |
|    n_updates            | 2050       |
|    policy_gradient_loss | -0.0462    |
|    value_loss           | 0.131      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 24.4     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 206      |
|    time_elapsed    | 543      |
|    total_timesteps | 421888   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 229       |
|    ep_rew_mean          | 24.1      |
| time/                   |           |
|    fps                  | 777       |
|    iterations           | 207       |
|    time_elapsed         | 545       |
|    total_timesteps      | 423936    |
| train/                  |           |
|    approx_kl            | 0.2617926 |
|    clip_fraction        | 0.33      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.366    |
|    explained_variance   | 0.619     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0493   |
|    n_updates            | 2060      |
|    policy_gradient_loss | -0.0451   |
|    value_loss           | 0.142     |
---------------------------------------
Eval num_timesteps=425000, episode_reward=25.70 +/- 5.33
Episode length: 225.30 +/- 48.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 225        |
|    mean_reward          | 25.7       |
| time/                   |            |
|    total_timesteps      | 425000     |
| train/                  |            |
|    approx_kl            | 0.23119025 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.376     |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0426    |
|    n_updates            | 2070       |
|    policy_gradient_loss | -0.0453    |
|    value_loss           | 0.131      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 24.5     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 208      |
|    time_elapsed    | 549      |
|    total_timesteps | 425984   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 231       |
|    ep_rew_mean          | 24.4      |
| time/                   |           |
|    fps                  | 776       |
|    iterations           | 209       |
|    time_elapsed         | 551       |
|    total_timesteps      | 428032    |
| train/                  |           |
|    approx_kl            | 0.2651734 |
|    clip_fraction        | 0.334     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.364    |
|    explained_variance   | 0.605     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0576   |
|    n_updates            | 2080      |
|    policy_gradient_loss | -0.0527   |
|    value_loss           | 0.132     |
---------------------------------------
Eval num_timesteps=430000, episode_reward=25.20 +/- 5.67
Episode length: 234.50 +/- 56.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 234       |
|    mean_reward          | 25.2      |
| time/                   |           |
|    total_timesteps      | 430000    |
| train/                  |           |
|    approx_kl            | 0.3157323 |
|    clip_fraction        | 0.333     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.373    |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0519   |
|    n_updates            | 2090      |
|    policy_gradient_loss | -0.0511   |
|    value_loss           | 0.148     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 230      |
|    ep_rew_mean     | 24.3     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 210      |
|    time_elapsed    | 554      |
|    total_timesteps | 430080   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 228       |
|    ep_rew_mean          | 24.2      |
| time/                   |           |
|    fps                  | 776       |
|    iterations           | 211       |
|    time_elapsed         | 556       |
|    total_timesteps      | 432128    |
| train/                  |           |
|    approx_kl            | 0.2704756 |
|    clip_fraction        | 0.326     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.365    |
|    explained_variance   | 0.607     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0357   |
|    n_updates            | 2100      |
|    policy_gradient_loss | -0.0475   |
|    value_loss           | 0.143     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 233        |
|    ep_rew_mean          | 24.8       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 212        |
|    time_elapsed         | 558        |
|    total_timesteps      | 434176     |
| train/                  |            |
|    approx_kl            | 0.28126928 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.346     |
|    explained_variance   | 0.571      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0332    |
|    n_updates            | 2110       |
|    policy_gradient_loss | -0.0445    |
|    value_loss           | 0.138      |
----------------------------------------
Eval num_timesteps=435000, episode_reward=24.80 +/- 3.60
Episode length: 231.40 +/- 38.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 231        |
|    mean_reward          | 24.8       |
| time/                   |            |
|    total_timesteps      | 435000     |
| train/                  |            |
|    approx_kl            | 0.30119196 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.352     |
|    explained_variance   | 0.536      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0603    |
|    n_updates            | 2120       |
|    policy_gradient_loss | -0.0467    |
|    value_loss           | 0.137      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 24.7     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 213      |
|    time_elapsed    | 562      |
|    total_timesteps | 436224   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 214        |
|    time_elapsed         | 564        |
|    total_timesteps      | 438272     |
| train/                  |            |
|    approx_kl            | 0.32165858 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.365     |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.044     |
|    n_updates            | 2130       |
|    policy_gradient_loss | -0.051     |
|    value_loss           | 0.15       |
----------------------------------------
Eval num_timesteps=440000, episode_reward=24.40 +/- 7.30
Episode length: 216.20 +/- 61.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 24.4       |
| time/                   |            |
|    total_timesteps      | 440000     |
| train/                  |            |
|    approx_kl            | 0.25140858 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.353     |
|    explained_variance   | 0.592      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0482    |
|    n_updates            | 2140       |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.145      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 24.9     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 215      |
|    time_elapsed    | 567      |
|    total_timesteps | 440320   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 233        |
|    ep_rew_mean          | 24.8       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 216        |
|    time_elapsed         | 569        |
|    total_timesteps      | 442368     |
| train/                  |            |
|    approx_kl            | 0.24534479 |
|    clip_fraction        | 0.331      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.382     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0488    |
|    n_updates            | 2150       |
|    policy_gradient_loss | -0.0521    |
|    value_loss           | 0.16       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 233        |
|    ep_rew_mean          | 24.6       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 217        |
|    time_elapsed         | 571        |
|    total_timesteps      | 444416     |
| train/                  |            |
|    approx_kl            | 0.22405624 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.367     |
|    explained_variance   | 0.593      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0309    |
|    n_updates            | 2160       |
|    policy_gradient_loss | -0.0484    |
|    value_loss           | 0.171      |
----------------------------------------
Eval num_timesteps=445000, episode_reward=25.50 +/- 4.88
Episode length: 242.90 +/- 55.76
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 243        |
|    mean_reward          | 25.5       |
| time/                   |            |
|    total_timesteps      | 445000     |
| train/                  |            |
|    approx_kl            | 0.23037532 |
|    clip_fraction        | 0.311      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.357     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0385    |
|    n_updates            | 2170       |
|    policy_gradient_loss | -0.0452    |
|    value_loss           | 0.145      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 24.8     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 218      |
|    time_elapsed    | 575      |
|    total_timesteps | 446464   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 233        |
|    ep_rew_mean          | 24.7       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 219        |
|    time_elapsed         | 577        |
|    total_timesteps      | 448512     |
| train/                  |            |
|    approx_kl            | 0.23291391 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.376     |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0166    |
|    n_updates            | 2180       |
|    policy_gradient_loss | -0.0481    |
|    value_loss           | 0.161      |
----------------------------------------
Eval num_timesteps=450000, episode_reward=26.70 +/- 5.14
Episode length: 248.10 +/- 55.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 248        |
|    mean_reward          | 26.7       |
| time/                   |            |
|    total_timesteps      | 450000     |
| train/                  |            |
|    approx_kl            | 0.25506127 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.343     |
|    explained_variance   | 0.609      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.038     |
|    n_updates            | 2190       |
|    policy_gradient_loss | -0.0438    |
|    value_loss           | 0.158      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 24.7     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 220      |
|    time_elapsed    | 581      |
|    total_timesteps | 450560   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 230        |
|    ep_rew_mean          | 24.5       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 221        |
|    time_elapsed         | 583        |
|    total_timesteps      | 452608     |
| train/                  |            |
|    approx_kl            | 0.23145631 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0154    |
|    n_updates            | 2200       |
|    policy_gradient_loss | -0.0472    |
|    value_loss           | 0.153      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 229        |
|    ep_rew_mean          | 24.3       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 222        |
|    time_elapsed         | 585        |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.21633466 |
|    clip_fraction        | 0.309      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.336     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0392    |
|    n_updates            | 2210       |
|    policy_gradient_loss | -0.0443    |
|    value_loss           | 0.144      |
----------------------------------------
Eval num_timesteps=455000, episode_reward=24.00 +/- 4.65
Episode length: 219.30 +/- 51.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 219        |
|    mean_reward          | 24         |
| time/                   |            |
|    total_timesteps      | 455000     |
| train/                  |            |
|    approx_kl            | 0.25822434 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.337     |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0342    |
|    n_updates            | 2220       |
|    policy_gradient_loss | -0.0425    |
|    value_loss           | 0.15       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 224      |
|    ep_rew_mean     | 23.8     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 223      |
|    time_elapsed    | 588      |
|    total_timesteps | 456704   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 224        |
|    ep_rew_mean          | 23.7       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 224        |
|    time_elapsed         | 590        |
|    total_timesteps      | 458752     |
| train/                  |            |
|    approx_kl            | 0.24725768 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.333     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0377    |
|    n_updates            | 2230       |
|    policy_gradient_loss | -0.0427    |
|    value_loss           | 0.158      |
----------------------------------------
Eval num_timesteps=460000, episode_reward=22.60 +/- 4.72
Episode length: 220.30 +/- 52.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 220        |
|    mean_reward          | 22.6       |
| time/                   |            |
|    total_timesteps      | 460000     |
| train/                  |            |
|    approx_kl            | 0.25136107 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.329     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0273    |
|    n_updates            | 2240       |
|    policy_gradient_loss | -0.0434    |
|    value_loss           | 0.142      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 221      |
|    ep_rew_mean     | 23.5     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 225      |
|    time_elapsed    | 594      |
|    total_timesteps | 460800   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 220       |
|    ep_rew_mean          | 23.4      |
| time/                   |           |
|    fps                  | 776       |
|    iterations           | 226       |
|    time_elapsed         | 596       |
|    total_timesteps      | 462848    |
| train/                  |           |
|    approx_kl            | 0.3453341 |
|    clip_fraction        | 0.318     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.338    |
|    explained_variance   | 0.597     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.048    |
|    n_updates            | 2250      |
|    policy_gradient_loss | -0.0456   |
|    value_loss           | 0.153     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 218        |
|    ep_rew_mean          | 23.3       |
| time/                   |            |
|    fps                  | 777        |
|    iterations           | 227        |
|    time_elapsed         | 598        |
|    total_timesteps      | 464896     |
| train/                  |            |
|    approx_kl            | 0.25382245 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.338     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0313    |
|    n_updates            | 2260       |
|    policy_gradient_loss | -0.0462    |
|    value_loss           | 0.145      |
----------------------------------------
Eval num_timesteps=465000, episode_reward=22.80 +/- 3.82
Episode length: 209.10 +/- 29.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 209        |
|    mean_reward          | 22.8       |
| time/                   |            |
|    total_timesteps      | 465000     |
| train/                  |            |
|    approx_kl            | 0.25678822 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.336     |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0289    |
|    n_updates            | 2270       |
|    policy_gradient_loss | -0.0475    |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 218      |
|    ep_rew_mean     | 23.3     |
| time/              |          |
|    fps             | 776      |
|    iterations      | 228      |
|    time_elapsed    | 601      |
|    total_timesteps | 466944   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 215        |
|    ep_rew_mean          | 23.1       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 229        |
|    time_elapsed         | 603        |
|    total_timesteps      | 468992     |
| train/                  |            |
|    approx_kl            | 0.25310642 |
|    clip_fraction        | 0.306      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.334     |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0268    |
|    n_updates            | 2280       |
|    policy_gradient_loss | -0.0441    |
|    value_loss           | 0.15       |
----------------------------------------
Eval num_timesteps=470000, episode_reward=23.80 +/- 6.40
Episode length: 229.30 +/- 67.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 229        |
|    mean_reward          | 23.8       |
| time/                   |            |
|    total_timesteps      | 470000     |
| train/                  |            |
|    approx_kl            | 0.24509759 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.032     |
|    n_updates            | 2290       |
|    policy_gradient_loss | -0.0459    |
|    value_loss           | 0.135      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 23       |
| time/              |          |
|    fps             | 775      |
|    iterations      | 230      |
|    time_elapsed    | 607      |
|    total_timesteps | 471040   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 214        |
|    ep_rew_mean          | 23.1       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 231        |
|    time_elapsed         | 609        |
|    total_timesteps      | 473088     |
| train/                  |            |
|    approx_kl            | 0.27848583 |
|    clip_fraction        | 0.304      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.329     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0365    |
|    n_updates            | 2300       |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.164      |
----------------------------------------
Eval num_timesteps=475000, episode_reward=25.80 +/- 5.10
Episode length: 224.10 +/- 38.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 224        |
|    mean_reward          | 25.8       |
| time/                   |            |
|    total_timesteps      | 475000     |
| train/                  |            |
|    approx_kl            | 0.23611681 |
|    clip_fraction        | 0.299      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.323     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0312    |
|    n_updates            | 2310       |
|    policy_gradient_loss | -0.0407    |
|    value_loss           | 0.163      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 211      |
|    ep_rew_mean     | 23       |
| time/              |          |
|    fps             | 775      |
|    iterations      | 232      |
|    time_elapsed    | 612      |
|    total_timesteps | 475136   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 213        |
|    ep_rew_mean          | 23.2       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 233        |
|    time_elapsed         | 614        |
|    total_timesteps      | 477184     |
| train/                  |            |
|    approx_kl            | 0.20756708 |
|    clip_fraction        | 0.305      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.593      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0339    |
|    n_updates            | 2320       |
|    policy_gradient_loss | -0.045     |
|    value_loss           | 0.156      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 211       |
|    ep_rew_mean          | 22.9      |
| time/                   |           |
|    fps                  | 777       |
|    iterations           | 234       |
|    time_elapsed         | 616       |
|    total_timesteps      | 479232    |
| train/                  |           |
|    approx_kl            | 0.3072971 |
|    clip_fraction        | 0.32      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.36     |
|    explained_variance   | 0.593     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0453   |
|    n_updates            | 2330      |
|    policy_gradient_loss | -0.0486   |
|    value_loss           | 0.148     |
---------------------------------------
Eval num_timesteps=480000, episode_reward=23.10 +/- 5.65
Episode length: 226.20 +/- 57.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 226        |
|    mean_reward          | 23.1       |
| time/                   |            |
|    total_timesteps      | 480000     |
| train/                  |            |
|    approx_kl            | 0.28299093 |
|    clip_fraction        | 0.322      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.354     |
|    explained_variance   | 0.556      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0466    |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.0528    |
|    value_loss           | 0.161      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | 23       |
| time/              |          |
|    fps             | 775      |
|    iterations      | 235      |
|    time_elapsed    | 620      |
|    total_timesteps | 481280   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 214        |
|    ep_rew_mean          | 23.1       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 236        |
|    time_elapsed         | 622        |
|    total_timesteps      | 483328     |
| train/                  |            |
|    approx_kl            | 0.24701029 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.6        |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0421    |
|    n_updates            | 2350       |
|    policy_gradient_loss | -0.0445    |
|    value_loss           | 0.152      |
----------------------------------------
Eval num_timesteps=485000, episode_reward=27.60 +/- 5.75
Episode length: 276.10 +/- 79.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 276        |
|    mean_reward          | 27.6       |
| time/                   |            |
|    total_timesteps      | 485000     |
| train/                  |            |
|    approx_kl            | 0.27258304 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.366     |
|    explained_variance   | 0.558      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0473    |
|    n_updates            | 2360       |
|    policy_gradient_loss | -0.0515    |
|    value_loss           | 0.131      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 213      |
|    ep_rew_mean     | 23       |
| time/              |          |
|    fps             | 775      |
|    iterations      | 237      |
|    time_elapsed    | 626      |
|    total_timesteps | 485376   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 212        |
|    ep_rew_mean          | 23         |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 238        |
|    time_elapsed         | 628        |
|    total_timesteps      | 487424     |
| train/                  |            |
|    approx_kl            | 0.25920695 |
|    clip_fraction        | 0.326      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.351     |
|    explained_variance   | 0.618      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0461    |
|    n_updates            | 2370       |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.143      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 212        |
|    ep_rew_mean          | 23.1       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 239        |
|    time_elapsed         | 630        |
|    total_timesteps      | 489472     |
| train/                  |            |
|    approx_kl            | 0.32271895 |
|    clip_fraction        | 0.345      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.352     |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0424    |
|    n_updates            | 2380       |
|    policy_gradient_loss | -0.0492    |
|    value_loss           | 0.136      |
----------------------------------------
Eval num_timesteps=490000, episode_reward=24.70 +/- 2.93
Episode length: 209.30 +/- 34.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 209       |
|    mean_reward          | 24.7      |
| time/                   |           |
|    total_timesteps      | 490000    |
| train/                  |           |
|    approx_kl            | 0.2282292 |
|    clip_fraction        | 0.323     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.366    |
|    explained_variance   | 0.648     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0332   |
|    n_updates            | 2390      |
|    policy_gradient_loss | -0.043    |
|    value_loss           | 0.158     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 214      |
|    ep_rew_mean     | 23.4     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 240      |
|    time_elapsed    | 633      |
|    total_timesteps | 491520   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 214        |
|    ep_rew_mean          | 23.3       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 241        |
|    time_elapsed         | 635        |
|    total_timesteps      | 493568     |
| train/                  |            |
|    approx_kl            | 0.21832928 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.352     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0473    |
|    n_updates            | 2400       |
|    policy_gradient_loss | -0.0459    |
|    value_loss           | 0.165      |
----------------------------------------
Eval num_timesteps=495000, episode_reward=26.90 +/- 3.96
Episode length: 243.80 +/- 41.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 244        |
|    mean_reward          | 26.9       |
| time/                   |            |
|    total_timesteps      | 495000     |
| train/                  |            |
|    approx_kl            | 0.26319247 |
|    clip_fraction        | 0.318      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0299    |
|    n_updates            | 2410       |
|    policy_gradient_loss | -0.0445    |
|    value_loss           | 0.157      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | 23.6     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 242      |
|    time_elapsed    | 639      |
|    total_timesteps | 495616   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 221        |
|    ep_rew_mean          | 23.9       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 243        |
|    time_elapsed         | 641        |
|    total_timesteps      | 497664     |
| train/                  |            |
|    approx_kl            | 0.26084638 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.349     |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.00538   |
|    n_updates            | 2420       |
|    policy_gradient_loss | -0.0493    |
|    value_loss           | 0.154      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 224        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 244        |
|    time_elapsed         | 643        |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.23171586 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.348     |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0381    |
|    n_updates            | 2430       |
|    policy_gradient_loss | -0.0493    |
|    value_loss           | 0.137      |
----------------------------------------
Eval num_timesteps=500000, episode_reward=26.10 +/- 3.42
Episode length: 240.30 +/- 48.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 240        |
|    mean_reward          | 26.1       |
| time/                   |            |
|    total_timesteps      | 500000     |
| train/                  |            |
|    approx_kl            | 0.28747827 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.349     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0351    |
|    n_updates            | 2440       |
|    policy_gradient_loss | -0.0506    |
|    value_loss           | 0.139      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 24.7     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 245      |
|    time_elapsed    | 646      |
|    total_timesteps | 501760   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 230        |
|    ep_rew_mean          | 24.9       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 246        |
|    time_elapsed         | 648        |
|    total_timesteps      | 503808     |
| train/                  |            |
|    approx_kl            | 0.22821608 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.366     |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0352    |
|    n_updates            | 2450       |
|    policy_gradient_loss | -0.0459    |
|    value_loss           | 0.148      |
----------------------------------------
Eval num_timesteps=505000, episode_reward=24.40 +/- 6.96
Episode length: 223.80 +/- 66.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 224        |
|    mean_reward          | 24.4       |
| time/                   |            |
|    total_timesteps      | 505000     |
| train/                  |            |
|    approx_kl            | 0.26426384 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.366     |
|    explained_variance   | 0.551      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0572    |
|    n_updates            | 2460       |
|    policy_gradient_loss | -0.0535    |
|    value_loss           | 0.155      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 229      |
|    ep_rew_mean     | 25.1     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 247      |
|    time_elapsed    | 652      |
|    total_timesteps | 505856   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 229       |
|    ep_rew_mean          | 25        |
| time/                   |           |
|    fps                  | 776       |
|    iterations           | 248       |
|    time_elapsed         | 654       |
|    total_timesteps      | 507904    |
| train/                  |           |
|    approx_kl            | 0.2807365 |
|    clip_fraction        | 0.309     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.325    |
|    explained_variance   | 0.632     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0415   |
|    n_updates            | 2470      |
|    policy_gradient_loss | -0.0482   |
|    value_loss           | 0.133     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 232        |
|    ep_rew_mean          | 25.3       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 249        |
|    time_elapsed         | 656        |
|    total_timesteps      | 509952     |
| train/                  |            |
|    approx_kl            | 0.21603072 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.348     |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0398    |
|    n_updates            | 2480       |
|    policy_gradient_loss | -0.0461    |
|    value_loss           | 0.134      |
----------------------------------------
Eval num_timesteps=510000, episode_reward=25.30 +/- 4.92
Episode length: 232.70 +/- 58.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 233        |
|    mean_reward          | 25.3       |
| time/                   |            |
|    total_timesteps      | 510000     |
| train/                  |            |
|    approx_kl            | 0.25207585 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.358     |
|    explained_variance   | 0.596      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0432    |
|    n_updates            | 2490       |
|    policy_gradient_loss | -0.05      |
|    value_loss           | 0.155      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 24.9     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 250      |
|    time_elapsed    | 660      |
|    total_timesteps | 512000   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 229        |
|    ep_rew_mean          | 24.8       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 251        |
|    time_elapsed         | 662        |
|    total_timesteps      | 514048     |
| train/                  |            |
|    approx_kl            | 0.24234068 |
|    clip_fraction        | 0.31       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.577      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0449    |
|    n_updates            | 2500       |
|    policy_gradient_loss | -0.0454    |
|    value_loss           | 0.137      |
----------------------------------------
Eval num_timesteps=515000, episode_reward=26.00 +/- 8.80
Episode length: 262.60 +/- 98.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 263       |
|    mean_reward          | 26        |
| time/                   |           |
|    total_timesteps      | 515000    |
| train/                  |           |
|    approx_kl            | 0.2847473 |
|    clip_fraction        | 0.33      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.355    |
|    explained_variance   | 0.566     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0466   |
|    n_updates            | 2510      |
|    policy_gradient_loss | -0.0447   |
|    value_loss           | 0.129     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 24.6     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 252      |
|    time_elapsed    | 665      |
|    total_timesteps | 516096   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 220        |
|    ep_rew_mean          | 23.8       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 253        |
|    time_elapsed         | 667        |
|    total_timesteps      | 518144     |
| train/                  |            |
|    approx_kl            | 0.24796835 |
|    clip_fraction        | 0.34       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.372     |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0463    |
|    n_updates            | 2520       |
|    policy_gradient_loss | -0.0514    |
|    value_loss           | 0.134      |
----------------------------------------
Eval num_timesteps=520000, episode_reward=25.30 +/- 8.81
Episode length: 239.50 +/- 84.57
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 240        |
|    mean_reward          | 25.3       |
| time/                   |            |
|    total_timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.22655138 |
|    clip_fraction        | 0.32       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.374     |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.058     |
|    n_updates            | 2530       |
|    policy_gradient_loss | -0.0499    |
|    value_loss           | 0.143      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 217      |
|    ep_rew_mean     | 23.5     |
| time/              |          |
|    fps             | 774      |
|    iterations      | 254      |
|    time_elapsed    | 671      |
|    total_timesteps | 520192   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 213        |
|    ep_rew_mean          | 23.1       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 255        |
|    time_elapsed         | 673        |
|    total_timesteps      | 522240     |
| train/                  |            |
|    approx_kl            | 0.21822332 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0566    |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.0529    |
|    value_loss           | 0.129      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 216        |
|    ep_rew_mean          | 23.3       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 256        |
|    time_elapsed         | 675        |
|    total_timesteps      | 524288     |
| train/                  |            |
|    approx_kl            | 0.29786456 |
|    clip_fraction        | 0.313      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.371     |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.043     |
|    n_updates            | 2550       |
|    policy_gradient_loss | -0.0487    |
|    value_loss           | 0.139      |
----------------------------------------
Eval num_timesteps=525000, episode_reward=27.30 +/- 4.71
Episode length: 269.20 +/- 59.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 269        |
|    mean_reward          | 27.3       |
| time/                   |            |
|    total_timesteps      | 525000     |
| train/                  |            |
|    approx_kl            | 0.25409076 |
|    clip_fraction        | 0.336      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.385     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0558    |
|    n_updates            | 2560       |
|    policy_gradient_loss | -0.0537    |
|    value_loss           | 0.138      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 215      |
|    ep_rew_mean     | 23.1     |
| time/              |          |
|    fps             | 774      |
|    iterations      | 257      |
|    time_elapsed    | 679      |
|    total_timesteps | 526336   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 216        |
|    ep_rew_mean          | 23.2       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 258        |
|    time_elapsed         | 681        |
|    total_timesteps      | 528384     |
| train/                  |            |
|    approx_kl            | 0.20794189 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.405     |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0401    |
|    n_updates            | 2570       |
|    policy_gradient_loss | -0.0516    |
|    value_loss           | 0.133      |
----------------------------------------
Eval num_timesteps=530000, episode_reward=23.50 +/- 6.20
Episode length: 221.20 +/- 51.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 221        |
|    mean_reward          | 23.5       |
| time/                   |            |
|    total_timesteps      | 530000     |
| train/                  |            |
|    approx_kl            | 0.24973443 |
|    clip_fraction        | 0.317      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.347     |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0593    |
|    n_updates            | 2580       |
|    policy_gradient_loss | -0.0507    |
|    value_loss           | 0.137      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 216      |
|    ep_rew_mean     | 23.2     |
| time/              |          |
|    fps             | 774      |
|    iterations      | 259      |
|    time_elapsed    | 684      |
|    total_timesteps | 530432   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 218       |
|    ep_rew_mean          | 23.4      |
| time/                   |           |
|    fps                  | 775       |
|    iterations           | 260       |
|    time_elapsed         | 686       |
|    total_timesteps      | 532480    |
| train/                  |           |
|    approx_kl            | 0.2668419 |
|    clip_fraction        | 0.33      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.369    |
|    explained_variance   | 0.624     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0441   |
|    n_updates            | 2590      |
|    policy_gradient_loss | -0.0451   |
|    value_loss           | 0.15      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 217        |
|    ep_rew_mean          | 23.1       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 261        |
|    time_elapsed         | 688        |
|    total_timesteps      | 534528     |
| train/                  |            |
|    approx_kl            | 0.25941938 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0269    |
|    n_updates            | 2600       |
|    policy_gradient_loss | -0.0464    |
|    value_loss           | 0.138      |
----------------------------------------
Eval num_timesteps=535000, episode_reward=19.70 +/- 4.96
Episode length: 193.10 +/- 64.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 193       |
|    mean_reward          | 19.7      |
| time/                   |           |
|    total_timesteps      | 535000    |
| train/                  |           |
|    approx_kl            | 0.2909968 |
|    clip_fraction        | 0.316     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.336    |
|    explained_variance   | 0.675     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0224   |
|    n_updates            | 2610      |
|    policy_gradient_loss | -0.0475   |
|    value_loss           | 0.139     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | 23.3     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 262      |
|    time_elapsed    | 692      |
|    total_timesteps | 536576   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 217        |
|    ep_rew_mean          | 23.1       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 263        |
|    time_elapsed         | 694        |
|    total_timesteps      | 538624     |
| train/                  |            |
|    approx_kl            | 0.24849288 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.368     |
|    explained_variance   | 0.631      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0411    |
|    n_updates            | 2620       |
|    policy_gradient_loss | -0.0508    |
|    value_loss           | 0.138      |
----------------------------------------
Eval num_timesteps=540000, episode_reward=23.60 +/- 5.26
Episode length: 216.50 +/- 56.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 216        |
|    mean_reward          | 23.6       |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.25370175 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0442    |
|    n_updates            | 2630       |
|    policy_gradient_loss | -0.0537    |
|    value_loss           | 0.145      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 222      |
|    ep_rew_mean     | 23.6     |
| time/              |          |
|    fps             | 774      |
|    iterations      | 264      |
|    time_elapsed    | 697      |
|    total_timesteps | 540672   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 220        |
|    ep_rew_mean          | 23.3       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 265        |
|    time_elapsed         | 699        |
|    total_timesteps      | 542720     |
| train/                  |            |
|    approx_kl            | 0.27041444 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0541    |
|    n_updates            | 2640       |
|    policy_gradient_loss | -0.052     |
|    value_loss           | 0.16       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 225        |
|    ep_rew_mean          | 23.6       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 266        |
|    time_elapsed         | 701        |
|    total_timesteps      | 544768     |
| train/                  |            |
|    approx_kl            | 0.26779953 |
|    clip_fraction        | 0.332      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.593      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.042     |
|    n_updates            | 2650       |
|    policy_gradient_loss | -0.0527    |
|    value_loss           | 0.141      |
----------------------------------------
Eval num_timesteps=545000, episode_reward=24.80 +/- 4.92
Episode length: 230.60 +/- 36.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 231        |
|    mean_reward          | 24.8       |
| time/                   |            |
|    total_timesteps      | 545000     |
| train/                  |            |
|    approx_kl            | 0.29821727 |
|    clip_fraction        | 0.335      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.357     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0564    |
|    n_updates            | 2660       |
|    policy_gradient_loss | -0.0482    |
|    value_loss           | 0.142      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 222      |
|    ep_rew_mean     | 23.3     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 267      |
|    time_elapsed    | 705      |
|    total_timesteps | 546816   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 225       |
|    ep_rew_mean          | 23.7      |
| time/                   |           |
|    fps                  | 775       |
|    iterations           | 268       |
|    time_elapsed         | 707       |
|    total_timesteps      | 548864    |
| train/                  |           |
|    approx_kl            | 0.3235168 |
|    clip_fraction        | 0.322     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.342    |
|    explained_variance   | 0.618     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0454   |
|    n_updates            | 2670      |
|    policy_gradient_loss | -0.0493   |
|    value_loss           | 0.168     |
---------------------------------------
Eval num_timesteps=550000, episode_reward=20.20 +/- 6.65
Episode length: 196.70 +/- 72.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 197       |
|    mean_reward          | 20.2      |
| time/                   |           |
|    total_timesteps      | 550000    |
| train/                  |           |
|    approx_kl            | 0.2561088 |
|    clip_fraction        | 0.312     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.365    |
|    explained_variance   | 0.647     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0451   |
|    n_updates            | 2680      |
|    policy_gradient_loss | -0.0462   |
|    value_loss           | 0.139     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | 23.3     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 269      |
|    time_elapsed    | 710      |
|    total_timesteps | 550912   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 223        |
|    ep_rew_mean          | 23.1       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 270        |
|    time_elapsed         | 712        |
|    total_timesteps      | 552960     |
| train/                  |            |
|    approx_kl            | 0.24548924 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.434     |
|    explained_variance   | 0.589      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0334    |
|    n_updates            | 2690       |
|    policy_gradient_loss | -0.0575    |
|    value_loss           | 0.145      |
----------------------------------------
Eval num_timesteps=555000, episode_reward=17.60 +/- 3.72
Episode length: 168.20 +/- 30.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 168       |
|    mean_reward          | 17.6      |
| time/                   |           |
|    total_timesteps      | 555000    |
| train/                  |           |
|    approx_kl            | 0.2826878 |
|    clip_fraction        | 0.357     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.415    |
|    explained_variance   | 0.585     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.045    |
|    n_updates            | 2700      |
|    policy_gradient_loss | -0.0555   |
|    value_loss           | 0.155     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | 23       |
| time/              |          |
|    fps             | 775      |
|    iterations      | 271      |
|    time_elapsed    | 715      |
|    total_timesteps | 555008   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 223        |
|    ep_rew_mean          | 22.9       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 272        |
|    time_elapsed         | 718        |
|    total_timesteps      | 557056     |
| train/                  |            |
|    approx_kl            | 0.24153948 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.397     |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.054     |
|    n_updates            | 2710       |
|    policy_gradient_loss | -0.0514    |
|    value_loss           | 0.143      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 221        |
|    ep_rew_mean          | 22.7       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 273        |
|    time_elapsed         | 720        |
|    total_timesteps      | 559104     |
| train/                  |            |
|    approx_kl            | 0.22421171 |
|    clip_fraction        | 0.359      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.397     |
|    explained_variance   | 0.559      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0538    |
|    n_updates            | 2720       |
|    policy_gradient_loss | -0.0537    |
|    value_loss           | 0.162      |
----------------------------------------
Eval num_timesteps=560000, episode_reward=25.90 +/- 4.66
Episode length: 249.40 +/- 39.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 249        |
|    mean_reward          | 25.9       |
| time/                   |            |
|    total_timesteps      | 560000     |
| train/                  |            |
|    approx_kl            | 0.25377572 |
|    clip_fraction        | 0.352      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.391     |
|    explained_variance   | 0.593      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0449    |
|    n_updates            | 2730       |
|    policy_gradient_loss | -0.05      |
|    value_loss           | 0.143      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | 22.8     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 274      |
|    time_elapsed    | 723      |
|    total_timesteps | 561152   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 225        |
|    ep_rew_mean          | 23         |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 275        |
|    time_elapsed         | 725        |
|    total_timesteps      | 563200     |
| train/                  |            |
|    approx_kl            | 0.37275678 |
|    clip_fraction        | 0.337      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.391     |
|    explained_variance   | 0.609      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0421    |
|    n_updates            | 2740       |
|    policy_gradient_loss | -0.0522    |
|    value_loss           | 0.162      |
----------------------------------------
Eval num_timesteps=565000, episode_reward=26.60 +/- 5.64
Episode length: 249.30 +/- 64.38
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 249        |
|    mean_reward          | 26.6       |
| time/                   |            |
|    total_timesteps      | 565000     |
| train/                  |            |
|    approx_kl            | 0.26507357 |
|    clip_fraction        | 0.328      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.365     |
|    explained_variance   | 0.57       |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0327    |
|    n_updates            | 2750       |
|    policy_gradient_loss | -0.0481    |
|    value_loss           | 0.165      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 227      |
|    ep_rew_mean     | 23.3     |
| time/              |          |
|    fps             | 774      |
|    iterations      | 276      |
|    time_elapsed    | 729      |
|    total_timesteps | 565248   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 226        |
|    ep_rew_mean          | 23.3       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 277        |
|    time_elapsed         | 731        |
|    total_timesteps      | 567296     |
| train/                  |            |
|    approx_kl            | 0.26293558 |
|    clip_fraction        | 0.334      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0311    |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.0487    |
|    value_loss           | 0.131      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 229        |
|    ep_rew_mean          | 23.5       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 278        |
|    time_elapsed         | 733        |
|    total_timesteps      | 569344     |
| train/                  |            |
|    approx_kl            | 0.27262655 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.365     |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0436    |
|    n_updates            | 2770       |
|    policy_gradient_loss | -0.048     |
|    value_loss           | 0.141      |
----------------------------------------
Eval num_timesteps=570000, episode_reward=25.00 +/- 4.20
Episode length: 240.90 +/- 59.51
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 241        |
|    mean_reward          | 25         |
| time/                   |            |
|    total_timesteps      | 570000     |
| train/                  |            |
|    approx_kl            | 0.22720084 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0535    |
|    n_updates            | 2780       |
|    policy_gradient_loss | -0.0485    |
|    value_loss           | 0.139      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 232      |
|    ep_rew_mean     | 23.9     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 279      |
|    time_elapsed    | 737      |
|    total_timesteps | 571392   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 233        |
|    ep_rew_mean          | 24.1       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 280        |
|    time_elapsed         | 739        |
|    total_timesteps      | 573440     |
| train/                  |            |
|    approx_kl            | 0.28820568 |
|    clip_fraction        | 0.333      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.349     |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0379    |
|    n_updates            | 2790       |
|    policy_gradient_loss | -0.0468    |
|    value_loss           | 0.142      |
----------------------------------------
Eval num_timesteps=575000, episode_reward=23.30 +/- 5.98
Episode length: 210.90 +/- 48.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 211        |
|    mean_reward          | 23.3       |
| time/                   |            |
|    total_timesteps      | 575000     |
| train/                  |            |
|    approx_kl            | 0.25023845 |
|    clip_fraction        | 0.323      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.354     |
|    explained_variance   | 0.702      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0415    |
|    n_updates            | 2800       |
|    policy_gradient_loss | -0.0469    |
|    value_loss           | 0.147      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 228      |
|    ep_rew_mean     | 23.9     |
| time/              |          |
|    fps             | 774      |
|    iterations      | 281      |
|    time_elapsed    | 742      |
|    total_timesteps | 575488   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 230        |
|    ep_rew_mean          | 24.3       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 282        |
|    time_elapsed         | 744        |
|    total_timesteps      | 577536     |
| train/                  |            |
|    approx_kl            | 0.22493502 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.363     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.049     |
|    n_updates            | 2810       |
|    policy_gradient_loss | -0.0499    |
|    value_loss           | 0.154      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 24.8       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 283        |
|    time_elapsed         | 746        |
|    total_timesteps      | 579584     |
| train/                  |            |
|    approx_kl            | 0.27005407 |
|    clip_fraction        | 0.329      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.353     |
|    explained_variance   | 0.538      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0299    |
|    n_updates            | 2820       |
|    policy_gradient_loss | -0.0496    |
|    value_loss           | 0.143      |
----------------------------------------
Eval num_timesteps=580000, episode_reward=23.80 +/- 3.25
Episode length: 223.80 +/- 27.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 224        |
|    mean_reward          | 23.8       |
| time/                   |            |
|    total_timesteps      | 580000     |
| train/                  |            |
|    approx_kl            | 0.23205042 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.354     |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0414    |
|    n_updates            | 2830       |
|    policy_gradient_loss | -0.0484    |
|    value_loss           | 0.132      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 238      |
|    ep_rew_mean     | 25.2     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 284      |
|    time_elapsed    | 750      |
|    total_timesteps | 581632   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 238        |
|    ep_rew_mean          | 25.4       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 285        |
|    time_elapsed         | 752        |
|    total_timesteps      | 583680     |
| train/                  |            |
|    approx_kl            | 0.24665274 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0407    |
|    n_updates            | 2840       |
|    policy_gradient_loss | -0.0466    |
|    value_loss           | 0.134      |
----------------------------------------
Eval num_timesteps=585000, episode_reward=23.20 +/- 7.65
Episode length: 203.70 +/- 70.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 204        |
|    mean_reward          | 23.2       |
| time/                   |            |
|    total_timesteps      | 585000     |
| train/                  |            |
|    approx_kl            | 0.21711367 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0327    |
|    n_updates            | 2850       |
|    policy_gradient_loss | -0.0461    |
|    value_loss           | 0.137      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 235      |
|    ep_rew_mean     | 25.3     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 286      |
|    time_elapsed    | 755      |
|    total_timesteps | 585728   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 237        |
|    ep_rew_mean          | 25.6       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 287        |
|    time_elapsed         | 757        |
|    total_timesteps      | 587776     |
| train/                  |            |
|    approx_kl            | 0.22405782 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.043     |
|    n_updates            | 2860       |
|    policy_gradient_loss | -0.0494    |
|    value_loss           | 0.144      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 236       |
|    ep_rew_mean          | 25.5      |
| time/                   |           |
|    fps                  | 776       |
|    iterations           | 288       |
|    time_elapsed         | 759       |
|    total_timesteps      | 589824    |
| train/                  |           |
|    approx_kl            | 0.2028037 |
|    clip_fraction        | 0.335     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.372    |
|    explained_variance   | 0.631     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0471   |
|    n_updates            | 2870      |
|    policy_gradient_loss | -0.045    |
|    value_loss           | 0.14      |
---------------------------------------
Eval num_timesteps=590000, episode_reward=25.80 +/- 5.53
Episode length: 232.80 +/- 58.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 233       |
|    mean_reward          | 25.8      |
| time/                   |           |
|    total_timesteps      | 590000    |
| train/                  |           |
|    approx_kl            | 0.3367672 |
|    clip_fraction        | 0.333     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.36     |
|    explained_variance   | 0.53      |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0442   |
|    n_updates            | 2880      |
|    policy_gradient_loss | -0.0523   |
|    value_loss           | 0.147     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 234      |
|    ep_rew_mean     | 25.5     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 289      |
|    time_elapsed    | 763      |
|    total_timesteps | 591872   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 234        |
|    ep_rew_mean          | 25.6       |
| time/                   |            |
|    fps                  | 776        |
|    iterations           | 290        |
|    time_elapsed         | 765        |
|    total_timesteps      | 593920     |
| train/                  |            |
|    approx_kl            | 0.26503497 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.341     |
|    explained_variance   | 0.565      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0405    |
|    n_updates            | 2890       |
|    policy_gradient_loss | -0.0446    |
|    value_loss           | 0.144      |
----------------------------------------
Eval num_timesteps=595000, episode_reward=26.40 +/- 7.09
Episode length: 242.20 +/- 75.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 242        |
|    mean_reward          | 26.4       |
| time/                   |            |
|    total_timesteps      | 595000     |
| train/                  |            |
|    approx_kl            | 0.23578447 |
|    clip_fraction        | 0.324      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.355     |
|    explained_variance   | 0.586      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0218    |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.0451    |
|    value_loss           | 0.141      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 231      |
|    ep_rew_mean     | 25.4     |
| time/              |          |
|    fps             | 775      |
|    iterations      | 291      |
|    time_elapsed    | 768      |
|    total_timesteps | 595968   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 235        |
|    ep_rew_mean          | 25.8       |
| time/                   |            |
|    fps                  | 775        |
|    iterations           | 292        |
|    time_elapsed         | 770        |
|    total_timesteps      | 598016     |
| train/                  |            |
|    approx_kl            | 0.27009755 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.356     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.00038    |
|    loss                 | -0.0468    |
|    n_updates            | 2910       |
|    policy_gradient_loss | -0.05      |
|    value_loss           | 0.15       |
----------------------------------------
Eval num_timesteps=600000, episode_reward=24.30 +/- 4.61
Episode length: 225.20 +/- 42.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 225       |
|    mean_reward          | 24.3      |
| time/                   |           |
|    total_timesteps      | 600000    |
| train/                  |           |
|    approx_kl            | 0.2337444 |
|    clip_fraction        | 0.337     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.373    |
|    explained_variance   | 0.618     |
|    learning_rate        | 0.00038   |
|    loss                 | -0.0487   |
|    n_updates            | 2920      |
|    policy_gradient_loss | -0.0471   |
|    value_loss           | 0.134     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 233      |
|    ep_rew_mean     | 25.6     |
| time/              |          |
|    fps             | 774      |
|    iterations      | 293      |
|    time_elapsed    | 774      |
|    total_timesteps | 600064   |
---------------------------------
/home/miguelvilla/anaconda3/envs/vizdoom/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'saves-tunning/defend-line/ppo-1/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
