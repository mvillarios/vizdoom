/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-510.72 +/- 83.44
Episode length: 22.58 +/- 9.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-532.36 +/- 61.97
Episode length: 24.48 +/- 8.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.5     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-513.77 +/- 67.32
Episode length: 25.14 +/- 7.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=-514.93 +/- 69.52
Episode length: 21.70 +/- 7.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 21.7     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.5     |
|    ep_rew_mean     | -465     |
| time/              |          |
|    fps             | 395      |
|    iterations      | 1        |
|    time_elapsed    | 5        |
|    total_timesteps | 2048     |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=2500, episode_reward=-361.81 +/- 154.14
Episode length: 18.90 +/- 7.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.9        |
|    mean_reward          | -362        |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.010905172 |
|    clip_fraction        | 0.0618      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -0.000152   |
|    learning_rate        | 0.0001      |
|    loss                 | 7.48e+03    |
|    n_updates            | 2042        |
|    policy_gradient_loss | -0.00557    |
|    value_loss           | 1.41e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=3000, episode_reward=-391.66 +/- 129.04
Episode length: 18.02 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-337.38 +/- 138.58
Episode length: 16.60 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -337     |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=-310.11 +/- 156.57
Episode length: 16.34 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.9     |
|    ep_rew_mean     | -443     |
| time/              |          |
|    fps             | 412      |
|    iterations      | 2        |
|    time_elapsed    | 9        |
|    total_timesteps | 4096     |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=4500, episode_reward=-536.05 +/- 68.74
Episode length: 23.30 +/- 8.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.3        |
|    mean_reward          | -536        |
| time/                   |             |
|    total_timesteps      | 4500        |
| train/                  |             |
|    approx_kl            | 0.011593505 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.04       |
|    explained_variance   | -0.000855   |
|    learning_rate        | 0.0001      |
|    loss                 | 5.81e+03    |
|    n_updates            | 2046        |
|    policy_gradient_loss | -0.00951    |
|    value_loss           | 1.19e+04    |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=-528.20 +/- 74.82
Episode length: 25.46 +/- 10.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.5     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-517.72 +/- 90.70
Episode length: 24.08 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.1     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=-504.56 +/- 90.01
Episode length: 24.32 +/- 9.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.3     |
|    mean_reward     | -505     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 27.7     |
|    ep_rew_mean     | -439     |
| time/              |          |
|    fps             | 399      |
|    iterations      | 3        |
|    time_elapsed    | 15       |
|    total_timesteps | 6144     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=6500, episode_reward=-527.74 +/- 86.32
Episode length: 23.52 +/- 8.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.5        |
|    mean_reward          | -528        |
| time/                   |             |
|    total_timesteps      | 6500        |
| train/                  |             |
|    approx_kl            | 0.006213357 |
|    clip_fraction        | 0.0461      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -2.01       |
|    explained_variance   | -0.0011     |
|    learning_rate        | 0.0001      |
|    loss                 | 9.68e+03    |
|    n_updates            | 2047        |
|    policy_gradient_loss | -0.00502    |
|    value_loss           | 1.48e+04    |
-----------------------------------------
Eval num_timesteps=7000, episode_reward=-532.20 +/- 69.42
Episode length: 24.16 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 24.2     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-533.68 +/- 63.83
Episode length: 26.00 +/- 9.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 26       |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=-537.86 +/- 67.49
Episode length: 25.32 +/- 9.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.3     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28       |
|    ep_rew_mean     | -432     |
| time/              |          |
|    fps             | 395      |
|    iterations      | 4        |
|    time_elapsed    | 20       |
|    total_timesteps | 8192     |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=8500, episode_reward=-525.21 +/- 65.41
Episode length: 24.42 +/- 9.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.4         |
|    mean_reward          | -525         |
| time/                   |              |
|    total_timesteps      | 8500         |
| train/                  |              |
|    approx_kl            | 0.0059793848 |
|    clip_fraction        | 0.0824       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.95        |
|    explained_variance   | -0.00286     |
|    learning_rate        | 0.0001       |
|    loss                 | 6.81e+03     |
|    n_updates            | 2048         |
|    policy_gradient_loss | -0.0075      |
|    value_loss           | 1.3e+04      |
------------------------------------------
Eval num_timesteps=9000, episode_reward=-533.61 +/- 71.16
Episode length: 25.88 +/- 7.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.9     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-524.49 +/- 74.31
Episode length: 22.56 +/- 8.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-516.22 +/- 79.45
Episode length: 23.58 +/- 8.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.6     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.5     |
|    ep_rew_mean     | -407     |
| time/              |          |
|    fps             | 394      |
|    iterations      | 5        |
|    time_elapsed    | 25       |
|    total_timesteps | 10240    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=10500, episode_reward=-525.83 +/- 67.75
Episode length: 24.68 +/- 9.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 24.7         |
|    mean_reward          | -526         |
| time/                   |              |
|    total_timesteps      | 10500        |
| train/                  |              |
|    approx_kl            | 0.0074507245 |
|    clip_fraction        | 0.066        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.83        |
|    explained_variance   | -0.00474     |
|    learning_rate        | 0.0001       |
|    loss                 | 5.16e+03     |
|    n_updates            | 2049         |
|    policy_gradient_loss | 0.000461     |
|    value_loss           | 1.17e+04     |
------------------------------------------
Eval num_timesteps=11000, episode_reward=-515.93 +/- 66.33
Episode length: 25.66 +/- 10.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.7     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-522.28 +/- 75.30
Episode length: 23.30 +/- 9.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.3     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=-529.98 +/- 79.88
Episode length: 23.72 +/- 7.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.7     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.9     |
|    ep_rew_mean     | -392     |
| time/              |          |
|    fps             | 393      |
|    iterations      | 6        |
|    time_elapsed    | 31       |
|    total_timesteps | 12288    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=12500, episode_reward=-519.63 +/- 75.64
Episode length: 23.76 +/- 7.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.8        |
|    mean_reward          | -520        |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.006831777 |
|    clip_fraction        | 0.0729      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.7        |
|    explained_variance   | -0.00683    |
|    learning_rate        | 0.0001      |
|    loss                 | 8.48e+03    |
|    n_updates            | 2050        |
|    policy_gradient_loss | 0.00586     |
|    value_loss           | 1.44e+04    |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=-514.49 +/- 73.42
Episode length: 23.20 +/- 8.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 23.2     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-539.25 +/- 75.27
Episode length: 25.12 +/- 8.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 25.1     |
|    mean_reward     | -539     |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=-509.54 +/- 79.50
Episode length: 22.62 +/- 10.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 22.6     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | -369     |
| time/              |          |
|    fps             | 394      |
|    iterations      | 7        |
|    time_elapsed    | 36       |
|    total_timesteps | 14336    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=14500, episode_reward=-136.97 +/- 180.37
Episode length: 15.88 +/- 5.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | -137        |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.006793005 |
|    clip_fraction        | 0.0649      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | -0.00609    |
|    learning_rate        | 0.0001      |
|    loss                 | 7.57e+03    |
|    n_updates            | 2051        |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 1.36e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-148.22 +/- 132.40
Episode length: 16.32 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-129.10 +/- 132.29
Episode length: 16.34 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
New best mean reward!
Eval num_timesteps=16000, episode_reward=-133.48 +/- 137.62
Episode length: 15.84 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | -337     |
| time/              |          |
|    fps             | 403      |
|    iterations      | 8        |
|    time_elapsed    | 40       |
|    total_timesteps | 16384    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=16500, episode_reward=-154.74 +/- 161.82
Episode length: 16.12 +/- 5.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -155        |
| time/                   |             |
|    total_timesteps      | 16500       |
| train/                  |             |
|    approx_kl            | 0.006597811 |
|    clip_fraction        | 0.0605      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.57       |
|    explained_variance   | -0.011      |
|    learning_rate        | 0.0001      |
|    loss                 | 5.71e+03    |
|    n_updates            | 2052        |
|    policy_gradient_loss | 0.0029      |
|    value_loss           | 1.24e+04    |
-----------------------------------------
Eval num_timesteps=17000, episode_reward=-171.87 +/- 178.28
Episode length: 16.00 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-136.10 +/- 125.88
Episode length: 15.66 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-134.48 +/- 157.01
Episode length: 16.68 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | -326     |
| time/              |          |
|    fps             | 411      |
|    iterations      | 9        |
|    time_elapsed    | 44       |
|    total_timesteps | 18432    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=18500, episode_reward=-121.93 +/- 124.75
Episode length: 15.86 +/- 4.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -122         |
| time/                   |              |
|    total_timesteps      | 18500        |
| train/                  |              |
|    approx_kl            | 0.0056573693 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.46        |
|    explained_variance   | -0.0142      |
|    learning_rate        | 0.0001       |
|    loss                 | 6.67e+03     |
|    n_updates            | 2053         |
|    policy_gradient_loss | 0.0014       |
|    value_loss           | 1.4e+04      |
------------------------------------------
New best mean reward!
Eval num_timesteps=19000, episode_reward=-196.35 +/- 142.66
Episode length: 14.80 +/- 3.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-145.31 +/- 161.86
Episode length: 16.02 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-169.64 +/- 147.06
Episode length: 15.68 +/- 3.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | -310     |
| time/              |          |
|    fps             | 418      |
|    iterations      | 10       |
|    time_elapsed    | 48       |
|    total_timesteps | 20480    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=20500, episode_reward=-133.66 +/- 182.81
Episode length: 16.58 +/- 5.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -134         |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 0.0061949915 |
|    clip_fraction        | 0.104        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | -0.0144      |
|    learning_rate        | 0.0001       |
|    loss                 | 7.16e+03     |
|    n_updates            | 2054         |
|    policy_gradient_loss | -0.00225     |
|    value_loss           | 1.22e+04     |
------------------------------------------
Eval num_timesteps=21000, episode_reward=-175.93 +/- 160.28
Episode length: 14.94 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-162.33 +/- 123.30
Episode length: 15.24 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-117.16 +/- 187.76
Episode length: 17.50 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
New best mean reward!
Eval num_timesteps=22500, episode_reward=-125.57 +/- 149.09
Episode length: 16.52 +/- 4.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | -313     |
| time/              |          |
|    fps             | 420      |
|    iterations      | 11       |
|    time_elapsed    | 53       |
|    total_timesteps | 22528    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=23000, episode_reward=-150.64 +/- 166.56
Episode length: 15.68 +/- 5.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | -151        |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.008923748 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | -0.0151     |
|    learning_rate        | 0.0001      |
|    loss                 | 5.66e+03    |
|    n_updates            | 2055        |
|    policy_gradient_loss | -0.00527    |
|    value_loss           | 1.16e+04    |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=-150.92 +/- 171.43
Episode length: 16.22 +/- 5.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-169.31 +/- 130.65
Episode length: 15.20 +/- 4.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=-144.80 +/- 118.62
Episode length: 15.64 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | -273     |
| time/              |          |
|    fps             | 426      |
|    iterations      | 12       |
|    time_elapsed    | 57       |
|    total_timesteps | 24576    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=25000, episode_reward=-129.46 +/- 172.07
Episode length: 16.78 +/- 5.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | -129        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.008071269 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | -0.0156     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.57e+03    |
|    n_updates            | 2056        |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 1.12e+04    |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=-137.64 +/- 155.67
Episode length: 16.20 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-199.18 +/- 129.61
Episode length: 14.58 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=-175.71 +/- 120.03
Episode length: 15.40 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | -254     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 13       |
|    time_elapsed    | 61       |
|    total_timesteps | 26624    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=27000, episode_reward=-136.22 +/- 180.01
Episode length: 16.28 +/- 5.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.3      |
|    mean_reward          | -136      |
| time/                   |           |
|    total_timesteps      | 27000     |
| train/                  |           |
|    approx_kl            | 0.0092321 |
|    clip_fraction        | 0.102     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.901    |
|    explained_variance   | -0.0159   |
|    learning_rate        | 0.0001    |
|    loss                 | 4.31e+03  |
|    n_updates            | 2057      |
|    policy_gradient_loss | -0.00476  |
|    value_loss           | 1.1e+04   |
---------------------------------------
Eval num_timesteps=27500, episode_reward=-151.59 +/- 170.07
Episode length: 15.90 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-155.92 +/- 184.16
Episode length: 15.58 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=-164.51 +/- 140.46
Episode length: 15.26 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -263     |
| time/              |          |
|    fps             | 435      |
|    iterations      | 14       |
|    time_elapsed    | 65       |
|    total_timesteps | 28672    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=29000, episode_reward=-154.70 +/- 147.99
Episode length: 15.92 +/- 4.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | -155        |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.005301277 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.808      |
|    explained_variance   | -0.0174     |
|    learning_rate        | 0.0001      |
|    loss                 | 5.5e+03     |
|    n_updates            | 2058        |
|    policy_gradient_loss | -0.000187   |
|    value_loss           | 1e+04       |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=-167.56 +/- 139.31
Episode length: 15.94 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-126.31 +/- 133.71
Episode length: 16.32 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=-150.92 +/- 158.53
Episode length: 16.12 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -234     |
| time/              |          |
|    fps             | 438      |
|    iterations      | 15       |
|    time_elapsed    | 70       |
|    total_timesteps | 30720    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=31000, episode_reward=-145.21 +/- 126.54
Episode length: 16.04 +/- 4.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -145         |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0085940845 |
|    clip_fraction        | 0.103        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.616       |
|    explained_variance   | -0.0225      |
|    learning_rate        | 0.0001       |
|    loss                 | 3.25e+03     |
|    n_updates            | 2059         |
|    policy_gradient_loss | -0.00874     |
|    value_loss           | 9.6e+03      |
------------------------------------------
Eval num_timesteps=31500, episode_reward=-116.55 +/- 166.41
Episode length: 16.98 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
New best mean reward!
Eval num_timesteps=32000, episode_reward=-126.82 +/- 142.92
Episode length: 16.64 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=-141.99 +/- 181.66
Episode length: 15.70 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -215     |
| time/              |          |
|    fps             | 440      |
|    iterations      | 16       |
|    time_elapsed    | 74       |
|    total_timesteps | 32768    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=33000, episode_reward=-136.78 +/- 175.37
Episode length: 15.66 +/- 5.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -137         |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0060079983 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.451       |
|    explained_variance   | -0.0205      |
|    learning_rate        | 0.0001       |
|    loss                 | 3.45e+03     |
|    n_updates            | 2060         |
|    policy_gradient_loss | -0.00443     |
|    value_loss           | 1.03e+04     |
------------------------------------------
Eval num_timesteps=33500, episode_reward=-151.06 +/- 131.00
Episode length: 15.50 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-176.79 +/- 150.82
Episode length: 15.02 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=-156.17 +/- 161.04
Episode length: 15.56 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -174     |
| time/              |          |
|    fps             | 444      |
|    iterations      | 17       |
|    time_elapsed    | 78       |
|    total_timesteps | 34816    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=35000, episode_reward=-199.62 +/- 136.50
Episode length: 14.78 +/- 4.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | -200        |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.004056598 |
|    clip_fraction        | 0.0378      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.341      |
|    explained_variance   | -0.0185     |
|    learning_rate        | 0.0001      |
|    loss                 | 5e+03       |
|    n_updates            | 2061        |
|    policy_gradient_loss | 0.000244    |
|    value_loss           | 9.26e+03    |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=-87.60 +/- 200.72
Episode length: 17.52 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -87.6    |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
New best mean reward!
Eval num_timesteps=36000, episode_reward=-154.12 +/- 136.38
Episode length: 15.76 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=-133.30 +/- 126.30
Episode length: 16.90 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 446      |
|    iterations      | 18       |
|    time_elapsed    | 82       |
|    total_timesteps | 36864    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=37000, episode_reward=-147.52 +/- 150.72
Episode length: 15.50 +/- 5.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -148        |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.005739249 |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.288      |
|    explained_variance   | -0.016      |
|    learning_rate        | 0.0001      |
|    loss                 | 4.78e+03    |
|    n_updates            | 2064        |
|    policy_gradient_loss | 0.00209     |
|    value_loss           | 7.97e+03    |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=-134.49 +/- 175.91
Episode length: 16.06 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-107.32 +/- 155.72
Episode length: 16.94 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=-189.86 +/- 137.97
Episode length: 14.72 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 448      |
|    iterations      | 19       |
|    time_elapsed    | 86       |
|    total_timesteps | 38912    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=39000, episode_reward=-123.48 +/- 155.56
Episode length: 16.70 +/- 5.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -123         |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0041618757 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.194       |
|    explained_variance   | -0.0112      |
|    learning_rate        | 0.0001       |
|    loss                 | 5.23e+03     |
|    n_updates            | 2067         |
|    policy_gradient_loss | 0.00119      |
|    value_loss           | 9.07e+03     |
------------------------------------------
Eval num_timesteps=39500, episode_reward=-169.93 +/- 131.02
Episode length: 15.14 +/- 3.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-181.78 +/- 134.28
Episode length: 14.48 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=-181.12 +/- 144.06
Episode length: 14.70 +/- 4.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -181     |
| time/              |          |
|    fps             | 451      |
|    iterations      | 20       |
|    time_elapsed    | 90       |
|    total_timesteps | 40960    |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=41000, episode_reward=-114.88 +/- 168.61
Episode length: 16.88 +/- 5.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.9        |
|    mean_reward          | -115        |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.004123014 |
|    clip_fraction        | 0.0335      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.263      |
|    explained_variance   | -0.0142     |
|    learning_rate        | 0.0001      |
|    loss                 | 5.71e+03    |
|    n_updates            | 2072        |
|    policy_gradient_loss | 0.000289    |
|    value_loss           | 1e+04       |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=-154.12 +/- 143.57
Episode length: 15.58 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-143.32 +/- 142.37
Episode length: 15.62 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=-128.51 +/- 152.75
Episode length: 16.22 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-149.34 +/- 125.11
Episode length: 16.22 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -179     |
| time/              |          |
|    fps             | 449      |
|    iterations      | 21       |
|    time_elapsed    | 95       |
|    total_timesteps | 43008    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=43500, episode_reward=-140.21 +/- 154.48
Episode length: 16.02 +/- 4.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -140        |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.002833906 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.22       |
|    explained_variance   | -0.0119     |
|    learning_rate        | 0.0001      |
|    loss                 | 5.31e+03    |
|    n_updates            | 2073        |
|    policy_gradient_loss | 0.000631    |
|    value_loss           | 9.67e+03    |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=-144.69 +/- 106.63
Episode length: 15.82 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-203.01 +/- 152.18
Episode length: 14.88 +/- 4.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-127.63 +/- 119.71
Episode length: 15.74 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 451      |
|    iterations      | 22       |
|    time_elapsed    | 99       |
|    total_timesteps | 45056    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=45500, episode_reward=-146.18 +/- 176.15
Episode length: 15.78 +/- 5.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -146         |
| time/                   |              |
|    total_timesteps      | 45500        |
| train/                  |              |
|    approx_kl            | 0.0040827566 |
|    clip_fraction        | 0.0299       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.286       |
|    explained_variance   | -0.0112      |
|    learning_rate        | 0.0001       |
|    loss                 | 3.77e+03     |
|    n_updates            | 2076         |
|    policy_gradient_loss | 0.00205      |
|    value_loss           | 8.73e+03     |
------------------------------------------
Eval num_timesteps=46000, episode_reward=-109.31 +/- 174.71
Episode length: 17.68 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-129.25 +/- 138.76
Episode length: 16.72 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=-124.78 +/- 188.91
Episode length: 16.54 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 452      |
|    iterations      | 23       |
|    time_elapsed    | 104      |
|    total_timesteps | 47104    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=47500, episode_reward=-147.23 +/- 157.17
Episode length: 16.12 +/- 4.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -147         |
| time/                   |              |
|    total_timesteps      | 47500        |
| train/                  |              |
|    approx_kl            | 0.0043968437 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.282       |
|    explained_variance   | -0.00953     |
|    learning_rate        | 0.0001       |
|    loss                 | 4.55e+03     |
|    n_updates            | 2078         |
|    policy_gradient_loss | 0.00442      |
|    value_loss           | 9.47e+03     |
------------------------------------------
Eval num_timesteps=48000, episode_reward=-173.78 +/- 135.11
Episode length: 16.42 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=-170.68 +/- 134.66
Episode length: 15.16 +/- 4.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-169.96 +/- 163.51
Episode length: 16.02 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 453      |
|    iterations      | 24       |
|    time_elapsed    | 108      |
|    total_timesteps | 49152    |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.04
Eval num_timesteps=49500, episode_reward=-145.48 +/- 152.48
Episode length: 16.90 +/- 5.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.9         |
|    mean_reward          | -145         |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 0.0070707235 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.343       |
|    explained_variance   | -0.01        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.33e+03     |
|    n_updates            | 2083         |
|    policy_gradient_loss | 0.000199     |
|    value_loss           | 9.38e+03     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-169.71 +/- 143.54
Episode length: 15.36 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=-155.39 +/- 122.78
Episode length: 14.94 +/- 4.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=-185.22 +/- 151.85
Episode length: 14.62 +/- 3.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 454      |
|    iterations      | 25       |
|    time_elapsed    | 112      |
|    total_timesteps | 51200    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=51500, episode_reward=-119.29 +/- 177.81
Episode length: 16.68 +/- 5.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -119        |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 0.012067924 |
|    clip_fraction        | 0.042       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.401      |
|    explained_variance   | -0.00734    |
|    learning_rate        | 0.0001      |
|    loss                 | 4.23e+03    |
|    n_updates            | 2086        |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 1.01e+04    |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=-122.54 +/- 185.30
Episode length: 16.54 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-175.88 +/- 124.77
Episode length: 15.44 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-135.04 +/- 148.84
Episode length: 16.22 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 455      |
|    iterations      | 26       |
|    time_elapsed    | 116      |
|    total_timesteps | 53248    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=53500, episode_reward=-150.37 +/- 120.73
Episode length: 16.00 +/- 4.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -150        |
| time/                   |             |
|    total_timesteps      | 53500       |
| train/                  |             |
|    approx_kl            | 0.006007362 |
|    clip_fraction        | 0.0294      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.274      |
|    explained_variance   | -0.0112     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.65e+03    |
|    n_updates            | 2089        |
|    policy_gradient_loss | 0.000834    |
|    value_loss           | 9.35e+03    |
-----------------------------------------
Eval num_timesteps=54000, episode_reward=-103.83 +/- 152.13
Episode length: 16.94 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-157.93 +/- 141.42
Episode length: 15.38 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=-153.65 +/- 147.75
Episode length: 14.96 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 456      |
|    iterations      | 27       |
|    time_elapsed    | 121      |
|    total_timesteps | 55296    |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=55500, episode_reward=-143.91 +/- 137.67
Episode length: 14.86 +/- 4.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.9         |
|    mean_reward          | -144         |
| time/                   |              |
|    total_timesteps      | 55500        |
| train/                  |              |
|    approx_kl            | 0.0055300314 |
|    clip_fraction        | 0.0332       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.358       |
|    explained_variance   | -0.0102      |
|    learning_rate        | 0.0001       |
|    loss                 | 6.08e+03     |
|    n_updates            | 2095         |
|    policy_gradient_loss | -0.00104     |
|    value_loss           | 8.29e+03     |
------------------------------------------
Eval num_timesteps=56000, episode_reward=-138.42 +/- 141.08
Episode length: 15.88 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-145.14 +/- 167.07
Episode length: 16.32 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-140.83 +/- 153.15
Episode length: 15.66 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 456      |
|    iterations      | 28       |
|    time_elapsed    | 125      |
|    total_timesteps | 57344    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=57500, episode_reward=-156.44 +/- 142.73
Episode length: 16.16 +/- 4.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -156         |
| time/                   |              |
|    total_timesteps      | 57500        |
| train/                  |              |
|    approx_kl            | 0.0041059144 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.28        |
|    explained_variance   | -0.0124      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.06e+03     |
|    n_updates            | 2097         |
|    policy_gradient_loss | 0.00388      |
|    value_loss           | 9.57e+03     |
------------------------------------------
Eval num_timesteps=58000, episode_reward=-133.04 +/- 177.12
Episode length: 16.22 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=-142.28 +/- 146.42
Episode length: 15.88 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=-156.84 +/- 136.03
Episode length: 15.74 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -192     |
| time/              |          |
|    fps             | 457      |
|    iterations      | 29       |
|    time_elapsed    | 129      |
|    total_timesteps | 59392    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=59500, episode_reward=-141.05 +/- 159.77
Episode length: 16.42 +/- 5.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -141         |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.0029515908 |
|    clip_fraction        | 0.0406       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.413       |
|    explained_variance   | -0.0117      |
|    learning_rate        | 0.0001       |
|    loss                 | 5.02e+03     |
|    n_updates            | 2098         |
|    policy_gradient_loss | 0.00288      |
|    value_loss           | 9.85e+03     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-151.38 +/- 138.30
Episode length: 15.46 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-125.97 +/- 143.85
Episode length: 16.46 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-138.73 +/- 169.61
Episode length: 16.06 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -216     |
| time/              |          |
|    fps             | 458      |
|    iterations      | 30       |
|    time_elapsed    | 133      |
|    total_timesteps | 61440    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=61500, episode_reward=-158.28 +/- 150.81
Episode length: 15.84 +/- 4.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -158         |
| time/                   |              |
|    total_timesteps      | 61500        |
| train/                  |              |
|    approx_kl            | 0.0056551467 |
|    clip_fraction        | 0.0813       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.481       |
|    explained_variance   | -0.0141      |
|    learning_rate        | 0.0001       |
|    loss                 | 4e+03        |
|    n_updates            | 2099         |
|    policy_gradient_loss | -0.00145     |
|    value_loss           | 9.4e+03      |
------------------------------------------
Eval num_timesteps=62000, episode_reward=-140.20 +/- 156.12
Episode length: 15.92 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-200.79 +/- 151.10
Episode length: 15.20 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=-182.69 +/- 154.04
Episode length: 15.98 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 460      |
|    iterations      | 31       |
|    time_elapsed    | 137      |
|    total_timesteps | 63488    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=63500, episode_reward=-145.71 +/- 152.23
Episode length: 16.54 +/- 5.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -146         |
| time/                   |              |
|    total_timesteps      | 63500        |
| train/                  |              |
|    approx_kl            | 0.0051439316 |
|    clip_fraction        | 0.0513       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.459       |
|    explained_variance   | -0.0155      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.92e+03     |
|    n_updates            | 2101         |
|    policy_gradient_loss | 0.000117     |
|    value_loss           | 1.01e+04     |
------------------------------------------
Eval num_timesteps=64000, episode_reward=-151.72 +/- 159.14
Episode length: 15.74 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-160.98 +/- 145.07
Episode length: 16.60 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-148.10 +/- 128.24
Episode length: 16.32 +/- 4.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=-162.55 +/- 131.25
Episode length: 16.10 +/- 4.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -204     |
| time/              |          |
|    fps             | 458      |
|    iterations      | 32       |
|    time_elapsed    | 142      |
|    total_timesteps | 65536    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=66000, episode_reward=-159.25 +/- 170.47
Episode length: 15.86 +/- 5.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -159         |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0047953334 |
|    clip_fraction        | 0.0567       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.588       |
|    explained_variance   | -0.0169      |
|    learning_rate        | 0.0001       |
|    loss                 | 5.9e+03      |
|    n_updates            | 2102         |
|    policy_gradient_loss | 0.000397     |
|    value_loss           | 9.23e+03     |
------------------------------------------
Eval num_timesteps=66500, episode_reward=-144.74 +/- 144.11
Episode length: 16.56 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-150.78 +/- 177.39
Episode length: 16.54 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=-134.93 +/- 156.17
Episode length: 16.06 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 459      |
|    iterations      | 33       |
|    time_elapsed    | 147      |
|    total_timesteps | 67584    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=68000, episode_reward=-164.33 +/- 143.25
Episode length: 15.74 +/- 4.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -164         |
| time/                   |              |
|    total_timesteps      | 68000        |
| train/                  |              |
|    approx_kl            | 0.0027872636 |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.399       |
|    explained_variance   | -0.0192      |
|    learning_rate        | 0.0001       |
|    loss                 | 3.72e+03     |
|    n_updates            | 2103         |
|    policy_gradient_loss | 8.22e-05     |
|    value_loss           | 8.8e+03      |
------------------------------------------
Eval num_timesteps=68500, episode_reward=-118.05 +/- 157.69
Episode length: 15.76 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-132.78 +/- 136.12
Episode length: 16.54 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=-172.45 +/- 120.42
Episode length: 15.18 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -189     |
| time/              |          |
|    fps             | 460      |
|    iterations      | 34       |
|    time_elapsed    | 151      |
|    total_timesteps | 69632    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=70000, episode_reward=-112.32 +/- 147.70
Episode length: 16.80 +/- 5.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | -112        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.006503956 |
|    clip_fraction        | 0.0461      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.476      |
|    explained_variance   | -0.014      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.87e+03    |
|    n_updates            | 2105        |
|    policy_gradient_loss | 0.00595     |
|    value_loss           | 8.49e+03    |
-----------------------------------------
Eval num_timesteps=70500, episode_reward=-153.52 +/- 169.39
Episode length: 16.04 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-183.67 +/- 176.72
Episode length: 15.48 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=-191.82 +/- 129.61
Episode length: 15.28 +/- 4.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -222     |
| time/              |          |
|    fps             | 461      |
|    iterations      | 35       |
|    time_elapsed    | 155      |
|    total_timesteps | 71680    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=72000, episode_reward=-162.23 +/- 156.67
Episode length: 15.32 +/- 5.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | -162        |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.008675898 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.497      |
|    explained_variance   | -0.0165     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.66e+03    |
|    n_updates            | 2106        |
|    policy_gradient_loss | 0.000438    |
|    value_loss           | 9.47e+03    |
-----------------------------------------
Eval num_timesteps=72500, episode_reward=-152.10 +/- 156.08
Episode length: 15.62 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-125.96 +/- 164.10
Episode length: 16.80 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=-162.28 +/- 172.89
Episode length: 15.96 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 462      |
|    iterations      | 36       |
|    time_elapsed    | 159      |
|    total_timesteps | 73728    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=74000, episode_reward=-158.86 +/- 151.87
Episode length: 16.14 +/- 4.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -159        |
| time/                   |             |
|    total_timesteps      | 74000       |
| train/                  |             |
|    approx_kl            | 0.003327796 |
|    clip_fraction        | 0.0357      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.454      |
|    explained_variance   | -0.0161     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.96e+03    |
|    n_updates            | 2107        |
|    policy_gradient_loss | 0.00412     |
|    value_loss           | 8.94e+03    |
-----------------------------------------
Eval num_timesteps=74500, episode_reward=-144.44 +/- 154.96
Episode length: 16.60 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-152.28 +/- 132.49
Episode length: 16.48 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=-150.76 +/- 164.12
Episode length: 16.06 +/- 5.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 463      |
|    iterations      | 37       |
|    time_elapsed    | 163      |
|    total_timesteps | 75776    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=76000, episode_reward=-152.17 +/- 148.33
Episode length: 15.98 +/- 4.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -152        |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.006120284 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.276      |
|    explained_variance   | -0.0121     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.7e+03     |
|    n_updates            | 2108        |
|    policy_gradient_loss | -0.000632   |
|    value_loss           | 9.04e+03    |
-----------------------------------------
Eval num_timesteps=76500, episode_reward=-162.04 +/- 139.23
Episode length: 15.40 +/- 4.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-151.44 +/- 166.18
Episode length: 15.56 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=-152.14 +/- 125.46
Episode length: 15.44 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 464      |
|    iterations      | 38       |
|    time_elapsed    | 167      |
|    total_timesteps | 77824    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=78000, episode_reward=-217.35 +/- 127.75
Episode length: 15.14 +/- 3.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.1         |
|    mean_reward          | -217         |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 0.0049172407 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.259       |
|    explained_variance   | -0.00725     |
|    learning_rate        | 0.0001       |
|    loss                 | 4.1e+03      |
|    n_updates            | 2110         |
|    policy_gradient_loss | 0.00668      |
|    value_loss           | 9.58e+03     |
------------------------------------------
Eval num_timesteps=78500, episode_reward=-170.95 +/- 136.73
Episode length: 15.28 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-141.82 +/- 157.55
Episode length: 16.90 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=-152.32 +/- 150.80
Episode length: 15.16 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 464      |
|    iterations      | 39       |
|    time_elapsed    | 171      |
|    total_timesteps | 79872    |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=80000, episode_reward=-166.45 +/- 177.86
Episode length: 16.22 +/- 5.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -166        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.011107285 |
|    clip_fraction        | 0.0399      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | -0.0074     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.65e+03    |
|    n_updates            | 2112        |
|    policy_gradient_loss | 0.0018      |
|    value_loss           | 9.51e+03    |
-----------------------------------------
Eval num_timesteps=80500, episode_reward=-139.42 +/- 163.18
Episode length: 16.00 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-158.96 +/- 122.93
Episode length: 16.34 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=-178.59 +/- 129.89
Episode length: 15.10 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -179     |
| time/              |          |
|    fps             | 465      |
|    iterations      | 40       |
|    time_elapsed    | 175      |
|    total_timesteps | 81920    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=82000, episode_reward=-180.33 +/- 117.17
Episode length: 14.94 +/- 3.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.9         |
|    mean_reward          | -180         |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 0.0054331208 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.323       |
|    explained_variance   | -0.00889     |
|    learning_rate        | 0.0001       |
|    loss                 | 5.94e+03     |
|    n_updates            | 2113         |
|    policy_gradient_loss | 0.00637      |
|    value_loss           | 9.59e+03     |
------------------------------------------
Eval num_timesteps=82500, episode_reward=-157.39 +/- 111.27
Episode length: 16.22 +/- 4.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-147.14 +/- 148.49
Episode length: 16.12 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=-145.60 +/- 146.30
Episode length: 16.06 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 466      |
|    iterations      | 41       |
|    time_elapsed    | 180      |
|    total_timesteps | 83968    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=84000, episode_reward=-109.19 +/- 162.08
Episode length: 16.80 +/- 5.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -109         |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0036275166 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.401       |
|    explained_variance   | -0.00779     |
|    learning_rate        | 0.0001       |
|    loss                 | 4.72e+03     |
|    n_updates            | 2114         |
|    policy_gradient_loss | 0.00208      |
|    value_loss           | 1.02e+04     |
------------------------------------------
Eval num_timesteps=84500, episode_reward=-135.42 +/- 150.06
Episode length: 15.86 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-175.19 +/- 141.25
Episode length: 15.94 +/- 4.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=-182.37 +/- 130.24
Episode length: 15.16 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-187.67 +/- 149.16
Episode length: 14.66 +/- 4.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -209     |
| time/              |          |
|    fps             | 465      |
|    iterations      | 42       |
|    time_elapsed    | 184      |
|    total_timesteps | 86016    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=86500, episode_reward=-161.75 +/- 122.11
Episode length: 15.72 +/- 3.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.7       |
|    mean_reward          | -162       |
| time/                   |            |
|    total_timesteps      | 86500      |
| train/                  |            |
|    approx_kl            | 0.00982321 |
|    clip_fraction        | 0.067      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.637     |
|    explained_variance   | -0.00574   |
|    learning_rate        | 0.0001     |
|    loss                 | 5.79e+03   |
|    n_updates            | 2115       |
|    policy_gradient_loss | 0.005      |
|    value_loss           | 1.04e+04   |
----------------------------------------
Eval num_timesteps=87000, episode_reward=-173.68 +/- 136.02
Episode length: 15.42 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-159.24 +/- 173.40
Episode length: 15.00 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=-216.27 +/- 139.45
Episode length: 14.52 +/- 3.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -206     |
| time/              |          |
|    fps             | 466      |
|    iterations      | 43       |
|    time_elapsed    | 188      |
|    total_timesteps | 88064    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=88500, episode_reward=-140.21 +/- 140.32
Episode length: 16.64 +/- 4.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -140         |
| time/                   |              |
|    total_timesteps      | 88500        |
| train/                  |              |
|    approx_kl            | 0.0045924513 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.62        |
|    explained_variance   | -0.00865     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.81e+03     |
|    n_updates            | 2116         |
|    policy_gradient_loss | -0.000554    |
|    value_loss           | 8.13e+03     |
------------------------------------------
Eval num_timesteps=89000, episode_reward=-138.95 +/- 132.11
Episode length: 16.34 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-168.89 +/- 127.31
Episode length: 15.34 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-183.48 +/- 152.87
Episode length: 15.42 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -191     |
| time/              |          |
|    fps             | 467      |
|    iterations      | 44       |
|    time_elapsed    | 192      |
|    total_timesteps | 90112    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=90500, episode_reward=-139.07 +/- 162.66
Episode length: 15.96 +/- 5.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -139        |
| time/                   |             |
|    total_timesteps      | 90500       |
| train/                  |             |
|    approx_kl            | 0.005191584 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.528      |
|    explained_variance   | -0.0115     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.53e+03    |
|    n_updates            | 2117        |
|    policy_gradient_loss | 0.00261     |
|    value_loss           | 8.99e+03    |
-----------------------------------------
Eval num_timesteps=91000, episode_reward=-130.45 +/- 162.15
Episode length: 15.96 +/- 5.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-160.39 +/- 136.06
Episode length: 15.42 +/- 5.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=-124.86 +/- 140.23
Episode length: 16.14 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 467      |
|    iterations      | 45       |
|    time_elapsed    | 197      |
|    total_timesteps | 92160    |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=92500, episode_reward=-145.90 +/- 165.71
Episode length: 16.04 +/- 4.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -146        |
| time/                   |             |
|    total_timesteps      | 92500       |
| train/                  |             |
|    approx_kl            | 0.008983523 |
|    clip_fraction        | 0.0433      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.42       |
|    explained_variance   | -0.00717    |
|    learning_rate        | 0.0001      |
|    loss                 | 4.52e+03    |
|    n_updates            | 2120        |
|    policy_gradient_loss | 0.00201     |
|    value_loss           | 8.72e+03    |
-----------------------------------------
Eval num_timesteps=93000, episode_reward=-165.42 +/- 163.00
Episode length: 15.78 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-132.92 +/- 162.31
Episode length: 16.24 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=-170.71 +/- 145.76
Episode length: 15.02 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -209     |
| time/              |          |
|    fps             | 468      |
|    iterations      | 46       |
|    time_elapsed    | 201      |
|    total_timesteps | 94208    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=94500, episode_reward=-139.04 +/- 160.59
Episode length: 16.28 +/- 5.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -139         |
| time/                   |              |
|    total_timesteps      | 94500        |
| train/                  |              |
|    approx_kl            | 0.0048212954 |
|    clip_fraction        | 0.0617       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.583       |
|    explained_variance   | -0.00484     |
|    learning_rate        | 0.0001       |
|    loss                 | 5.95e+03     |
|    n_updates            | 2121         |
|    policy_gradient_loss | 0.00133      |
|    value_loss           | 1.01e+04     |
------------------------------------------
Eval num_timesteps=95000, episode_reward=-120.51 +/- 185.88
Episode length: 17.06 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-144.83 +/- 177.29
Episode length: 16.12 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=-166.53 +/- 140.35
Episode length: 15.76 +/- 4.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 468      |
|    iterations      | 47       |
|    time_elapsed    | 205      |
|    total_timesteps | 96256    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=96500, episode_reward=-158.59 +/- 124.18
Episode length: 15.94 +/- 4.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -159         |
| time/                   |              |
|    total_timesteps      | 96500        |
| train/                  |              |
|    approx_kl            | 0.0044696787 |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.407       |
|    explained_variance   | -0.00417     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.78e+03     |
|    n_updates            | 2122         |
|    policy_gradient_loss | 0.0013       |
|    value_loss           | 8.1e+03      |
------------------------------------------
Eval num_timesteps=97000, episode_reward=-151.81 +/- 142.85
Episode length: 15.40 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-171.46 +/- 131.16
Episode length: 15.34 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=-144.17 +/- 173.65
Episode length: 15.48 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -188     |
| time/              |          |
|    fps             | 469      |
|    iterations      | 48       |
|    time_elapsed    | 209      |
|    total_timesteps | 98304    |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=98500, episode_reward=-145.37 +/- 166.04
Episode length: 16.30 +/- 5.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -145         |
| time/                   |              |
|    total_timesteps      | 98500        |
| train/                  |              |
|    approx_kl            | 0.0029324936 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.516       |
|    explained_variance   | -0.00914     |
|    learning_rate        | 0.0001       |
|    loss                 | 5.48e+03     |
|    n_updates            | 2123         |
|    policy_gradient_loss | 2.64e-05     |
|    value_loss           | 8.91e+03     |
------------------------------------------
Eval num_timesteps=99000, episode_reward=-195.92 +/- 146.09
Episode length: 14.96 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-148.60 +/- 153.87
Episode length: 15.58 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-156.37 +/- 163.61
Episode length: 15.68 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 469      |
|    iterations      | 49       |
|    time_elapsed    | 213      |
|    total_timesteps | 100352   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=100500, episode_reward=-156.97 +/- 156.77
Episode length: 15.52 +/- 5.02
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.5       |
|    mean_reward          | -157       |
| time/                   |            |
|    total_timesteps      | 100500     |
| train/                  |            |
|    approx_kl            | 0.00483756 |
|    clip_fraction        | 0.0463     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.393     |
|    explained_variance   | -0.00389   |
|    learning_rate        | 0.0001     |
|    loss                 | 4.79e+03   |
|    n_updates            | 2124       |
|    policy_gradient_loss | 0.00386    |
|    value_loss           | 1.07e+04   |
----------------------------------------
Eval num_timesteps=101000, episode_reward=-141.50 +/- 172.39
Episode length: 16.18 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-171.92 +/- 126.58
Episode length: 15.78 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=-154.83 +/- 135.87
Episode length: 15.88 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 470      |
|    iterations      | 50       |
|    time_elapsed    | 217      |
|    total_timesteps | 102400   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=102500, episode_reward=-164.89 +/- 181.33
Episode length: 15.64 +/- 5.29
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -165        |
| time/                   |             |
|    total_timesteps      | 102500      |
| train/                  |             |
|    approx_kl            | 0.006950264 |
|    clip_fraction        | 0.0609      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.565      |
|    explained_variance   | -0.00145    |
|    learning_rate        | 0.0001      |
|    loss                 | 3.81e+03    |
|    n_updates            | 2125        |
|    policy_gradient_loss | 0.00212     |
|    value_loss           | 9.95e+03    |
-----------------------------------------
Eval num_timesteps=103000, episode_reward=-160.28 +/- 129.91
Episode length: 14.56 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-163.83 +/- 134.98
Episode length: 15.34 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=-135.17 +/- 130.02
Episode length: 16.38 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -222     |
| time/              |          |
|    fps             | 470      |
|    iterations      | 51       |
|    time_elapsed    | 221      |
|    total_timesteps | 104448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=104500, episode_reward=-125.15 +/- 118.46
Episode length: 16.10 +/- 4.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -125        |
| time/                   |             |
|    total_timesteps      | 104500      |
| train/                  |             |
|    approx_kl            | 0.006759923 |
|    clip_fraction        | 0.0953      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.561      |
|    explained_variance   | -0.00415    |
|    learning_rate        | 0.0001      |
|    loss                 | 4.35e+03    |
|    n_updates            | 2126        |
|    policy_gradient_loss | 0.00893     |
|    value_loss           | 1.02e+04    |
-----------------------------------------
Eval num_timesteps=105000, episode_reward=-111.41 +/- 186.13
Episode length: 16.02 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-109.45 +/- 153.29
Episode length: 16.72 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=-150.23 +/- 159.51
Episode length: 16.36 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -201     |
| time/              |          |
|    fps             | 471      |
|    iterations      | 52       |
|    time_elapsed    | 226      |
|    total_timesteps | 106496   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=106500, episode_reward=-152.81 +/- 160.85
Episode length: 15.54 +/- 4.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -153         |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0113032935 |
|    clip_fraction        | 0.119        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.439       |
|    explained_variance   | -0.0085      |
|    learning_rate        | 0.0001       |
|    loss                 | 3.71e+03     |
|    n_updates            | 2127         |
|    policy_gradient_loss | -0.000623    |
|    value_loss           | 7.75e+03     |
------------------------------------------
Eval num_timesteps=107000, episode_reward=-127.34 +/- 104.20
Episode length: 16.38 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-164.19 +/- 128.52
Episode length: 15.22 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-169.27 +/- 153.46
Episode length: 16.28 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=-155.90 +/- 151.27
Episode length: 15.72 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 470      |
|    iterations      | 53       |
|    time_elapsed    | 230      |
|    total_timesteps | 108544   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=109000, episode_reward=-152.93 +/- 162.93
Episode length: 15.54 +/- 5.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -153        |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.003174065 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.381      |
|    explained_variance   | -0.00451    |
|    learning_rate        | 0.0001      |
|    loss                 | 4.58e+03    |
|    n_updates            | 2128        |
|    policy_gradient_loss | -0.0012     |
|    value_loss           | 1.04e+04    |
-----------------------------------------
Eval num_timesteps=109500, episode_reward=-149.70 +/- 141.20
Episode length: 16.50 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-159.23 +/- 124.81
Episode length: 15.16 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=-143.27 +/- 148.23
Episode length: 15.98 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -189     |
| time/              |          |
|    fps             | 470      |
|    iterations      | 54       |
|    time_elapsed    | 234      |
|    total_timesteps | 110592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=111000, episode_reward=-165.66 +/- 161.34
Episode length: 15.18 +/- 5.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | -166        |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.006370807 |
|    clip_fraction        | 0.0696      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.462      |
|    explained_variance   | -9.3e-06    |
|    learning_rate        | 0.0001      |
|    loss                 | 4.45e+03    |
|    n_updates            | 2129        |
|    policy_gradient_loss | 0.00455     |
|    value_loss           | 8.02e+03    |
-----------------------------------------
Eval num_timesteps=111500, episode_reward=-145.68 +/- 141.30
Episode length: 16.34 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-141.32 +/- 160.55
Episode length: 15.26 +/- 5.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=-163.50 +/- 162.96
Episode length: 15.10 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 471      |
|    iterations      | 55       |
|    time_elapsed    | 238      |
|    total_timesteps | 112640   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.04
Eval num_timesteps=113000, episode_reward=-130.86 +/- 138.99
Episode length: 16.04 +/- 4.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -131         |
| time/                   |              |
|    total_timesteps      | 113000       |
| train/                  |              |
|    approx_kl            | 0.0060323733 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.388       |
|    explained_variance   | 0.00328      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.74e+03     |
|    n_updates            | 2130         |
|    policy_gradient_loss | 0.000942     |
|    value_loss           | 8.57e+03     |
------------------------------------------
Eval num_timesteps=113500, episode_reward=-136.96 +/- 180.52
Episode length: 16.00 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-141.48 +/- 174.53
Episode length: 16.70 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=-113.55 +/- 120.13
Episode length: 16.98 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -114     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 471      |
|    iterations      | 56       |
|    time_elapsed    | 243      |
|    total_timesteps | 114688   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=115000, episode_reward=-153.64 +/- 146.91
Episode length: 15.28 +/- 4.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -154         |
| time/                   |              |
|    total_timesteps      | 115000       |
| train/                  |              |
|    approx_kl            | 0.0038940848 |
|    clip_fraction        | 0.0511       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.494       |
|    explained_variance   | 0.00344      |
|    learning_rate        | 0.0001       |
|    loss                 | 3.57e+03     |
|    n_updates            | 2131         |
|    policy_gradient_loss | 0.00128      |
|    value_loss           | 9.13e+03     |
------------------------------------------
Eval num_timesteps=115500, episode_reward=-141.06 +/- 161.79
Episode length: 16.58 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-170.10 +/- 173.80
Episode length: 15.54 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=-118.40 +/- 177.83
Episode length: 16.60 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -203     |
| time/              |          |
|    fps             | 471      |
|    iterations      | 57       |
|    time_elapsed    | 247      |
|    total_timesteps | 116736   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=117000, episode_reward=-138.91 +/- 150.42
Episode length: 15.50 +/- 5.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -139        |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.018067667 |
|    clip_fraction        | 0.0478      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.347      |
|    explained_variance   | 0.00402     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.49e+03    |
|    n_updates            | 2133        |
|    policy_gradient_loss | 0.00119     |
|    value_loss           | 9.89e+03    |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=-141.49 +/- 177.25
Episode length: 15.90 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-180.18 +/- 139.68
Episode length: 15.40 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=-154.83 +/- 172.36
Episode length: 16.10 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 472      |
|    iterations      | 58       |
|    time_elapsed    | 251      |
|    total_timesteps | 118784   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=119000, episode_reward=-191.34 +/- 154.37
Episode length: 15.44 +/- 4.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -191         |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 0.0068740896 |
|    clip_fraction        | 0.0859       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.591       |
|    explained_variance   | 0.00571      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.36e+03     |
|    n_updates            | 2134         |
|    policy_gradient_loss | 0.0184       |
|    value_loss           | 9.9e+03      |
------------------------------------------
Eval num_timesteps=119500, episode_reward=-118.78 +/- 149.36
Episode length: 16.26 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-140.83 +/- 193.33
Episode length: 16.90 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=-131.92 +/- 150.49
Episode length: 16.70 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -224     |
| time/              |          |
|    fps             | 472      |
|    iterations      | 59       |
|    time_elapsed    | 255      |
|    total_timesteps | 120832   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=121000, episode_reward=-136.42 +/- 172.92
Episode length: 16.82 +/- 4.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | -136        |
| time/                   |             |
|    total_timesteps      | 121000      |
| train/                  |             |
|    approx_kl            | 0.006654653 |
|    clip_fraction        | 0.076       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.569      |
|    explained_variance   | 0.00373     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.95e+03    |
|    n_updates            | 2135        |
|    policy_gradient_loss | 0.000557    |
|    value_loss           | 9e+03       |
-----------------------------------------
Eval num_timesteps=121500, episode_reward=-147.20 +/- 159.97
Episode length: 16.22 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-143.77 +/- 154.22
Episode length: 16.10 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=-129.29 +/- 150.91
Episode length: 16.74 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 472      |
|    iterations      | 60       |
|    time_elapsed    | 259      |
|    total_timesteps | 122880   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=123000, episode_reward=-144.95 +/- 167.60
Episode length: 16.48 +/- 5.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -145         |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0059324726 |
|    clip_fraction        | 0.0977       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.461       |
|    explained_variance   | 0.00148      |
|    learning_rate        | 0.0001       |
|    loss                 | 5.52e+03     |
|    n_updates            | 2136         |
|    policy_gradient_loss | -0.0119      |
|    value_loss           | 1.04e+04     |
------------------------------------------
Eval num_timesteps=123500, episode_reward=-134.36 +/- 160.90
Episode length: 16.60 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-156.42 +/- 155.58
Episode length: 15.52 +/- 4.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=-135.03 +/- 165.84
Episode length: 16.50 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 61       |
|    time_elapsed    | 264      |
|    total_timesteps | 124928   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=125000, episode_reward=-172.05 +/- 148.58
Episode length: 15.86 +/- 4.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | -172        |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.004391624 |
|    clip_fraction        | 0.0518      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.387      |
|    explained_variance   | 0.0074      |
|    learning_rate        | 0.0001      |
|    loss                 | 5.35e+03    |
|    n_updates            | 2137        |
|    policy_gradient_loss | 0.00742     |
|    value_loss           | 8.58e+03    |
-----------------------------------------
Eval num_timesteps=125500, episode_reward=-147.98 +/- 120.04
Episode length: 15.62 +/- 3.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-174.16 +/- 135.31
Episode length: 15.16 +/- 4.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=-144.25 +/- 171.21
Episode length: 16.06 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -195     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 62       |
|    time_elapsed    | 268      |
|    total_timesteps | 126976   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=127000, episode_reward=-176.96 +/- 124.30
Episode length: 14.86 +/- 3.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | -177        |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.005587689 |
|    clip_fraction        | 0.0667      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.49       |
|    explained_variance   | 0.00842     |
|    learning_rate        | 0.0001      |
|    loss                 | 3.49e+03    |
|    n_updates            | 2138        |
|    policy_gradient_loss | 0.00624     |
|    value_loss           | 8.04e+03    |
-----------------------------------------
Eval num_timesteps=127500, episode_reward=-157.25 +/- 123.48
Episode length: 14.88 +/- 3.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-170.46 +/- 121.60
Episode length: 14.90 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-158.37 +/- 187.11
Episode length: 15.68 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-123.82 +/- 149.45
Episode length: 16.10 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -191     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 63       |
|    time_elapsed    | 272      |
|    total_timesteps | 129024   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=129500, episode_reward=-143.51 +/- 97.84
Episode length: 16.28 +/- 4.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -144        |
| time/                   |             |
|    total_timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.004897743 |
|    clip_fraction        | 0.0563      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.348      |
|    explained_variance   | 0.00865     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.99e+03    |
|    n_updates            | 2139        |
|    policy_gradient_loss | -0.001      |
|    value_loss           | 8.92e+03    |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=-126.31 +/- 201.87
Episode length: 17.02 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-186.00 +/- 138.54
Episode length: 14.80 +/- 4.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=-152.40 +/- 141.83
Episode length: 14.96 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 64       |
|    time_elapsed    | 276      |
|    total_timesteps | 131072   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=131500, episode_reward=-125.91 +/- 191.53
Episode length: 17.08 +/- 5.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.1         |
|    mean_reward          | -126         |
| time/                   |              |
|    total_timesteps      | 131500       |
| train/                  |              |
|    approx_kl            | 0.0030374292 |
|    clip_fraction        | 0.0337       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.282       |
|    explained_variance   | 0.00939      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.77e+03     |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.00116     |
|    value_loss           | 9.65e+03     |
------------------------------------------
Eval num_timesteps=132000, episode_reward=-139.19 +/- 130.84
Episode length: 15.62 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-125.93 +/- 170.03
Episode length: 16.20 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-172.32 +/- 160.78
Episode length: 15.54 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 65       |
|    time_elapsed    | 280      |
|    total_timesteps | 133120   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=133500, episode_reward=-154.56 +/- 168.80
Episode length: 15.54 +/- 5.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -155         |
| time/                   |              |
|    total_timesteps      | 133500       |
| train/                  |              |
|    approx_kl            | 0.0072119962 |
|    clip_fraction        | 0.0672       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.358       |
|    explained_variance   | 0.0114       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.45e+03     |
|    n_updates            | 2142         |
|    policy_gradient_loss | -0.000608    |
|    value_loss           | 9.5e+03      |
------------------------------------------
Eval num_timesteps=134000, episode_reward=-176.22 +/- 129.43
Episode length: 15.04 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-123.73 +/- 183.74
Episode length: 16.88 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=-117.88 +/- 175.80
Episode length: 17.46 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -189     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 66       |
|    time_elapsed    | 285      |
|    total_timesteps | 135168   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=135500, episode_reward=-111.73 +/- 180.12
Episode length: 17.06 +/- 6.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.1        |
|    mean_reward          | -112        |
| time/                   |             |
|    total_timesteps      | 135500      |
| train/                  |             |
|    approx_kl            | 0.004957921 |
|    clip_fraction        | 0.0482      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.517      |
|    explained_variance   | 0.0123      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.48e+03    |
|    n_updates            | 2143        |
|    policy_gradient_loss | 0.00186     |
|    value_loss           | 9.18e+03    |
-----------------------------------------
Eval num_timesteps=136000, episode_reward=-158.11 +/- 145.70
Episode length: 15.54 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-152.15 +/- 154.54
Episode length: 15.52 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-112.55 +/- 196.41
Episode length: 16.86 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 67       |
|    time_elapsed    | 289      |
|    total_timesteps | 137216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=137500, episode_reward=-160.80 +/- 108.37
Episode length: 14.84 +/- 3.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -161         |
| time/                   |              |
|    total_timesteps      | 137500       |
| train/                  |              |
|    approx_kl            | 0.0056539034 |
|    clip_fraction        | 0.0618       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.426       |
|    explained_variance   | 0.0138       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.84e+03     |
|    n_updates            | 2144         |
|    policy_gradient_loss | 4.94e-05     |
|    value_loss           | 8.96e+03     |
------------------------------------------
Eval num_timesteps=138000, episode_reward=-147.72 +/- 132.69
Episode length: 15.92 +/- 4.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-121.60 +/- 183.37
Episode length: 16.64 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=-131.80 +/- 151.78
Episode length: 16.28 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 68       |
|    time_elapsed    | 293      |
|    total_timesteps | 139264   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=139500, episode_reward=-159.59 +/- 118.64
Episode length: 16.14 +/- 4.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -160        |
| time/                   |             |
|    total_timesteps      | 139500      |
| train/                  |             |
|    approx_kl            | 0.010493628 |
|    clip_fraction        | 0.0477      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.417      |
|    explained_variance   | 0.0159      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.62e+03    |
|    n_updates            | 2146        |
|    policy_gradient_loss | 0.0024      |
|    value_loss           | 9.26e+03    |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=-141.34 +/- 138.35
Episode length: 16.02 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-152.41 +/- 152.13
Episode length: 16.22 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-120.71 +/- 218.07
Episode length: 17.52 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -209     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 69       |
|    time_elapsed    | 297      |
|    total_timesteps | 141312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=141500, episode_reward=-179.70 +/- 142.46
Episode length: 16.76 +/- 4.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -180         |
| time/                   |              |
|    total_timesteps      | 141500       |
| train/                  |              |
|    approx_kl            | 0.0072335624 |
|    clip_fraction        | 0.0664       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.537       |
|    explained_variance   | 0.0178       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.37e+03     |
|    n_updates            | 2147         |
|    policy_gradient_loss | 0.0176       |
|    value_loss           | 9.02e+03     |
------------------------------------------
Eval num_timesteps=142000, episode_reward=-152.26 +/- 147.21
Episode length: 16.88 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-203.61 +/- 128.97
Episode length: 15.68 +/- 4.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=-192.54 +/- 138.71
Episode length: 16.08 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -200     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 70       |
|    time_elapsed    | 302      |
|    total_timesteps | 143360   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=143500, episode_reward=-124.36 +/- 186.07
Episode length: 16.68 +/- 5.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -124         |
| time/                   |              |
|    total_timesteps      | 143500       |
| train/                  |              |
|    approx_kl            | 0.0064763497 |
|    clip_fraction        | 0.0898       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.55        |
|    explained_variance   | 0.0153       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.61e+03     |
|    n_updates            | 2148         |
|    policy_gradient_loss | 0.00739      |
|    value_loss           | 7.39e+03     |
------------------------------------------
Eval num_timesteps=144000, episode_reward=-152.00 +/- 99.04
Episode length: 14.80 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-174.80 +/- 154.82
Episode length: 15.84 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-145.45 +/- 147.21
Episode length: 16.32 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 71       |
|    time_elapsed    | 306      |
|    total_timesteps | 145408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=145500, episode_reward=-174.39 +/- 123.16
Episode length: 15.98 +/- 5.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -174         |
| time/                   |              |
|    total_timesteps      | 145500       |
| train/                  |              |
|    approx_kl            | 0.0041866913 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.433       |
|    explained_variance   | 0.0198       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.95e+03     |
|    n_updates            | 2149         |
|    policy_gradient_loss | -0.00195     |
|    value_loss           | 9.26e+03     |
------------------------------------------
Eval num_timesteps=146000, episode_reward=-170.71 +/- 139.99
Episode length: 17.10 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-176.96 +/- 138.72
Episode length: 15.72 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=-187.93 +/- 135.66
Episode length: 16.58 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 72       |
|    time_elapsed    | 310      |
|    total_timesteps | 147456   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=147500, episode_reward=-132.60 +/- 157.85
Episode length: 15.58 +/- 5.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -133        |
| time/                   |             |
|    total_timesteps      | 147500      |
| train/                  |             |
|    approx_kl            | 0.006002425 |
|    clip_fraction        | 0.0914      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.471      |
|    explained_variance   | 0.0172      |
|    learning_rate        | 0.0001      |
|    loss                 | 6.47e+03    |
|    n_updates            | 2150        |
|    policy_gradient_loss | 0.00194     |
|    value_loss           | 9.58e+03    |
-----------------------------------------
Eval num_timesteps=148000, episode_reward=-132.86 +/- 140.61
Episode length: 16.32 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-151.39 +/- 146.48
Episode length: 16.52 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-193.26 +/- 151.07
Episode length: 14.74 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-110.80 +/- 136.51
Episode length: 16.40 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 73       |
|    time_elapsed    | 315      |
|    total_timesteps | 149504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=150000, episode_reward=-142.68 +/- 159.85
Episode length: 16.58 +/- 5.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -143        |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.004474704 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.461      |
|    explained_variance   | 0.0211      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.81e+03    |
|    n_updates            | 2151        |
|    policy_gradient_loss | 0.0026      |
|    value_loss           | 9.27e+03    |
-----------------------------------------
Eval num_timesteps=150500, episode_reward=-125.27 +/- 149.45
Episode length: 17.04 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-145.80 +/- 134.80
Episode length: 14.96 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=-131.73 +/- 172.78
Episode length: 16.42 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 74       |
|    time_elapsed    | 319      |
|    total_timesteps | 151552   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=152000, episode_reward=-167.64 +/- 144.88
Episode length: 15.00 +/- 4.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | -168         |
| time/                   |              |
|    total_timesteps      | 152000       |
| train/                  |              |
|    approx_kl            | 0.0038965882 |
|    clip_fraction        | 0.0604       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.363       |
|    explained_variance   | 0.0243       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.66e+03     |
|    n_updates            | 2152         |
|    policy_gradient_loss | 0.00511      |
|    value_loss           | 8.92e+03     |
------------------------------------------
Eval num_timesteps=152500, episode_reward=-154.94 +/- 118.33
Episode length: 14.88 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-177.83 +/- 155.02
Episode length: 15.28 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=-166.91 +/- 122.94
Episode length: 15.28 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 75       |
|    time_elapsed    | 323      |
|    total_timesteps | 153600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.07
Eval num_timesteps=154000, episode_reward=-154.98 +/- 144.92
Episode length: 16.46 +/- 4.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -155        |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.010610402 |
|    clip_fraction        | 0.0998      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.307      |
|    explained_variance   | 0.0238      |
|    learning_rate        | 0.0001      |
|    loss                 | 4.14e+03    |
|    n_updates            | 2153        |
|    policy_gradient_loss | 0.0125      |
|    value_loss           | 9.42e+03    |
-----------------------------------------
Eval num_timesteps=154500, episode_reward=-156.02 +/- 174.43
Episode length: 15.98 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-152.31 +/- 174.19
Episode length: 16.32 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=-97.85 +/- 146.76
Episode length: 17.70 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -97.8    |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 76       |
|    time_elapsed    | 327      |
|    total_timesteps | 155648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=156000, episode_reward=-174.96 +/- 139.59
Episode length: 17.32 +/- 5.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.3         |
|    mean_reward          | -175         |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0041468823 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.468       |
|    explained_variance   | 0.0295       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.58e+03     |
|    n_updates            | 2154         |
|    policy_gradient_loss | 0.00245      |
|    value_loss           | 8.8e+03      |
------------------------------------------
Eval num_timesteps=156500, episode_reward=-219.26 +/- 131.51
Episode length: 15.86 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-210.85 +/- 134.35
Episode length: 16.32 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=-209.48 +/- 95.01
Episode length: 16.04 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -214     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 77       |
|    time_elapsed    | 331      |
|    total_timesteps | 157696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=158000, episode_reward=-138.24 +/- 134.58
Episode length: 17.22 +/- 4.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | -138         |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0035503975 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.528       |
|    explained_variance   | 0.0396       |
|    learning_rate        | 0.0001       |
|    loss                 | 5.92e+03     |
|    n_updates            | 2155         |
|    policy_gradient_loss | 0.00191      |
|    value_loss           | 9.53e+03     |
------------------------------------------
Eval num_timesteps=158500, episode_reward=-166.52 +/- 137.67
Episode length: 15.94 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=-152.08 +/- 140.21
Episode length: 17.12 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=-153.88 +/- 144.07
Episode length: 17.70 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -222     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 78       |
|    time_elapsed    | 335      |
|    total_timesteps | 159744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=160000, episode_reward=-191.38 +/- 115.14
Episode length: 15.94 +/- 4.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | -191        |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.005360626 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.553      |
|    explained_variance   | 0.0408      |
|    learning_rate        | 0.0001      |
|    loss                 | 4.73e+03    |
|    n_updates            | 2156        |
|    policy_gradient_loss | 0.000966    |
|    value_loss           | 9.86e+03    |
-----------------------------------------
Eval num_timesteps=160500, episode_reward=-198.54 +/- 151.04
Episode length: 16.32 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-190.27 +/- 153.93
Episode length: 16.86 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=-197.39 +/- 135.28
Episode length: 16.20 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 79       |
|    time_elapsed    | 340      |
|    total_timesteps | 161792   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=162000, episode_reward=-140.12 +/- 148.46
Episode length: 16.02 +/- 4.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -140        |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.004899336 |
|    clip_fraction        | 0.0651      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.463      |
|    explained_variance   | 0.0433      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.18e+03    |
|    n_updates            | 2157        |
|    policy_gradient_loss | -0.00303    |
|    value_loss           | 7.63e+03    |
-----------------------------------------
Eval num_timesteps=162500, episode_reward=-134.80 +/- 170.69
Episode length: 17.12 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-128.79 +/- 126.89
Episode length: 16.22 +/- 4.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=-192.05 +/- 144.56
Episode length: 15.26 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -181     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 80       |
|    time_elapsed    | 344      |
|    total_timesteps | 163840   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=164000, episode_reward=-163.58 +/- 108.78
Episode length: 14.62 +/- 3.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | -164         |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0070725027 |
|    clip_fraction        | 0.0807       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.475       |
|    explained_variance   | 0.0425       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.65e+03     |
|    n_updates            | 2158         |
|    policy_gradient_loss | -0.00384     |
|    value_loss           | 9.09e+03     |
------------------------------------------
Eval num_timesteps=164500, episode_reward=-131.39 +/- 145.57
Episode length: 16.94 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-153.37 +/- 128.88
Episode length: 15.94 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=-185.36 +/- 130.59
Episode length: 14.80 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 81       |
|    time_elapsed    | 348      |
|    total_timesteps | 165888   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=166000, episode_reward=-156.16 +/- 139.34
Episode length: 15.36 +/- 4.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | -156        |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.008378905 |
|    clip_fraction        | 0.0922      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.497      |
|    explained_variance   | 0.0424      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.49e+03    |
|    n_updates            | 2160        |
|    policy_gradient_loss | 0.00214     |
|    value_loss           | 8.39e+03    |
-----------------------------------------
Eval num_timesteps=166500, episode_reward=-158.30 +/- 150.14
Episode length: 16.08 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-137.42 +/- 121.77
Episode length: 15.68 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=-114.99 +/- 145.54
Episode length: 16.58 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 82       |
|    time_elapsed    | 352      |
|    total_timesteps | 167936   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=168000, episode_reward=-155.80 +/- 167.82
Episode length: 16.00 +/- 5.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -156        |
| time/                   |             |
|    total_timesteps      | 168000      |
| train/                  |             |
|    approx_kl            | 0.006897073 |
|    clip_fraction        | 0.0641      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.49       |
|    explained_variance   | 0.0454      |
|    learning_rate        | 0.0001      |
|    loss                 | 4.97e+03    |
|    n_updates            | 2162        |
|    policy_gradient_loss | 0.000622    |
|    value_loss           | 9.02e+03    |
-----------------------------------------
Eval num_timesteps=168500, episode_reward=-115.55 +/- 130.57
Episode length: 17.62 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-174.32 +/- 149.96
Episode length: 14.92 +/- 4.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=-133.45 +/- 142.38
Episode length: 15.62 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | -212     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 83       |
|    time_elapsed    | 356      |
|    total_timesteps | 169984   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=170000, episode_reward=-181.58 +/- 114.38
Episode length: 15.00 +/- 4.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | -182        |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.004981663 |
|    clip_fraction        | 0.0807      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.458      |
|    explained_variance   | 0.0519      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.78e+03    |
|    n_updates            | 2164        |
|    policy_gradient_loss | 0.00552     |
|    value_loss           | 9.73e+03    |
-----------------------------------------
Eval num_timesteps=170500, episode_reward=-149.66 +/- 153.49
Episode length: 15.46 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=-169.36 +/- 153.84
Episode length: 15.86 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=-156.02 +/- 160.38
Episode length: 16.92 +/- 5.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-149.05 +/- 162.20
Episode length: 16.16 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 84       |
|    time_elapsed    | 361      |
|    total_timesteps | 172032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=172500, episode_reward=-134.26 +/- 110.26
Episode length: 16.60 +/- 4.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -134         |
| time/                   |              |
|    total_timesteps      | 172500       |
| train/                  |              |
|    approx_kl            | 0.0043686866 |
|    clip_fraction        | 0.0561       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.483       |
|    explained_variance   | 0.0478       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.55e+03     |
|    n_updates            | 2165         |
|    policy_gradient_loss | 0.00097      |
|    value_loss           | 8.78e+03     |
------------------------------------------
Eval num_timesteps=173000, episode_reward=-173.67 +/- 139.83
Episode length: 15.38 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-175.64 +/- 149.68
Episode length: 15.20 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=-166.96 +/- 137.88
Episode length: 14.50 +/- 3.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 85       |
|    time_elapsed    | 365      |
|    total_timesteps | 174080   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=174500, episode_reward=-166.98 +/- 130.39
Episode length: 17.18 +/- 4.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | -167         |
| time/                   |              |
|    total_timesteps      | 174500       |
| train/                  |              |
|    approx_kl            | 0.0051574437 |
|    clip_fraction        | 0.0656       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.388       |
|    explained_variance   | 0.0586       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.8e+03      |
|    n_updates            | 2166         |
|    policy_gradient_loss | -0.00146     |
|    value_loss           | 9.01e+03     |
------------------------------------------
Eval num_timesteps=175000, episode_reward=-186.36 +/- 139.63
Episode length: 16.96 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-218.92 +/- 134.29
Episode length: 16.06 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=-159.82 +/- 123.75
Episode length: 17.28 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 86       |
|    time_elapsed    | 369      |
|    total_timesteps | 176128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=176500, episode_reward=-150.32 +/- 137.81
Episode length: 14.88 +/- 4.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.9        |
|    mean_reward          | -150        |
| time/                   |             |
|    total_timesteps      | 176500      |
| train/                  |             |
|    approx_kl            | 0.005984293 |
|    clip_fraction        | 0.0926      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.379      |
|    explained_variance   | 0.065       |
|    learning_rate        | 0.0001      |
|    loss                 | 7.28e+03    |
|    n_updates            | 2167        |
|    policy_gradient_loss | 0.00481     |
|    value_loss           | 1.1e+04     |
-----------------------------------------
Eval num_timesteps=177000, episode_reward=-138.34 +/- 137.55
Episode length: 16.54 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-161.00 +/- 119.03
Episode length: 15.46 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=-123.56 +/- 176.68
Episode length: 17.74 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 87       |
|    time_elapsed    | 374      |
|    total_timesteps | 178176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=178500, episode_reward=-164.86 +/- 131.93
Episode length: 16.28 +/- 5.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -165         |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0059904614 |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.382       |
|    explained_variance   | 0.0714       |
|    learning_rate        | 0.0001       |
|    loss                 | 4.4e+03      |
|    n_updates            | 2168         |
|    policy_gradient_loss | 0.0117       |
|    value_loss           | 8.46e+03     |
------------------------------------------
Eval num_timesteps=179000, episode_reward=-188.38 +/- 128.00
Episode length: 16.72 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-182.46 +/- 151.04
Episode length: 15.92 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-203.58 +/- 151.88
Episode length: 15.90 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 88       |
|    time_elapsed    | 378      |
|    total_timesteps | 180224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=180500, episode_reward=-163.62 +/- 168.73
Episode length: 15.18 +/- 5.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | -164        |
| time/                   |             |
|    total_timesteps      | 180500      |
| train/                  |             |
|    approx_kl            | 0.005201291 |
|    clip_fraction        | 0.0503      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.403      |
|    explained_variance   | 0.0761      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.74e+03    |
|    n_updates            | 2169        |
|    policy_gradient_loss | -0.0056     |
|    value_loss           | 8.85e+03    |
-----------------------------------------
Eval num_timesteps=181000, episode_reward=-167.36 +/- 132.41
Episode length: 15.10 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-125.91 +/- 123.63
Episode length: 17.18 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=-165.14 +/- 138.48
Episode length: 15.32 +/- 4.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -181     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 89       |
|    time_elapsed    | 382      |
|    total_timesteps | 182272   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=182500, episode_reward=-153.29 +/- 146.14
Episode length: 15.54 +/- 4.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -153        |
| time/                   |             |
|    total_timesteps      | 182500      |
| train/                  |             |
|    approx_kl            | 0.003009883 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.348      |
|    explained_variance   | 0.077       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.82e+03    |
|    n_updates            | 2171        |
|    policy_gradient_loss | -0.000976   |
|    value_loss           | 8.87e+03    |
-----------------------------------------
Eval num_timesteps=183000, episode_reward=-159.08 +/- 141.84
Episode length: 16.38 +/- 4.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-163.01 +/- 131.69
Episode length: 15.22 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=-121.79 +/- 141.97
Episode length: 16.86 +/- 4.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 90       |
|    time_elapsed    | 386      |
|    total_timesteps | 184320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=184500, episode_reward=-139.95 +/- 152.17
Episode length: 17.62 +/- 5.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | -140         |
| time/                   |              |
|    total_timesteps      | 184500       |
| train/                  |              |
|    approx_kl            | 0.0054872595 |
|    clip_fraction        | 0.0675       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.369       |
|    explained_variance   | 0.0884       |
|    learning_rate        | 0.0001       |
|    loss                 | 5e+03        |
|    n_updates            | 2172         |
|    policy_gradient_loss | 0.00059      |
|    value_loss           | 9.27e+03     |
------------------------------------------
Eval num_timesteps=185000, episode_reward=-193.55 +/- 143.24
Episode length: 16.26 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=-176.65 +/- 153.36
Episode length: 17.10 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=-173.82 +/- 158.96
Episode length: 17.16 +/- 5.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 91       |
|    time_elapsed    | 390      |
|    total_timesteps | 186368   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=186500, episode_reward=-185.32 +/- 166.32
Episode length: 17.50 +/- 6.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | -185        |
| time/                   |             |
|    total_timesteps      | 186500      |
| train/                  |             |
|    approx_kl            | 0.006517367 |
|    clip_fraction        | 0.0498      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.4        |
|    explained_variance   | 0.0899      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.21e+03    |
|    n_updates            | 2174        |
|    policy_gradient_loss | 0.00264     |
|    value_loss           | 8.74e+03    |
-----------------------------------------
Eval num_timesteps=187000, episode_reward=-200.82 +/- 134.25
Episode length: 15.68 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-181.08 +/- 156.52
Episode length: 16.90 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=-193.84 +/- 127.13
Episode length: 16.66 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 92       |
|    time_elapsed    | 395      |
|    total_timesteps | 188416   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=188500, episode_reward=-191.08 +/- 162.40
Episode length: 16.54 +/- 5.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.5       |
|    mean_reward          | -191       |
| time/                   |            |
|    total_timesteps      | 188500     |
| train/                  |            |
|    approx_kl            | 0.00682824 |
|    clip_fraction        | 0.0893     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.455     |
|    explained_variance   | 0.0854     |
|    learning_rate        | 0.0001     |
|    loss                 | 4.7e+03    |
|    n_updates            | 2175       |
|    policy_gradient_loss | 0.00473    |
|    value_loss           | 9.49e+03   |
----------------------------------------
Eval num_timesteps=189000, episode_reward=-191.11 +/- 116.40
Episode length: 16.40 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-183.47 +/- 112.05
Episode length: 16.64 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-200.81 +/- 151.93
Episode length: 16.76 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 93       |
|    time_elapsed    | 399      |
|    total_timesteps | 190464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=190500, episode_reward=-180.31 +/- 180.99
Episode length: 17.12 +/- 6.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.1         |
|    mean_reward          | -180         |
| time/                   |              |
|    total_timesteps      | 190500       |
| train/                  |              |
|    approx_kl            | 0.0038086544 |
|    clip_fraction        | 0.0359       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.377       |
|    explained_variance   | 0.0975       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.76e+03     |
|    n_updates            | 2176         |
|    policy_gradient_loss | 0.00145      |
|    value_loss           | 8.07e+03     |
------------------------------------------
Eval num_timesteps=191000, episode_reward=-183.65 +/- 139.72
Episode length: 16.22 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-197.46 +/- 149.52
Episode length: 17.02 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-213.57 +/- 134.43
Episode length: 15.82 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-202.49 +/- 127.00
Episode length: 15.82 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 94       |
|    time_elapsed    | 404      |
|    total_timesteps | 192512   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=193000, episode_reward=-171.91 +/- 137.76
Episode length: 16.26 +/- 5.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -172        |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.005045161 |
|    clip_fraction        | 0.0651      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.366      |
|    explained_variance   | 0.103       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.02e+03    |
|    n_updates            | 2177        |
|    policy_gradient_loss | 0.00285     |
|    value_loss           | 8.98e+03    |
-----------------------------------------
Eval num_timesteps=193500, episode_reward=-165.26 +/- 174.68
Episode length: 17.00 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-182.59 +/- 174.21
Episode length: 16.66 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=-176.81 +/- 171.43
Episode length: 17.60 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -222     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 95       |
|    time_elapsed    | 408      |
|    total_timesteps | 194560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=195000, episode_reward=-216.65 +/- 124.63
Episode length: 15.62 +/- 4.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -217         |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 0.0061288583 |
|    clip_fraction        | 0.0922       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.443       |
|    explained_variance   | 0.107        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.14e+03     |
|    n_updates            | 2178         |
|    policy_gradient_loss | -0.0029      |
|    value_loss           | 9.3e+03      |
------------------------------------------
Eval num_timesteps=195500, episode_reward=-197.59 +/- 136.43
Episode length: 16.16 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=-186.32 +/- 131.93
Episode length: 17.34 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=-164.02 +/- 153.60
Episode length: 17.54 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -204     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 96       |
|    time_elapsed    | 412      |
|    total_timesteps | 196608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=197000, episode_reward=-186.36 +/- 175.26
Episode length: 16.62 +/- 5.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -186         |
| time/                   |              |
|    total_timesteps      | 197000       |
| train/                  |              |
|    approx_kl            | 0.0051895767 |
|    clip_fraction        | 0.0605       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.406       |
|    explained_variance   | 0.113        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.28e+03     |
|    n_updates            | 2179         |
|    policy_gradient_loss | -0.00249     |
|    value_loss           | 8.59e+03     |
------------------------------------------
Eval num_timesteps=197500, episode_reward=-175.54 +/- 119.35
Episode length: 15.86 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-197.54 +/- 125.75
Episode length: 15.66 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=-189.72 +/- 129.45
Episode length: 15.46 +/- 4.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -211     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 97       |
|    time_elapsed    | 416      |
|    total_timesteps | 198656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=199000, episode_reward=-169.73 +/- 140.74
Episode length: 16.14 +/- 4.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -170        |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.006207313 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.378      |
|    explained_variance   | 0.117       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.61e+03    |
|    n_updates            | 2180        |
|    policy_gradient_loss | 0.00186     |
|    value_loss           | 9.69e+03    |
-----------------------------------------
Eval num_timesteps=199500, episode_reward=-156.83 +/- 138.37
Episode length: 16.02 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-159.59 +/- 161.32
Episode length: 16.28 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=-158.28 +/- 147.48
Episode length: 16.48 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 98       |
|    time_elapsed    | 421      |
|    total_timesteps | 200704   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=201000, episode_reward=-151.59 +/- 138.62
Episode length: 15.46 +/- 5.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -152        |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.004261154 |
|    clip_fraction        | 0.0318      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.382      |
|    explained_variance   | 0.121       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.76e+03    |
|    n_updates            | 2181        |
|    policy_gradient_loss | 0.00725     |
|    value_loss           | 8.76e+03    |
-----------------------------------------
Eval num_timesteps=201500, episode_reward=-184.54 +/- 165.41
Episode length: 15.18 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-161.54 +/- 151.01
Episode length: 15.62 +/- 4.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=-150.63 +/- 160.10
Episode length: 16.88 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 99       |
|    time_elapsed    | 425      |
|    total_timesteps | 202752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=203000, episode_reward=-174.14 +/- 153.79
Episode length: 17.24 +/- 5.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | -174         |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 0.0060196365 |
|    clip_fraction        | 0.0781       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.467       |
|    explained_variance   | 0.123        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.89e+03     |
|    n_updates            | 2182         |
|    policy_gradient_loss | 0.00537      |
|    value_loss           | 8.45e+03     |
------------------------------------------
Eval num_timesteps=203500, episode_reward=-200.02 +/- 128.50
Episode length: 15.76 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-208.98 +/- 129.10
Episode length: 15.64 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=-166.94 +/- 164.79
Episode length: 16.82 +/- 5.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 100      |
|    time_elapsed    | 429      |
|    total_timesteps | 204800   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=205000, episode_reward=-168.67 +/- 127.61
Episode length: 15.72 +/- 4.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | -169        |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.005892525 |
|    clip_fraction        | 0.0831      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.442      |
|    explained_variance   | 0.136       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.21e+03    |
|    n_updates            | 2184        |
|    policy_gradient_loss | 0.00461     |
|    value_loss           | 7.67e+03    |
-----------------------------------------
Eval num_timesteps=205500, episode_reward=-176.31 +/- 133.97
Episode length: 15.26 +/- 4.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-151.89 +/- 179.92
Episode length: 16.20 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=-157.40 +/- 152.44
Episode length: 15.94 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 101      |
|    time_elapsed    | 433      |
|    total_timesteps | 206848   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=207000, episode_reward=-216.60 +/- 127.29
Episode length: 16.80 +/- 4.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.8       |
|    mean_reward          | -217       |
| time/                   |            |
|    total_timesteps      | 207000     |
| train/                  |            |
|    approx_kl            | 0.01302701 |
|    clip_fraction        | 0.0576     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.392     |
|    explained_variance   | 0.141      |
|    learning_rate        | 0.0001     |
|    loss                 | 3.84e+03   |
|    n_updates            | 2187       |
|    policy_gradient_loss | 0.00339    |
|    value_loss           | 8.32e+03   |
----------------------------------------
Eval num_timesteps=207500, episode_reward=-197.00 +/- 126.66
Episode length: 16.12 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=-192.34 +/- 114.21
Episode length: 16.28 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=-209.83 +/- 104.90
Episode length: 14.98 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 102      |
|    time_elapsed    | 438      |
|    total_timesteps | 208896   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=209000, episode_reward=-164.67 +/- 149.31
Episode length: 15.74 +/- 5.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -165         |
| time/                   |              |
|    total_timesteps      | 209000       |
| train/                  |              |
|    approx_kl            | 0.0061585535 |
|    clip_fraction        | 0.0703       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.397       |
|    explained_variance   | 0.144        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.49e+03     |
|    n_updates            | 2188         |
|    policy_gradient_loss | 0.00137      |
|    value_loss           | 8.34e+03     |
------------------------------------------
Eval num_timesteps=209500, episode_reward=-177.01 +/- 124.37
Episode length: 15.74 +/- 3.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-150.97 +/- 146.22
Episode length: 15.54 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=-174.20 +/- 138.34
Episode length: 15.36 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -188     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 103      |
|    time_elapsed    | 442      |
|    total_timesteps | 210944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=211000, episode_reward=-219.96 +/- 138.07
Episode length: 15.08 +/- 4.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | -220        |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.004255065 |
|    clip_fraction        | 0.0566      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.368      |
|    explained_variance   | 0.15        |
|    learning_rate        | 0.0001      |
|    loss                 | 4.37e+03    |
|    n_updates            | 2189        |
|    policy_gradient_loss | 0.00717     |
|    value_loss           | 7.59e+03    |
-----------------------------------------
Eval num_timesteps=211500, episode_reward=-156.35 +/- 161.28
Episode length: 16.88 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-136.39 +/- 111.29
Episode length: 18.58 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.6     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=-191.62 +/- 119.04
Episode length: 15.98 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 104      |
|    time_elapsed    | 446      |
|    total_timesteps | 212992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=213000, episode_reward=-197.25 +/- 135.23
Episode length: 16.36 +/- 4.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -197         |
| time/                   |              |
|    total_timesteps      | 213000       |
| train/                  |              |
|    approx_kl            | 0.0068912534 |
|    clip_fraction        | 0.0856       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.394       |
|    explained_variance   | 0.157        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.3e+03      |
|    n_updates            | 2190         |
|    policy_gradient_loss | 0.00694      |
|    value_loss           | 8.45e+03     |
------------------------------------------
Eval num_timesteps=213500, episode_reward=-205.16 +/- 125.41
Episode length: 15.42 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=-168.11 +/- 151.58
Episode length: 16.32 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=-185.42 +/- 147.63
Episode length: 16.40 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=-174.21 +/- 163.94
Episode length: 16.28 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 105      |
|    time_elapsed    | 451      |
|    total_timesteps | 215040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=215500, episode_reward=-178.89 +/- 128.05
Episode length: 16.28 +/- 4.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -179         |
| time/                   |              |
|    total_timesteps      | 215500       |
| train/                  |              |
|    approx_kl            | 0.0049555977 |
|    clip_fraction        | 0.0649       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.449       |
|    explained_variance   | 0.169        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.67e+03     |
|    n_updates            | 2191         |
|    policy_gradient_loss | 0.00269      |
|    value_loss           | 7.44e+03     |
------------------------------------------
Eval num_timesteps=216000, episode_reward=-156.30 +/- 104.47
Episode length: 17.28 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=-164.27 +/- 142.57
Episode length: 16.82 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=-132.45 +/- 143.28
Episode length: 17.88 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 106      |
|    time_elapsed    | 455      |
|    total_timesteps | 217088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=217500, episode_reward=-157.28 +/- 149.86
Episode length: 15.28 +/- 4.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -157         |
| time/                   |              |
|    total_timesteps      | 217500       |
| train/                  |              |
|    approx_kl            | 0.0024009426 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.406       |
|    explained_variance   | 0.168        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.9e+03      |
|    n_updates            | 2192         |
|    policy_gradient_loss | -0.00237     |
|    value_loss           | 9.26e+03     |
------------------------------------------
Eval num_timesteps=218000, episode_reward=-144.43 +/- 169.96
Episode length: 16.14 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=-123.94 +/- 173.10
Episode length: 16.82 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=-145.64 +/- 145.56
Episode length: 16.68 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | -142     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 107      |
|    time_elapsed    | 459      |
|    total_timesteps | 219136   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=219500, episode_reward=-199.56 +/- 117.04
Episode length: 14.82 +/- 4.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | -200        |
| time/                   |             |
|    total_timesteps      | 219500      |
| train/                  |             |
|    approx_kl            | 0.006616716 |
|    clip_fraction        | 0.0759      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.358      |
|    explained_variance   | 0.167       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.79e+03    |
|    n_updates            | 2193        |
|    policy_gradient_loss | 0.00692     |
|    value_loss           | 7.11e+03    |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=-118.86 +/- 211.51
Episode length: 16.10 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=-161.51 +/- 150.80
Episode length: 15.06 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=-181.32 +/- 130.47
Episode length: 15.12 +/- 3.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -190     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 108      |
|    time_elapsed    | 463      |
|    total_timesteps | 221184   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=221500, episode_reward=-159.00 +/- 159.45
Episode length: 15.50 +/- 4.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -159        |
| time/                   |             |
|    total_timesteps      | 221500      |
| train/                  |             |
|    approx_kl            | 0.012822976 |
|    clip_fraction        | 0.0582      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.315      |
|    explained_variance   | 0.178       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.46e+03    |
|    n_updates            | 2195        |
|    policy_gradient_loss | 8.78e-05    |
|    value_loss           | 8.11e+03    |
-----------------------------------------
Eval num_timesteps=222000, episode_reward=-139.96 +/- 148.52
Episode length: 15.92 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=-136.71 +/- 173.59
Episode length: 15.78 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=-186.47 +/- 135.21
Episode length: 15.30 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | -147     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 109      |
|    time_elapsed    | 468      |
|    total_timesteps | 223232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=223500, episode_reward=-138.47 +/- 182.61
Episode length: 16.42 +/- 5.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -138         |
| time/                   |              |
|    total_timesteps      | 223500       |
| train/                  |              |
|    approx_kl            | 0.0050134785 |
|    clip_fraction        | 0.0756       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.373       |
|    explained_variance   | 0.192        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.79e+03     |
|    n_updates            | 2196         |
|    policy_gradient_loss | 0.00329      |
|    value_loss           | 7.44e+03     |
------------------------------------------
Eval num_timesteps=224000, episode_reward=-163.26 +/- 133.10
Episode length: 15.74 +/- 4.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=-191.55 +/- 112.64
Episode length: 14.96 +/- 3.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=-127.98 +/- 168.85
Episode length: 16.24 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 110      |
|    time_elapsed    | 472      |
|    total_timesteps | 225280   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=225500, episode_reward=-160.93 +/- 150.94
Episode length: 15.46 +/- 4.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.5        |
|    mean_reward          | -161        |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 0.005924809 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.304      |
|    explained_variance   | 0.171       |
|    learning_rate        | 0.0001      |
|    loss                 | 6.73e+03    |
|    n_updates            | 2197        |
|    policy_gradient_loss | -2.26e-05   |
|    value_loss           | 9.16e+03    |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=-109.33 +/- 168.66
Episode length: 16.96 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=-151.58 +/- 172.32
Episode length: 15.58 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=-185.52 +/- 133.78
Episode length: 14.92 +/- 4.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 111      |
|    time_elapsed    | 476      |
|    total_timesteps | 227328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.05
Eval num_timesteps=227500, episode_reward=-168.30 +/- 132.71
Episode length: 15.60 +/- 4.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -168         |
| time/                   |              |
|    total_timesteps      | 227500       |
| train/                  |              |
|    approx_kl            | 0.0043279855 |
|    clip_fraction        | 0.0383       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.274       |
|    explained_variance   | 0.163        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.39e+03     |
|    n_updates            | 2198         |
|    policy_gradient_loss | 0.00424      |
|    value_loss           | 8.23e+03     |
------------------------------------------
Eval num_timesteps=228000, episode_reward=-98.06 +/- 158.88
Episode length: 18.04 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | -98.1    |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-163.68 +/- 136.84
Episode length: 15.00 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=-125.85 +/- 153.62
Episode length: 16.52 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -177     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 112      |
|    time_elapsed    | 480      |
|    total_timesteps | 229376   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=229500, episode_reward=-157.31 +/- 117.80
Episode length: 15.08 +/- 4.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | -157        |
| time/                   |             |
|    total_timesteps      | 229500      |
| train/                  |             |
|    approx_kl            | 0.006494334 |
|    clip_fraction        | 0.0523      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.342      |
|    explained_variance   | 0.168       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.6e+03     |
|    n_updates            | 2200        |
|    policy_gradient_loss | 0.00606     |
|    value_loss           | 8.38e+03    |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=-139.01 +/- 161.01
Episode length: 16.90 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=-143.00 +/- 131.98
Episode length: 15.50 +/- 4.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=-128.70 +/- 138.94
Episode length: 15.86 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -192     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 113      |
|    time_elapsed    | 484      |
|    total_timesteps | 231424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=231500, episode_reward=-178.17 +/- 160.95
Episode length: 14.58 +/- 4.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | -178         |
| time/                   |              |
|    total_timesteps      | 231500       |
| train/                  |              |
|    approx_kl            | 0.0046547283 |
|    clip_fraction        | 0.0714       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.401       |
|    explained_variance   | 0.203        |
|    learning_rate        | 0.0001       |
|    loss                 | 3e+03        |
|    n_updates            | 2201         |
|    policy_gradient_loss | 0.0019       |
|    value_loss           | 7.17e+03     |
------------------------------------------
Eval num_timesteps=232000, episode_reward=-110.07 +/- 184.11
Episode length: 17.18 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-167.85 +/- 126.54
Episode length: 16.28 +/- 4.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=-137.80 +/- 153.45
Episode length: 16.22 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 114      |
|    time_elapsed    | 488      |
|    total_timesteps | 233472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=233500, episode_reward=-157.61 +/- 136.73
Episode length: 16.56 +/- 5.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -158        |
| time/                   |             |
|    total_timesteps      | 233500      |
| train/                  |             |
|    approx_kl            | 0.005128307 |
|    clip_fraction        | 0.0535      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.376      |
|    explained_variance   | 0.182       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.05e+03    |
|    n_updates            | 2202        |
|    policy_gradient_loss | -0.000382   |
|    value_loss           | 8.08e+03    |
-----------------------------------------
Eval num_timesteps=234000, episode_reward=-149.85 +/- 164.75
Episode length: 17.30 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=-132.15 +/- 150.78
Episode length: 16.82 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=-190.62 +/- 135.11
Episode length: 15.92 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=-175.20 +/- 148.68
Episode length: 15.90 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 115      |
|    time_elapsed    | 493      |
|    total_timesteps | 235520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=236000, episode_reward=-145.96 +/- 157.22
Episode length: 15.80 +/- 5.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -146         |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 0.0044910233 |
|    clip_fraction        | 0.0588       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.377       |
|    explained_variance   | 0.228        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.45e+03     |
|    n_updates            | 2203         |
|    policy_gradient_loss | 0.00388      |
|    value_loss           | 7.88e+03     |
------------------------------------------
Eval num_timesteps=236500, episode_reward=-150.99 +/- 159.86
Episode length: 15.78 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=-141.03 +/- 161.31
Episode length: 15.86 +/- 4.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=-141.14 +/- 147.74
Episode length: 15.76 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 116      |
|    time_elapsed    | 497      |
|    total_timesteps | 237568   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=238000, episode_reward=-133.15 +/- 154.34
Episode length: 16.26 +/- 4.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -133        |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.005120673 |
|    clip_fraction        | 0.0465      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.342      |
|    explained_variance   | 0.19        |
|    learning_rate        | 0.0001      |
|    loss                 | 3.9e+03     |
|    n_updates            | 2205        |
|    policy_gradient_loss | 0.00299     |
|    value_loss           | 8.85e+03    |
-----------------------------------------
Eval num_timesteps=238500, episode_reward=-176.98 +/- 154.02
Episode length: 14.96 +/- 4.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-142.79 +/- 129.65
Episode length: 15.84 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=-122.37 +/- 156.40
Episode length: 16.48 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -174     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 117      |
|    time_elapsed    | 502      |
|    total_timesteps | 239616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=240000, episode_reward=-140.73 +/- 180.95
Episode length: 16.62 +/- 5.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -141         |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0048432914 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.352       |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.13e+03     |
|    n_updates            | 2206         |
|    policy_gradient_loss | 0.00657      |
|    value_loss           | 7.92e+03     |
------------------------------------------
Eval num_timesteps=240500, episode_reward=-154.47 +/- 139.51
Episode length: 15.96 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=-130.09 +/- 137.14
Episode length: 16.42 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=-185.16 +/- 126.87
Episode length: 14.42 +/- 3.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 118      |
|    time_elapsed    | 506      |
|    total_timesteps | 241664   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=242000, episode_reward=-157.01 +/- 170.14
Episode length: 15.68 +/- 4.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | -157        |
| time/                   |             |
|    total_timesteps      | 242000      |
| train/                  |             |
|    approx_kl            | 0.005958421 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.379      |
|    explained_variance   | 0.259       |
|    learning_rate        | 0.0001      |
|    loss                 | 5e+03       |
|    n_updates            | 2208        |
|    policy_gradient_loss | 0.00125     |
|    value_loss           | 7.8e+03     |
-----------------------------------------
Eval num_timesteps=242500, episode_reward=-135.71 +/- 176.56
Episode length: 16.32 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=-136.70 +/- 150.08
Episode length: 16.36 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=-132.12 +/- 151.77
Episode length: 16.58 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 119      |
|    time_elapsed    | 510      |
|    total_timesteps | 243712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=244000, episode_reward=-139.83 +/- 154.68
Episode length: 16.32 +/- 5.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -140         |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0039373855 |
|    clip_fraction        | 0.0397       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.351       |
|    explained_variance   | 0.253        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.28e+03     |
|    n_updates            | 2209         |
|    policy_gradient_loss | 0.00341      |
|    value_loss           | 5.83e+03     |
------------------------------------------
Eval num_timesteps=244500, episode_reward=-148.56 +/- 150.55
Episode length: 16.22 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=-204.29 +/- 129.21
Episode length: 15.60 +/- 3.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=-113.41 +/- 164.83
Episode length: 15.94 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 120      |
|    time_elapsed    | 514      |
|    total_timesteps | 245760   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=246000, episode_reward=-181.43 +/- 122.56
Episode length: 15.90 +/- 4.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.9       |
|    mean_reward          | -181       |
| time/                   |            |
|    total_timesteps      | 246000     |
| train/                  |            |
|    approx_kl            | 0.00466396 |
|    clip_fraction        | 0.069      |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.29      |
|    explained_variance   | 0.231      |
|    learning_rate        | 0.0001     |
|    loss                 | 4.2e+03    |
|    n_updates            | 2210       |
|    policy_gradient_loss | 0.00194    |
|    value_loss           | 7.97e+03   |
----------------------------------------
Eval num_timesteps=246500, episode_reward=-156.15 +/- 168.42
Episode length: 16.56 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=-204.45 +/- 122.13
Episode length: 14.90 +/- 3.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=-180.72 +/- 95.90
Episode length: 15.84 +/- 3.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 121      |
|    time_elapsed    | 518      |
|    total_timesteps | 247808   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=248000, episode_reward=-147.11 +/- 148.63
Episode length: 15.96 +/- 4.88
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -147        |
| time/                   |             |
|    total_timesteps      | 248000      |
| train/                  |             |
|    approx_kl            | 0.007681459 |
|    clip_fraction        | 0.0972      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.301      |
|    explained_variance   | 0.248       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.16e+03    |
|    n_updates            | 2211        |
|    policy_gradient_loss | 0.00592     |
|    value_loss           | 7.09e+03    |
-----------------------------------------
Eval num_timesteps=248500, episode_reward=-169.88 +/- 145.18
Episode length: 15.18 +/- 4.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=-161.58 +/- 127.00
Episode length: 15.54 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=-180.54 +/- 146.97
Episode length: 15.34 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 122      |
|    time_elapsed    | 522      |
|    total_timesteps | 249856   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=250000, episode_reward=-157.71 +/- 146.84
Episode length: 16.68 +/- 5.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -158         |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0042554475 |
|    clip_fraction        | 0.0636       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.317       |
|    explained_variance   | 0.247        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.43e+03     |
|    n_updates            | 2212         |
|    policy_gradient_loss | 0.00651      |
|    value_loss           | 7.66e+03     |
------------------------------------------
Eval num_timesteps=250500, episode_reward=-221.54 +/- 138.39
Episode length: 14.84 +/- 3.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=-140.43 +/- 113.57
Episode length: 16.26 +/- 4.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=-156.56 +/- 135.44
Episode length: 15.40 +/- 4.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -195     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 123      |
|    time_elapsed    | 526      |
|    total_timesteps | 251904   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=252000, episode_reward=-182.46 +/- 144.73
Episode length: 16.00 +/- 4.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -182         |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 0.0048494963 |
|    clip_fraction        | 0.0703       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.335       |
|    explained_variance   | 0.25         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.36e+03     |
|    n_updates            | 2213         |
|    policy_gradient_loss | 0.00302      |
|    value_loss           | 8.07e+03     |
------------------------------------------
Eval num_timesteps=252500, episode_reward=-129.33 +/- 143.12
Episode length: 16.12 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=-135.95 +/- 145.39
Episode length: 16.98 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=-139.28 +/- 128.63
Episode length: 15.80 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -192     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 124      |
|    time_elapsed    | 531      |
|    total_timesteps | 253952   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=254000, episode_reward=-196.13 +/- 135.24
Episode length: 16.26 +/- 4.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -196        |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.005141601 |
|    clip_fraction        | 0.0659      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.323      |
|    explained_variance   | 0.258       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.95e+03    |
|    n_updates            | 2215        |
|    policy_gradient_loss | 0.00485     |
|    value_loss           | 6.81e+03    |
-----------------------------------------
Eval num_timesteps=254500, episode_reward=-193.11 +/- 143.34
Episode length: 17.14 +/- 5.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=-175.87 +/- 148.09
Episode length: 17.86 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=-197.60 +/- 122.40
Episode length: 16.28 +/- 4.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-182.99 +/- 104.22
Episode length: 15.52 +/- 4.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 125      |
|    time_elapsed    | 535      |
|    total_timesteps | 256000   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=256500, episode_reward=-207.51 +/- 130.69
Episode length: 16.42 +/- 4.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -208         |
| time/                   |              |
|    total_timesteps      | 256500       |
| train/                  |              |
|    approx_kl            | 0.0061087366 |
|    clip_fraction        | 0.0731       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.314       |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.11e+03     |
|    n_updates            | 2216         |
|    policy_gradient_loss | 0.00848      |
|    value_loss           | 7.72e+03     |
------------------------------------------
Eval num_timesteps=257000, episode_reward=-178.72 +/- 159.44
Episode length: 17.20 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=-188.60 +/- 122.58
Episode length: 16.52 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=-166.73 +/- 127.32
Episode length: 17.76 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -201     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 126      |
|    time_elapsed    | 540      |
|    total_timesteps | 258048   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=258500, episode_reward=-180.20 +/- 158.70
Episode length: 17.04 +/- 5.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | -180        |
| time/                   |             |
|    total_timesteps      | 258500      |
| train/                  |             |
|    approx_kl            | 0.004577396 |
|    clip_fraction        | 0.0579      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.315      |
|    explained_variance   | 0.259       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.52e+03    |
|    n_updates            | 2219        |
|    policy_gradient_loss | 0.00269     |
|    value_loss           | 6.89e+03    |
-----------------------------------------
Eval num_timesteps=259000, episode_reward=-157.25 +/- 139.71
Episode length: 17.16 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=-179.04 +/- 127.17
Episode length: 17.26 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-192.17 +/- 130.51
Episode length: 16.42 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -201     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 127      |
|    time_elapsed    | 544      |
|    total_timesteps | 260096   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=260500, episode_reward=-194.20 +/- 132.90
Episode length: 15.74 +/- 4.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | -194        |
| time/                   |             |
|    total_timesteps      | 260500      |
| train/                  |             |
|    approx_kl            | 0.006698754 |
|    clip_fraction        | 0.0692      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.306      |
|    explained_variance   | 0.262       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.89e+03    |
|    n_updates            | 2220        |
|    policy_gradient_loss | 0.00247     |
|    value_loss           | 7.47e+03    |
-----------------------------------------
Eval num_timesteps=261000, episode_reward=-176.27 +/- 131.83
Episode length: 16.22 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=-170.41 +/- 180.46
Episode length: 16.64 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=-190.44 +/- 149.99
Episode length: 16.84 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 128      |
|    time_elapsed    | 548      |
|    total_timesteps | 262144   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=262500, episode_reward=-166.03 +/- 168.02
Episode length: 15.06 +/- 4.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.1        |
|    mean_reward          | -166        |
| time/                   |             |
|    total_timesteps      | 262500      |
| train/                  |             |
|    approx_kl            | 0.005078357 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.357      |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.0001      |
|    loss                 | 3.67e+03    |
|    n_updates            | 2221        |
|    policy_gradient_loss | -0.00199    |
|    value_loss           | 7.27e+03    |
-----------------------------------------
Eval num_timesteps=263000, episode_reward=-164.32 +/- 150.92
Episode length: 14.88 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=-156.24 +/- 171.30
Episode length: 17.14 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=-172.82 +/- 139.80
Episode length: 15.46 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 129      |
|    time_elapsed    | 552      |
|    total_timesteps | 264192   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=264500, episode_reward=-131.79 +/- 166.31
Episode length: 17.62 +/- 5.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | -132         |
| time/                   |              |
|    total_timesteps      | 264500       |
| train/                  |              |
|    approx_kl            | 0.0048727463 |
|    clip_fraction        | 0.0542       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.327       |
|    explained_variance   | 0.273        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.59e+03     |
|    n_updates            | 2224         |
|    policy_gradient_loss | 0.0024       |
|    value_loss           | 6.92e+03     |
------------------------------------------
Eval num_timesteps=265000, episode_reward=-160.87 +/- 154.47
Episode length: 16.18 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=-145.31 +/- 151.46
Episode length: 16.64 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=-135.47 +/- 130.67
Episode length: 15.98 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 130      |
|    time_elapsed    | 557      |
|    total_timesteps | 266240   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=266500, episode_reward=-125.10 +/- 140.95
Episode length: 17.32 +/- 5.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.3         |
|    mean_reward          | -125         |
| time/                   |              |
|    total_timesteps      | 266500       |
| train/                  |              |
|    approx_kl            | 0.0046033375 |
|    clip_fraction        | 0.0602       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.363       |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.61e+03     |
|    n_updates            | 2225         |
|    policy_gradient_loss | 0.00412      |
|    value_loss           | 7.47e+03     |
------------------------------------------
Eval num_timesteps=267000, episode_reward=-155.50 +/- 149.47
Episode length: 15.88 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=-156.58 +/- 141.77
Episode length: 16.40 +/- 4.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=-143.76 +/- 159.53
Episode length: 15.48 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -183     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 131      |
|    time_elapsed    | 561      |
|    total_timesteps | 268288   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=268500, episode_reward=-139.05 +/- 155.14
Episode length: 16.06 +/- 5.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -139         |
| time/                   |              |
|    total_timesteps      | 268500       |
| train/                  |              |
|    approx_kl            | 0.0051722797 |
|    clip_fraction        | 0.0533       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.337       |
|    explained_variance   | 0.297        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.62e+03     |
|    n_updates            | 2226         |
|    policy_gradient_loss | 0.000467     |
|    value_loss           | 7.04e+03     |
------------------------------------------
Eval num_timesteps=269000, episode_reward=-145.63 +/- 158.13
Episode length: 16.06 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=-132.88 +/- 141.27
Episode length: 15.32 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-151.80 +/- 137.24
Episode length: 15.96 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 132      |
|    time_elapsed    | 565      |
|    total_timesteps | 270336   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=270500, episode_reward=-158.97 +/- 143.30
Episode length: 15.88 +/- 4.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | -159        |
| time/                   |             |
|    total_timesteps      | 270500      |
| train/                  |             |
|    approx_kl            | 0.013249891 |
|    clip_fraction        | 0.0524      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.305      |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.48e+03    |
|    n_updates            | 2228        |
|    policy_gradient_loss | 0.00315     |
|    value_loss           | 7.4e+03     |
-----------------------------------------
Eval num_timesteps=271000, episode_reward=-165.18 +/- 121.98
Episode length: 15.56 +/- 3.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=-184.34 +/- 178.06
Episode length: 15.38 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=-150.92 +/- 179.12
Episode length: 15.56 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 133      |
|    time_elapsed    | 569      |
|    total_timesteps | 272384   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=272500, episode_reward=-149.28 +/- 126.37
Episode length: 15.62 +/- 4.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -149        |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 0.002982392 |
|    clip_fraction        | 0.0308      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.253      |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.15e+03    |
|    n_updates            | 2229        |
|    policy_gradient_loss | 0.00516     |
|    value_loss           | 7.31e+03    |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=-132.17 +/- 161.05
Episode length: 16.30 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=-154.85 +/- 167.35
Episode length: 15.66 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=-155.65 +/- 130.71
Episode length: 14.70 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -162     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 134      |
|    time_elapsed    | 573      |
|    total_timesteps | 274432   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.04
Eval num_timesteps=274500, episode_reward=-144.42 +/- 169.98
Episode length: 16.36 +/- 5.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -144        |
| time/                   |             |
|    total_timesteps      | 274500      |
| train/                  |             |
|    approx_kl            | 0.020435715 |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.286      |
|    explained_variance   | 0.254       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.43e+03    |
|    n_updates            | 2231        |
|    policy_gradient_loss | 0.000817    |
|    value_loss           | 7.75e+03    |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=-157.75 +/- 116.92
Episode length: 15.30 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=-169.41 +/- 167.70
Episode length: 15.74 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=-142.28 +/- 148.64
Episode length: 16.18 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -174     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 135      |
|    time_elapsed    | 577      |
|    total_timesteps | 276480   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=276500, episode_reward=-192.85 +/- 131.08
Episode length: 15.18 +/- 4.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -193         |
| time/                   |              |
|    total_timesteps      | 276500       |
| train/                  |              |
|    approx_kl            | 0.0060785166 |
|    clip_fraction        | 0.0848       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.363       |
|    explained_variance   | 0.309        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.32e+03     |
|    n_updates            | 2232         |
|    policy_gradient_loss | 0.00111      |
|    value_loss           | 5.86e+03     |
------------------------------------------
Eval num_timesteps=277000, episode_reward=-186.01 +/- 135.79
Episode length: 15.72 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=-163.51 +/- 135.60
Episode length: 16.50 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=-159.32 +/- 137.64
Episode length: 16.98 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=-179.64 +/- 133.58
Episode length: 16.12 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 136      |
|    time_elapsed    | 582      |
|    total_timesteps | 278528   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=279000, episode_reward=-147.16 +/- 143.48
Episode length: 16.32 +/- 5.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -147         |
| time/                   |              |
|    total_timesteps      | 279000       |
| train/                  |              |
|    approx_kl            | 0.0048880167 |
|    clip_fraction        | 0.0677       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.363       |
|    explained_variance   | 0.271        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.27e+03     |
|    n_updates            | 2233         |
|    policy_gradient_loss | 0.0049       |
|    value_loss           | 6.96e+03     |
------------------------------------------
Eval num_timesteps=279500, episode_reward=-161.47 +/- 124.11
Episode length: 15.82 +/- 4.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-152.32 +/- 129.91
Episode length: 16.50 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=-105.76 +/- 189.35
Episode length: 18.06 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.1     |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 137      |
|    time_elapsed    | 586      |
|    total_timesteps | 280576   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=281000, episode_reward=-136.62 +/- 193.90
Episode length: 16.26 +/- 6.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -137        |
| time/                   |             |
|    total_timesteps      | 281000      |
| train/                  |             |
|    approx_kl            | 0.017363945 |
|    clip_fraction        | 0.0559      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.337      |
|    explained_variance   | 0.274       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.56e+03    |
|    n_updates            | 2235        |
|    policy_gradient_loss | 0.00374     |
|    value_loss           | 6.63e+03    |
-----------------------------------------
Eval num_timesteps=281500, episode_reward=-160.08 +/- 154.05
Episode length: 16.34 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=-157.16 +/- 135.04
Episode length: 15.88 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=-134.07 +/- 141.19
Episode length: 16.86 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 138      |
|    time_elapsed    | 590      |
|    total_timesteps | 282624   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=283000, episode_reward=-153.23 +/- 119.49
Episode length: 17.02 +/- 5.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -153         |
| time/                   |              |
|    total_timesteps      | 283000       |
| train/                  |              |
|    approx_kl            | 0.0047137327 |
|    clip_fraction        | 0.0739       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.338       |
|    explained_variance   | 0.24         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.75e+03     |
|    n_updates            | 2237         |
|    policy_gradient_loss | 0.00318      |
|    value_loss           | 7.14e+03     |
------------------------------------------
Eval num_timesteps=283500, episode_reward=-169.17 +/- 149.38
Episode length: 16.16 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=-185.31 +/- 159.73
Episode length: 16.24 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=-188.35 +/- 136.63
Episode length: 15.74 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -194     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 139      |
|    time_elapsed    | 595      |
|    total_timesteps | 284672   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=285000, episode_reward=-189.74 +/- 148.20
Episode length: 15.76 +/- 5.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -190        |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.005053339 |
|    clip_fraction        | 0.0361      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.375      |
|    explained_variance   | 0.289       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.38e+03    |
|    n_updates            | 2238        |
|    policy_gradient_loss | 0.000751    |
|    value_loss           | 7.6e+03     |
-----------------------------------------
Eval num_timesteps=285500, episode_reward=-150.32 +/- 124.10
Episode length: 17.14 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=-167.18 +/- 129.11
Episode length: 16.58 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=-183.10 +/- 136.45
Episode length: 16.38 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -203     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 140      |
|    time_elapsed    | 599      |
|    total_timesteps | 286720   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=287000, episode_reward=-148.27 +/- 140.06
Episode length: 16.30 +/- 5.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -148        |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 0.005242252 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.393      |
|    explained_variance   | 0.279       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.94e+03    |
|    n_updates            | 2239        |
|    policy_gradient_loss | 0.00302     |
|    value_loss           | 7.65e+03    |
-----------------------------------------
Eval num_timesteps=287500, episode_reward=-144.64 +/- 170.03
Episode length: 16.26 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=-177.37 +/- 147.40
Episode length: 16.52 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=-131.56 +/- 156.20
Episode length: 18.06 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.1     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -224     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 141      |
|    time_elapsed    | 603      |
|    total_timesteps | 288768   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=289000, episode_reward=-150.66 +/- 157.97
Episode length: 16.80 +/- 5.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | -151        |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.011258017 |
|    clip_fraction        | 0.0994      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.38       |
|    explained_variance   | 0.282       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.48e+03    |
|    n_updates            | 2241        |
|    policy_gradient_loss | 0.003       |
|    value_loss           | 7.77e+03    |
-----------------------------------------
Eval num_timesteps=289500, episode_reward=-143.42 +/- 171.58
Episode length: 16.52 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-145.32 +/- 158.40
Episode length: 16.16 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=-142.98 +/- 144.34
Episode length: 16.64 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -205     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 142      |
|    time_elapsed    | 607      |
|    total_timesteps | 290816   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=291000, episode_reward=-171.14 +/- 126.48
Episode length: 15.24 +/- 4.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | -171        |
| time/                   |             |
|    total_timesteps      | 291000      |
| train/                  |             |
|    approx_kl            | 0.007902219 |
|    clip_fraction        | 0.0716      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.396      |
|    explained_variance   | 0.274       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.83e+03    |
|    n_updates            | 2245        |
|    policy_gradient_loss | 0.00155     |
|    value_loss           | 6.8e+03     |
-----------------------------------------
Eval num_timesteps=291500, episode_reward=-148.84 +/- 148.22
Episode length: 15.00 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=-131.78 +/- 134.88
Episode length: 16.52 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=-172.06 +/- 158.96
Episode length: 15.58 +/- 4.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -208     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 143      |
|    time_elapsed    | 612      |
|    total_timesteps | 292864   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=293000, episode_reward=-153.94 +/- 125.99
Episode length: 15.76 +/- 4.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -154        |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.022684835 |
|    clip_fraction        | 0.0774      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.39       |
|    explained_variance   | 0.334       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.58e+03    |
|    n_updates            | 2248        |
|    policy_gradient_loss | 0.00675     |
|    value_loss           | 7.21e+03    |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=-146.27 +/- 165.16
Episode length: 16.06 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=-148.12 +/- 146.27
Episode length: 15.68 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=-177.93 +/- 128.43
Episode length: 15.02 +/- 4.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 144      |
|    time_elapsed    | 616      |
|    total_timesteps | 294912   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=295000, episode_reward=-151.14 +/- 137.87
Episode length: 15.42 +/- 4.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -151         |
| time/                   |              |
|    total_timesteps      | 295000       |
| train/                  |              |
|    approx_kl            | 0.0051490455 |
|    clip_fraction        | 0.0697       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.375       |
|    explained_variance   | 0.311        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.77e+03     |
|    n_updates            | 2251         |
|    policy_gradient_loss | 0.000446     |
|    value_loss           | 6.6e+03      |
------------------------------------------
Eval num_timesteps=295500, episode_reward=-177.25 +/- 167.78
Episode length: 15.76 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=-146.67 +/- 153.41
Episode length: 16.64 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=-140.43 +/- 174.83
Episode length: 17.46 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -203     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 145      |
|    time_elapsed    | 620      |
|    total_timesteps | 296960   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=297000, episode_reward=-154.20 +/- 121.27
Episode length: 15.30 +/- 4.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -154         |
| time/                   |              |
|    total_timesteps      | 297000       |
| train/                  |              |
|    approx_kl            | 0.0033429386 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.384       |
|    explained_variance   | 0.339        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.88e+03     |
|    n_updates            | 2252         |
|    policy_gradient_loss | -0.0052      |
|    value_loss           | 7.04e+03     |
------------------------------------------
Eval num_timesteps=297500, episode_reward=-108.40 +/- 182.25
Episode length: 16.84 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=-129.18 +/- 166.76
Episode length: 16.64 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=-160.20 +/- 141.91
Episode length: 15.62 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=-161.04 +/- 168.64
Episode length: 15.12 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 146      |
|    time_elapsed    | 625      |
|    total_timesteps | 299008   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=299500, episode_reward=-124.02 +/- 151.52
Episode length: 17.56 +/- 4.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.6         |
|    mean_reward          | -124         |
| time/                   |              |
|    total_timesteps      | 299500       |
| train/                  |              |
|    approx_kl            | 0.0068659415 |
|    clip_fraction        | 0.0621       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.342       |
|    explained_variance   | 0.278        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.95e+03     |
|    n_updates            | 2255         |
|    policy_gradient_loss | 0.0022       |
|    value_loss           | 7e+03        |
------------------------------------------
Eval num_timesteps=300000, episode_reward=-138.36 +/- 122.85
Episode length: 16.04 +/- 4.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=-119.47 +/- 178.04
Episode length: 17.02 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=-134.83 +/- 164.87
Episode length: 15.94 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 147      |
|    time_elapsed    | 629      |
|    total_timesteps | 301056   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=301500, episode_reward=-172.42 +/- 122.95
Episode length: 16.06 +/- 4.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -172        |
| time/                   |             |
|    total_timesteps      | 301500      |
| train/                  |             |
|    approx_kl            | 0.006205837 |
|    clip_fraction        | 0.0708      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.357      |
|    explained_variance   | 0.281       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.34e+03    |
|    n_updates            | 2258        |
|    policy_gradient_loss | 0.00603     |
|    value_loss           | 7.55e+03    |
-----------------------------------------
Eval num_timesteps=302000, episode_reward=-123.03 +/- 138.31
Episode length: 15.78 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=-153.16 +/- 152.41
Episode length: 16.60 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=-131.30 +/- 131.30
Episode length: 16.74 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 148      |
|    time_elapsed    | 633      |
|    total_timesteps | 303104   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=303500, episode_reward=-150.99 +/- 160.13
Episode length: 15.84 +/- 5.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -151         |
| time/                   |              |
|    total_timesteps      | 303500       |
| train/                  |              |
|    approx_kl            | 0.0041291434 |
|    clip_fraction        | 0.0664       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.35        |
|    explained_variance   | 0.281        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.79e+03     |
|    n_updates            | 2259         |
|    policy_gradient_loss | -0.0018      |
|    value_loss           | 8.05e+03     |
------------------------------------------
Eval num_timesteps=304000, episode_reward=-159.68 +/- 136.46
Episode length: 15.18 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=-152.39 +/- 159.70
Episode length: 16.20 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=-129.31 +/- 144.07
Episode length: 16.00 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 149      |
|    time_elapsed    | 638      |
|    total_timesteps | 305152   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=305500, episode_reward=-156.80 +/- 155.35
Episode length: 16.14 +/- 4.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -157        |
| time/                   |             |
|    total_timesteps      | 305500      |
| train/                  |             |
|    approx_kl            | 0.003907794 |
|    clip_fraction        | 0.044       |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.263      |
|    explained_variance   | 0.27        |
|    learning_rate        | 0.0001      |
|    loss                 | 2.65e+03    |
|    n_updates            | 2260        |
|    policy_gradient_loss | 0.00484     |
|    value_loss           | 6.5e+03     |
-----------------------------------------
Eval num_timesteps=306000, episode_reward=-176.30 +/- 120.47
Episode length: 15.20 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=-137.94 +/- 161.97
Episode length: 16.44 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=-151.51 +/- 127.40
Episode length: 15.80 +/- 4.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 150      |
|    time_elapsed    | 642      |
|    total_timesteps | 307200   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=307500, episode_reward=-80.83 +/- 183.55
Episode length: 17.14 +/- 5.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.1         |
|    mean_reward          | -80.8        |
| time/                   |              |
|    total_timesteps      | 307500       |
| train/                  |              |
|    approx_kl            | 0.0065528164 |
|    clip_fraction        | 0.0813       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.318       |
|    explained_variance   | 0.326        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.19e+03     |
|    n_updates            | 2261         |
|    policy_gradient_loss | 0.00898      |
|    value_loss           | 6.57e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=308000, episode_reward=-192.40 +/- 122.76
Episode length: 15.84 +/- 4.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=-127.18 +/- 171.70
Episode length: 17.00 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=-162.51 +/- 144.80
Episode length: 15.62 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 151      |
|    time_elapsed    | 646      |
|    total_timesteps | 309248   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=309500, episode_reward=-187.17 +/- 138.05
Episode length: 15.20 +/- 4.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -187         |
| time/                   |              |
|    total_timesteps      | 309500       |
| train/                  |              |
|    approx_kl            | 0.0070998594 |
|    clip_fraction        | 0.0613       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.283       |
|    explained_variance   | 0.263        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.62e+03     |
|    n_updates            | 2263         |
|    policy_gradient_loss | 0.0033       |
|    value_loss           | 7.63e+03     |
------------------------------------------
Eval num_timesteps=310000, episode_reward=-185.58 +/- 134.26
Episode length: 15.62 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=-190.97 +/- 119.55
Episode length: 15.56 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=-130.30 +/- 158.71
Episode length: 17.92 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 152      |
|    time_elapsed    | 650      |
|    total_timesteps | 311296   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=311500, episode_reward=-166.01 +/- 162.18
Episode length: 15.02 +/- 4.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | -166        |
| time/                   |             |
|    total_timesteps      | 311500      |
| train/                  |             |
|    approx_kl            | 0.005234161 |
|    clip_fraction        | 0.0499      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.255      |
|    explained_variance   | 0.298       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.92e+03    |
|    n_updates            | 2265        |
|    policy_gradient_loss | 9.2e-06     |
|    value_loss           | 6.98e+03    |
-----------------------------------------
Eval num_timesteps=312000, episode_reward=-142.88 +/- 175.32
Episode length: 16.32 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=-112.11 +/- 174.28
Episode length: 16.82 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -112     |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=-144.99 +/- 122.51
Episode length: 15.14 +/- 4.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -183     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 153      |
|    time_elapsed    | 654      |
|    total_timesteps | 313344   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=313500, episode_reward=-155.10 +/- 139.64
Episode length: 16.98 +/- 4.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -155         |
| time/                   |              |
|    total_timesteps      | 313500       |
| train/                  |              |
|    approx_kl            | 0.0040503503 |
|    clip_fraction        | 0.0551       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.269       |
|    explained_variance   | 0.304        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.02e+03     |
|    n_updates            | 2266         |
|    policy_gradient_loss | -0.000445    |
|    value_loss           | 6.86e+03     |
------------------------------------------
Eval num_timesteps=314000, episode_reward=-139.31 +/- 149.06
Episode length: 16.74 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=-186.93 +/- 134.25
Episode length: 15.96 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=-176.28 +/- 147.13
Episode length: 17.02 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -207     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 154      |
|    time_elapsed    | 659      |
|    total_timesteps | 315392   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=315500, episode_reward=-170.65 +/- 132.56
Episode length: 15.24 +/- 5.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -171         |
| time/                   |              |
|    total_timesteps      | 315500       |
| train/                  |              |
|    approx_kl            | 0.0053718253 |
|    clip_fraction        | 0.0706       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.272       |
|    explained_variance   | 0.31         |
|    learning_rate        | 0.0001       |
|    loss                 | 4.55e+03     |
|    n_updates            | 2267         |
|    policy_gradient_loss | -0.0016      |
|    value_loss           | 7.38e+03     |
------------------------------------------
Eval num_timesteps=316000, episode_reward=-134.53 +/- 136.58
Episode length: 17.28 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=-161.37 +/- 141.94
Episode length: 17.46 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=-111.46 +/- 130.32
Episode length: 17.58 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 155      |
|    time_elapsed    | 663      |
|    total_timesteps | 317440   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=317500, episode_reward=-131.00 +/- 118.16
Episode length: 16.76 +/- 4.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -131         |
| time/                   |              |
|    total_timesteps      | 317500       |
| train/                  |              |
|    approx_kl            | 0.0047811205 |
|    clip_fraction        | 0.0656       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.251       |
|    explained_variance   | 0.351        |
|    learning_rate        | 0.0001       |
|    loss                 | 4e+03        |
|    n_updates            | 2268         |
|    policy_gradient_loss | 0.00475      |
|    value_loss           | 7.4e+03      |
------------------------------------------
Eval num_timesteps=318000, episode_reward=-153.15 +/- 169.59
Episode length: 15.40 +/- 5.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=-113.08 +/- 162.23
Episode length: 16.70 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=-169.98 +/- 138.85
Episode length: 15.74 +/- 4.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 156      |
|    time_elapsed    | 667      |
|    total_timesteps | 319488   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=319500, episode_reward=-135.16 +/- 162.72
Episode length: 15.88 +/- 5.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -135         |
| time/                   |              |
|    total_timesteps      | 319500       |
| train/                  |              |
|    approx_kl            | 0.0046663866 |
|    clip_fraction        | 0.0667       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.214       |
|    explained_variance   | 0.318        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.9e+03      |
|    n_updates            | 2269         |
|    policy_gradient_loss | 0.000809     |
|    value_loss           | 7.08e+03     |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-138.21 +/- 194.15
Episode length: 16.14 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=-125.74 +/- 160.10
Episode length: 17.14 +/- 5.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=-144.39 +/- 161.06
Episode length: 15.98 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=-152.07 +/- 182.69
Episode length: 15.92 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -158     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 157      |
|    time_elapsed    | 672      |
|    total_timesteps | 321536   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=322000, episode_reward=-136.85 +/- 137.01
Episode length: 16.32 +/- 4.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -137        |
| time/                   |             |
|    total_timesteps      | 322000      |
| train/                  |             |
|    approx_kl            | 0.003694933 |
|    clip_fraction        | 0.0411      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.204      |
|    explained_variance   | 0.294       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.33e+03    |
|    n_updates            | 2270        |
|    policy_gradient_loss | 0.00208     |
|    value_loss           | 7.58e+03    |
-----------------------------------------
Eval num_timesteps=322500, episode_reward=-152.13 +/- 172.50
Episode length: 16.30 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=-140.25 +/- 132.64
Episode length: 16.38 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=-154.00 +/- 162.40
Episode length: 15.54 +/- 5.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -153     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 158      |
|    time_elapsed    | 676      |
|    total_timesteps | 323584   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=324000, episode_reward=-180.51 +/- 169.51
Episode length: 14.74 +/- 5.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.7        |
|    mean_reward          | -181        |
| time/                   |             |
|    total_timesteps      | 324000      |
| train/                  |             |
|    approx_kl            | 0.018792259 |
|    clip_fraction        | 0.0241      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.271       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.21e+03    |
|    n_updates            | 2272        |
|    policy_gradient_loss | 0.00244     |
|    value_loss           | 6.94e+03    |
-----------------------------------------
Eval num_timesteps=324500, episode_reward=-145.34 +/- 141.49
Episode length: 16.26 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-160.76 +/- 136.88
Episode length: 15.92 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=-135.41 +/- 174.47
Episode length: 16.76 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 159      |
|    time_elapsed    | 680      |
|    total_timesteps | 325632   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=326000, episode_reward=-118.52 +/- 157.86
Episode length: 16.36 +/- 5.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -119         |
| time/                   |              |
|    total_timesteps      | 326000       |
| train/                  |              |
|    approx_kl            | 0.0032369008 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.126       |
|    explained_variance   | 0.31         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.71e+03     |
|    n_updates            | 2273         |
|    policy_gradient_loss | -0.000454    |
|    value_loss           | 7.54e+03     |
------------------------------------------
Eval num_timesteps=326500, episode_reward=-88.54 +/- 159.89
Episode length: 18.12 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.1     |
|    mean_reward     | -88.5    |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=-157.06 +/- 131.57
Episode length: 16.08 +/- 4.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=-143.30 +/- 135.97
Episode length: 16.34 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 160      |
|    time_elapsed    | 685      |
|    total_timesteps | 327680   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=328000, episode_reward=-168.30 +/- 117.58
Episode length: 14.84 +/- 4.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -168         |
| time/                   |              |
|    total_timesteps      | 328000       |
| train/                  |              |
|    approx_kl            | 0.0040191594 |
|    clip_fraction        | 0.0335       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.303        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.51e+03     |
|    n_updates            | 2274         |
|    policy_gradient_loss | -0.00106     |
|    value_loss           | 6.42e+03     |
------------------------------------------
Eval num_timesteps=328500, episode_reward=-147.60 +/- 137.89
Episode length: 15.96 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=-144.86 +/- 138.73
Episode length: 16.06 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=-161.69 +/- 146.65
Episode length: 14.62 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 161      |
|    time_elapsed    | 689      |
|    total_timesteps | 329728   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=330000, episode_reward=-178.63 +/- 145.03
Episode length: 15.22 +/- 4.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.2        |
|    mean_reward          | -179        |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.008233684 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.228      |
|    explained_variance   | 0.282       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.92e+03    |
|    n_updates            | 2276        |
|    policy_gradient_loss | 0.000991    |
|    value_loss           | 7.35e+03    |
-----------------------------------------
Eval num_timesteps=330500, episode_reward=-172.84 +/- 129.86
Episode length: 16.16 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=-163.71 +/- 132.49
Episode length: 15.94 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=-181.35 +/- 150.85
Episode length: 14.88 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -170     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 162      |
|    time_elapsed    | 693      |
|    total_timesteps | 331776   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=332000, episode_reward=-155.13 +/- 175.87
Episode length: 16.54 +/- 5.33
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -155         |
| time/                   |              |
|    total_timesteps      | 332000       |
| train/                  |              |
|    approx_kl            | 0.0033202355 |
|    clip_fraction        | 0.0462       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.25        |
|    explained_variance   | 0.276        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.01e+03     |
|    n_updates            | 2279         |
|    policy_gradient_loss | 0.000869     |
|    value_loss           | 7.27e+03     |
------------------------------------------
Eval num_timesteps=332500, episode_reward=-176.79 +/- 159.28
Episode length: 15.36 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=-150.36 +/- 140.21
Episode length: 15.34 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=-142.95 +/- 160.90
Episode length: 15.62 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 163      |
|    time_elapsed    | 697      |
|    total_timesteps | 333824   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=334000, episode_reward=-140.94 +/- 169.04
Episode length: 16.82 +/- 5.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.8        |
|    mean_reward          | -141        |
| time/                   |             |
|    total_timesteps      | 334000      |
| train/                  |             |
|    approx_kl            | 0.004896694 |
|    clip_fraction        | 0.0709      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.221      |
|    explained_variance   | 0.283       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.98e+03    |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 7.38e+03    |
-----------------------------------------
Eval num_timesteps=334500, episode_reward=-152.60 +/- 147.17
Episode length: 16.10 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=-166.09 +/- 144.79
Episode length: 16.02 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=-146.09 +/- 126.76
Episode length: 15.50 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 164      |
|    time_elapsed    | 701      |
|    total_timesteps | 335872   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=336000, episode_reward=-157.81 +/- 135.13
Episode length: 15.34 +/- 4.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | -158        |
| time/                   |             |
|    total_timesteps      | 336000      |
| train/                  |             |
|    approx_kl            | 0.005670919 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.291      |
|    explained_variance   | 0.303       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.8e+03     |
|    n_updates            | 2281        |
|    policy_gradient_loss | -0.00135    |
|    value_loss           | 7.03e+03    |
-----------------------------------------
Eval num_timesteps=336500, episode_reward=-133.79 +/- 178.07
Episode length: 17.24 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=-177.05 +/- 190.00
Episode length: 15.74 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=-137.71 +/- 154.07
Episode length: 15.26 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 165      |
|    time_elapsed    | 705      |
|    total_timesteps | 337920   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=338000, episode_reward=-130.58 +/- 182.87
Episode length: 16.14 +/- 5.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -131         |
| time/                   |              |
|    total_timesteps      | 338000       |
| train/                  |              |
|    approx_kl            | 0.0049672015 |
|    clip_fraction        | 0.0616       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.322        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.76e+03     |
|    n_updates            | 2282         |
|    policy_gradient_loss | -0.000262    |
|    value_loss           | 7.05e+03     |
------------------------------------------
Eval num_timesteps=338500, episode_reward=-146.74 +/- 126.97
Episode length: 16.12 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=-119.03 +/- 156.99
Episode length: 17.52 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=-143.29 +/- 152.09
Episode length: 15.48 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 166      |
|    time_elapsed    | 709      |
|    total_timesteps | 339968   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=340000, episode_reward=-160.32 +/- 122.33
Episode length: 15.84 +/- 3.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -160         |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0067390176 |
|    clip_fraction        | 0.0721       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.189       |
|    explained_variance   | 0.301        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.95e+03     |
|    n_updates            | 2284         |
|    policy_gradient_loss | 0.00179      |
|    value_loss           | 6.77e+03     |
------------------------------------------
Eval num_timesteps=340500, episode_reward=-143.19 +/- 174.57
Episode length: 16.18 +/- 5.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=-161.44 +/- 163.83
Episode length: 16.10 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=-169.26 +/- 148.25
Episode length: 15.24 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=-168.79 +/- 140.97
Episode length: 15.52 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -155     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 167      |
|    time_elapsed    | 714      |
|    total_timesteps | 342016   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=342500, episode_reward=-176.69 +/- 131.36
Episode length: 16.08 +/- 4.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -177        |
| time/                   |             |
|    total_timesteps      | 342500      |
| train/                  |             |
|    approx_kl            | 0.004467292 |
|    clip_fraction        | 0.0656      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.296       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.52e+03    |
|    n_updates            | 2285        |
|    policy_gradient_loss | 0.0052      |
|    value_loss           | 5.85e+03    |
-----------------------------------------
Eval num_timesteps=343000, episode_reward=-162.43 +/- 147.40
Episode length: 16.20 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=-185.57 +/- 153.23
Episode length: 15.76 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=-131.75 +/- 155.30
Episode length: 15.82 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 168      |
|    time_elapsed    | 718      |
|    total_timesteps | 344064   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=344500, episode_reward=-128.84 +/- 164.29
Episode length: 15.42 +/- 5.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -129         |
| time/                   |              |
|    total_timesteps      | 344500       |
| train/                  |              |
|    approx_kl            | 0.0037946228 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.18        |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.24e+03     |
|    n_updates            | 2286         |
|    policy_gradient_loss | 0.000641     |
|    value_loss           | 6.46e+03     |
------------------------------------------
Eval num_timesteps=345000, episode_reward=-147.88 +/- 142.50
Episode length: 15.66 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=-162.34 +/- 145.83
Episode length: 14.52 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=-145.81 +/- 150.64
Episode length: 16.02 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 169      |
|    time_elapsed    | 722      |
|    total_timesteps | 346112   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=346500, episode_reward=-130.63 +/- 181.08
Episode length: 16.70 +/- 5.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | -131        |
| time/                   |             |
|    total_timesteps      | 346500      |
| train/                  |             |
|    approx_kl            | 0.006821522 |
|    clip_fraction        | 0.0538      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.248      |
|    explained_variance   | 0.281       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.43e+03    |
|    n_updates            | 2288        |
|    policy_gradient_loss | 0.00373     |
|    value_loss           | 8.51e+03    |
-----------------------------------------
Eval num_timesteps=347000, episode_reward=-118.46 +/- 141.21
Episode length: 16.86 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=-109.31 +/- 165.31
Episode length: 17.42 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=-144.02 +/- 178.14
Episode length: 16.10 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -185     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 170      |
|    time_elapsed    | 727      |
|    total_timesteps | 348160   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=348500, episode_reward=-172.05 +/- 132.27
Episode length: 15.14 +/- 4.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.1         |
|    mean_reward          | -172         |
| time/                   |              |
|    total_timesteps      | 348500       |
| train/                  |              |
|    approx_kl            | 0.0068508163 |
|    clip_fraction        | 0.122        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.367       |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.08e+03     |
|    n_updates            | 2289         |
|    policy_gradient_loss | 0.00503      |
|    value_loss           | 6.65e+03     |
------------------------------------------
Eval num_timesteps=349000, episode_reward=-149.92 +/- 163.52
Episode length: 16.60 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=-152.79 +/- 161.94
Episode length: 16.56 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=-148.11 +/- 170.22
Episode length: 15.60 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | -205     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 171      |
|    time_elapsed    | 731      |
|    total_timesteps | 350208   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=350500, episode_reward=-132.91 +/- 161.38
Episode length: 16.64 +/- 5.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -133         |
| time/                   |              |
|    total_timesteps      | 350500       |
| train/                  |              |
|    approx_kl            | 0.0053427424 |
|    clip_fraction        | 0.0806       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.341       |
|    explained_variance   | 0.306        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.02e+03     |
|    n_updates            | 2290         |
|    policy_gradient_loss | 0.00163      |
|    value_loss           | 8.15e+03     |
------------------------------------------
Eval num_timesteps=351000, episode_reward=-131.28 +/- 166.20
Episode length: 17.22 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=-165.38 +/- 167.35
Episode length: 15.62 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=-120.31 +/- 157.52
Episode length: 16.92 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 172      |
|    time_elapsed    | 735      |
|    total_timesteps | 352256   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=352500, episode_reward=-162.34 +/- 137.54
Episode length: 15.58 +/- 5.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -162        |
| time/                   |             |
|    total_timesteps      | 352500      |
| train/                  |             |
|    approx_kl            | 0.005489139 |
|    clip_fraction        | 0.0429      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.304      |
|    explained_variance   | 0.28        |
|    learning_rate        | 0.0001      |
|    loss                 | 3.8e+03     |
|    n_updates            | 2292        |
|    policy_gradient_loss | 0.00401     |
|    value_loss           | 6.83e+03    |
-----------------------------------------
Eval num_timesteps=353000, episode_reward=-174.58 +/- 157.65
Episode length: 15.58 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=-103.80 +/- 165.68
Episode length: 17.52 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=-162.43 +/- 120.36
Episode length: 15.28 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 173      |
|    time_elapsed    | 739      |
|    total_timesteps | 354304   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=354500, episode_reward=-144.42 +/- 164.43
Episode length: 16.10 +/- 5.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -144        |
| time/                   |             |
|    total_timesteps      | 354500      |
| train/                  |             |
|    approx_kl            | 0.004588178 |
|    clip_fraction        | 0.0433      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.262      |
|    explained_variance   | 0.299       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.55e+03    |
|    n_updates            | 2293        |
|    policy_gradient_loss | 0.00224     |
|    value_loss           | 7.09e+03    |
-----------------------------------------
Eval num_timesteps=355000, episode_reward=-161.01 +/- 122.83
Episode length: 15.10 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=-163.14 +/- 127.69
Episode length: 15.04 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=-167.66 +/- 138.01
Episode length: 14.62 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -181     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 174      |
|    time_elapsed    | 743      |
|    total_timesteps | 356352   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=356500, episode_reward=-182.38 +/- 154.38
Episode length: 15.64 +/- 5.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -182        |
| time/                   |             |
|    total_timesteps      | 356500      |
| train/                  |             |
|    approx_kl            | 0.006806043 |
|    clip_fraction        | 0.0593      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.302      |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.56e+03    |
|    n_updates            | 2296        |
|    policy_gradient_loss | 0.0041      |
|    value_loss           | 6.72e+03    |
-----------------------------------------
Eval num_timesteps=357000, episode_reward=-146.37 +/- 137.49
Episode length: 16.70 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=-174.62 +/- 139.74
Episode length: 15.32 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=-122.87 +/- 145.74
Episode length: 16.72 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -160     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 175      |
|    time_elapsed    | 748      |
|    total_timesteps | 358400   |
---------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Eval num_timesteps=358500, episode_reward=-144.39 +/- 140.10
Episode length: 17.70 +/- 5.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.7         |
|    mean_reward          | -144         |
| time/                   |              |
|    total_timesteps      | 358500       |
| train/                  |              |
|    approx_kl            | 0.0049209874 |
|    clip_fraction        | 0.0614       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.323       |
|    explained_variance   | 0.313        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.55e+03     |
|    n_updates            | 2302         |
|    policy_gradient_loss | 0.00298      |
|    value_loss           | 6.51e+03     |
------------------------------------------
Eval num_timesteps=359000, episode_reward=-173.24 +/- 169.85
Episode length: 16.48 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=-166.09 +/- 140.87
Episode length: 16.78 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-167.36 +/- 131.20
Episode length: 16.70 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 176      |
|    time_elapsed    | 752      |
|    total_timesteps | 360448   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=360500, episode_reward=-120.47 +/- 169.07
Episode length: 18.12 +/- 5.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18.1         |
|    mean_reward          | -120         |
| time/                   |              |
|    total_timesteps      | 360500       |
| train/                  |              |
|    approx_kl            | 0.0042943447 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.327       |
|    explained_variance   | 0.339        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.01e+03     |
|    n_updates            | 2303         |
|    policy_gradient_loss | 0.00331      |
|    value_loss           | 5.87e+03     |
------------------------------------------
Eval num_timesteps=361000, episode_reward=-137.79 +/- 180.99
Episode length: 16.66 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=-149.90 +/- 156.39
Episode length: 16.24 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=-135.73 +/- 134.68
Episode length: 16.80 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -193     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 177      |
|    time_elapsed    | 757      |
|    total_timesteps | 362496   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=362500, episode_reward=-173.52 +/- 104.91
Episode length: 15.96 +/- 3.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -174         |
| time/                   |              |
|    total_timesteps      | 362500       |
| train/                  |              |
|    approx_kl            | 0.0046968847 |
|    clip_fraction        | 0.061        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.336       |
|    explained_variance   | 0.306        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.46e+03     |
|    n_updates            | 2307         |
|    policy_gradient_loss | 0.00292      |
|    value_loss           | 7.96e+03     |
------------------------------------------
Eval num_timesteps=363000, episode_reward=-125.43 +/- 129.37
Episode length: 17.76 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=-165.86 +/- 120.17
Episode length: 16.22 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=-142.88 +/- 146.07
Episode length: 17.12 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=-139.43 +/- 130.10
Episode length: 17.92 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 178      |
|    time_elapsed    | 762      |
|    total_timesteps | 364544   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
Eval num_timesteps=365000, episode_reward=-184.28 +/- 147.57
Episode length: 15.64 +/- 5.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -184        |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.006134265 |
|    clip_fraction        | 0.0649      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.322       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.02e+03    |
|    n_updates            | 2311        |
|    policy_gradient_loss | 4e-05       |
|    value_loss           | 6.24e+03    |
-----------------------------------------
Eval num_timesteps=365500, episode_reward=-130.55 +/- 151.69
Episode length: 17.04 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=-170.59 +/- 154.03
Episode length: 16.16 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=-139.36 +/- 121.65
Episode length: 18.18 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -188     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 179      |
|    time_elapsed    | 766      |
|    total_timesteps | 366592   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=367000, episode_reward=-157.29 +/- 131.92
Episode length: 16.20 +/- 4.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -157         |
| time/                   |              |
|    total_timesteps      | 367000       |
| train/                  |              |
|    approx_kl            | 0.0049912655 |
|    clip_fraction        | 0.0576       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.36        |
|    explained_variance   | 0.36         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.21e+03     |
|    n_updates            | 2312         |
|    policy_gradient_loss | 0.00186      |
|    value_loss           | 6.1e+03      |
------------------------------------------
Eval num_timesteps=367500, episode_reward=-157.88 +/- 150.53
Episode length: 16.74 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=-111.30 +/- 142.81
Episode length: 18.12 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.1     |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=-162.64 +/- 137.74
Episode length: 15.90 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 180      |
|    time_elapsed    | 770      |
|    total_timesteps | 368640   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=369000, episode_reward=-144.03 +/- 138.96
Episode length: 16.24 +/- 4.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -144         |
| time/                   |              |
|    total_timesteps      | 369000       |
| train/                  |              |
|    approx_kl            | 0.0069261095 |
|    clip_fraction        | 0.0493       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.341       |
|    explained_variance   | 0.306        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.67e+03     |
|    n_updates            | 2314         |
|    policy_gradient_loss | 0.00212      |
|    value_loss           | 6.53e+03     |
------------------------------------------
Eval num_timesteps=369500, episode_reward=-143.99 +/- 178.00
Episode length: 16.66 +/- 5.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=-152.29 +/- 136.54
Episode length: 16.32 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=-134.94 +/- 152.89
Episode length: 16.30 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 181      |
|    time_elapsed    | 775      |
|    total_timesteps | 370688   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.03
Eval num_timesteps=371000, episode_reward=-109.17 +/- 148.47
Episode length: 17.12 +/- 4.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.1        |
|    mean_reward          | -109        |
| time/                   |             |
|    total_timesteps      | 371000      |
| train/                  |             |
|    approx_kl            | 0.014246645 |
|    clip_fraction        | 0.0664      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.31       |
|    explained_variance   | 0.344       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.88e+03    |
|    n_updates            | 2317        |
|    policy_gradient_loss | 0.000823    |
|    value_loss           | 6.71e+03    |
-----------------------------------------
Eval num_timesteps=371500, episode_reward=-161.01 +/- 166.87
Episode length: 15.38 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=-138.58 +/- 163.82
Episode length: 17.16 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=-154.21 +/- 138.55
Episode length: 16.32 +/- 4.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 182      |
|    time_elapsed    | 779      |
|    total_timesteps | 372736   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=373000, episode_reward=-137.75 +/- 164.38
Episode length: 17.06 +/- 5.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.1        |
|    mean_reward          | -138        |
| time/                   |             |
|    total_timesteps      | 373000      |
| train/                  |             |
|    approx_kl            | 0.004062269 |
|    clip_fraction        | 0.0656      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.25       |
|    explained_variance   | 0.337       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.83e+03    |
|    n_updates            | 2318        |
|    policy_gradient_loss | 0.00247     |
|    value_loss           | 6.55e+03    |
-----------------------------------------
Eval num_timesteps=373500, episode_reward=-161.21 +/- 142.34
Episode length: 16.44 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=-180.35 +/- 136.01
Episode length: 15.48 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=-135.82 +/- 138.21
Episode length: 15.76 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 183      |
|    time_elapsed    | 783      |
|    total_timesteps | 374784   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=375000, episode_reward=-141.93 +/- 173.64
Episode length: 16.30 +/- 5.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -142        |
| time/                   |             |
|    total_timesteps      | 375000      |
| train/                  |             |
|    approx_kl            | 0.006098844 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.306      |
|    explained_variance   | 0.312       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.36e+03    |
|    n_updates            | 2321        |
|    policy_gradient_loss | 0.00156     |
|    value_loss           | 6.63e+03    |
-----------------------------------------
Eval num_timesteps=375500, episode_reward=-152.11 +/- 143.93
Episode length: 16.08 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=-161.43 +/- 153.19
Episode length: 16.02 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=-174.63 +/- 167.00
Episode length: 15.64 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 184      |
|    time_elapsed    | 787      |
|    total_timesteps | 376832   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=377000, episode_reward=-179.94 +/- 162.78
Episode length: 15.30 +/- 4.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -180         |
| time/                   |              |
|    total_timesteps      | 377000       |
| train/                  |              |
|    approx_kl            | 0.0063340436 |
|    clip_fraction        | 0.0488       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.256       |
|    explained_variance   | 0.318        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.16e+03     |
|    n_updates            | 2323         |
|    policy_gradient_loss | 0.00454      |
|    value_loss           | 6.4e+03      |
------------------------------------------
Eval num_timesteps=377500, episode_reward=-170.10 +/- 139.06
Episode length: 14.98 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=-135.82 +/- 177.75
Episode length: 16.78 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=-133.88 +/- 121.13
Episode length: 16.48 +/- 4.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 185      |
|    time_elapsed    | 791      |
|    total_timesteps | 378880   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=379000, episode_reward=-174.46 +/- 145.51
Episode length: 17.02 +/- 5.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -174         |
| time/                   |              |
|    total_timesteps      | 379000       |
| train/                  |              |
|    approx_kl            | 0.0058339373 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.216       |
|    explained_variance   | 0.338        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.38e+03     |
|    n_updates            | 2327         |
|    policy_gradient_loss | 0.00368      |
|    value_loss           | 6.67e+03     |
------------------------------------------
Eval num_timesteps=379500, episode_reward=-217.56 +/- 151.34
Episode length: 15.22 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-180.19 +/- 138.47
Episode length: 16.22 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=-158.27 +/- 135.51
Episode length: 16.72 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -172     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 186      |
|    time_elapsed    | 796      |
|    total_timesteps | 380928   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=381000, episode_reward=-133.68 +/- 151.45
Episode length: 16.18 +/- 5.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -134         |
| time/                   |              |
|    total_timesteps      | 381000       |
| train/                  |              |
|    approx_kl            | 0.0059623783 |
|    clip_fraction        | 0.0503       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.233       |
|    explained_variance   | 0.313        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.29e+03     |
|    n_updates            | 2329         |
|    policy_gradient_loss | 0.001        |
|    value_loss           | 7.48e+03     |
------------------------------------------
Eval num_timesteps=381500, episode_reward=-148.76 +/- 140.24
Episode length: 15.82 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=-157.35 +/- 126.89
Episode length: 15.92 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=-193.00 +/- 142.86
Episode length: 15.32 +/- 4.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 187      |
|    time_elapsed    | 800      |
|    total_timesteps | 382976   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=383000, episode_reward=-186.29 +/- 145.04
Episode length: 15.42 +/- 4.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.4        |
|    mean_reward          | -186        |
| time/                   |             |
|    total_timesteps      | 383000      |
| train/                  |             |
|    approx_kl            | 0.004200502 |
|    clip_fraction        | 0.0478      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.218      |
|    explained_variance   | 0.384       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.57e+03    |
|    n_updates            | 2333        |
|    policy_gradient_loss | 0.00304     |
|    value_loss           | 5.85e+03    |
-----------------------------------------
Eval num_timesteps=383500, episode_reward=-182.24 +/- 147.34
Episode length: 16.16 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=-185.10 +/- 130.23
Episode length: 15.52 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=-169.74 +/- 133.90
Episode length: 15.82 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=-147.62 +/- 136.49
Episode length: 17.32 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 188      |
|    time_elapsed    | 805      |
|    total_timesteps | 385024   |
---------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Eval num_timesteps=385500, episode_reward=-127.36 +/- 143.31
Episode length: 15.72 +/- 4.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -127         |
| time/                   |              |
|    total_timesteps      | 385500       |
| train/                  |              |
|    approx_kl            | 0.0039659436 |
|    clip_fraction        | 0.0482       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.221       |
|    explained_variance   | 0.352        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.5e+03      |
|    n_updates            | 2338         |
|    policy_gradient_loss | 0.00125      |
|    value_loss           | 6.14e+03     |
------------------------------------------
Eval num_timesteps=386000, episode_reward=-168.27 +/- 129.81
Episode length: 15.64 +/- 4.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=-149.22 +/- 136.15
Episode length: 16.08 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=-126.64 +/- 147.76
Episode length: 16.20 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 189      |
|    time_elapsed    | 809      |
|    total_timesteps | 387072   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=387500, episode_reward=-148.20 +/- 150.09
Episode length: 16.56 +/- 5.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -148        |
| time/                   |             |
|    total_timesteps      | 387500      |
| train/                  |             |
|    approx_kl            | 0.019002773 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.214      |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.88e+03    |
|    n_updates            | 2340        |
|    policy_gradient_loss | 0.000182    |
|    value_loss           | 6.23e+03    |
-----------------------------------------
Eval num_timesteps=388000, episode_reward=-166.14 +/- 137.53
Episode length: 15.84 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=-142.67 +/- 111.35
Episode length: 16.30 +/- 3.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=-145.14 +/- 160.86
Episode length: 16.38 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 190      |
|    time_elapsed    | 814      |
|    total_timesteps | 389120   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=389500, episode_reward=-166.48 +/- 151.17
Episode length: 16.90 +/- 5.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.9         |
|    mean_reward          | -166         |
| time/                   |              |
|    total_timesteps      | 389500       |
| train/                  |              |
|    approx_kl            | 0.0038687347 |
|    clip_fraction        | 0.054        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.205       |
|    explained_variance   | 0.337        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.79e+03     |
|    n_updates            | 2341         |
|    policy_gradient_loss | 0.00206      |
|    value_loss           | 6.51e+03     |
------------------------------------------
Eval num_timesteps=390000, episode_reward=-145.92 +/- 137.35
Episode length: 17.56 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=-165.88 +/- 113.32
Episode length: 16.02 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=-162.04 +/- 125.91
Episode length: 16.64 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | -148     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 191      |
|    time_elapsed    | 818      |
|    total_timesteps | 391168   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=391500, episode_reward=-139.52 +/- 154.00
Episode length: 16.02 +/- 5.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -140         |
| time/                   |              |
|    total_timesteps      | 391500       |
| train/                  |              |
|    approx_kl            | 0.0052244426 |
|    clip_fraction        | 0.0766       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.216       |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.22e+03     |
|    n_updates            | 2343         |
|    policy_gradient_loss | 0.00208      |
|    value_loss           | 6.02e+03     |
------------------------------------------
Eval num_timesteps=392000, episode_reward=-157.03 +/- 165.62
Episode length: 15.42 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=-146.57 +/- 118.45
Episode length: 15.84 +/- 4.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=-132.27 +/- 171.46
Episode length: 16.24 +/- 5.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 192      |
|    time_elapsed    | 822      |
|    total_timesteps | 393216   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=393500, episode_reward=-144.24 +/- 158.69
Episode length: 16.12 +/- 5.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.1        |
|    mean_reward          | -144        |
| time/                   |             |
|    total_timesteps      | 393500      |
| train/                  |             |
|    approx_kl            | 0.004166548 |
|    clip_fraction        | 0.0719      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.325       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.88e+03    |
|    n_updates            | 2344        |
|    policy_gradient_loss | 0.00368     |
|    value_loss           | 6.66e+03    |
-----------------------------------------
Eval num_timesteps=394000, episode_reward=-147.55 +/- 132.41
Episode length: 16.58 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=-176.90 +/- 167.21
Episode length: 16.32 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=-155.29 +/- 137.64
Episode length: 17.10 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 193      |
|    time_elapsed    | 826      |
|    total_timesteps | 395264   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.03
Eval num_timesteps=395500, episode_reward=-174.51 +/- 187.48
Episode length: 15.56 +/- 5.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -175         |
| time/                   |              |
|    total_timesteps      | 395500       |
| train/                  |              |
|    approx_kl            | 0.0043888283 |
|    clip_fraction        | 0.0568       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.207       |
|    explained_variance   | 0.323        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.32e+03     |
|    n_updates            | 2348         |
|    policy_gradient_loss | 0.00345      |
|    value_loss           | 6.77e+03     |
------------------------------------------
Eval num_timesteps=396000, episode_reward=-132.10 +/- 154.17
Episode length: 16.70 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=-146.19 +/- 121.41
Episode length: 15.50 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=-118.67 +/- 169.82
Episode length: 16.78 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -151     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 194      |
|    time_elapsed    | 831      |
|    total_timesteps | 397312   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=397500, episode_reward=-151.90 +/- 156.37
Episode length: 16.18 +/- 5.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -152         |
| time/                   |              |
|    total_timesteps      | 397500       |
| train/                  |              |
|    approx_kl            | 0.0052658124 |
|    clip_fraction        | 0.0842       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.191       |
|    explained_variance   | 0.34         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.68e+03     |
|    n_updates            | 2349         |
|    policy_gradient_loss | 0.00631      |
|    value_loss           | 6.77e+03     |
------------------------------------------
Eval num_timesteps=398000, episode_reward=-157.45 +/- 116.44
Episode length: 16.14 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=-178.87 +/- 138.89
Episode length: 15.52 +/- 4.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=-151.11 +/- 126.08
Episode length: 15.50 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -182     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 195      |
|    time_elapsed    | 835      |
|    total_timesteps | 399360   |
---------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Eval num_timesteps=399500, episode_reward=-143.13 +/- 151.48
Episode length: 17.22 +/- 5.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.2        |
|    mean_reward          | -143        |
| time/                   |             |
|    total_timesteps      | 399500      |
| train/                  |             |
|    approx_kl            | 0.004916048 |
|    clip_fraction        | 0.0375      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.215      |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.61e+03    |
|    n_updates            | 2353        |
|    policy_gradient_loss | 0.00394     |
|    value_loss           | 6.12e+03    |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=-130.07 +/- 184.17
Episode length: 16.50 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=-147.09 +/- 141.82
Episode length: 16.30 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=-164.23 +/- 119.12
Episode length: 14.78 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 196      |
|    time_elapsed    | 839      |
|    total_timesteps | 401408   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=401500, episode_reward=-160.93 +/- 112.89
Episode length: 16.16 +/- 3.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -161        |
| time/                   |             |
|    total_timesteps      | 401500      |
| train/                  |             |
|    approx_kl            | 0.006904004 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.232       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.28e+03    |
|    n_updates            | 2354        |
|    policy_gradient_loss | 0.00386     |
|    value_loss           | 7.32e+03    |
-----------------------------------------
Eval num_timesteps=402000, episode_reward=-175.51 +/- 140.92
Episode length: 14.86 +/- 3.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=402500, episode_reward=-150.94 +/- 138.82
Episode length: 15.98 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 402500   |
---------------------------------
Eval num_timesteps=403000, episode_reward=-141.22 +/- 154.28
Episode length: 15.46 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 197      |
|    time_elapsed    | 844      |
|    total_timesteps | 403456   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=403500, episode_reward=-129.69 +/- 154.54
Episode length: 16.82 +/- 5.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -130         |
| time/                   |              |
|    total_timesteps      | 403500       |
| train/                  |              |
|    approx_kl            | 0.0069966004 |
|    clip_fraction        | 0.0337       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.141       |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.77e+03     |
|    n_updates            | 2356         |
|    policy_gradient_loss | 0.00465      |
|    value_loss           | 6.92e+03     |
------------------------------------------
Eval num_timesteps=404000, episode_reward=-165.88 +/- 137.40
Episode length: 14.68 +/- 4.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=404500, episode_reward=-128.14 +/- 161.97
Episode length: 16.08 +/- 5.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 404500   |
---------------------------------
Eval num_timesteps=405000, episode_reward=-115.22 +/- 183.86
Episode length: 17.36 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=405500, episode_reward=-173.29 +/- 128.85
Episode length: 15.44 +/- 3.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 405500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -150     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 198      |
|    time_elapsed    | 848      |
|    total_timesteps | 405504   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=406000, episode_reward=-146.92 +/- 151.08
Episode length: 15.72 +/- 5.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -147         |
| time/                   |              |
|    total_timesteps      | 406000       |
| train/                  |              |
|    approx_kl            | 0.0025013955 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.188       |
|    explained_variance   | 0.318        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.35e+03     |
|    n_updates            | 2357         |
|    policy_gradient_loss | 0.00403      |
|    value_loss           | 5.72e+03     |
------------------------------------------
Eval num_timesteps=406500, episode_reward=-158.58 +/- 138.26
Episode length: 15.92 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 406500   |
---------------------------------
Eval num_timesteps=407000, episode_reward=-140.20 +/- 166.87
Episode length: 15.84 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=407500, episode_reward=-184.37 +/- 160.53
Episode length: 14.72 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 407500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 199      |
|    time_elapsed    | 852      |
|    total_timesteps | 407552   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=408000, episode_reward=-194.65 +/- 128.54
Episode length: 16.46 +/- 4.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -195         |
| time/                   |              |
|    total_timesteps      | 408000       |
| train/                  |              |
|    approx_kl            | 0.0029404208 |
|    clip_fraction        | 0.0639       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.208       |
|    explained_variance   | 0.28         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.65e+03     |
|    n_updates            | 2360         |
|    policy_gradient_loss | 0.000842     |
|    value_loss           | 6.44e+03     |
------------------------------------------
Eval num_timesteps=408500, episode_reward=-177.86 +/- 107.83
Episode length: 16.46 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 408500   |
---------------------------------
Eval num_timesteps=409000, episode_reward=-135.84 +/- 161.04
Episode length: 18.22 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.2     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=409500, episode_reward=-151.99 +/- 128.12
Episode length: 16.60 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 409500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -152     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 200      |
|    time_elapsed    | 857      |
|    total_timesteps | 409600   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=410000, episode_reward=-167.78 +/- 162.30
Episode length: 17.52 +/- 5.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | -168        |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.006576723 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.209      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.43e+03    |
|    n_updates            | 2361        |
|    policy_gradient_loss | 0.00747     |
|    value_loss           | 6.25e+03    |
-----------------------------------------
Eval num_timesteps=410500, episode_reward=-173.54 +/- 140.35
Episode length: 16.22 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 410500   |
---------------------------------
Eval num_timesteps=411000, episode_reward=-138.56 +/- 159.57
Episode length: 17.90 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=411500, episode_reward=-179.46 +/- 159.74
Episode length: 16.88 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 411500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 201      |
|    time_elapsed    | 861      |
|    total_timesteps | 411648   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=412000, episode_reward=-164.84 +/- 119.49
Episode length: 16.62 +/- 4.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -165         |
| time/                   |              |
|    total_timesteps      | 412000       |
| train/                  |              |
|    approx_kl            | 0.0036431116 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.202       |
|    explained_variance   | 0.355        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.02e+03     |
|    n_updates            | 2362         |
|    policy_gradient_loss | 0.00272      |
|    value_loss           | 6.32e+03     |
------------------------------------------
Eval num_timesteps=412500, episode_reward=-160.94 +/- 135.92
Episode length: 17.88 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 412500   |
---------------------------------
Eval num_timesteps=413000, episode_reward=-150.59 +/- 134.81
Episode length: 16.88 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=413500, episode_reward=-190.07 +/- 128.59
Episode length: 15.66 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 413500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -187     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 202      |
|    time_elapsed    | 865      |
|    total_timesteps | 413696   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=414000, episode_reward=-178.42 +/- 147.26
Episode length: 17.00 +/- 5.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | -178        |
| time/                   |             |
|    total_timesteps      | 414000      |
| train/                  |             |
|    approx_kl            | 0.003196377 |
|    clip_fraction        | 0.0299      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.369       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.46e+03    |
|    n_updates            | 2363        |
|    policy_gradient_loss | 0.000804    |
|    value_loss           | 6.06e+03    |
-----------------------------------------
Eval num_timesteps=414500, episode_reward=-169.01 +/- 134.67
Episode length: 16.60 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 414500   |
---------------------------------
Eval num_timesteps=415000, episode_reward=-186.30 +/- 135.63
Episode length: 17.08 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=415500, episode_reward=-155.53 +/- 170.86
Episode length: 16.82 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 415500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -179     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 203      |
|    time_elapsed    | 870      |
|    total_timesteps | 415744   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=416000, episode_reward=-155.32 +/- 167.47
Episode length: 17.98 +/- 5.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | -155         |
| time/                   |              |
|    total_timesteps      | 416000       |
| train/                  |              |
|    approx_kl            | 0.0022016924 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.134       |
|    explained_variance   | 0.404        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.78e+03     |
|    n_updates            | 2364         |
|    policy_gradient_loss | 0.000735     |
|    value_loss           | 5.57e+03     |
------------------------------------------
Eval num_timesteps=416500, episode_reward=-161.93 +/- 127.55
Episode length: 16.76 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 416500   |
---------------------------------
Eval num_timesteps=417000, episode_reward=-190.18 +/- 129.07
Episode length: 16.06 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=417500, episode_reward=-159.91 +/- 121.50
Episode length: 16.82 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 417500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.6     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 204      |
|    time_elapsed    | 874      |
|    total_timesteps | 417792   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=418000, episode_reward=-190.71 +/- 156.43
Episode length: 15.64 +/- 5.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.6        |
|    mean_reward          | -191        |
| time/                   |             |
|    total_timesteps      | 418000      |
| train/                  |             |
|    approx_kl            | 0.009551565 |
|    clip_fraction        | 0.0224      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.253       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.34e+03    |
|    n_updates            | 2367        |
|    policy_gradient_loss | 0.00151     |
|    value_loss           | 6.8e+03     |
-----------------------------------------
Eval num_timesteps=418500, episode_reward=-193.70 +/- 115.32
Episode length: 15.60 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 418500   |
---------------------------------
Eval num_timesteps=419000, episode_reward=-152.89 +/- 117.99
Episode length: 17.00 +/- 5.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=419500, episode_reward=-171.50 +/- 120.01
Episode length: 17.44 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 419500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -186     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 205      |
|    time_elapsed    | 878      |
|    total_timesteps | 419840   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=420000, episode_reward=-149.33 +/- 141.16
Episode length: 17.96 +/- 5.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | -149         |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0037160218 |
|    clip_fraction        | 0.028        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.117       |
|    explained_variance   | 0.353        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.89e+03     |
|    n_updates            | 2369         |
|    policy_gradient_loss | 0.00165      |
|    value_loss           | 7.82e+03     |
------------------------------------------
Eval num_timesteps=420500, episode_reward=-187.64 +/- 157.15
Episode length: 16.98 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 420500   |
---------------------------------
Eval num_timesteps=421000, episode_reward=-155.98 +/- 122.26
Episode length: 17.36 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=421500, episode_reward=-165.95 +/- 123.99
Episode length: 15.86 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 421500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -163     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 206      |
|    time_elapsed    | 883      |
|    total_timesteps | 421888   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=422000, episode_reward=-190.19 +/- 152.24
Episode length: 15.32 +/- 5.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | -190        |
| time/                   |             |
|    total_timesteps      | 422000      |
| train/                  |             |
|    approx_kl            | 0.013873073 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.095      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.61e+03    |
|    n_updates            | 2372        |
|    policy_gradient_loss | -0.000979   |
|    value_loss           | 6.96e+03    |
-----------------------------------------
Eval num_timesteps=422500, episode_reward=-174.20 +/- 119.38
Episode length: 16.84 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 422500   |
---------------------------------
Eval num_timesteps=423000, episode_reward=-179.57 +/- 136.48
Episode length: 15.44 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=423500, episode_reward=-164.29 +/- 178.38
Episode length: 17.36 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 423500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -168     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 207      |
|    time_elapsed    | 887      |
|    total_timesteps | 423936   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=424000, episode_reward=-198.04 +/- 138.78
Episode length: 15.78 +/- 4.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -198        |
| time/                   |             |
|    total_timesteps      | 424000      |
| train/                  |             |
|    approx_kl            | 0.001492123 |
|    clip_fraction        | 0.00521     |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0608     |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.19e+03    |
|    n_updates            | 2373        |
|    policy_gradient_loss | -0.000146   |
|    value_loss           | 6.29e+03    |
-----------------------------------------
Eval num_timesteps=424500, episode_reward=-190.38 +/- 132.63
Episode length: 15.98 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 424500   |
---------------------------------
Eval num_timesteps=425000, episode_reward=-174.17 +/- 124.88
Episode length: 17.38 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=425500, episode_reward=-108.59 +/- 140.65
Episode length: 19.14 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.1     |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 425500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -173     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 208      |
|    time_elapsed    | 891      |
|    total_timesteps | 425984   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=426000, episode_reward=-193.24 +/- 122.99
Episode length: 16.16 +/- 5.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -193         |
| time/                   |              |
|    total_timesteps      | 426000       |
| train/                  |              |
|    approx_kl            | 0.0019270856 |
|    clip_fraction        | 0.00639      |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.0744      |
|    explained_variance   | 0.41         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.65e+03     |
|    n_updates            | 2374         |
|    policy_gradient_loss | 0.00496      |
|    value_loss           | 5.89e+03     |
------------------------------------------
Eval num_timesteps=426500, episode_reward=-221.83 +/- 123.87
Episode length: 15.40 +/- 4.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 426500   |
---------------------------------
Eval num_timesteps=427000, episode_reward=-175.29 +/- 139.14
Episode length: 17.02 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=427500, episode_reward=-189.07 +/- 136.23
Episode length: 16.86 +/- 5.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 427500   |
---------------------------------
Eval num_timesteps=428000, episode_reward=-205.39 +/- 123.83
Episode length: 16.32 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -197     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 209      |
|    time_elapsed    | 896      |
|    total_timesteps | 428032   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=428500, episode_reward=-194.37 +/- 139.96
Episode length: 15.98 +/- 5.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -194        |
| time/                   |             |
|    total_timesteps      | 428500      |
| train/                  |             |
|    approx_kl            | 0.003757183 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.0894     |
|    explained_variance   | 0.424       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.98e+03    |
|    n_updates            | 2375        |
|    policy_gradient_loss | 0.00337     |
|    value_loss           | 5.79e+03    |
-----------------------------------------
Eval num_timesteps=429000, episode_reward=-211.07 +/- 127.27
Episode length: 16.06 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 429000   |
---------------------------------
Eval num_timesteps=429500, episode_reward=-164.68 +/- 126.41
Episode length: 17.22 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 429500   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-188.83 +/- 122.78
Episode length: 16.28 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -213     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 210      |
|    time_elapsed    | 900      |
|    total_timesteps | 430080   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=430500, episode_reward=-161.20 +/- 178.67
Episode length: 17.36 +/- 6.71
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.4       |
|    mean_reward          | -161       |
| time/                   |            |
|    total_timesteps      | 430500     |
| train/                  |            |
|    approx_kl            | 0.00521088 |
|    clip_fraction        | 0.0329     |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.155     |
|    explained_variance   | 0.449      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.71e+03   |
|    n_updates            | 2377       |
|    policy_gradient_loss | 0.00164    |
|    value_loss           | 5.51e+03   |
----------------------------------------
Eval num_timesteps=431000, episode_reward=-190.21 +/- 154.59
Episode length: 16.82 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 431000   |
---------------------------------
Eval num_timesteps=431500, episode_reward=-188.64 +/- 136.38
Episode length: 16.80 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 431500   |
---------------------------------
Eval num_timesteps=432000, episode_reward=-150.21 +/- 168.45
Episode length: 17.66 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -189     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 211      |
|    time_elapsed    | 904      |
|    total_timesteps | 432128   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=432500, episode_reward=-191.55 +/- 139.74
Episode length: 16.06 +/- 5.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -192         |
| time/                   |              |
|    total_timesteps      | 432500       |
| train/                  |              |
|    approx_kl            | 0.0024865933 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.152       |
|    explained_variance   | 0.412        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.97e+03     |
|    n_updates            | 2378         |
|    policy_gradient_loss | 0.00294      |
|    value_loss           | 6.42e+03     |
------------------------------------------
Eval num_timesteps=433000, episode_reward=-144.14 +/- 151.11
Episode length: 17.60 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 433000   |
---------------------------------
Eval num_timesteps=433500, episode_reward=-174.52 +/- 158.80
Episode length: 17.08 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 433500   |
---------------------------------
Eval num_timesteps=434000, episode_reward=-199.21 +/- 134.17
Episode length: 16.90 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -156     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 212      |
|    time_elapsed    | 909      |
|    total_timesteps | 434176   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=434500, episode_reward=-162.74 +/- 128.77
Episode length: 15.90 +/- 5.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -163         |
| time/                   |              |
|    total_timesteps      | 434500       |
| train/                  |              |
|    approx_kl            | 0.0058804816 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.182       |
|    explained_variance   | 0.334        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.5e+03      |
|    n_updates            | 2379         |
|    policy_gradient_loss | 0.00631      |
|    value_loss           | 6.01e+03     |
------------------------------------------
Eval num_timesteps=435000, episode_reward=-183.41 +/- 107.37
Episode length: 15.44 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 435000   |
---------------------------------
Eval num_timesteps=435500, episode_reward=-197.43 +/- 126.39
Episode length: 15.66 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 435500   |
---------------------------------
Eval num_timesteps=436000, episode_reward=-163.93 +/- 117.81
Episode length: 16.48 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 213      |
|    time_elapsed    | 913      |
|    total_timesteps | 436224   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=436500, episode_reward=-194.65 +/- 167.94
Episode length: 16.02 +/- 5.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -195        |
| time/                   |             |
|    total_timesteps      | 436500      |
| train/                  |             |
|    approx_kl            | 0.006680677 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.379       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.27e+03    |
|    n_updates            | 2380        |
|    policy_gradient_loss | 0.00376     |
|    value_loss           | 8.2e+03     |
-----------------------------------------
Eval num_timesteps=437000, episode_reward=-183.33 +/- 147.25
Episode length: 15.64 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 437000   |
---------------------------------
Eval num_timesteps=437500, episode_reward=-164.10 +/- 104.01
Episode length: 16.66 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 437500   |
---------------------------------
Eval num_timesteps=438000, episode_reward=-196.35 +/- 122.60
Episode length: 15.94 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -175     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 214      |
|    time_elapsed    | 917      |
|    total_timesteps | 438272   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=438500, episode_reward=-153.29 +/- 173.73
Episode length: 15.92 +/- 6.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | -153        |
| time/                   |             |
|    total_timesteps      | 438500      |
| train/                  |             |
|    approx_kl            | 0.002429607 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.174      |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.84e+03    |
|    n_updates            | 2382        |
|    policy_gradient_loss | 0.000507    |
|    value_loss           | 6.53e+03    |
-----------------------------------------
Eval num_timesteps=439000, episode_reward=-153.25 +/- 165.99
Episode length: 16.16 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 439000   |
---------------------------------
Eval num_timesteps=439500, episode_reward=-128.08 +/- 139.22
Episode length: 17.32 +/- 4.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 439500   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-125.29 +/- 198.99
Episode length: 16.86 +/- 5.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 215      |
|    time_elapsed    | 921      |
|    total_timesteps | 440320   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=440500, episode_reward=-134.90 +/- 153.20
Episode length: 17.44 +/- 5.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.4         |
|    mean_reward          | -135         |
| time/                   |              |
|    total_timesteps      | 440500       |
| train/                  |              |
|    approx_kl            | 0.0038018858 |
|    clip_fraction        | 0.0645       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.149       |
|    explained_variance   | 0.422        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.25e+03     |
|    n_updates            | 2383         |
|    policy_gradient_loss | 0.000493     |
|    value_loss           | 6.22e+03     |
------------------------------------------
Eval num_timesteps=441000, episode_reward=-184.67 +/- 135.30
Episode length: 15.72 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 441000   |
---------------------------------
Eval num_timesteps=441500, episode_reward=-155.69 +/- 98.64
Episode length: 17.26 +/- 4.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 441500   |
---------------------------------
Eval num_timesteps=442000, episode_reward=-142.69 +/- 155.33
Episode length: 17.42 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 216      |
|    time_elapsed    | 926      |
|    total_timesteps | 442368   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=442500, episode_reward=-164.51 +/- 152.47
Episode length: 15.50 +/- 4.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 15.5      |
|    mean_reward          | -165      |
| time/                   |           |
|    total_timesteps      | 442500    |
| train/                  |           |
|    approx_kl            | 0.0071341 |
|    clip_fraction        | 0.0678    |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.168    |
|    explained_variance   | 0.38      |
|    learning_rate        | 0.0001    |
|    loss                 | 2.59e+03  |
|    n_updates            | 2386      |
|    policy_gradient_loss | 0.00248   |
|    value_loss           | 6.41e+03  |
---------------------------------------
Eval num_timesteps=443000, episode_reward=-158.33 +/- 151.49
Episode length: 16.56 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 443000   |
---------------------------------
Eval num_timesteps=443500, episode_reward=-159.29 +/- 134.55
Episode length: 15.76 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 443500   |
---------------------------------
Eval num_timesteps=444000, episode_reward=-146.28 +/- 191.53
Episode length: 15.54 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -142     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 217      |
|    time_elapsed    | 930      |
|    total_timesteps | 444416   |
---------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Eval num_timesteps=444500, episode_reward=-132.41 +/- 145.89
Episode length: 16.60 +/- 5.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.6        |
|    mean_reward          | -132        |
| time/                   |             |
|    total_timesteps      | 444500      |
| train/                  |             |
|    approx_kl            | 0.004176489 |
|    clip_fraction        | 0.0447      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 0.352       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.03e+03    |
|    n_updates            | 2393        |
|    policy_gradient_loss | 0.00147     |
|    value_loss           | 6.78e+03    |
-----------------------------------------
Eval num_timesteps=445000, episode_reward=-102.54 +/- 183.28
Episode length: 17.84 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -103     |
| time/              |          |
|    total_timesteps | 445000   |
---------------------------------
Eval num_timesteps=445500, episode_reward=-114.83 +/- 146.17
Episode length: 16.62 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 445500   |
---------------------------------
Eval num_timesteps=446000, episode_reward=-140.20 +/- 155.53
Episode length: 16.64 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -171     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 218      |
|    time_elapsed    | 934      |
|    total_timesteps | 446464   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=446500, episode_reward=-127.26 +/- 153.08
Episode length: 15.60 +/- 4.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -127         |
| time/                   |              |
|    total_timesteps      | 446500       |
| train/                  |              |
|    approx_kl            | 0.0045010587 |
|    clip_fraction        | 0.065        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.147       |
|    explained_variance   | 0.362        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.47e+03     |
|    n_updates            | 2394         |
|    policy_gradient_loss | -0.000916    |
|    value_loss           | 6.28e+03     |
------------------------------------------
Eval num_timesteps=447000, episode_reward=-133.16 +/- 155.98
Episode length: 16.32 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 447000   |
---------------------------------
Eval num_timesteps=447500, episode_reward=-171.37 +/- 142.90
Episode length: 15.34 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 447500   |
---------------------------------
Eval num_timesteps=448000, episode_reward=-101.01 +/- 172.88
Episode length: 16.58 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -101     |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=448500, episode_reward=-128.13 +/- 130.25
Episode length: 16.86 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 448500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 219      |
|    time_elapsed    | 939      |
|    total_timesteps | 448512   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.06
Eval num_timesteps=449000, episode_reward=-186.06 +/- 127.06
Episode length: 15.30 +/- 4.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | -186        |
| time/                   |             |
|    total_timesteps      | 449000      |
| train/                  |             |
|    approx_kl            | 0.009825217 |
|    clip_fraction        | 0.0206      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.112      |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.54e+03    |
|    n_updates            | 2396        |
|    policy_gradient_loss | 0.00229     |
|    value_loss           | 6.78e+03    |
-----------------------------------------
Eval num_timesteps=449500, episode_reward=-157.77 +/- 132.31
Episode length: 15.70 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 449500   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-146.67 +/- 139.92
Episode length: 16.38 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=450500, episode_reward=-124.55 +/- 152.09
Episode length: 16.72 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 450500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 220      |
|    time_elapsed    | 943      |
|    total_timesteps | 450560   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=451000, episode_reward=-144.89 +/- 195.83
Episode length: 16.62 +/- 6.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -145         |
| time/                   |              |
|    total_timesteps      | 451000       |
| train/                  |              |
|    approx_kl            | 0.0032456669 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.171       |
|    explained_variance   | 0.334        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.87e+03     |
|    n_updates            | 2397         |
|    policy_gradient_loss | 0.000759     |
|    value_loss           | 6.94e+03     |
------------------------------------------
Eval num_timesteps=451500, episode_reward=-189.13 +/- 133.61
Episode length: 15.12 +/- 4.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 451500   |
---------------------------------
Eval num_timesteps=452000, episode_reward=-160.47 +/- 138.90
Episode length: 15.16 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=452500, episode_reward=-146.49 +/- 152.07
Episode length: 16.02 +/- 5.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 452500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -146     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 221      |
|    time_elapsed    | 947      |
|    total_timesteps | 452608   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=453000, episode_reward=-181.57 +/- 127.92
Episode length: 15.86 +/- 4.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.9        |
|    mean_reward          | -182        |
| time/                   |             |
|    total_timesteps      | 453000      |
| train/                  |             |
|    approx_kl            | 0.005372421 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.189      |
|    explained_variance   | 0.298       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.27e+03    |
|    n_updates            | 2398        |
|    policy_gradient_loss | 0.00831     |
|    value_loss           | 7.57e+03    |
-----------------------------------------
Eval num_timesteps=453500, episode_reward=-182.34 +/- 144.78
Episode length: 16.22 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 453500   |
---------------------------------
Eval num_timesteps=454000, episode_reward=-141.70 +/- 146.46
Episode length: 18.00 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=454500, episode_reward=-192.62 +/- 142.86
Episode length: 16.04 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 454500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -156     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 222      |
|    time_elapsed    | 952      |
|    total_timesteps | 454656   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=455000, episode_reward=-166.19 +/- 149.06
Episode length: 16.58 +/- 5.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -166         |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0044138334 |
|    clip_fraction        | 0.0703       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.203       |
|    explained_variance   | 0.327        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.33e+03     |
|    n_updates            | 2399         |
|    policy_gradient_loss | -0.0037      |
|    value_loss           | 6.9e+03      |
------------------------------------------
Eval num_timesteps=455500, episode_reward=-173.07 +/- 138.87
Episode length: 16.38 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 455500   |
---------------------------------
Eval num_timesteps=456000, episode_reward=-173.80 +/- 115.98
Episode length: 15.96 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=456500, episode_reward=-150.85 +/- 141.68
Episode length: 17.40 +/- 5.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 456500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -164     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 223      |
|    time_elapsed    | 956      |
|    total_timesteps | 456704   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=457000, episode_reward=-157.41 +/- 120.26
Episode length: 17.04 +/- 5.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -157         |
| time/                   |              |
|    total_timesteps      | 457000       |
| train/                  |              |
|    approx_kl            | 0.0054108505 |
|    clip_fraction        | 0.0259       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.155       |
|    explained_variance   | 0.404        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.92e+03     |
|    n_updates            | 2401         |
|    policy_gradient_loss | 0.00245      |
|    value_loss           | 5.43e+03     |
------------------------------------------
Eval num_timesteps=457500, episode_reward=-197.49 +/- 137.30
Episode length: 17.02 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 457500   |
---------------------------------
Eval num_timesteps=458000, episode_reward=-173.63 +/- 124.34
Episode length: 16.16 +/- 4.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=458500, episode_reward=-193.31 +/- 136.14
Episode length: 16.28 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 458500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | -157     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 224      |
|    time_elapsed    | 960      |
|    total_timesteps | 458752   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=459000, episode_reward=-193.14 +/- 129.21
Episode length: 16.06 +/- 4.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -193         |
| time/                   |              |
|    total_timesteps      | 459000       |
| train/                  |              |
|    approx_kl            | 0.0030317735 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.183       |
|    explained_variance   | 0.431        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.62e+03     |
|    n_updates            | 2402         |
|    policy_gradient_loss | 0.000147     |
|    value_loss           | 6.41e+03     |
------------------------------------------
Eval num_timesteps=459500, episode_reward=-165.18 +/- 136.86
Episode length: 16.54 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 459500   |
---------------------------------
Eval num_timesteps=460000, episode_reward=-162.86 +/- 110.77
Episode length: 16.12 +/- 4.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=460500, episode_reward=-173.21 +/- 129.36
Episode length: 17.22 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 460500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 225      |
|    time_elapsed    | 964      |
|    total_timesteps | 460800   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=461000, episode_reward=-160.20 +/- 181.35
Episode length: 16.70 +/- 6.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -160         |
| time/                   |              |
|    total_timesteps      | 461000       |
| train/                  |              |
|    approx_kl            | 0.0058690845 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.217       |
|    explained_variance   | 0.389        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.82e+03     |
|    n_updates            | 2404         |
|    policy_gradient_loss | 0.0034       |
|    value_loss           | 6.63e+03     |
------------------------------------------
Eval num_timesteps=461500, episode_reward=-123.49 +/- 119.75
Episode length: 16.70 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 461500   |
---------------------------------
Eval num_timesteps=462000, episode_reward=-161.28 +/- 113.32
Episode length: 16.00 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=462500, episode_reward=-159.24 +/- 149.25
Episode length: 16.26 +/- 5.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 462500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -184     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 226      |
|    time_elapsed    | 969      |
|    total_timesteps | 462848   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=463000, episode_reward=-144.13 +/- 140.34
Episode length: 16.30 +/- 5.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -144        |
| time/                   |             |
|    total_timesteps      | 463000      |
| train/                  |             |
|    approx_kl            | 0.003738219 |
|    clip_fraction        | 0.0164      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.374       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.37e+03    |
|    n_updates            | 2405        |
|    policy_gradient_loss | 0.0025      |
|    value_loss           | 5.95e+03    |
-----------------------------------------
Eval num_timesteps=463500, episode_reward=-141.19 +/- 148.61
Episode length: 16.78 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 463500   |
---------------------------------
Eval num_timesteps=464000, episode_reward=-178.16 +/- 144.92
Episode length: 15.88 +/- 5.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=464500, episode_reward=-155.91 +/- 163.19
Episode length: 16.74 +/- 5.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 464500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -167     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 227      |
|    time_elapsed    | 973      |
|    total_timesteps | 464896   |
---------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Eval num_timesteps=465000, episode_reward=-142.51 +/- 138.13
Episode length: 16.48 +/- 5.02
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -143        |
| time/                   |             |
|    total_timesteps      | 465000      |
| train/                  |             |
|    approx_kl            | 0.005895444 |
|    clip_fraction        | 0.0534      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 0.363       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.53e+03    |
|    n_updates            | 2408        |
|    policy_gradient_loss | 0.002       |
|    value_loss           | 6.34e+03    |
-----------------------------------------
Eval num_timesteps=465500, episode_reward=-153.74 +/- 116.20
Episode length: 15.46 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 465500   |
---------------------------------
Eval num_timesteps=466000, episode_reward=-164.50 +/- 147.37
Episode length: 15.46 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=466500, episode_reward=-127.32 +/- 157.28
Episode length: 16.22 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 466500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 228      |
|    time_elapsed    | 977      |
|    total_timesteps | 466944   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=467000, episode_reward=-184.98 +/- 130.21
Episode length: 15.62 +/- 5.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.6         |
|    mean_reward          | -185         |
| time/                   |              |
|    total_timesteps      | 467000       |
| train/                  |              |
|    approx_kl            | 0.0025871934 |
|    clip_fraction        | 0.0367       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.159       |
|    explained_variance   | 0.399        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.26e+03     |
|    n_updates            | 2409         |
|    policy_gradient_loss | 0.0029       |
|    value_loss           | 5.86e+03     |
------------------------------------------
Eval num_timesteps=467500, episode_reward=-173.02 +/- 132.50
Episode length: 16.48 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 467500   |
---------------------------------
Eval num_timesteps=468000, episode_reward=-165.55 +/- 132.59
Episode length: 15.70 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=468500, episode_reward=-181.25 +/- 113.85
Episode length: 15.46 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 468500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -143     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 229      |
|    time_elapsed    | 981      |
|    total_timesteps | 468992   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=469000, episode_reward=-150.68 +/- 137.47
Episode length: 17.72 +/- 4.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.7         |
|    mean_reward          | -151         |
| time/                   |              |
|    total_timesteps      | 469000       |
| train/                  |              |
|    approx_kl            | 0.0033646603 |
|    clip_fraction        | 0.0335       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.17        |
|    explained_variance   | 0.342        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.67e+03     |
|    n_updates            | 2410         |
|    policy_gradient_loss | 0.00694      |
|    value_loss           | 6.57e+03     |
------------------------------------------
Eval num_timesteps=469500, episode_reward=-108.52 +/- 180.43
Episode length: 18.26 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.3     |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 469500   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-124.68 +/- 141.70
Episode length: 18.30 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18.3     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=470500, episode_reward=-201.86 +/- 162.57
Episode length: 15.90 +/- 5.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 470500   |
---------------------------------
Eval num_timesteps=471000, episode_reward=-154.93 +/- 152.83
Episode length: 17.36 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -174     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 230      |
|    time_elapsed    | 986      |
|    total_timesteps | 471040   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=471500, episode_reward=-176.36 +/- 144.79
Episode length: 15.08 +/- 4.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.1         |
|    mean_reward          | -176         |
| time/                   |              |
|    total_timesteps      | 471500       |
| train/                  |              |
|    approx_kl            | 0.0075509804 |
|    clip_fraction        | 0.0982       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.185       |
|    explained_variance   | 0.408        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.31e+03     |
|    n_updates            | 2411         |
|    policy_gradient_loss | -0.0118      |
|    value_loss           | 5.13e+03     |
------------------------------------------
Eval num_timesteps=472000, episode_reward=-149.55 +/- 135.43
Episode length: 16.82 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -150     |
| time/              |          |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=472500, episode_reward=-131.81 +/- 157.11
Episode length: 16.26 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 472500   |
---------------------------------
Eval num_timesteps=473000, episode_reward=-133.67 +/- 136.28
Episode length: 16.04 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -154     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 231      |
|    time_elapsed    | 991      |
|    total_timesteps | 473088   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=473500, episode_reward=-148.99 +/- 125.23
Episode length: 15.54 +/- 4.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.5       |
|    mean_reward          | -149       |
| time/                   |            |
|    total_timesteps      | 473500     |
| train/                  |            |
|    approx_kl            | 0.00201153 |
|    clip_fraction        | 0.00684    |
|    clip_range           | 0.2        |
|    clip_range_vf        | 0.2        |
|    entropy_loss         | -0.181     |
|    explained_variance   | 0.357      |
|    learning_rate        | 0.0001     |
|    loss                 | 2.92e+03   |
|    n_updates            | 2412       |
|    policy_gradient_loss | -0.000631  |
|    value_loss           | 5.93e+03   |
----------------------------------------
Eval num_timesteps=474000, episode_reward=-154.68 +/- 171.09
Episode length: 15.64 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=474500, episode_reward=-142.73 +/- 160.72
Episode length: 16.08 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 474500   |
---------------------------------
Eval num_timesteps=475000, episode_reward=-134.06 +/- 171.01
Episode length: 15.82 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -176     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 232      |
|    time_elapsed    | 995      |
|    total_timesteps | 475136   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=475500, episode_reward=-187.35 +/- 129.09
Episode length: 17.32 +/- 5.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.3         |
|    mean_reward          | -187         |
| time/                   |              |
|    total_timesteps      | 475500       |
| train/                  |              |
|    approx_kl            | 0.0064832214 |
|    clip_fraction        | 0.0659       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.197       |
|    explained_variance   | 0.332        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.71e+03     |
|    n_updates            | 2414         |
|    policy_gradient_loss | 0.000217     |
|    value_loss           | 6.33e+03     |
------------------------------------------
Eval num_timesteps=476000, episode_reward=-178.56 +/- 142.64
Episode length: 16.88 +/- 5.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=476500, episode_reward=-138.89 +/- 123.79
Episode length: 17.48 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 476500   |
---------------------------------
Eval num_timesteps=477000, episode_reward=-169.50 +/- 132.00
Episode length: 16.80 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -166     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 233      |
|    time_elapsed    | 999      |
|    total_timesteps | 477184   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=477500, episode_reward=-131.75 +/- 160.46
Episode length: 17.70 +/- 5.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 17.7      |
|    mean_reward          | -132      |
| time/                   |           |
|    total_timesteps      | 477500    |
| train/                  |           |
|    approx_kl            | 0.0032601 |
|    clip_fraction        | 0.012     |
|    clip_range           | 0.2       |
|    clip_range_vf        | 0.2       |
|    entropy_loss         | -0.21     |
|    explained_variance   | 0.421     |
|    learning_rate        | 0.0001    |
|    loss                 | 3.85e+03  |
|    n_updates            | 2415      |
|    policy_gradient_loss | 0.00806   |
|    value_loss           | 6.14e+03  |
---------------------------------------
Eval num_timesteps=478000, episode_reward=-167.80 +/- 161.39
Episode length: 17.38 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 478000   |
---------------------------------
Eval num_timesteps=478500, episode_reward=-173.76 +/- 157.36
Episode length: 16.56 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -174     |
| time/              |          |
|    total_timesteps | 478500   |
---------------------------------
Eval num_timesteps=479000, episode_reward=-147.06 +/- 145.21
Episode length: 17.96 +/- 5.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -189     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 234      |
|    time_elapsed    | 1003     |
|    total_timesteps | 479232   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=479500, episode_reward=-175.40 +/- 126.71
Episode length: 16.36 +/- 5.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -175         |
| time/                   |              |
|    total_timesteps      | 479500       |
| train/                  |              |
|    approx_kl            | 0.0056532645 |
|    clip_fraction        | 0.082        |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.206       |
|    explained_variance   | 0.381        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.13e+03     |
|    n_updates            | 2416         |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 6.19e+03     |
------------------------------------------
Eval num_timesteps=480000, episode_reward=-204.97 +/- 134.26
Episode length: 16.66 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=480500, episode_reward=-181.72 +/- 163.55
Episode length: 15.64 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 480500   |
---------------------------------
Eval num_timesteps=481000, episode_reward=-147.68 +/- 149.49
Episode length: 16.88 +/- 5.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -192     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 235      |
|    time_elapsed    | 1008     |
|    total_timesteps | 481280   |
---------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Eval num_timesteps=481500, episode_reward=-188.38 +/- 140.91
Episode length: 15.54 +/- 5.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -188         |
| time/                   |              |
|    total_timesteps      | 481500       |
| train/                  |              |
|    approx_kl            | 0.0042966856 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.167       |
|    explained_variance   | 0.401        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.89e+03     |
|    n_updates            | 2418         |
|    policy_gradient_loss | 0.00138      |
|    value_loss           | 6.09e+03     |
------------------------------------------
Eval num_timesteps=482000, episode_reward=-166.82 +/- 120.14
Episode length: 15.70 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 482000   |
---------------------------------
Eval num_timesteps=482500, episode_reward=-204.78 +/- 125.34
Episode length: 15.80 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 482500   |
---------------------------------
Eval num_timesteps=483000, episode_reward=-176.74 +/- 133.97
Episode length: 15.82 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -180     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 236      |
|    time_elapsed    | 1012     |
|    total_timesteps | 483328   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=483500, episode_reward=-170.53 +/- 136.71
Episode length: 16.70 +/- 4.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -171         |
| time/                   |              |
|    total_timesteps      | 483500       |
| train/                  |              |
|    approx_kl            | 0.0025254206 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.201       |
|    explained_variance   | 0.421        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.31e+03     |
|    n_updates            | 2419         |
|    policy_gradient_loss | 0.0107       |
|    value_loss           | 6.4e+03      |
------------------------------------------
Eval num_timesteps=484000, episode_reward=-164.38 +/- 149.36
Episode length: 17.24 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=484500, episode_reward=-174.96 +/- 127.54
Episode length: 17.10 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 484500   |
---------------------------------
Eval num_timesteps=485000, episode_reward=-164.49 +/- 158.60
Episode length: 16.50 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 237      |
|    time_elapsed    | 1016     |
|    total_timesteps | 485376   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=485500, episode_reward=-172.04 +/- 128.20
Episode length: 16.46 +/- 5.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -172         |
| time/                   |              |
|    total_timesteps      | 485500       |
| train/                  |              |
|    approx_kl            | 0.0062525184 |
|    clip_fraction        | 0.0906       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.195       |
|    explained_variance   | 0.334        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.74e+03     |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.00468     |
|    value_loss           | 7.31e+03     |
------------------------------------------
Eval num_timesteps=486000, episode_reward=-141.62 +/- 111.66
Episode length: 16.44 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=486500, episode_reward=-139.20 +/- 130.76
Episode length: 15.84 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 486500   |
---------------------------------
Eval num_timesteps=487000, episode_reward=-134.60 +/- 130.35
Episode length: 17.04 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -159     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 238      |
|    time_elapsed    | 1020     |
|    total_timesteps | 487424   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=487500, episode_reward=-157.89 +/- 137.25
Episode length: 16.98 +/- 5.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | -158        |
| time/                   |             |
|    total_timesteps      | 487500      |
| train/                  |             |
|    approx_kl            | 0.003726507 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.222      |
|    explained_variance   | 0.385       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.79e+03    |
|    n_updates            | 2421        |
|    policy_gradient_loss | 0.000819    |
|    value_loss           | 5.99e+03    |
-----------------------------------------
Eval num_timesteps=488000, episode_reward=-188.71 +/- 124.77
Episode length: 16.18 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=488500, episode_reward=-205.95 +/- 185.45
Episode length: 16.72 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 488500   |
---------------------------------
Eval num_timesteps=489000, episode_reward=-132.04 +/- 118.03
Episode length: 17.84 +/- 5.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -161     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 239      |
|    time_elapsed    | 1025     |
|    total_timesteps | 489472   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=489500, episode_reward=-213.33 +/- 142.25
Episode length: 15.52 +/- 5.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -213         |
| time/                   |              |
|    total_timesteps      | 489500       |
| train/                  |              |
|    approx_kl            | 0.0056385924 |
|    clip_fraction        | 0.0871       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.213       |
|    explained_variance   | 0.393        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.35e+03     |
|    n_updates            | 2422         |
|    policy_gradient_loss | 0.00166      |
|    value_loss           | 6.7e+03      |
------------------------------------------
Eval num_timesteps=490000, episode_reward=-169.53 +/- 164.10
Episode length: 17.68 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
Eval num_timesteps=490500, episode_reward=-186.89 +/- 109.82
Episode length: 15.78 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 490500   |
---------------------------------
Eval num_timesteps=491000, episode_reward=-159.14 +/- 125.22
Episode length: 17.78 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.8     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=491500, episode_reward=-155.36 +/- 153.94
Episode length: 17.98 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 18       |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 491500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -202     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 240      |
|    time_elapsed    | 1030     |
|    total_timesteps | 491520   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.06
Eval num_timesteps=492000, episode_reward=-156.27 +/- 140.12
Episode length: 17.50 +/- 5.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.5        |
|    mean_reward          | -156        |
| time/                   |             |
|    total_timesteps      | 492000      |
| train/                  |             |
|    approx_kl            | 0.007460785 |
|    clip_fraction        | 0.0377      |
|    clip_range           | 0.2         |
|    clip_range_vf        | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.3e+03     |
|    n_updates            | 2423        |
|    policy_gradient_loss | -0.000323   |
|    value_loss           | 5.97e+03    |
-----------------------------------------
Eval num_timesteps=492500, episode_reward=-168.20 +/- 155.16
Episode length: 17.08 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 492500   |
---------------------------------
Eval num_timesteps=493000, episode_reward=-197.68 +/- 156.76
Episode length: 16.18 +/- 5.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=493500, episode_reward=-192.30 +/- 145.44
Episode length: 16.86 +/- 5.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 493500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | -169     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 241      |
|    time_elapsed    | 1034     |
|    total_timesteps | 493568   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=494000, episode_reward=-172.39 +/- 158.37
Episode length: 16.46 +/- 5.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -172         |
| time/                   |              |
|    total_timesteps      | 494000       |
| train/                  |              |
|    approx_kl            | 0.0030117661 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.122       |
|    explained_variance   | 0.405        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.76e+03     |
|    n_updates            | 2424         |
|    policy_gradient_loss | 0.00604      |
|    value_loss           | 7.31e+03     |
------------------------------------------
Eval num_timesteps=494500, episode_reward=-199.82 +/- 147.83
Episode length: 16.58 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 494500   |
---------------------------------
Eval num_timesteps=495000, episode_reward=-169.20 +/- 158.62
Episode length: 16.72 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=495500, episode_reward=-123.06 +/- 150.36
Episode length: 19.10 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 19.1     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 495500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -198     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 242      |
|    time_elapsed    | 1038     |
|    total_timesteps | 495616   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=496000, episode_reward=-177.03 +/- 122.93
Episode length: 16.10 +/- 5.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -177         |
| time/                   |              |
|    total_timesteps      | 496000       |
| train/                  |              |
|    approx_kl            | 0.0033254095 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.128       |
|    explained_variance   | 0.472        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.33e+03     |
|    n_updates            | 2425         |
|    policy_gradient_loss | 0.000853     |
|    value_loss           | 6.42e+03     |
------------------------------------------
Eval num_timesteps=496500, episode_reward=-166.07 +/- 166.25
Episode length: 16.04 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 496500   |
---------------------------------
Eval num_timesteps=497000, episode_reward=-165.34 +/- 125.89
Episode length: 16.46 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=497500, episode_reward=-189.43 +/- 122.67
Episode length: 15.42 +/- 4.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 497500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -178     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 243      |
|    time_elapsed    | 1042     |
|    total_timesteps | 497664   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
Eval num_timesteps=498000, episode_reward=-178.78 +/- 124.64
Episode length: 16.30 +/- 4.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -179         |
| time/                   |              |
|    total_timesteps      | 498000       |
| train/                  |              |
|    approx_kl            | 0.0049204305 |
|    clip_fraction        | 0.0493       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.173       |
|    explained_variance   | 0.428        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.25e+03     |
|    n_updates            | 2426         |
|    policy_gradient_loss | -0.00575     |
|    value_loss           | 6.68e+03     |
------------------------------------------
Eval num_timesteps=498500, episode_reward=-170.25 +/- 154.96
Episode length: 16.86 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 498500   |
---------------------------------
Eval num_timesteps=499000, episode_reward=-187.57 +/- 117.67
Episode length: 15.92 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=499500, episode_reward=-186.62 +/- 121.61
Episode length: 16.56 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 499500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 244      |
|    time_elapsed    | 1047     |
|    total_timesteps | 499712   |
---------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Eval num_timesteps=500000, episode_reward=-127.39 +/- 159.32
Episode length: 16.98 +/- 5.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -127         |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0031913607 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    clip_range_vf        | 0.2          |
|    entropy_loss         | -0.177       |
|    explained_variance   | 0.342        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.61e+03     |
|    n_updates            | 2427         |
|    policy_gradient_loss | 0.00173      |
|    value_loss           | 6.36e+03     |
------------------------------------------
Eval num_timesteps=500500, episode_reward=-143.97 +/- 150.99
Episode length: 16.34 +/- 5.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 500500   |
---------------------------------
Eval num_timesteps=501000, episode_reward=-150.54 +/- 154.98
Episode length: 16.14 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=501500, episode_reward=-159.70 +/- 119.69
Episode length: 16.32 +/- 4.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 501500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -165     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 245      |
|    time_elapsed    | 1051     |
|    total_timesteps | 501760   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-5-last/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Early stopping at step 1 due to reaching max kl: 0.02
