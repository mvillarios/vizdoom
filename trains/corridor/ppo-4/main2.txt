/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Eval num_timesteps=500, episode_reward=1378.57 +/- 719.57
Episode length: 35.88 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=1385.46 +/- 688.60
Episode length: 36.60 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 299      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
Eval num_timesteps=1500, episode_reward=1172.13 +/- 588.88
Episode length: 35.16 +/- 6.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.2      |
|    mean_reward          | 1.17e+03  |
| time/                   |           |
|    total_timesteps      | 1500      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.15e-10 |
|    explained_variance   | 0.466     |
|    learning_rate        | 0.001     |
|    loss                 | 4.05e+04  |
|    n_updates            | 220       |
|    policy_gradient_loss | 3.43e-10  |
|    value_loss           | 9.37e+04  |
---------------------------------------
Eval num_timesteps=2000, episode_reward=1137.57 +/- 598.59
Episode length: 34.52 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 280      |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=1253.19 +/- 690.08
Episode length: 35.00 +/- 6.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 1.25e+03  |
| time/                   |           |
|    total_timesteps      | 2500      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.68e-08 |
|    explained_variance   | 0.177     |
|    learning_rate        | 0.001     |
|    loss                 | 4.06e+04  |
|    n_updates            | 230       |
|    policy_gradient_loss | 5.94e-10  |
|    value_loss           | 7.58e+04  |
---------------------------------------
Eval num_timesteps=3000, episode_reward=1227.40 +/- 645.30
Episode length: 35.36 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 277      |
|    iterations      | 3        |
|    time_elapsed    | 11       |
|    total_timesteps | 3072     |
---------------------------------
Eval num_timesteps=3500, episode_reward=1216.40 +/- 626.79
Episode length: 35.46 +/- 6.54
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.5           |
|    mean_reward          | 1.22e+03       |
| time/                   |                |
|    total_timesteps      | 3500           |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.21e-07      |
|    explained_variance   | 0.34           |
|    learning_rate        | 0.001          |
|    loss                 | 3.06e+04       |
|    n_updates            | 240            |
|    policy_gradient_loss | 9.78e-09       |
|    value_loss           | 7.72e+04       |
--------------------------------------------
Eval num_timesteps=4000, episode_reward=1092.78 +/- 544.82
Episode length: 34.48 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 281      |
|    iterations      | 4        |
|    time_elapsed    | 14       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=1278.72 +/- 732.38
Episode length: 34.58 +/- 7.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 1.28e+03      |
| time/                   |               |
|    total_timesteps      | 4500          |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.38e-07     |
|    explained_variance   | 0.163         |
|    learning_rate        | 0.001         |
|    loss                 | 2.69e+04      |
|    n_updates            | 250           |
|    policy_gradient_loss | -2.75e-07     |
|    value_loss           | 6.84e+04      |
-------------------------------------------
Eval num_timesteps=5000, episode_reward=1279.89 +/- 705.45
Episode length: 34.92 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 280      |
|    iterations      | 5        |
|    time_elapsed    | 18       |
|    total_timesteps | 5120     |
---------------------------------
Eval num_timesteps=5500, episode_reward=1177.94 +/- 633.07
Episode length: 34.52 +/- 6.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 1.18e+03      |
| time/                   |               |
|    total_timesteps      | 5500          |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.59e-07     |
|    explained_variance   | 0.519         |
|    learning_rate        | 0.001         |
|    loss                 | 1.82e+04      |
|    n_updates            | 260           |
|    policy_gradient_loss | 3.34e-08      |
|    value_loss           | 4.83e+04      |
-------------------------------------------
Eval num_timesteps=6000, episode_reward=1387.96 +/- 716.56
Episode length: 36.04 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 278      |
|    iterations      | 6        |
|    time_elapsed    | 22       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=1314.47 +/- 650.95
Episode length: 36.66 +/- 5.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.7         |
|    mean_reward          | 1.31e+03     |
| time/                   |              |
|    total_timesteps      | 6500         |
| train/                  |              |
|    approx_kl            | 5.820766e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.58e-08    |
|    explained_variance   | 0.388        |
|    learning_rate        | 0.001        |
|    loss                 | 3.21e+04     |
|    n_updates            | 270          |
|    policy_gradient_loss | -1.04e-09    |
|    value_loss           | 6.05e+04     |
------------------------------------------
Eval num_timesteps=7000, episode_reward=1253.33 +/- 663.16
Episode length: 35.06 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 279      |
|    iterations      | 7        |
|    time_elapsed    | 25       |
|    total_timesteps | 7168     |
---------------------------------
Eval num_timesteps=7500, episode_reward=1261.10 +/- 648.44
Episode length: 35.74 +/- 6.20
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.7           |
|    mean_reward          | 1.26e+03       |
| time/                   |                |
|    total_timesteps      | 7500           |
| train/                  |                |
|    approx_kl            | -1.7462298e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.53e-06      |
|    explained_variance   | 0.196          |
|    learning_rate        | 0.001          |
|    loss                 | 3.09e+04       |
|    n_updates            | 280            |
|    policy_gradient_loss | 1.14e-08       |
|    value_loss           | 6.8e+04        |
--------------------------------------------
Eval num_timesteps=8000, episode_reward=1211.49 +/- 589.52
Episode length: 35.78 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 280      |
|    iterations      | 8        |
|    time_elapsed    | 29       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=1163.06 +/- 624.56
Episode length: 34.64 +/- 6.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.6      |
|    mean_reward          | 1.16e+03  |
| time/                   |           |
|    total_timesteps      | 8500      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.01e-07 |
|    explained_variance   | 0.37      |
|    learning_rate        | 0.001     |
|    loss                 | 1.98e+04  |
|    n_updates            | 290       |
|    policy_gradient_loss | 5.94e-10  |
|    value_loss           | 6.2e+04   |
---------------------------------------
Eval num_timesteps=9000, episode_reward=1098.20 +/- 623.38
Episode length: 33.48 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 9        |
|    time_elapsed    | 32       |
|    total_timesteps | 9216     |
---------------------------------
Eval num_timesteps=9500, episode_reward=1273.12 +/- 651.21
Episode length: 35.82 +/- 6.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35.8      |
|    mean_reward          | 1.27e+03  |
| time/                   |           |
|    total_timesteps      | 9500      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.65e-08 |
|    explained_variance   | 0.187     |
|    learning_rate        | 0.001     |
|    loss                 | 2.99e+04  |
|    n_updates            | 300       |
|    policy_gradient_loss | -2.57e-08 |
|    value_loss           | 7.63e+04  |
---------------------------------------
Eval num_timesteps=10000, episode_reward=1041.53 +/- 576.85
Episode length: 33.22 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 10       |
|    time_elapsed    | 36       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=1233.84 +/- 671.36
Episode length: 34.92 +/- 6.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 1.23e+03  |
| time/                   |           |
|    total_timesteps      | 10500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -2.83e-07 |
|    explained_variance   | 0.368     |
|    learning_rate        | 0.001     |
|    loss                 | 2.28e+04  |
|    n_updates            | 310       |
|    policy_gradient_loss | -1.57e-10 |
|    value_loss           | 6.16e+04  |
---------------------------------------
Eval num_timesteps=11000, episode_reward=1310.26 +/- 682.99
Episode length: 35.98 +/- 6.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 11       |
|    time_elapsed    | 39       |
|    total_timesteps | 11264    |
---------------------------------
Eval num_timesteps=11500, episode_reward=1120.70 +/- 571.59
Episode length: 35.00 +/- 6.58
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35             |
|    mean_reward          | 1.12e+03       |
| time/                   |                |
|    total_timesteps      | 11500          |
| train/                  |                |
|    approx_kl            | -1.7462298e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.13e-07      |
|    explained_variance   | 0.401          |
|    learning_rate        | 0.001          |
|    loss                 | 3.03e+04       |
|    n_updates            | 320            |
|    policy_gradient_loss | 1.71e-08       |
|    value_loss           | 6.84e+04       |
--------------------------------------------
Eval num_timesteps=12000, episode_reward=1207.84 +/- 657.45
Episode length: 34.46 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 12       |
|    time_elapsed    | 43       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=1269.64 +/- 612.14
Episode length: 36.38 +/- 5.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 1.27e+03      |
| time/                   |               |
|    total_timesteps      | 12500         |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.51e-08     |
|    explained_variance   | 0.34          |
|    learning_rate        | 0.001         |
|    loss                 | 1.92e+04      |
|    n_updates            | 330           |
|    policy_gradient_loss | -3.79e-09     |
|    value_loss           | 5.96e+04      |
-------------------------------------------
Eval num_timesteps=13000, episode_reward=1159.28 +/- 628.30
Episode length: 34.60 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 13       |
|    time_elapsed    | 46       |
|    total_timesteps | 13312    |
---------------------------------
Eval num_timesteps=13500, episode_reward=1127.67 +/- 562.84
Episode length: 35.04 +/- 5.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 35        |
|    mean_reward          | 1.13e+03  |
| time/                   |           |
|    total_timesteps      | 13500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.58e-07 |
|    explained_variance   | 0.455     |
|    learning_rate        | 0.001     |
|    loss                 | 2.21e+04  |
|    n_updates            | 340       |
|    policy_gradient_loss | -8.67e-10 |
|    value_loss           | 4.79e+04  |
---------------------------------------
Eval num_timesteps=14000, episode_reward=1069.72 +/- 517.41
Episode length: 34.38 +/- 5.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 14       |
|    time_elapsed    | 50       |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=1121.05 +/- 639.97
Episode length: 33.62 +/- 6.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.6          |
|    mean_reward          | 1.12e+03      |
| time/                   |               |
|    total_timesteps      | 14500         |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.4e-07      |
|    explained_variance   | 0.342         |
|    learning_rate        | 0.001         |
|    loss                 | 2.17e+04      |
|    n_updates            | 350           |
|    policy_gradient_loss | -6.2e-09      |
|    value_loss           | 6.97e+04      |
-------------------------------------------
Eval num_timesteps=15000, episode_reward=1241.33 +/- 609.12
Episode length: 36.24 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 15       |
|    time_elapsed    | 53       |
|    total_timesteps | 15360    |
---------------------------------
Eval num_timesteps=15500, episode_reward=1274.70 +/- 708.04
Episode length: 34.98 +/- 6.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 1.27e+03      |
| time/                   |               |
|    total_timesteps      | 15500         |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.92e-07     |
|    explained_variance   | 0.273         |
|    learning_rate        | 0.001         |
|    loss                 | 3.03e+04      |
|    n_updates            | 360           |
|    policy_gradient_loss | -3.07e-08     |
|    value_loss           | 6.98e+04      |
-------------------------------------------
Eval num_timesteps=16000, episode_reward=1111.47 +/- 598.36
Episode length: 33.76 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 16       |
|    time_elapsed    | 57       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=1201.49 +/- 633.83
Episode length: 35.22 +/- 6.75
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.2           |
|    mean_reward          | 1.2e+03        |
| time/                   |                |
|    total_timesteps      | 16500          |
| train/                  |                |
|    approx_kl            | -4.0745363e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -4.61e-06      |
|    explained_variance   | 0.182          |
|    learning_rate        | 0.001          |
|    loss                 | 4.12e+04       |
|    n_updates            | 370            |
|    policy_gradient_loss | -5.18e-08      |
|    value_loss           | 7.54e+04       |
--------------------------------------------
Eval num_timesteps=17000, episode_reward=1201.19 +/- 622.78
Episode length: 35.14 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 287      |
|    iterations      | 17       |
|    time_elapsed    | 60       |
|    total_timesteps | 17408    |
---------------------------------
Eval num_timesteps=17500, episode_reward=1116.40 +/- 613.01
Episode length: 34.28 +/- 7.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 1.12e+03      |
| time/                   |               |
|    total_timesteps      | 17500         |
| train/                  |               |
|    approx_kl            | -4.656613e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.98e-06     |
|    explained_variance   | 0.264         |
|    learning_rate        | 0.001         |
|    loss                 | 3.37e+04      |
|    n_updates            | 380           |
|    policy_gradient_loss | -1.51e-07     |
|    value_loss           | 7.04e+04      |
-------------------------------------------
Eval num_timesteps=18000, episode_reward=1201.40 +/- 625.73
Episode length: 34.96 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 1.34e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 18       |
|    time_elapsed    | 64       |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=1380.34 +/- 662.14
Episode length: 36.88 +/- 5.76
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.9           |
|    mean_reward          | 1.38e+03       |
| time/                   |                |
|    total_timesteps      | 18500          |
| train/                  |                |
|    approx_kl            | -2.3283064e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.54e-06      |
|    explained_variance   | 0.345          |
|    learning_rate        | 0.001          |
|    loss                 | 2.47e+04       |
|    n_updates            | 390            |
|    policy_gradient_loss | -7.44e-08      |
|    value_loss           | 6.76e+04       |
--------------------------------------------
Eval num_timesteps=19000, episode_reward=1229.44 +/- 685.35
Episode length: 34.60 +/- 7.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.2     |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 19       |
|    time_elapsed    | 68       |
|    total_timesteps | 19456    |
---------------------------------
Eval num_timesteps=19500, episode_reward=1161.66 +/- 583.30
Episode length: 35.00 +/- 5.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 1.16e+03      |
| time/                   |               |
|    total_timesteps      | 19500         |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.08e-07     |
|    explained_variance   | 0.159         |
|    learning_rate        | 0.001         |
|    loss                 | 2.71e+04      |
|    n_updates            | 400           |
|    policy_gradient_loss | -4.7e-07      |
|    value_loss           | 7.59e+04      |
-------------------------------------------
Eval num_timesteps=20000, episode_reward=1155.50 +/- 668.52
Episode length: 33.78 +/- 8.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.3     |
|    ep_rew_mean     | 1.39e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 20       |
|    time_elapsed    | 71       |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=1250.07 +/- 633.94
Episode length: 36.08 +/- 6.23
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.1           |
|    mean_reward          | 1.25e+03       |
| time/                   |                |
|    total_timesteps      | 20500          |
| train/                  |                |
|    approx_kl            | -3.4924597e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.73e-06      |
|    explained_variance   | 0.209          |
|    learning_rate        | 0.001          |
|    loss                 | 1.86e+04       |
|    n_updates            | 410            |
|    policy_gradient_loss | -2.97e-07      |
|    value_loss           | 7.06e+04       |
--------------------------------------------
Eval num_timesteps=21000, episode_reward=1165.84 +/- 642.00
Episode length: 34.12 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=1050.19 +/- 529.96
Episode length: 34.14 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 21       |
|    time_elapsed    | 76       |
|    total_timesteps | 21504    |
---------------------------------
Eval num_timesteps=22000, episode_reward=1444.89 +/- 699.95
Episode length: 37.32 +/- 5.69
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 37.3           |
|    mean_reward          | 1.44e+03       |
| time/                   |                |
|    total_timesteps      | 22000          |
| train/                  |                |
|    approx_kl            | -4.0745363e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.51e-06      |
|    explained_variance   | 0.384          |
|    learning_rate        | 0.001          |
|    loss                 | 2.4e+04        |
|    n_updates            | 420            |
|    policy_gradient_loss | 1.28e-07       |
|    value_loss           | 5.85e+04       |
--------------------------------------------
New best mean reward!
Eval num_timesteps=22500, episode_reward=1221.69 +/- 649.88
Episode length: 35.06 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 22       |
|    time_elapsed    | 79       |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=1212.67 +/- 656.19
Episode length: 34.60 +/- 6.61
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.6           |
|    mean_reward          | 1.21e+03       |
| time/                   |                |
|    total_timesteps      | 23000          |
| train/                  |                |
|    approx_kl            | -1.3969839e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.01e-05      |
|    explained_variance   | 0.227          |
|    learning_rate        | 0.001          |
|    loss                 | 1.36e+04       |
|    n_updates            | 430            |
|    policy_gradient_loss | 6.76e-07       |
|    value_loss           | 4.77e+04       |
--------------------------------------------
Eval num_timesteps=23500, episode_reward=1356.34 +/- 719.39
Episode length: 35.78 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 23       |
|    time_elapsed    | 83       |
|    total_timesteps | 23552    |
---------------------------------
Eval num_timesteps=24000, episode_reward=1081.27 +/- 525.33
Episode length: 34.92 +/- 6.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.9      |
|    mean_reward          | 1.08e+03  |
| time/                   |           |
|    total_timesteps      | 24000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.95e-06 |
|    explained_variance   | 0.423     |
|    learning_rate        | 0.001     |
|    loss                 | 2.07e+04  |
|    n_updates            | 440       |
|    policy_gradient_loss | -9.26e-07 |
|    value_loss           | 5.39e+04  |
---------------------------------------
Eval num_timesteps=24500, episode_reward=1333.99 +/- 668.56
Episode length: 36.66 +/- 6.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 24       |
|    time_elapsed    | 86       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=1161.18 +/- 579.05
Episode length: 35.26 +/- 6.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 1.16e+03      |
| time/                   |               |
|    total_timesteps      | 25000         |
| train/                  |               |
|    approx_kl            | -5.820766e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.41e-05     |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.001         |
|    loss                 | 1.59e+04      |
|    n_updates            | 450           |
|    policy_gradient_loss | 1.86e-07      |
|    value_loss           | 5.57e+04      |
-------------------------------------------
Eval num_timesteps=25500, episode_reward=1232.14 +/- 636.02
Episode length: 35.22 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 25       |
|    time_elapsed    | 90       |
|    total_timesteps | 25600    |
---------------------------------
Eval num_timesteps=26000, episode_reward=1325.06 +/- 707.74
Episode length: 35.42 +/- 6.84
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.4           |
|    mean_reward          | 1.33e+03       |
| time/                   |                |
|    total_timesteps      | 26000          |
| train/                  |                |
|    approx_kl            | -1.1641532e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.8e-05       |
|    explained_variance   | 0.0112         |
|    learning_rate        | 0.001          |
|    loss                 | 1.96e+04       |
|    n_updates            | 460            |
|    policy_gradient_loss | -2.7e-07       |
|    value_loss           | 7.67e+04       |
--------------------------------------------
Eval num_timesteps=26500, episode_reward=1204.94 +/- 593.20
Episode length: 35.98 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 282      |
|    iterations      | 26       |
|    time_elapsed    | 94       |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=1171.49 +/- 622.62
Episode length: 34.72 +/- 7.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 1.17e+03      |
| time/                   |               |
|    total_timesteps      | 27000         |
| train/                  |               |
|    approx_kl            | -9.895302e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.45e-05     |
|    explained_variance   | 0.315         |
|    learning_rate        | 0.001         |
|    loss                 | 1.64e+04      |
|    n_updates            | 470           |
|    policy_gradient_loss | -5.02e-07     |
|    value_loss           | 6.86e+04      |
-------------------------------------------
Eval num_timesteps=27500, episode_reward=1215.64 +/- 599.49
Episode length: 35.82 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 27       |
|    time_elapsed    | 97       |
|    total_timesteps | 27648    |
---------------------------------
Eval num_timesteps=28000, episode_reward=1047.23 +/- 571.97
Episode length: 33.20 +/- 6.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.2          |
|    mean_reward          | 1.05e+03      |
| time/                   |               |
|    total_timesteps      | 28000         |
| train/                  |               |
|    approx_kl            | -4.656613e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.26e-05     |
|    explained_variance   | 0.296         |
|    learning_rate        | 0.001         |
|    loss                 | 2.77e+04      |
|    n_updates            | 480           |
|    policy_gradient_loss | 1.39e-07      |
|    value_loss           | 6.65e+04      |
-------------------------------------------
Eval num_timesteps=28500, episode_reward=1043.62 +/- 542.43
Episode length: 33.80 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 28       |
|    time_elapsed    | 101      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=1387.26 +/- 655.19
Episode length: 37.22 +/- 5.58
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 37.2           |
|    mean_reward          | 1.39e+03       |
| time/                   |                |
|    total_timesteps      | 29000          |
| train/                  |                |
|    approx_kl            | -4.0745363e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -4.92e-06      |
|    explained_variance   | 0.468          |
|    learning_rate        | 0.001          |
|    loss                 | 1.56e+04       |
|    n_updates            | 490            |
|    policy_gradient_loss | -1.41e-07      |
|    value_loss           | 5.83e+04       |
--------------------------------------------
Eval num_timesteps=29500, episode_reward=1199.55 +/- 627.28
Episode length: 35.18 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 29       |
|    time_elapsed    | 104      |
|    total_timesteps | 29696    |
---------------------------------
Eval num_timesteps=30000, episode_reward=1142.88 +/- 590.24
Episode length: 34.78 +/- 6.34
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.8           |
|    mean_reward          | 1.14e+03       |
| time/                   |                |
|    total_timesteps      | 30000          |
| train/                  |                |
|    approx_kl            | -1.2223609e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.54e-06      |
|    explained_variance   | 0.321          |
|    learning_rate        | 0.001          |
|    loss                 | 1.78e+04       |
|    n_updates            | 500            |
|    policy_gradient_loss | -8.84e-08      |
|    value_loss           | 5.61e+04       |
--------------------------------------------
Eval num_timesteps=30500, episode_reward=1261.95 +/- 665.64
Episode length: 35.46 +/- 7.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 30       |
|    time_elapsed    | 108      |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=1287.67 +/- 697.53
Episode length: 35.36 +/- 6.84
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.4           |
|    mean_reward          | 1.29e+03       |
| time/                   |                |
|    total_timesteps      | 31000          |
| train/                  |                |
|    approx_kl            | -1.0477379e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.56e-05      |
|    explained_variance   | 0.383          |
|    learning_rate        | 0.001          |
|    loss                 | 1.69e+04       |
|    n_updates            | 510            |
|    policy_gradient_loss | -1.51e-09      |
|    value_loss           | 4.57e+04       |
--------------------------------------------
Eval num_timesteps=31500, episode_reward=1192.27 +/- 663.92
Episode length: 34.38 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    fps             | 283      |
|    iterations      | 31       |
|    time_elapsed    | 111      |
|    total_timesteps | 31744    |
---------------------------------
Eval num_timesteps=32000, episode_reward=1308.05 +/- 753.91
Episode length: 34.30 +/- 8.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 1.31e+03      |
| time/                   |               |
|    total_timesteps      | 32000         |
| train/                  |               |
|    approx_kl            | -4.656613e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.63e-06     |
|    explained_variance   | 0.34          |
|    learning_rate        | 0.001         |
|    loss                 | 1.97e+04      |
|    n_updates            | 520           |
|    policy_gradient_loss | -3.93e-07     |
|    value_loss           | 5.67e+04      |
-------------------------------------------
Eval num_timesteps=32500, episode_reward=1245.86 +/- 692.54
Episode length: 34.40 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 32       |
|    time_elapsed    | 115      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=1120.36 +/- 608.00
Episode length: 34.30 +/- 6.73
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.3           |
|    mean_reward          | 1.12e+03       |
| time/                   |                |
|    total_timesteps      | 33000          |
| train/                  |                |
|    approx_kl            | -1.1059456e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.42e-06      |
|    explained_variance   | 0.487          |
|    learning_rate        | 0.001          |
|    loss                 | 1.54e+04       |
|    n_updates            | 530            |
|    policy_gradient_loss | -1.49e-08      |
|    value_loss           | 4.98e+04       |
--------------------------------------------
Eval num_timesteps=33500, episode_reward=1188.12 +/- 644.14
Episode length: 34.38 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 33       |
|    time_elapsed    | 118      |
|    total_timesteps | 33792    |
---------------------------------
Eval num_timesteps=34000, episode_reward=1220.59 +/- 652.28
Episode length: 35.30 +/- 6.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 1.22e+03      |
| time/                   |               |
|    total_timesteps      | 34000         |
| train/                  |               |
|    approx_kl            | -7.566996e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.03e-05     |
|    explained_variance   | 0.244         |
|    learning_rate        | 0.001         |
|    loss                 | 2.25e+04      |
|    n_updates            | 540           |
|    policy_gradient_loss | -4.78e-07     |
|    value_loss           | 5.68e+04      |
-------------------------------------------
Eval num_timesteps=34500, episode_reward=1140.98 +/- 630.90
Episode length: 33.84 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 34       |
|    time_elapsed    | 122      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=1215.99 +/- 664.88
Episode length: 34.96 +/- 7.34
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35             |
|    mean_reward          | 1.22e+03       |
| time/                   |                |
|    total_timesteps      | 35000          |
| train/                  |                |
|    approx_kl            | -8.1490725e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -4.9e-06       |
|    explained_variance   | 0.296          |
|    learning_rate        | 0.001          |
|    loss                 | 2.41e+04       |
|    n_updates            | 550            |
|    policy_gradient_loss | -1.54e-07      |
|    value_loss           | 5.93e+04       |
--------------------------------------------
Eval num_timesteps=35500, episode_reward=1302.21 +/- 665.86
Episode length: 35.90 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 35       |
|    time_elapsed    | 125      |
|    total_timesteps | 35840    |
---------------------------------
Eval num_timesteps=36000, episode_reward=1284.99 +/- 666.50
Episode length: 35.62 +/- 6.23
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.6           |
|    mean_reward          | 1.28e+03       |
| time/                   |                |
|    total_timesteps      | 36000          |
| train/                  |                |
|    approx_kl            | -1.5133992e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.85e-05      |
|    explained_variance   | 0.39           |
|    learning_rate        | 0.001          |
|    loss                 | 1.79e+04       |
|    n_updates            | 560            |
|    policy_gradient_loss | -7.96e-08      |
|    value_loss           | 6.43e+04       |
--------------------------------------------
Eval num_timesteps=36500, episode_reward=1215.80 +/- 613.50
Episode length: 35.22 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 36       |
|    time_elapsed    | 129      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=1250.43 +/- 723.67
Episode length: 33.86 +/- 7.42
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.9           |
|    mean_reward          | 1.25e+03       |
| time/                   |                |
|    total_timesteps      | 37000          |
| train/                  |                |
|    approx_kl            | -1.8044375e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.95e-05      |
|    explained_variance   | 0.143          |
|    learning_rate        | 0.001          |
|    loss                 | 1.42e+04       |
|    n_updates            | 570            |
|    policy_gradient_loss | -3.52e-07      |
|    value_loss           | 5.91e+04       |
--------------------------------------------
Eval num_timesteps=37500, episode_reward=1371.16 +/- 663.90
Episode length: 36.98 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 37       |
|    time_elapsed    | 132      |
|    total_timesteps | 37888    |
---------------------------------
Eval num_timesteps=38000, episode_reward=1219.76 +/- 654.08
Episode length: 35.00 +/- 6.87
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35             |
|    mean_reward          | 1.22e+03       |
| time/                   |                |
|    total_timesteps      | 38000          |
| train/                  |                |
|    approx_kl            | -6.9849193e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -4.2e-05       |
|    explained_variance   | 0.13           |
|    learning_rate        | 0.001          |
|    loss                 | 1.96e+04       |
|    n_updates            | 580            |
|    policy_gradient_loss | 3.5e-07        |
|    value_loss           | 6.19e+04       |
--------------------------------------------
Eval num_timesteps=38500, episode_reward=1089.43 +/- 590.02
Episode length: 33.72 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 38       |
|    time_elapsed    | 136      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=1093.53 +/- 581.16
Episode length: 33.90 +/- 6.54
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.9           |
|    mean_reward          | 1.09e+03       |
| time/                   |                |
|    total_timesteps      | 39000          |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.9e-05       |
|    explained_variance   | 0.28           |
|    learning_rate        | 0.001          |
|    loss                 | 1.65e+04       |
|    n_updates            | 590            |
|    policy_gradient_loss | -4.72e-07      |
|    value_loss           | 6.13e+04       |
--------------------------------------------
Eval num_timesteps=39500, episode_reward=1096.45 +/- 581.79
Episode length: 33.90 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 39       |
|    time_elapsed    | 139      |
|    total_timesteps | 39936    |
---------------------------------
Eval num_timesteps=40000, episode_reward=1283.32 +/- 638.02
Episode length: 35.82 +/- 5.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 1.28e+03      |
| time/                   |               |
|    total_timesteps      | 40000         |
| train/                  |               |
|    approx_kl            | -7.566996e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.22e-05     |
|    explained_variance   | 0.133         |
|    learning_rate        | 0.001         |
|    loss                 | 3.13e+04      |
|    n_updates            | 600           |
|    policy_gradient_loss | 2.29e-07      |
|    value_loss           | 7.55e+04      |
-------------------------------------------
Eval num_timesteps=40500, episode_reward=1135.38 +/- 595.01
Episode length: 34.26 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 40       |
|    time_elapsed    | 142      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=1181.99 +/- 646.42
Episode length: 34.30 +/- 7.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 1.18e+03      |
| time/                   |               |
|    total_timesteps      | 41000         |
| train/                  |               |
|    approx_kl            | -5.820766e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.08e-06     |
|    explained_variance   | 0.345         |
|    learning_rate        | 0.001         |
|    loss                 | 1.77e+04      |
|    n_updates            | 610           |
|    policy_gradient_loss | -2.08e-07     |
|    value_loss           | 5.49e+04      |
-------------------------------------------
Eval num_timesteps=41500, episode_reward=1157.56 +/- 593.89
Episode length: 35.04 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 286      |
|    iterations      | 41       |
|    time_elapsed    | 146      |
|    total_timesteps | 41984    |
---------------------------------
Eval num_timesteps=42000, episode_reward=1066.21 +/- 524.93
Episode length: 34.52 +/- 6.43
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.5           |
|    mean_reward          | 1.07e+03       |
| time/                   |                |
|    total_timesteps      | 42000          |
| train/                  |                |
|    approx_kl            | -6.4028427e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.6e-06       |
|    explained_variance   | 0.367          |
|    learning_rate        | 0.001          |
|    loss                 | 1.96e+04       |
|    n_updates            | 620            |
|    policy_gradient_loss | -6.14e-08      |
|    value_loss           | 6.09e+04       |
--------------------------------------------
Eval num_timesteps=42500, episode_reward=1209.85 +/- 582.57
Episode length: 35.80 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=1068.19 +/- 599.29
Episode length: 33.28 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 42       |
|    time_elapsed    | 151      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=1307.89 +/- 721.90
Episode length: 35.42 +/- 7.50
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.4           |
|    mean_reward          | 1.31e+03       |
| time/                   |                |
|    total_timesteps      | 43500          |
| train/                  |                |
|    approx_kl            | -1.3969839e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.31e-06      |
|    explained_variance   | 0.356          |
|    learning_rate        | 0.001          |
|    loss                 | 2.54e+04       |
|    n_updates            | 630            |
|    policy_gradient_loss | 1.92e-07       |
|    value_loss           | 6.25e+04       |
--------------------------------------------
Eval num_timesteps=44000, episode_reward=1171.12 +/- 679.31
Episode length: 33.38 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 43       |
|    time_elapsed    | 154      |
|    total_timesteps | 44032    |
---------------------------------
Eval num_timesteps=44500, episode_reward=1223.16 +/- 618.72
Episode length: 35.94 +/- 6.30
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.9           |
|    mean_reward          | 1.22e+03       |
| time/                   |                |
|    total_timesteps      | 44500          |
| train/                  |                |
|    approx_kl            | -1.9790605e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.45e-05      |
|    explained_variance   | 0.138          |
|    learning_rate        | 0.001          |
|    loss                 | 1.88e+04       |
|    n_updates            | 640            |
|    policy_gradient_loss | -3.38e-07      |
|    value_loss           | 6.41e+04       |
--------------------------------------------
Eval num_timesteps=45000, episode_reward=1254.19 +/- 698.85
Episode length: 34.82 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 44       |
|    time_elapsed    | 158      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=1277.19 +/- 606.55
Episode length: 36.74 +/- 5.57
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.7           |
|    mean_reward          | 1.28e+03       |
| time/                   |                |
|    total_timesteps      | 45500          |
| train/                  |                |
|    approx_kl            | -1.7462298e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.38e-06      |
|    explained_variance   | 0.474          |
|    learning_rate        | 0.001          |
|    loss                 | 1.75e+04       |
|    n_updates            | 650            |
|    policy_gradient_loss | -2.81e-07      |
|    value_loss           | 4.76e+04       |
--------------------------------------------
Eval num_timesteps=46000, episode_reward=1304.80 +/- 668.03
Episode length: 36.20 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 45       |
|    time_elapsed    | 161      |
|    total_timesteps | 46080    |
---------------------------------
Eval num_timesteps=46500, episode_reward=1111.07 +/- 615.73
Episode length: 34.22 +/- 7.27
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.2           |
|    mean_reward          | 1.11e+03       |
| time/                   |                |
|    total_timesteps      | 46500          |
| train/                  |                |
|    approx_kl            | -1.3387762e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.1e-06       |
|    explained_variance   | 0.367          |
|    learning_rate        | 0.001          |
|    loss                 | 1.98e+04       |
|    n_updates            | 660            |
|    policy_gradient_loss | -1.46e-07      |
|    value_loss           | 5.29e+04       |
--------------------------------------------
Eval num_timesteps=47000, episode_reward=1224.95 +/- 711.10
Episode length: 34.06 +/- 7.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 46       |
|    time_elapsed    | 165      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=1185.27 +/- 641.70
Episode length: 34.50 +/- 7.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 1.19e+03      |
| time/                   |               |
|    total_timesteps      | 47500         |
| train/                  |               |
|    approx_kl            | -2.386514e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.26e-05     |
|    explained_variance   | 0.245         |
|    learning_rate        | 0.001         |
|    loss                 | 1.67e+04      |
|    n_updates            | 670           |
|    policy_gradient_loss | -4.54e-07     |
|    value_loss           | 5.45e+04      |
-------------------------------------------
Eval num_timesteps=48000, episode_reward=1154.23 +/- 595.98
Episode length: 35.32 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 47       |
|    time_elapsed    | 168      |
|    total_timesteps | 48128    |
---------------------------------
Eval num_timesteps=48500, episode_reward=1172.14 +/- 610.92
Episode length: 35.00 +/- 6.15
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35             |
|    mean_reward          | 1.17e+03       |
| time/                   |                |
|    total_timesteps      | 48500          |
| train/                  |                |
|    approx_kl            | -8.1490725e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.17e-05      |
|    explained_variance   | 0.255          |
|    learning_rate        | 0.001          |
|    loss                 | 2.37e+04       |
|    n_updates            | 680            |
|    policy_gradient_loss | -2.27e-07      |
|    value_loss           | 6.17e+04       |
--------------------------------------------
Eval num_timesteps=49000, episode_reward=1153.86 +/- 587.72
Episode length: 35.12 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 48       |
|    time_elapsed    | 172      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=1325.61 +/- 650.36
Episode length: 36.80 +/- 6.76
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.8           |
|    mean_reward          | 1.33e+03       |
| time/                   |                |
|    total_timesteps      | 49500          |
| train/                  |                |
|    approx_kl            | -2.2700988e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -4.08e-05      |
|    explained_variance   | 0.263          |
|    learning_rate        | 0.001          |
|    loss                 | 2.76e+04       |
|    n_updates            | 690            |
|    policy_gradient_loss | -1.39e-07      |
|    value_loss           | 6.33e+04       |
--------------------------------------------
Eval num_timesteps=50000, episode_reward=1210.91 +/- 588.28
Episode length: 36.00 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 49       |
|    time_elapsed    | 175      |
|    total_timesteps | 50176    |
---------------------------------
Eval num_timesteps=50500, episode_reward=1200.45 +/- 588.17
Episode length: 35.62 +/- 5.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 1.2e+03       |
| time/                   |               |
|    total_timesteps      | 50500         |
| train/                  |               |
|    approx_kl            | -8.731149e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.88e-06     |
|    explained_variance   | 0.303         |
|    learning_rate        | 0.001         |
|    loss                 | 2.23e+04      |
|    n_updates            | 700           |
|    policy_gradient_loss | -7.7e-07      |
|    value_loss           | 5.87e+04      |
-------------------------------------------
Eval num_timesteps=51000, episode_reward=1269.98 +/- 647.42
Episode length: 35.78 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 50       |
|    time_elapsed    | 179      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=1253.09 +/- 729.23
Episode length: 33.98 +/- 7.81
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34             |
|    mean_reward          | 1.25e+03       |
| time/                   |                |
|    total_timesteps      | 51500          |
| train/                  |                |
|    approx_kl            | -3.4924597e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.94e-06      |
|    explained_variance   | 0.23           |
|    learning_rate        | 0.001          |
|    loss                 | 2.08e+04       |
|    n_updates            | 710            |
|    policy_gradient_loss | -1.76e-07      |
|    value_loss           | 5.93e+04       |
--------------------------------------------
Eval num_timesteps=52000, episode_reward=1137.76 +/- 558.89
Episode length: 35.12 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.34e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 51       |
|    time_elapsed    | 182      |
|    total_timesteps | 52224    |
---------------------------------
Eval num_timesteps=52500, episode_reward=1259.82 +/- 728.41
Episode length: 34.52 +/- 8.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 1.26e+03      |
| time/                   |               |
|    total_timesteps      | 52500         |
| train/                  |               |
|    approx_kl            | -9.313226e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.33e-06     |
|    explained_variance   | 0.266         |
|    learning_rate        | 0.001         |
|    loss                 | 1.94e+04      |
|    n_updates            | 720           |
|    policy_gradient_loss | -2.13e-07     |
|    value_loss           | 6.46e+04      |
-------------------------------------------
Eval num_timesteps=53000, episode_reward=1183.43 +/- 640.56
Episode length: 34.18 +/- 6.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 52       |
|    time_elapsed    | 186      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=1315.71 +/- 675.86
Episode length: 36.02 +/- 6.17
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36             |
|    mean_reward          | 1.32e+03       |
| time/                   |                |
|    total_timesteps      | 53500          |
| train/                  |                |
|    approx_kl            | -4.0745363e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.9e-06       |
|    explained_variance   | 0.204          |
|    learning_rate        | 0.001          |
|    loss                 | 1.94e+04       |
|    n_updates            | 730            |
|    policy_gradient_loss | -5.01e-08      |
|    value_loss           | 8.44e+04       |
--------------------------------------------
Eval num_timesteps=54000, episode_reward=1172.51 +/- 651.72
Episode length: 34.20 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 53       |
|    time_elapsed    | 189      |
|    total_timesteps | 54272    |
---------------------------------
Eval num_timesteps=54500, episode_reward=1199.42 +/- 563.63
Episode length: 36.46 +/- 5.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.5          |
|    mean_reward          | 1.2e+03       |
| time/                   |               |
|    total_timesteps      | 54500         |
| train/                  |               |
|    approx_kl            | -7.566996e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.97e-06     |
|    explained_variance   | 0.361         |
|    learning_rate        | 0.001         |
|    loss                 | 2.17e+04      |
|    n_updates            | 740           |
|    policy_gradient_loss | -8.09e-08     |
|    value_loss           | 6.27e+04      |
-------------------------------------------
Eval num_timesteps=55000, episode_reward=1184.03 +/- 632.29
Episode length: 34.72 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 54       |
|    time_elapsed    | 193      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=1224.28 +/- 633.66
Episode length: 35.34 +/- 6.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 1.22e+03      |
| time/                   |               |
|    total_timesteps      | 55500         |
| train/                  |               |
|    approx_kl            | -8.731149e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.68e-06     |
|    explained_variance   | 0.244         |
|    learning_rate        | 0.001         |
|    loss                 | 2.41e+04      |
|    n_updates            | 750           |
|    policy_gradient_loss | 2.49e-08      |
|    value_loss           | 6.66e+04      |
-------------------------------------------
Eval num_timesteps=56000, episode_reward=1172.55 +/- 610.33
Episode length: 34.94 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 55       |
|    time_elapsed    | 196      |
|    total_timesteps | 56320    |
---------------------------------
Eval num_timesteps=56500, episode_reward=1080.05 +/- 520.14
Episode length: 34.70 +/- 6.21
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.7           |
|    mean_reward          | 1.08e+03       |
| time/                   |                |
|    total_timesteps      | 56500          |
| train/                  |                |
|    approx_kl            | -1.4551915e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.51e-05      |
|    explained_variance   | 0.23           |
|    learning_rate        | 0.001          |
|    loss                 | 2.22e+04       |
|    n_updates            | 760            |
|    policy_gradient_loss | -5.35e-07      |
|    value_loss           | 6.94e+04       |
--------------------------------------------
Eval num_timesteps=57000, episode_reward=1030.54 +/- 587.56
Episode length: 32.64 +/- 7.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.6     |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 56       |
|    time_elapsed    | 200      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=1378.07 +/- 693.80
Episode length: 36.64 +/- 6.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 1.38e+03      |
| time/                   |               |
|    total_timesteps      | 57500         |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.82e-06     |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.001         |
|    loss                 | 1.11e+04      |
|    n_updates            | 770           |
|    policy_gradient_loss | -1.03e-06     |
|    value_loss           | 6.08e+04      |
-------------------------------------------
Eval num_timesteps=58000, episode_reward=1411.60 +/- 668.41
Episode length: 37.54 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.5     |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 57       |
|    time_elapsed    | 203      |
|    total_timesteps | 58368    |
---------------------------------
Eval num_timesteps=58500, episode_reward=1137.55 +/- 597.62
Episode length: 34.62 +/- 6.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 1.14e+03      |
| time/                   |               |
|    total_timesteps      | 58500         |
| train/                  |               |
|    approx_kl            | -9.895302e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.51e-06     |
|    explained_variance   | 0.318         |
|    learning_rate        | 0.001         |
|    loss                 | 2e+04         |
|    n_updates            | 780           |
|    policy_gradient_loss | 2.89e-08      |
|    value_loss           | 5.75e+04      |
-------------------------------------------
Eval num_timesteps=59000, episode_reward=1282.98 +/- 669.83
Episode length: 35.50 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 58       |
|    time_elapsed    | 207      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=1270.23 +/- 652.79
Episode length: 36.08 +/- 6.61
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.1           |
|    mean_reward          | 1.27e+03       |
| time/                   |                |
|    total_timesteps      | 59500          |
| train/                  |                |
|    approx_kl            | -1.1059456e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.07e-05      |
|    explained_variance   | 0.204          |
|    learning_rate        | 0.001          |
|    loss                 | 1.52e+04       |
|    n_updates            | 790            |
|    policy_gradient_loss | -2.2e-07       |
|    value_loss           | 5.95e+04       |
--------------------------------------------
Eval num_timesteps=60000, episode_reward=1241.69 +/- 645.74
Episode length: 35.52 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 59       |
|    time_elapsed    | 210      |
|    total_timesteps | 60416    |
---------------------------------
Eval num_timesteps=60500, episode_reward=1165.58 +/- 651.93
Episode length: 33.94 +/- 6.99
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.9           |
|    mean_reward          | 1.17e+03       |
| time/                   |                |
|    total_timesteps      | 60500          |
| train/                  |                |
|    approx_kl            | -1.0477379e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2e-05         |
|    explained_variance   | 0.287          |
|    learning_rate        | 0.001          |
|    loss                 | 1.42e+04       |
|    n_updates            | 800            |
|    policy_gradient_loss | -4.52e-07      |
|    value_loss           | 5.98e+04       |
--------------------------------------------
Eval num_timesteps=61000, episode_reward=1028.18 +/- 496.21
Episode length: 34.00 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 60       |
|    time_elapsed    | 214      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=1403.63 +/- 699.03
Episode length: 36.44 +/- 6.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.4         |
|    mean_reward          | 1.4e+03      |
| time/                   |              |
|    total_timesteps      | 61500        |
| train/                  |              |
|    approx_kl            | -2.73576e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.9e-05     |
|    explained_variance   | 0.236        |
|    learning_rate        | 0.001        |
|    loss                 | 2.08e+04     |
|    n_updates            | 810          |
|    policy_gradient_loss | -4.8e-07     |
|    value_loss           | 5.42e+04     |
------------------------------------------
Eval num_timesteps=62000, episode_reward=1119.75 +/- 612.29
Episode length: 34.00 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 61       |
|    time_elapsed    | 217      |
|    total_timesteps | 62464    |
---------------------------------
Eval num_timesteps=62500, episode_reward=1244.04 +/- 637.66
Episode length: 35.72 +/- 6.24
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.7           |
|    mean_reward          | 1.24e+03       |
| time/                   |                |
|    total_timesteps      | 62500          |
| train/                  |                |
|    approx_kl            | -1.3387762e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.3e-05       |
|    explained_variance   | 0.22           |
|    learning_rate        | 0.001          |
|    loss                 | 2.37e+04       |
|    n_updates            | 820            |
|    policy_gradient_loss | -4.13e-07      |
|    value_loss           | 6.51e+04       |
--------------------------------------------
Eval num_timesteps=63000, episode_reward=1382.48 +/- 684.87
Episode length: 36.38 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 62       |
|    time_elapsed    | 221      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=1228.16 +/- 645.37
Episode length: 35.50 +/- 6.33
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.5           |
|    mean_reward          | 1.23e+03       |
| time/                   |                |
|    total_timesteps      | 63500          |
| train/                  |                |
|    approx_kl            | -2.1536835e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.35e-05      |
|    explained_variance   | 0.175          |
|    learning_rate        | 0.001          |
|    loss                 | 1.61e+04       |
|    n_updates            | 830            |
|    policy_gradient_loss | -1.99e-07      |
|    value_loss           | 6.71e+04       |
--------------------------------------------
Eval num_timesteps=64000, episode_reward=1334.69 +/- 704.33
Episode length: 35.54 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=1378.74 +/- 717.58
Episode length: 36.10 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 63       |
|    time_elapsed    | 226      |
|    total_timesteps | 64512    |
---------------------------------
Eval num_timesteps=65000, episode_reward=1284.87 +/- 636.68
Episode length: 36.06 +/- 5.62
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.1           |
|    mean_reward          | 1.28e+03       |
| time/                   |                |
|    total_timesteps      | 65000          |
| train/                  |                |
|    approx_kl            | -2.6775524e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.3e-06       |
|    explained_variance   | 0.0958         |
|    learning_rate        | 0.001          |
|    loss                 | 2.31e+04       |
|    n_updates            | 840            |
|    policy_gradient_loss | -1.58e-07      |
|    value_loss           | 6.96e+04       |
--------------------------------------------
Eval num_timesteps=65500, episode_reward=1270.06 +/- 687.45
Episode length: 34.98 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 64       |
|    time_elapsed    | 229      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=1283.47 +/- 672.12
Episode length: 35.60 +/- 5.95
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.6           |
|    mean_reward          | 1.28e+03       |
| time/                   |                |
|    total_timesteps      | 66000          |
| train/                  |                |
|    approx_kl            | -1.9208528e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.46e-05      |
|    explained_variance   | 0.209          |
|    learning_rate        | 0.001          |
|    loss                 | 1.96e+04       |
|    n_updates            | 850            |
|    policy_gradient_loss | -2.02e-07      |
|    value_loss           | 5.61e+04       |
--------------------------------------------
Eval num_timesteps=66500, episode_reward=1203.05 +/- 664.73
Episode length: 34.80 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 65       |
|    time_elapsed    | 233      |
|    total_timesteps | 66560    |
---------------------------------
Eval num_timesteps=67000, episode_reward=1168.54 +/- 579.94
Episode length: 35.38 +/- 5.91
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.4           |
|    mean_reward          | 1.17e+03       |
| time/                   |                |
|    total_timesteps      | 67000          |
| train/                  |                |
|    approx_kl            | -1.0477379e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.01e-05      |
|    explained_variance   | 0.191          |
|    learning_rate        | 0.001          |
|    loss                 | 1.27e+04       |
|    n_updates            | 860            |
|    policy_gradient_loss | -1.62e-07      |
|    value_loss           | 5.35e+04       |
--------------------------------------------
Eval num_timesteps=67500, episode_reward=1308.85 +/- 686.28
Episode length: 35.76 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 66       |
|    time_elapsed    | 236      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=1339.53 +/- 660.31
Episode length: 36.94 +/- 5.55
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.9           |
|    mean_reward          | 1.34e+03       |
| time/                   |                |
|    total_timesteps      | 68000          |
| train/                  |                |
|    approx_kl            | -2.7939677e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -8.85e-06      |
|    explained_variance   | 0.136          |
|    learning_rate        | 0.001          |
|    loss                 | 1.76e+04       |
|    n_updates            | 870            |
|    policy_gradient_loss | -2.69e-08      |
|    value_loss           | 6.53e+04       |
--------------------------------------------
Eval num_timesteps=68500, episode_reward=1273.87 +/- 673.96
Episode length: 35.14 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 67       |
|    time_elapsed    | 240      |
|    total_timesteps | 68608    |
---------------------------------
Eval num_timesteps=69000, episode_reward=1260.03 +/- 662.52
Episode length: 35.70 +/- 6.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 1.26e+03      |
| time/                   |               |
|    total_timesteps      | 69000         |
| train/                  |               |
|    approx_kl            | -7.566996e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.91e-06     |
|    explained_variance   | 0.182         |
|    learning_rate        | 0.001         |
|    loss                 | 1.9e+04       |
|    n_updates            | 880           |
|    policy_gradient_loss | -1.17e-07     |
|    value_loss           | 6.81e+04      |
-------------------------------------------
Eval num_timesteps=69500, episode_reward=1242.32 +/- 649.55
Episode length: 35.66 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 68       |
|    time_elapsed    | 243      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=1208.47 +/- 663.94
Episode length: 34.74 +/- 7.27
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.7           |
|    mean_reward          | 1.21e+03       |
| time/                   |                |
|    total_timesteps      | 70000          |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.41e-06      |
|    explained_variance   | 0.401          |
|    learning_rate        | 0.001          |
|    loss                 | 1.86e+04       |
|    n_updates            | 890            |
|    policy_gradient_loss | -2.71e-08      |
|    value_loss           | 4.96e+04       |
--------------------------------------------
Eval num_timesteps=70500, episode_reward=1222.53 +/- 677.57
Episode length: 34.58 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 69       |
|    time_elapsed    | 247      |
|    total_timesteps | 70656    |
---------------------------------
Eval num_timesteps=71000, episode_reward=1313.37 +/- 686.73
Episode length: 35.54 +/- 7.09
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.5           |
|    mean_reward          | 1.31e+03       |
| time/                   |                |
|    total_timesteps      | 71000          |
| train/                  |                |
|    approx_kl            | -1.3969839e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.41e-06      |
|    explained_variance   | 0.342          |
|    learning_rate        | 0.001          |
|    loss                 | 1.31e+04       |
|    n_updates            | 900            |
|    policy_gradient_loss | -5.56e-08      |
|    value_loss           | 5.73e+04       |
--------------------------------------------
Eval num_timesteps=71500, episode_reward=1156.86 +/- 656.44
Episode length: 33.80 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 70       |
|    time_elapsed    | 250      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=1239.73 +/- 666.28
Episode length: 34.84 +/- 6.21
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.8           |
|    mean_reward          | 1.24e+03       |
| time/                   |                |
|    total_timesteps      | 72000          |
| train/                  |                |
|    approx_kl            | -4.1909516e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.75e-05      |
|    explained_variance   | 0.252          |
|    learning_rate        | 0.001          |
|    loss                 | 1.85e+04       |
|    n_updates            | 910            |
|    policy_gradient_loss | -1.09e-07      |
|    value_loss           | 5.84e+04       |
--------------------------------------------
Eval num_timesteps=72500, episode_reward=1277.16 +/- 718.62
Episode length: 34.50 +/- 7.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 71       |
|    time_elapsed    | 254      |
|    total_timesteps | 72704    |
---------------------------------
Eval num_timesteps=73000, episode_reward=1177.57 +/- 607.62
Episode length: 35.48 +/- 6.02
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.5           |
|    mean_reward          | 1.18e+03       |
| time/                   |                |
|    total_timesteps      | 73000          |
| train/                  |                |
|    approx_kl            | -1.9208528e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.38e-05      |
|    explained_variance   | 0.159          |
|    learning_rate        | 0.001          |
|    loss                 | 1.35e+04       |
|    n_updates            | 920            |
|    policy_gradient_loss | -2.2e-07       |
|    value_loss           | 5.43e+04       |
--------------------------------------------
Eval num_timesteps=73500, episode_reward=1345.88 +/- 657.73
Episode length: 36.68 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 72       |
|    time_elapsed    | 257      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=1254.75 +/- 655.50
Episode length: 35.36 +/- 6.17
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.4           |
|    mean_reward          | 1.25e+03       |
| time/                   |                |
|    total_timesteps      | 74000          |
| train/                  |                |
|    approx_kl            | -1.9790605e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.15e-05      |
|    explained_variance   | 0.132          |
|    learning_rate        | 0.001          |
|    loss                 | 1.03e+04       |
|    n_updates            | 930            |
|    policy_gradient_loss | -4.18e-07      |
|    value_loss           | 5.12e+04       |
--------------------------------------------
Eval num_timesteps=74500, episode_reward=1106.23 +/- 576.19
Episode length: 33.80 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 73       |
|    time_elapsed    | 261      |
|    total_timesteps | 74752    |
---------------------------------
Eval num_timesteps=75000, episode_reward=1226.61 +/- 712.85
Episode length: 33.88 +/- 7.65
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.9           |
|    mean_reward          | 1.23e+03       |
| time/                   |                |
|    total_timesteps      | 75000          |
| train/                  |                |
|    approx_kl            | -3.1432137e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.14e-06      |
|    explained_variance   | 0.17           |
|    learning_rate        | 0.001          |
|    loss                 | 1.68e+04       |
|    n_updates            | 940            |
|    policy_gradient_loss | -1.25e-07      |
|    value_loss           | 6.08e+04       |
--------------------------------------------
Eval num_timesteps=75500, episode_reward=1162.53 +/- 617.43
Episode length: 34.78 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 74       |
|    time_elapsed    | 264      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=1296.69 +/- 665.93
Episode length: 35.92 +/- 6.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 1.3e+03       |
| time/                   |               |
|    total_timesteps      | 76000         |
| train/                  |               |
|    approx_kl            | -5.820766e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.24e-06     |
|    explained_variance   | 0.1           |
|    learning_rate        | 0.001         |
|    loss                 | 1.86e+04      |
|    n_updates            | 950           |
|    policy_gradient_loss | -7.97e-07     |
|    value_loss           | 5.07e+04      |
-------------------------------------------
Eval num_timesteps=76500, episode_reward=1090.88 +/- 620.44
Episode length: 33.46 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 286      |
|    iterations      | 75       |
|    time_elapsed    | 268      |
|    total_timesteps | 76800    |
---------------------------------
Eval num_timesteps=77000, episode_reward=1044.84 +/- 494.81
Episode length: 34.30 +/- 6.43
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.3           |
|    mean_reward          | 1.04e+03       |
| time/                   |                |
|    total_timesteps      | 77000          |
| train/                  |                |
|    approx_kl            | -1.1059456e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000175      |
|    explained_variance   | 0.132          |
|    learning_rate        | 0.001          |
|    loss                 | 1.66e+04       |
|    n_updates            | 960            |
|    policy_gradient_loss | -3.63e-07      |
|    value_loss           | 6.33e+04       |
--------------------------------------------
Eval num_timesteps=77500, episode_reward=1318.22 +/- 651.10
Episode length: 36.66 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 76       |
|    time_elapsed    | 271      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=1337.90 +/- 660.29
Episode length: 36.54 +/- 5.60
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.5           |
|    mean_reward          | 1.34e+03       |
| time/                   |                |
|    total_timesteps      | 78000          |
| train/                  |                |
|    approx_kl            | -1.3387762e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.45e-05      |
|    explained_variance   | 0.237          |
|    learning_rate        | 0.001          |
|    loss                 | 1.75e+04       |
|    n_updates            | 970            |
|    policy_gradient_loss | -1.29e-06      |
|    value_loss           | 6.13e+04       |
--------------------------------------------
Eval num_timesteps=78500, episode_reward=1138.61 +/- 562.55
Episode length: 34.80 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.34e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 77       |
|    time_elapsed    | 275      |
|    total_timesteps | 78848    |
---------------------------------
Eval num_timesteps=79000, episode_reward=1208.36 +/- 670.10
Episode length: 34.64 +/- 7.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 1.21e+03      |
| time/                   |               |
|    total_timesteps      | 79000         |
| train/                  |               |
|    approx_kl            | -9.895302e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.07e-05     |
|    explained_variance   | 0.0818        |
|    learning_rate        | 0.001         |
|    loss                 | 2.35e+04      |
|    n_updates            | 980           |
|    policy_gradient_loss | -8.92e-07     |
|    value_loss           | 6.96e+04      |
-------------------------------------------
Eval num_timesteps=79500, episode_reward=1268.43 +/- 614.10
Episode length: 36.36 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 1.39e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 78       |
|    time_elapsed    | 279      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=1257.01 +/- 656.39
Episode length: 35.26 +/- 6.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 1.26e+03      |
| time/                   |               |
|    total_timesteps      | 80000         |
| train/                  |               |
|    approx_kl            | -2.910383e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.2e-05      |
|    explained_variance   | 0.199         |
|    learning_rate        | 0.001         |
|    loss                 | 1.55e+04      |
|    n_updates            | 990           |
|    policy_gradient_loss | -2.46e-07     |
|    value_loss           | 6.05e+04      |
-------------------------------------------
Eval num_timesteps=80500, episode_reward=1337.12 +/- 664.64
Episode length: 36.50 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 79       |
|    time_elapsed    | 282      |
|    total_timesteps | 80896    |
---------------------------------
Eval num_timesteps=81000, episode_reward=1339.43 +/- 693.89
Episode length: 35.78 +/- 6.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 1.34e+03      |
| time/                   |               |
|    total_timesteps      | 81000         |
| train/                  |               |
|    approx_kl            | -3.608875e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.98e-05     |
|    explained_variance   | 0.239         |
|    learning_rate        | 0.001         |
|    loss                 | 1.42e+04      |
|    n_updates            | 1000          |
|    policy_gradient_loss | -2.97e-07     |
|    value_loss           | 5.29e+04      |
-------------------------------------------
Eval num_timesteps=81500, episode_reward=1312.57 +/- 677.17
Episode length: 35.72 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 80       |
|    time_elapsed    | 286      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=1179.00 +/- 644.03
Episode length: 34.62 +/- 6.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 1.18e+03      |
| time/                   |               |
|    total_timesteps      | 82000         |
| train/                  |               |
|    approx_kl            | -2.561137e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.09e-05     |
|    explained_variance   | 0.171         |
|    learning_rate        | 0.001         |
|    loss                 | 1.41e+04      |
|    n_updates            | 1010          |
|    policy_gradient_loss | -2.54e-07     |
|    value_loss           | 5.72e+04      |
-------------------------------------------
Eval num_timesteps=82500, episode_reward=1299.29 +/- 697.49
Episode length: 35.44 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 81       |
|    time_elapsed    | 289      |
|    total_timesteps | 82944    |
---------------------------------
Eval num_timesteps=83000, episode_reward=1323.68 +/- 645.57
Episode length: 36.70 +/- 5.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.7          |
|    mean_reward          | 1.32e+03      |
| time/                   |               |
|    total_timesteps      | 83000         |
| train/                  |               |
|    approx_kl            | -5.122274e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.71e-05     |
|    explained_variance   | 0.201         |
|    learning_rate        | 0.001         |
|    loss                 | 1.72e+04      |
|    n_updates            | 1020          |
|    policy_gradient_loss | -5.2e-07      |
|    value_loss           | 5.48e+04      |
-------------------------------------------
Eval num_timesteps=83500, episode_reward=1307.54 +/- 652.80
Episode length: 36.30 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 82       |
|    time_elapsed    | 293      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=1335.05 +/- 636.89
Episode length: 37.28 +/- 5.35
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 37.3           |
|    mean_reward          | 1.34e+03       |
| time/                   |                |
|    total_timesteps      | 84000          |
| train/                  |                |
|    approx_kl            | -5.0640665e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.21e-05      |
|    explained_variance   | 0.132          |
|    learning_rate        | 0.001          |
|    loss                 | 2.06e+04       |
|    n_updates            | 1030           |
|    policy_gradient_loss | -3.35e-07      |
|    value_loss           | 5.61e+04       |
--------------------------------------------
Eval num_timesteps=84500, episode_reward=1203.58 +/- 664.46
Episode length: 34.56 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 83       |
|    time_elapsed    | 296      |
|    total_timesteps | 84992    |
---------------------------------
Eval num_timesteps=85000, episode_reward=1372.39 +/- 697.26
Episode length: 36.10 +/- 6.11
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.1           |
|    mean_reward          | 1.37e+03       |
| time/                   |                |
|    total_timesteps      | 85000          |
| train/                  |                |
|    approx_kl            | -2.4447218e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.73e-05      |
|    explained_variance   | 0.212          |
|    learning_rate        | 0.001          |
|    loss                 | 1.97e+04       |
|    n_updates            | 1040           |
|    policy_gradient_loss | -4.69e-07      |
|    value_loss           | 6.22e+04       |
--------------------------------------------
Eval num_timesteps=85500, episode_reward=1259.20 +/- 614.72
Episode length: 35.86 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=1296.30 +/- 728.88
Episode length: 34.60 +/- 7.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 84       |
|    time_elapsed    | 301      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=1372.36 +/- 698.95
Episode length: 36.36 +/- 6.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 1.37e+03      |
| time/                   |               |
|    total_timesteps      | 86500         |
| train/                  |               |
|    approx_kl            | -8.731149e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.99e-06     |
|    explained_variance   | 0.235         |
|    learning_rate        | 0.001         |
|    loss                 | 1.9e+04       |
|    n_updates            | 1050          |
|    policy_gradient_loss | -3.56e-07     |
|    value_loss           | 5.45e+04      |
-------------------------------------------
Eval num_timesteps=87000, episode_reward=1072.38 +/- 591.40
Episode length: 33.02 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 85       |
|    time_elapsed    | 305      |
|    total_timesteps | 87040    |
---------------------------------
Eval num_timesteps=87500, episode_reward=1209.00 +/- 658.25
Episode length: 34.40 +/- 6.81
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.4           |
|    mean_reward          | 1.21e+03       |
| time/                   |                |
|    total_timesteps      | 87500          |
| train/                  |                |
|    approx_kl            | -2.5029294e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.13e-06      |
|    explained_variance   | 0.355          |
|    learning_rate        | 0.001          |
|    loss                 | 2.42e+04       |
|    n_updates            | 1060           |
|    policy_gradient_loss | -1.68e-07      |
|    value_loss           | 6.11e+04       |
--------------------------------------------
Eval num_timesteps=88000, episode_reward=1243.25 +/- 670.09
Episode length: 35.18 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 86       |
|    time_elapsed    | 308      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=1110.51 +/- 621.37
Episode length: 33.24 +/- 7.30
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.2           |
|    mean_reward          | 1.11e+03       |
| time/                   |                |
|    total_timesteps      | 88500          |
| train/                  |                |
|    approx_kl            | -1.7462298e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.48e-06      |
|    explained_variance   | 0.244          |
|    learning_rate        | 0.001          |
|    loss                 | 2.68e+04       |
|    n_updates            | 1070           |
|    policy_gradient_loss | -1.4e-07       |
|    value_loss           | 6.88e+04       |
--------------------------------------------
Eval num_timesteps=89000, episode_reward=994.70 +/- 524.70
Episode length: 33.14 +/- 7.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 995      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 87       |
|    time_elapsed    | 311      |
|    total_timesteps | 89088    |
---------------------------------
Eval num_timesteps=89500, episode_reward=1230.00 +/- 645.12
Episode length: 35.26 +/- 6.56
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.3           |
|    mean_reward          | 1.23e+03       |
| time/                   |                |
|    total_timesteps      | 89500          |
| train/                  |                |
|    approx_kl            | -1.0826625e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.85e-05      |
|    explained_variance   | 0.406          |
|    learning_rate        | 0.001          |
|    loss                 | 1.4e+04        |
|    n_updates            | 1080           |
|    policy_gradient_loss | -2.72e-08      |
|    value_loss           | 4.82e+04       |
--------------------------------------------
Eval num_timesteps=90000, episode_reward=1197.06 +/- 598.65
Episode length: 35.38 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 88       |
|    time_elapsed    | 315      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=1304.88 +/- 656.03
Episode length: 36.36 +/- 5.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 1.3e+03       |
| time/                   |               |
|    total_timesteps      | 90500         |
| train/                  |               |
|    approx_kl            | 5.0640665e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000103     |
|    explained_variance   | 0.152         |
|    learning_rate        | 0.001         |
|    loss                 | 1.31e+04      |
|    n_updates            | 1090          |
|    policy_gradient_loss | -6.87e-05     |
|    value_loss           | 6.28e+04      |
-------------------------------------------
Eval num_timesteps=91000, episode_reward=1171.91 +/- 643.70
Episode length: 34.12 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.4     |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 89       |
|    time_elapsed    | 319      |
|    total_timesteps | 91136    |
---------------------------------
Eval num_timesteps=91500, episode_reward=1454.55 +/- 745.56
Episode length: 36.20 +/- 7.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.2          |
|    mean_reward          | 1.45e+03      |
| time/                   |               |
|    total_timesteps      | 91500         |
| train/                  |               |
|    approx_kl            | 1.7462298e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.89e-05     |
|    explained_variance   | 0.112         |
|    learning_rate        | 0.001         |
|    loss                 | 1.87e+04      |
|    n_updates            | 1100          |
|    policy_gradient_loss | -1.32e-06     |
|    value_loss           | 7.11e+04      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=92000, episode_reward=1156.09 +/- 624.26
Episode length: 34.66 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 90       |
|    time_elapsed    | 322      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=1196.18 +/- 670.11
Episode length: 34.24 +/- 7.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 1.2e+03       |
| time/                   |               |
|    total_timesteps      | 92500         |
| train/                  |               |
|    approx_kl            | -8.731149e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.21e-05     |
|    explained_variance   | 0.242         |
|    learning_rate        | 0.001         |
|    loss                 | 1.96e+04      |
|    n_updates            | 1110          |
|    policy_gradient_loss | -2.11e-06     |
|    value_loss           | 5.85e+04      |
-------------------------------------------
Eval num_timesteps=93000, episode_reward=1197.68 +/- 629.26
Episode length: 34.80 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 91       |
|    time_elapsed    | 326      |
|    total_timesteps | 93184    |
---------------------------------
Eval num_timesteps=93500, episode_reward=1133.89 +/- 593.92
Episode length: 34.38 +/- 6.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 1.13e+03      |
| time/                   |               |
|    total_timesteps      | 93500         |
| train/                  |               |
|    approx_kl            | 6.4028427e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.94e-05     |
|    explained_variance   | 0.155         |
|    learning_rate        | 0.001         |
|    loss                 | 1.53e+04      |
|    n_updates            | 1120          |
|    policy_gradient_loss | -2.52e-06     |
|    value_loss           | 5.71e+04      |
-------------------------------------------
Eval num_timesteps=94000, episode_reward=1360.86 +/- 679.86
Episode length: 36.64 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 92       |
|    time_elapsed    | 329      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=1124.35 +/- 567.93
Episode length: 34.52 +/- 6.33
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.5           |
|    mean_reward          | 1.12e+03       |
| time/                   |                |
|    total_timesteps      | 94500          |
| train/                  |                |
|    approx_kl            | -2.4447218e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.53e-05      |
|    explained_variance   | 0.287          |
|    learning_rate        | 0.001          |
|    loss                 | 1.76e+04       |
|    n_updates            | 1130           |
|    policy_gradient_loss | -2.55e-06      |
|    value_loss           | 5.33e+04       |
--------------------------------------------
Eval num_timesteps=95000, episode_reward=1353.23 +/- 691.46
Episode length: 36.36 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 93       |
|    time_elapsed    | 333      |
|    total_timesteps | 95232    |
---------------------------------
Eval num_timesteps=95500, episode_reward=1258.23 +/- 683.59
Episode length: 34.60 +/- 6.57
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.6           |
|    mean_reward          | 1.26e+03       |
| time/                   |                |
|    total_timesteps      | 95500          |
| train/                  |                |
|    approx_kl            | -2.5029294e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.56e-05      |
|    explained_variance   | 0.228          |
|    learning_rate        | 0.001          |
|    loss                 | 1.96e+04       |
|    n_updates            | 1140           |
|    policy_gradient_loss | -9.33e-07      |
|    value_loss           | 5.34e+04       |
--------------------------------------------
Eval num_timesteps=96000, episode_reward=1190.60 +/- 617.81
Episode length: 35.48 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 94       |
|    time_elapsed    | 336      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=1037.37 +/- 537.89
Episode length: 33.32 +/- 6.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.3          |
|    mean_reward          | 1.04e+03      |
| time/                   |               |
|    total_timesteps      | 96500         |
| train/                  |               |
|    approx_kl            | -7.799827e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.49e-05     |
|    explained_variance   | 0.0396        |
|    learning_rate        | 0.001         |
|    loss                 | 1.88e+04      |
|    n_updates            | 1150          |
|    policy_gradient_loss | -9.06e-07     |
|    value_loss           | 5.11e+04      |
-------------------------------------------
Eval num_timesteps=97000, episode_reward=1317.62 +/- 674.75
Episode length: 36.02 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 95       |
|    time_elapsed    | 340      |
|    total_timesteps | 97280    |
---------------------------------
Eval num_timesteps=97500, episode_reward=1134.87 +/- 635.03
Episode length: 33.72 +/- 7.08
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.7           |
|    mean_reward          | 1.13e+03       |
| time/                   |                |
|    total_timesteps      | 97500          |
| train/                  |                |
|    approx_kl            | -3.7252903e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.45e-05      |
|    explained_variance   | 0.295          |
|    learning_rate        | 0.001          |
|    loss                 | 1.72e+04       |
|    n_updates            | 1160           |
|    policy_gradient_loss | -6.78e-07      |
|    value_loss           | 6.03e+04       |
--------------------------------------------
Eval num_timesteps=98000, episode_reward=1237.31 +/- 649.85
Episode length: 35.76 +/- 7.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 96       |
|    time_elapsed    | 343      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=1186.68 +/- 684.84
Episode length: 33.94 +/- 8.05
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.9           |
|    mean_reward          | 1.19e+03       |
| time/                   |                |
|    total_timesteps      | 98500          |
| train/                  |                |
|    approx_kl            | -2.9685907e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.54e-05      |
|    explained_variance   | 0.252          |
|    learning_rate        | 0.001          |
|    loss                 | 1.99e+04       |
|    n_updates            | 1170           |
|    policy_gradient_loss | -4.47e-07      |
|    value_loss           | 5.37e+04       |
--------------------------------------------
Eval num_timesteps=99000, episode_reward=1435.09 +/- 648.21
Episode length: 37.98 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 38       |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 97       |
|    time_elapsed    | 347      |
|    total_timesteps | 99328    |
---------------------------------
Eval num_timesteps=99500, episode_reward=1121.83 +/- 612.93
Episode length: 33.84 +/- 7.04
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.8           |
|    mean_reward          | 1.12e+03       |
| time/                   |                |
|    total_timesteps      | 99500          |
| train/                  |                |
|    approx_kl            | -1.4551915e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.01e-05      |
|    explained_variance   | 0.273          |
|    learning_rate        | 0.001          |
|    loss                 | 1.85e+04       |
|    n_updates            | 1180           |
|    policy_gradient_loss | -4.21e-07      |
|    value_loss           | 6.93e+04       |
--------------------------------------------
Eval num_timesteps=100000, episode_reward=1262.26 +/- 661.84
Episode length: 35.84 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 98       |
|    time_elapsed    | 350      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=1160.42 +/- 581.35
Episode length: 35.28 +/- 6.13
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.3           |
|    mean_reward          | 1.16e+03       |
| time/                   |                |
|    total_timesteps      | 100500         |
| train/                  |                |
|    approx_kl            | -4.8894435e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.62e-05      |
|    explained_variance   | 0.13           |
|    learning_rate        | 0.001          |
|    loss                 | 1.44e+04       |
|    n_updates            | 1190           |
|    policy_gradient_loss | -8.36e-07      |
|    value_loss           | 5.45e+04       |
--------------------------------------------
Eval num_timesteps=101000, episode_reward=1182.23 +/- 611.16
Episode length: 35.28 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 99       |
|    time_elapsed    | 354      |
|    total_timesteps | 101376   |
---------------------------------
Eval num_timesteps=101500, episode_reward=1362.95 +/- 703.41
Episode length: 36.18 +/- 6.81
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.2           |
|    mean_reward          | 1.36e+03       |
| time/                   |                |
|    total_timesteps      | 101500         |
| train/                  |                |
|    approx_kl            | -2.5029294e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.66e-05      |
|    explained_variance   | 0.159          |
|    learning_rate        | 0.001          |
|    loss                 | 2.22e+04       |
|    n_updates            | 1200           |
|    policy_gradient_loss | -1.1e-06       |
|    value_loss           | 6.14e+04       |
--------------------------------------------
Eval num_timesteps=102000, episode_reward=1181.78 +/- 673.11
Episode length: 33.98 +/- 7.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 100      |
|    time_elapsed    | 357      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=1316.00 +/- 646.41
Episode length: 36.44 +/- 5.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 1.32e+03      |
| time/                   |               |
|    total_timesteps      | 102500        |
| train/                  |               |
|    approx_kl            | -4.656613e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.48e-05     |
|    explained_variance   | 0.29          |
|    learning_rate        | 0.001         |
|    loss                 | 2.06e+04      |
|    n_updates            | 1210          |
|    policy_gradient_loss | -2.84e-06     |
|    value_loss           | 5.84e+04      |
-------------------------------------------
Eval num_timesteps=103000, episode_reward=1159.08 +/- 589.15
Episode length: 34.84 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 101      |
|    time_elapsed    | 361      |
|    total_timesteps | 103424   |
---------------------------------
Eval num_timesteps=103500, episode_reward=1148.66 +/- 588.26
Episode length: 34.62 +/- 6.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 1.15e+03      |
| time/                   |               |
|    total_timesteps      | 103500        |
| train/                  |               |
|    approx_kl            | -1.542503e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.76e-05     |
|    explained_variance   | 0.34          |
|    learning_rate        | 0.001         |
|    loss                 | 2.43e+04      |
|    n_updates            | 1220          |
|    policy_gradient_loss | -2.53e-07     |
|    value_loss           | 5.45e+04      |
-------------------------------------------
Eval num_timesteps=104000, episode_reward=1142.38 +/- 597.89
Episode length: 34.68 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 102      |
|    time_elapsed    | 364      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=1320.92 +/- 655.24
Episode length: 36.62 +/- 6.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.6      |
|    mean_reward          | 1.32e+03  |
| time/                   |           |
|    total_timesteps      | 104500    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000127 |
|    explained_variance   | 0.177     |
|    learning_rate        | 0.001     |
|    loss                 | 8.24e+03  |
|    n_updates            | 1230      |
|    policy_gradient_loss | -8.53e-07 |
|    value_loss           | 3.9e+04   |
---------------------------------------
Eval num_timesteps=105000, episode_reward=1252.22 +/- 661.94
Episode length: 35.32 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 103      |
|    time_elapsed    | 368      |
|    total_timesteps | 105472   |
---------------------------------
Eval num_timesteps=105500, episode_reward=1235.50 +/- 677.90
Episode length: 34.86 +/- 7.01
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.9           |
|    mean_reward          | 1.24e+03       |
| time/                   |                |
|    total_timesteps      | 105500         |
| train/                  |                |
|    approx_kl            | -1.3969839e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000131      |
|    explained_variance   | 0.226          |
|    learning_rate        | 0.001          |
|    loss                 | 1.89e+04       |
|    n_updates            | 1240           |
|    policy_gradient_loss | -3.26e-06      |
|    value_loss           | 4.57e+04       |
--------------------------------------------
Eval num_timesteps=106000, episode_reward=1166.20 +/- 647.42
Episode length: 33.84 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 286      |
|    iterations      | 104      |
|    time_elapsed    | 372      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=1177.53 +/- 627.81
Episode length: 35.08 +/- 7.19
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.1           |
|    mean_reward          | 1.18e+03       |
| time/                   |                |
|    total_timesteps      | 106500         |
| train/                  |                |
|    approx_kl            | -2.3283064e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.21e-05      |
|    explained_variance   | 0.23           |
|    learning_rate        | 0.001          |
|    loss                 | 1.32e+04       |
|    n_updates            | 1250           |
|    policy_gradient_loss | -3.01e-06      |
|    value_loss           | 4.8e+04        |
--------------------------------------------
Eval num_timesteps=107000, episode_reward=1224.10 +/- 628.86
Episode length: 35.90 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=1268.36 +/- 656.63
Episode length: 35.46 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 105      |
|    time_elapsed    | 376      |
|    total_timesteps | 107520   |
---------------------------------
Eval num_timesteps=108000, episode_reward=1374.30 +/- 667.32
Episode length: 37.30 +/- 5.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.3          |
|    mean_reward          | 1.37e+03      |
| time/                   |               |
|    total_timesteps      | 108000        |
| train/                  |               |
|    approx_kl            | -2.386514e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.82e-05     |
|    explained_variance   | 0.34          |
|    learning_rate        | 0.001         |
|    loss                 | 1.38e+04      |
|    n_updates            | 1260          |
|    policy_gradient_loss | -1.53e-06     |
|    value_loss           | 4.95e+04      |
-------------------------------------------
Eval num_timesteps=108500, episode_reward=1109.92 +/- 542.32
Episode length: 35.34 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 106      |
|    time_elapsed    | 380      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=1216.41 +/- 647.58
Episode length: 35.02 +/- 6.24
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35             |
|    mean_reward          | 1.22e+03       |
| time/                   |                |
|    total_timesteps      | 109000         |
| train/                  |                |
|    approx_kl            | -1.4551915e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.77e-05      |
|    explained_variance   | 0.278          |
|    learning_rate        | 0.001          |
|    loss                 | 1.07e+04       |
|    n_updates            | 1270           |
|    policy_gradient_loss | -8.54e-07      |
|    value_loss           | 5.24e+04       |
--------------------------------------------
Eval num_timesteps=109500, episode_reward=1194.55 +/- 675.32
Episode length: 34.48 +/- 7.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 107      |
|    time_elapsed    | 383      |
|    total_timesteps | 109568   |
---------------------------------
Eval num_timesteps=110000, episode_reward=1267.50 +/- 651.29
Episode length: 35.68 +/- 6.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 1.27e+03      |
| time/                   |               |
|    total_timesteps      | 110000        |
| train/                  |               |
|    approx_kl            | -9.371433e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.31e-05     |
|    explained_variance   | 0.336         |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+04      |
|    n_updates            | 1280          |
|    policy_gradient_loss | -6.43e-07     |
|    value_loss           | 4.25e+04      |
-------------------------------------------
Eval num_timesteps=110500, episode_reward=1314.92 +/- 719.50
Episode length: 35.68 +/- 7.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 108      |
|    time_elapsed    | 387      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=1199.33 +/- 635.92
Episode length: 35.08 +/- 6.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 1.2e+03       |
| time/                   |               |
|    total_timesteps      | 111000        |
| train/                  |               |
|    approx_kl            | -9.720679e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.32e-05     |
|    explained_variance   | 0.237         |
|    learning_rate        | 0.001         |
|    loss                 | 1.03e+04      |
|    n_updates            | 1290          |
|    policy_gradient_loss | -1.08e-06     |
|    value_loss           | 4.38e+04      |
-------------------------------------------
Eval num_timesteps=111500, episode_reward=1165.41 +/- 613.76
Episode length: 34.78 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 109      |
|    time_elapsed    | 391      |
|    total_timesteps | 111616   |
---------------------------------
Eval num_timesteps=112000, episode_reward=1147.83 +/- 631.41
Episode length: 34.14 +/- 6.62
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.1           |
|    mean_reward          | 1.15e+03       |
| time/                   |                |
|    total_timesteps      | 112000         |
| train/                  |                |
|    approx_kl            | -3.7252903e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.92e-05      |
|    explained_variance   | 0.302          |
|    learning_rate        | 0.001          |
|    loss                 | 7.87e+03       |
|    n_updates            | 1300           |
|    policy_gradient_loss | -1.03e-06      |
|    value_loss           | 4.03e+04       |
--------------------------------------------
Eval num_timesteps=112500, episode_reward=1176.81 +/- 703.11
Episode length: 32.94 +/- 7.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.9     |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 110      |
|    time_elapsed    | 394      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=1273.65 +/- 649.43
Episode length: 35.58 +/- 6.23
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.6           |
|    mean_reward          | 1.27e+03       |
| time/                   |                |
|    total_timesteps      | 113000         |
| train/                  |                |
|    approx_kl            | -1.5133992e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.85e-05      |
|    explained_variance   | 0.331          |
|    learning_rate        | 0.001          |
|    loss                 | 1.46e+04       |
|    n_updates            | 1310           |
|    policy_gradient_loss | -1.19e-06      |
|    value_loss           | 4.48e+04       |
--------------------------------------------
Eval num_timesteps=113500, episode_reward=1199.12 +/- 660.51
Episode length: 34.50 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 111      |
|    time_elapsed    | 398      |
|    total_timesteps | 113664   |
---------------------------------
Eval num_timesteps=114000, episode_reward=1161.62 +/- 584.72
Episode length: 35.40 +/- 6.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 1.16e+03      |
| time/                   |               |
|    total_timesteps      | 114000        |
| train/                  |               |
|    approx_kl            | 7.6252036e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.77e-05     |
|    explained_variance   | 0.341         |
|    learning_rate        | 0.001         |
|    loss                 | 1.61e+04      |
|    n_updates            | 1320          |
|    policy_gradient_loss | -1.02e-05     |
|    value_loss           | 5.7e+04       |
-------------------------------------------
Eval num_timesteps=114500, episode_reward=1161.19 +/- 554.30
Episode length: 35.98 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 112      |
|    time_elapsed    | 401      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=1296.26 +/- 692.15
Episode length: 35.36 +/- 6.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 1.3e+03       |
| time/                   |               |
|    total_timesteps      | 115000        |
| train/                  |               |
|    approx_kl            | -3.085006e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.15e-05     |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+04      |
|    n_updates            | 1330          |
|    policy_gradient_loss | -2.81e-06     |
|    value_loss           | 5.04e+04      |
-------------------------------------------
Eval num_timesteps=115500, episode_reward=1211.73 +/- 652.99
Episode length: 35.16 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 113      |
|    time_elapsed    | 405      |
|    total_timesteps | 115712   |
---------------------------------
Eval num_timesteps=116000, episode_reward=1375.45 +/- 723.07
Episode length: 35.84 +/- 6.69
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.8           |
|    mean_reward          | 1.38e+03       |
| time/                   |                |
|    total_timesteps      | 116000         |
| train/                  |                |
|    approx_kl            | -2.6775524e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -4.7e-05       |
|    explained_variance   | 0.258          |
|    learning_rate        | 0.001          |
|    loss                 | 7.28e+03       |
|    n_updates            | 1340           |
|    policy_gradient_loss | -1.55e-06      |
|    value_loss           | 4.1e+04        |
--------------------------------------------
Eval num_timesteps=116500, episode_reward=1082.34 +/- 589.35
Episode length: 33.50 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 114      |
|    time_elapsed    | 408      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=1130.44 +/- 599.76
Episode length: 34.32 +/- 6.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 1.13e+03      |
| time/                   |               |
|    total_timesteps      | 117000        |
| train/                  |               |
|    approx_kl            | -7.741619e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.87e-05     |
|    explained_variance   | 0.0146        |
|    learning_rate        | 0.001         |
|    loss                 | 1.5e+04       |
|    n_updates            | 1350          |
|    policy_gradient_loss | -1.12e-06     |
|    value_loss           | 5.44e+04      |
-------------------------------------------
Eval num_timesteps=117500, episode_reward=1237.67 +/- 642.43
Episode length: 35.62 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 115      |
|    time_elapsed    | 412      |
|    total_timesteps | 117760   |
---------------------------------
Eval num_timesteps=118000, episode_reward=1205.79 +/- 663.48
Episode length: 34.98 +/- 6.82
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35             |
|    mean_reward          | 1.21e+03       |
| time/                   |                |
|    total_timesteps      | 118000         |
| train/                  |                |
|    approx_kl            | -2.2118911e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.95e-05      |
|    explained_variance   | 0.342          |
|    learning_rate        | 0.001          |
|    loss                 | 1.67e+04       |
|    n_updates            | 1360           |
|    policy_gradient_loss | -1.93e-06      |
|    value_loss           | 4.52e+04       |
--------------------------------------------
Eval num_timesteps=118500, episode_reward=1300.53 +/- 664.75
Episode length: 36.30 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 116      |
|    time_elapsed    | 415      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=1290.63 +/- 671.36
Episode length: 35.40 +/- 6.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 1.29e+03      |
| time/                   |               |
|    total_timesteps      | 119000        |
| train/                  |               |
|    approx_kl            | 2.9685907e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000143     |
|    explained_variance   | 0.173         |
|    learning_rate        | 0.001         |
|    loss                 | 9.99e+03      |
|    n_updates            | 1370          |
|    policy_gradient_loss | -3.91e-06     |
|    value_loss           | 4.2e+04       |
-------------------------------------------
Eval num_timesteps=119500, episode_reward=1300.03 +/- 655.72
Episode length: 36.16 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 117      |
|    time_elapsed    | 419      |
|    total_timesteps | 119808   |
---------------------------------
Eval num_timesteps=120000, episode_reward=1326.65 +/- 677.25
Episode length: 36.30 +/- 6.58
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.3           |
|    mean_reward          | 1.33e+03       |
| time/                   |                |
|    total_timesteps      | 120000         |
| train/                  |                |
|    approx_kl            | -3.4924597e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.53e-05      |
|    explained_variance   | 0.309          |
|    learning_rate        | 0.001          |
|    loss                 | 1.92e+04       |
|    n_updates            | 1380           |
|    policy_gradient_loss | -1.72e-06      |
|    value_loss           | 5.38e+04       |
--------------------------------------------
Eval num_timesteps=120500, episode_reward=1330.94 +/- 703.78
Episode length: 35.52 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 118      |
|    time_elapsed    | 422      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=1191.76 +/- 674.89
Episode length: 34.26 +/- 7.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 1.19e+03      |
| time/                   |               |
|    total_timesteps      | 121000        |
| train/                  |               |
|    approx_kl            | -4.598405e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.28e-05     |
|    explained_variance   | 0.119         |
|    learning_rate        | 0.001         |
|    loss                 | 1.6e+04       |
|    n_updates            | 1390          |
|    policy_gradient_loss | -1.14e-06     |
|    value_loss           | 5.88e+04      |
-------------------------------------------
Eval num_timesteps=121500, episode_reward=1126.43 +/- 637.43
Episode length: 33.70 +/- 7.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 119      |
|    time_elapsed    | 426      |
|    total_timesteps | 121856   |
---------------------------------
Eval num_timesteps=122000, episode_reward=1229.43 +/- 649.45
Episode length: 35.30 +/- 6.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 1.23e+03      |
| time/                   |               |
|    total_timesteps      | 122000        |
| train/                  |               |
|    approx_kl            | -3.434252e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.01e-05     |
|    explained_variance   | 0.292         |
|    learning_rate        | 0.001         |
|    loss                 | 2.03e+04      |
|    n_updates            | 1400          |
|    policy_gradient_loss | -9.99e-07     |
|    value_loss           | 5.15e+04      |
-------------------------------------------
Eval num_timesteps=122500, episode_reward=1161.75 +/- 580.34
Episode length: 35.02 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 120      |
|    time_elapsed    | 429      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=1145.12 +/- 665.14
Episode length: 33.42 +/- 7.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 1.15e+03      |
| time/                   |               |
|    total_timesteps      | 123000        |
| train/                  |               |
|    approx_kl            | 2.3283064e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000133     |
|    explained_variance   | 0.23          |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+04      |
|    n_updates            | 1410          |
|    policy_gradient_loss | -2.48e-06     |
|    value_loss           | 5.1e+04       |
-------------------------------------------
Eval num_timesteps=123500, episode_reward=1172.24 +/- 617.67
Episode length: 35.16 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 121      |
|    time_elapsed    | 433      |
|    total_timesteps | 123904   |
---------------------------------
Eval num_timesteps=124000, episode_reward=1382.32 +/- 665.74
Episode length: 37.30 +/- 6.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.3          |
|    mean_reward          | 1.38e+03      |
| time/                   |               |
|    total_timesteps      | 124000        |
| train/                  |               |
|    approx_kl            | 2.9685907e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000159     |
|    explained_variance   | 0.19          |
|    learning_rate        | 0.001         |
|    loss                 | 7.67e+03      |
|    n_updates            | 1420          |
|    policy_gradient_loss | -1.42e-05     |
|    value_loss           | 4.63e+04      |
-------------------------------------------
Eval num_timesteps=124500, episode_reward=1354.56 +/- 682.53
Episode length: 36.24 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 122      |
|    time_elapsed    | 436      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=1412.01 +/- 715.85
Episode length: 35.90 +/- 6.77
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.9           |
|    mean_reward          | 1.41e+03       |
| time/                   |                |
|    total_timesteps      | 125000         |
| train/                  |                |
|    approx_kl            | -1.8626451e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.37e-05      |
|    explained_variance   | 0.228          |
|    learning_rate        | 0.001          |
|    loss                 | 1.03e+04       |
|    n_updates            | 1430           |
|    policy_gradient_loss | -2.86e-06      |
|    value_loss           | 4.84e+04       |
--------------------------------------------
Eval num_timesteps=125500, episode_reward=1285.80 +/- 632.49
Episode length: 36.50 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 123      |
|    time_elapsed    | 440      |
|    total_timesteps | 125952   |
---------------------------------
Eval num_timesteps=126000, episode_reward=1194.67 +/- 599.20
Episode length: 35.30 +/- 5.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 1.19e+03      |
| time/                   |               |
|    total_timesteps      | 126000        |
| train/                  |               |
|    approx_kl            | -5.180482e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.09e-05     |
|    explained_variance   | 0.301         |
|    learning_rate        | 0.001         |
|    loss                 | 8.27e+03      |
|    n_updates            | 1440          |
|    policy_gradient_loss | -2.72e-06     |
|    value_loss           | 4.49e+04      |
-------------------------------------------
Eval num_timesteps=126500, episode_reward=1233.61 +/- 618.72
Episode length: 35.88 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 124      |
|    time_elapsed    | 444      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=1144.71 +/- 592.15
Episode length: 34.90 +/- 6.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 1.14e+03      |
| time/                   |               |
|    total_timesteps      | 127000        |
| train/                  |               |
|    approx_kl            | -4.947651e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.63e-05     |
|    explained_variance   | 0.163         |
|    learning_rate        | 0.001         |
|    loss                 | 1.53e+04      |
|    n_updates            | 1450          |
|    policy_gradient_loss | -2.8e-06      |
|    value_loss           | 4.56e+04      |
-------------------------------------------
Eval num_timesteps=127500, episode_reward=1212.88 +/- 621.50
Episode length: 35.70 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=1311.22 +/- 688.23
Episode length: 36.12 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 125      |
|    time_elapsed    | 448      |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=1261.63 +/- 656.63
Episode length: 35.58 +/- 6.40
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.6           |
|    mean_reward          | 1.26e+03       |
| time/                   |                |
|    total_timesteps      | 128500         |
| train/                  |                |
|    approx_kl            | -2.2118911e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00014       |
|    explained_variance   | 0.142          |
|    learning_rate        | 0.001          |
|    loss                 | 1.15e+04       |
|    n_updates            | 1460           |
|    policy_gradient_loss | -3.92e-06      |
|    value_loss           | 4.66e+04       |
--------------------------------------------
Eval num_timesteps=129000, episode_reward=1145.95 +/- 593.09
Episode length: 35.04 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 126      |
|    time_elapsed    | 452      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=1296.14 +/- 627.09
Episode length: 36.74 +/- 5.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.7          |
|    mean_reward          | 1.3e+03       |
| time/                   |               |
|    total_timesteps      | 129500        |
| train/                  |               |
|    approx_kl            | 1.1059456e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00013      |
|    explained_variance   | 0.102         |
|    learning_rate        | 0.001         |
|    loss                 | 1.7e+04       |
|    n_updates            | 1470          |
|    policy_gradient_loss | -5.45e-06     |
|    value_loss           | 5.58e+04      |
-------------------------------------------
Eval num_timesteps=130000, episode_reward=1236.37 +/- 662.84
Episode length: 34.98 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 127      |
|    time_elapsed    | 455      |
|    total_timesteps | 130048   |
---------------------------------
Eval num_timesteps=130500, episode_reward=1208.85 +/- 660.43
Episode length: 34.58 +/- 6.85
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.6           |
|    mean_reward          | 1.21e+03       |
| time/                   |                |
|    total_timesteps      | 130500         |
| train/                  |                |
|    approx_kl            | -2.9685907e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.21e-05      |
|    explained_variance   | 0.16           |
|    learning_rate        | 0.001          |
|    loss                 | 1.33e+04       |
|    n_updates            | 1480           |
|    policy_gradient_loss | -2.96e-06      |
|    value_loss           | 6.02e+04       |
--------------------------------------------
Eval num_timesteps=131000, episode_reward=1339.36 +/- 661.64
Episode length: 36.84 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 128      |
|    time_elapsed    | 459      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=1285.45 +/- 699.60
Episode length: 35.00 +/- 6.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 1.29e+03     |
| time/                   |              |
|    total_timesteps      | 131500       |
| train/                  |              |
|    approx_kl            | 9.138603e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000142    |
|    explained_variance   | 0.176        |
|    learning_rate        | 0.001        |
|    loss                 | 1.37e+04     |
|    n_updates            | 1490         |
|    policy_gradient_loss | -1.78e-05    |
|    value_loss           | 5.47e+04     |
------------------------------------------
Eval num_timesteps=132000, episode_reward=1228.32 +/- 640.51
Episode length: 35.22 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 129      |
|    time_elapsed    | 463      |
|    total_timesteps | 132096   |
---------------------------------
Eval num_timesteps=132500, episode_reward=1188.54 +/- 602.65
Episode length: 35.36 +/- 6.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 1.19e+03      |
| time/                   |               |
|    total_timesteps      | 132500        |
| train/                  |               |
|    approx_kl            | 2.4447218e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000145     |
|    explained_variance   | 0.0892        |
|    learning_rate        | 0.001         |
|    loss                 | 1.49e+04      |
|    n_updates            | 1500          |
|    policy_gradient_loss | -3.27e-06     |
|    value_loss           | 5.18e+04      |
-------------------------------------------
Eval num_timesteps=133000, episode_reward=1296.36 +/- 659.46
Episode length: 36.04 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 130      |
|    time_elapsed    | 466      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=1234.64 +/- 641.72
Episode length: 35.28 +/- 6.34
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.3           |
|    mean_reward          | 1.23e+03       |
| time/                   |                |
|    total_timesteps      | 133500         |
| train/                  |                |
|    approx_kl            | -1.0477379e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000114      |
|    explained_variance   | 0.209          |
|    learning_rate        | 0.001          |
|    loss                 | 1.62e+04       |
|    n_updates            | 1510           |
|    policy_gradient_loss | -2.71e-06      |
|    value_loss           | 5.79e+04       |
--------------------------------------------
Eval num_timesteps=134000, episode_reward=1262.05 +/- 652.73
Episode length: 35.50 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 131      |
|    time_elapsed    | 470      |
|    total_timesteps | 134144   |
---------------------------------
Eval num_timesteps=134500, episode_reward=1242.50 +/- 668.98
Episode length: 34.76 +/- 6.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 1.24e+03      |
| time/                   |               |
|    total_timesteps      | 134500        |
| train/                  |               |
|    approx_kl            | -4.656613e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.36e-05     |
|    explained_variance   | 0.243         |
|    learning_rate        | 0.001         |
|    loss                 | 1.62e+04      |
|    n_updates            | 1520          |
|    policy_gradient_loss | -3.21e-06     |
|    value_loss           | 5.54e+04      |
-------------------------------------------
Eval num_timesteps=135000, episode_reward=1174.17 +/- 638.25
Episode length: 34.42 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 132      |
|    time_elapsed    | 473      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=1209.78 +/- 690.69
Episode length: 34.14 +/- 7.41
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.1           |
|    mean_reward          | 1.21e+03       |
| time/                   |                |
|    total_timesteps      | 135500         |
| train/                  |                |
|    approx_kl            | -3.4924597e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.61e-05      |
|    explained_variance   | 0.206          |
|    learning_rate        | 0.001          |
|    loss                 | 1.37e+04       |
|    n_updates            | 1530           |
|    policy_gradient_loss | -3.92e-06      |
|    value_loss           | 5.28e+04       |
--------------------------------------------
Eval num_timesteps=136000, episode_reward=1247.04 +/- 633.05
Episode length: 35.60 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 133      |
|    time_elapsed    | 477      |
|    total_timesteps | 136192   |
---------------------------------
Eval num_timesteps=136500, episode_reward=1324.25 +/- 641.91
Episode length: 36.74 +/- 6.09
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.7           |
|    mean_reward          | 1.32e+03       |
| time/                   |                |
|    total_timesteps      | 136500         |
| train/                  |                |
|    approx_kl            | -1.7462298e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000171      |
|    explained_variance   | 0.199          |
|    learning_rate        | 0.001          |
|    loss                 | 5.46e+03       |
|    n_updates            | 1540           |
|    policy_gradient_loss | -5.1e-06       |
|    value_loss           | 3.63e+04       |
--------------------------------------------
Eval num_timesteps=137000, episode_reward=1325.91 +/- 637.81
Episode length: 36.82 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 134      |
|    time_elapsed    | 480      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=1342.34 +/- 661.03
Episode length: 36.46 +/- 6.01
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.5           |
|    mean_reward          | 1.34e+03       |
| time/                   |                |
|    total_timesteps      | 137500         |
| train/                  |                |
|    approx_kl            | -4.8894435e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.72e-05      |
|    explained_variance   | 0.236          |
|    learning_rate        | 0.001          |
|    loss                 | 7.64e+03       |
|    n_updates            | 1550           |
|    policy_gradient_loss | -4.69e-06      |
|    value_loss           | 4.11e+04       |
--------------------------------------------
Eval num_timesteps=138000, episode_reward=1418.30 +/- 680.51
Episode length: 36.96 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 135      |
|    time_elapsed    | 484      |
|    total_timesteps | 138240   |
---------------------------------
Eval num_timesteps=138500, episode_reward=1143.23 +/- 630.66
Episode length: 33.82 +/- 6.78
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.8           |
|    mean_reward          | 1.14e+03       |
| time/                   |                |
|    total_timesteps      | 138500         |
| train/                  |                |
|    approx_kl            | -2.2118911e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.85e-05      |
|    explained_variance   | 0.221          |
|    learning_rate        | 0.001          |
|    loss                 | 1.3e+04        |
|    n_updates            | 1560           |
|    policy_gradient_loss | -1.46e-06      |
|    value_loss           | 5.34e+04       |
--------------------------------------------
Eval num_timesteps=139000, episode_reward=1222.65 +/- 641.70
Episode length: 35.12 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 136      |
|    time_elapsed    | 487      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=1413.16 +/- 726.50
Episode length: 36.42 +/- 6.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.4         |
|    mean_reward          | 1.41e+03     |
| time/                   |              |
|    total_timesteps      | 139500       |
| train/                  |              |
|    approx_kl            | 4.703179e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000174    |
|    explained_variance   | 0.16         |
|    learning_rate        | 0.001        |
|    loss                 | 1.46e+04     |
|    n_updates            | 1570         |
|    policy_gradient_loss | -2.91e-05    |
|    value_loss           | 5.11e+04     |
------------------------------------------
Eval num_timesteps=140000, episode_reward=1256.29 +/- 660.90
Episode length: 35.76 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 137      |
|    time_elapsed    | 491      |
|    total_timesteps | 140288   |
---------------------------------
Eval num_timesteps=140500, episode_reward=1224.69 +/- 681.26
Episode length: 34.54 +/- 7.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 1.22e+03      |
| time/                   |               |
|    total_timesteps      | 140500        |
| train/                  |               |
|    approx_kl            | 3.4924597e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000111     |
|    explained_variance   | 0.274         |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+04      |
|    n_updates            | 1580          |
|    policy_gradient_loss | -6.71e-06     |
|    value_loss           | 5.77e+04      |
-------------------------------------------
Eval num_timesteps=141000, episode_reward=1083.43 +/- 592.99
Episode length: 33.44 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 138      |
|    time_elapsed    | 494      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=1158.89 +/- 583.76
Episode length: 35.12 +/- 6.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 1.16e+03      |
| time/                   |               |
|    total_timesteps      | 141500        |
| train/                  |               |
|    approx_kl            | -2.561137e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000106     |
|    explained_variance   | 0.29          |
|    learning_rate        | 0.001         |
|    loss                 | 2.2e+04       |
|    n_updates            | 1590          |
|    policy_gradient_loss | -3.18e-06     |
|    value_loss           | 6.15e+04      |
-------------------------------------------
Eval num_timesteps=142000, episode_reward=1070.36 +/- 570.12
Episode length: 33.72 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 139      |
|    time_elapsed    | 498      |
|    total_timesteps | 142336   |
---------------------------------
Eval num_timesteps=142500, episode_reward=1143.83 +/- 629.17
Episode length: 34.06 +/- 6.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.1          |
|    mean_reward          | 1.14e+03      |
| time/                   |               |
|    total_timesteps      | 142500        |
| train/                  |               |
|    approx_kl            | 6.2282197e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000266     |
|    explained_variance   | 0.134         |
|    learning_rate        | 0.001         |
|    loss                 | 1.39e+04      |
|    n_updates            | 1600          |
|    policy_gradient_loss | -3.17e-06     |
|    value_loss           | 6.05e+04      |
-------------------------------------------
Eval num_timesteps=143000, episode_reward=1240.24 +/- 636.15
Episode length: 35.80 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 140      |
|    time_elapsed    | 501      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=1160.15 +/- 631.24
Episode length: 34.92 +/- 7.36
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.9           |
|    mean_reward          | 1.16e+03       |
| time/                   |                |
|    total_timesteps      | 143500         |
| train/                  |                |
|    approx_kl            | -1.1059456e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000113      |
|    explained_variance   | 0.321          |
|    learning_rate        | 0.001          |
|    loss                 | 1.47e+04       |
|    n_updates            | 1610           |
|    policy_gradient_loss | -2.74e-06      |
|    value_loss           | 4.63e+04       |
--------------------------------------------
Eval num_timesteps=144000, episode_reward=1148.99 +/- 593.87
Episode length: 35.12 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 141      |
|    time_elapsed    | 505      |
|    total_timesteps | 144384   |
---------------------------------
Eval num_timesteps=144500, episode_reward=1259.24 +/- 688.49
Episode length: 34.90 +/- 6.86
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.9           |
|    mean_reward          | 1.26e+03       |
| time/                   |                |
|    total_timesteps      | 144500         |
| train/                  |                |
|    approx_kl            | -2.3283064e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000112      |
|    explained_variance   | 0.349          |
|    learning_rate        | 0.001          |
|    loss                 | 9.06e+03       |
|    n_updates            | 1620           |
|    policy_gradient_loss | -3.36e-06      |
|    value_loss           | 4.12e+04       |
--------------------------------------------
Eval num_timesteps=145000, episode_reward=1112.77 +/- 532.96
Episode length: 35.08 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 142      |
|    time_elapsed    | 509      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=1305.41 +/- 657.44
Episode length: 36.28 +/- 6.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 1.31e+03      |
| time/                   |               |
|    total_timesteps      | 145500        |
| train/                  |               |
|    approx_kl            | 1.5716068e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000105     |
|    explained_variance   | 0.288         |
|    learning_rate        | 0.001         |
|    loss                 | 9.79e+03      |
|    n_updates            | 1630          |
|    policy_gradient_loss | -6.05e-06     |
|    value_loss           | 5.36e+04      |
-------------------------------------------
Eval num_timesteps=146000, episode_reward=1281.38 +/- 643.96
Episode length: 35.92 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 143      |
|    time_elapsed    | 512      |
|    total_timesteps | 146432   |
---------------------------------
Eval num_timesteps=146500, episode_reward=1210.89 +/- 631.25
Episode length: 35.40 +/- 6.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 1.21e+03     |
| time/                   |              |
|    total_timesteps      | 146500       |
| train/                  |              |
|    approx_kl            | 9.895302e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000149    |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.001        |
|    loss                 | 1.69e+04     |
|    n_updates            | 1640         |
|    policy_gradient_loss | -3.38e-06    |
|    value_loss           | 4.77e+04     |
------------------------------------------
Eval num_timesteps=147000, episode_reward=1126.17 +/- 595.51
Episode length: 34.60 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 144      |
|    time_elapsed    | 516      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=1306.25 +/- 688.67
Episode length: 35.44 +/- 6.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 1.31e+03      |
| time/                   |               |
|    total_timesteps      | 147500        |
| train/                  |               |
|    approx_kl            | 1.1990778e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000183     |
|    explained_variance   | 0.198         |
|    learning_rate        | 0.001         |
|    loss                 | 9.47e+03      |
|    n_updates            | 1650          |
|    policy_gradient_loss | -2.25e-06     |
|    value_loss           | 4.66e+04      |
-------------------------------------------
Eval num_timesteps=148000, episode_reward=1384.75 +/- 692.58
Episode length: 36.72 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 145      |
|    time_elapsed    | 519      |
|    total_timesteps | 148480   |
---------------------------------
Eval num_timesteps=148500, episode_reward=1288.55 +/- 633.29
Episode length: 36.44 +/- 5.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 1.29e+03      |
| time/                   |               |
|    total_timesteps      | 148500        |
| train/                  |               |
|    approx_kl            | 3.2247044e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000487     |
|    explained_variance   | 0.133         |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+04      |
|    n_updates            | 1660          |
|    policy_gradient_loss | -3.37e-05     |
|    value_loss           | 4.93e+04      |
-------------------------------------------
Eval num_timesteps=149000, episode_reward=1250.60 +/- 695.54
Episode length: 34.46 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=1210.69 +/- 623.13
Episode length: 35.34 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 146      |
|    time_elapsed    | 524      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=1062.99 +/- 566.84
Episode length: 33.86 +/- 6.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.9          |
|    mean_reward          | 1.06e+03      |
| time/                   |               |
|    total_timesteps      | 150000        |
| train/                  |               |
|    approx_kl            | 4.0745363e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000137     |
|    explained_variance   | 0.288         |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+04      |
|    n_updates            | 1670          |
|    policy_gradient_loss | -1.98e-05     |
|    value_loss           | 4.04e+04      |
-------------------------------------------
Eval num_timesteps=150500, episode_reward=1260.33 +/- 692.20
Episode length: 35.14 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 147      |
|    time_elapsed    | 527      |
|    total_timesteps | 150528   |
---------------------------------
Eval num_timesteps=151000, episode_reward=1145.53 +/- 597.32
Episode length: 34.88 +/- 6.48
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.9           |
|    mean_reward          | 1.15e+03       |
| time/                   |                |
|    total_timesteps      | 151000         |
| train/                  |                |
|    approx_kl            | -2.0372681e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.36e-05      |
|    explained_variance   | 0.252          |
|    learning_rate        | 0.001          |
|    loss                 | 1.15e+04       |
|    n_updates            | 1680           |
|    policy_gradient_loss | -2.2e-06       |
|    value_loss           | 4.71e+04       |
--------------------------------------------
Eval num_timesteps=151500, episode_reward=1346.16 +/- 688.87
Episode length: 35.96 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 148      |
|    time_elapsed    | 531      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=1198.21 +/- 626.30
Episode length: 34.98 +/- 6.17
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35             |
|    mean_reward          | 1.2e+03        |
| time/                   |                |
|    total_timesteps      | 152000         |
| train/                  |                |
|    approx_kl            | -5.2386895e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.42e-05      |
|    explained_variance   | 0.132          |
|    learning_rate        | 0.001          |
|    loss                 | 1.31e+04       |
|    n_updates            | 1690           |
|    policy_gradient_loss | -1.7e-06       |
|    value_loss           | 5.8e+04        |
--------------------------------------------
Eval num_timesteps=152500, episode_reward=1074.01 +/- 524.25
Episode length: 34.48 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 149      |
|    time_elapsed    | 535      |
|    total_timesteps | 152576   |
---------------------------------
Eval num_timesteps=153000, episode_reward=1325.79 +/- 704.32
Episode length: 35.68 +/- 6.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 1.33e+03      |
| time/                   |               |
|    total_timesteps      | 153000        |
| train/                  |               |
|    approx_kl            | 5.9371814e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00024      |
|    explained_variance   | -0.0689       |
|    learning_rate        | 0.001         |
|    loss                 | 1.71e+04      |
|    n_updates            | 1700          |
|    policy_gradient_loss | -9.75e-06     |
|    value_loss           | 5.26e+04      |
-------------------------------------------
Eval num_timesteps=153500, episode_reward=1091.87 +/- 543.43
Episode length: 34.40 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 150      |
|    time_elapsed    | 538      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=1170.60 +/- 652.08
Episode length: 34.04 +/- 7.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34            |
|    mean_reward          | 1.17e+03      |
| time/                   |               |
|    total_timesteps      | 154000        |
| train/                  |               |
|    approx_kl            | -2.386514e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000172     |
|    explained_variance   | 0.342         |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+04      |
|    n_updates            | 1710          |
|    policy_gradient_loss | -5.34e-06     |
|    value_loss           | 4.19e+04      |
-------------------------------------------
Eval num_timesteps=154500, episode_reward=1378.35 +/- 691.50
Episode length: 36.66 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 151      |
|    time_elapsed    | 542      |
|    total_timesteps | 154624   |
---------------------------------
Eval num_timesteps=155000, episode_reward=1269.64 +/- 623.99
Episode length: 36.40 +/- 5.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 1.27e+03      |
| time/                   |               |
|    total_timesteps      | 155000        |
| train/                  |               |
|    approx_kl            | 1.6880222e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000143     |
|    explained_variance   | 0.293         |
|    learning_rate        | 0.001         |
|    loss                 | 1.66e+04      |
|    n_updates            | 1720          |
|    policy_gradient_loss | -6.14e-06     |
|    value_loss           | 4.73e+04      |
-------------------------------------------
Eval num_timesteps=155500, episode_reward=1228.72 +/- 646.35
Episode length: 35.14 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 152      |
|    time_elapsed    | 545      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=1192.05 +/- 599.43
Episode length: 35.60 +/- 6.10
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.6           |
|    mean_reward          | 1.19e+03       |
| time/                   |                |
|    total_timesteps      | 156000         |
| train/                  |                |
|    approx_kl            | -1.2223609e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -8.45e-05      |
|    explained_variance   | 0.29           |
|    learning_rate        | 0.001          |
|    loss                 | 1.88e+04       |
|    n_updates            | 1730           |
|    policy_gradient_loss | -2.97e-06      |
|    value_loss           | 5.37e+04       |
--------------------------------------------
Eval num_timesteps=156500, episode_reward=1018.23 +/- 585.85
Episode length: 32.42 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.4     |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 153      |
|    time_elapsed    | 549      |
|    total_timesteps | 156672   |
---------------------------------
Eval num_timesteps=157000, episode_reward=1276.19 +/- 678.92
Episode length: 35.20 +/- 6.70
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.2           |
|    mean_reward          | 1.28e+03       |
| time/                   |                |
|    total_timesteps      | 157000         |
| train/                  |                |
|    approx_kl            | -3.4924597e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -8.11e-05      |
|    explained_variance   | 0.202          |
|    learning_rate        | 0.001          |
|    loss                 | 1.76e+04       |
|    n_updates            | 1740           |
|    policy_gradient_loss | -2.33e-06      |
|    value_loss           | 6.08e+04       |
--------------------------------------------
Eval num_timesteps=157500, episode_reward=1167.93 +/- 614.31
Episode length: 34.94 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 154      |
|    time_elapsed    | 552      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=1218.36 +/- 619.40
Episode length: 35.62 +/- 6.36
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.6           |
|    mean_reward          | 1.22e+03       |
| time/                   |                |
|    total_timesteps      | 158000         |
| train/                  |                |
|    approx_kl            | -6.6356733e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.87e-05      |
|    explained_variance   | 0.228          |
|    learning_rate        | 0.001          |
|    loss                 | 2.95e+04       |
|    n_updates            | 1750           |
|    policy_gradient_loss | -2.11e-06      |
|    value_loss           | 5.75e+04       |
--------------------------------------------
Eval num_timesteps=158500, episode_reward=1107.52 +/- 577.83
Episode length: 34.18 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 155      |
|    time_elapsed    | 556      |
|    total_timesteps | 158720   |
---------------------------------
Eval num_timesteps=159000, episode_reward=1203.37 +/- 627.64
Episode length: 34.88 +/- 6.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 1.2e+03       |
| time/                   |               |
|    total_timesteps      | 159000        |
| train/                  |               |
|    approx_kl            | -8.731149e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.79e-05     |
|    explained_variance   | 0.349         |
|    learning_rate        | 0.001         |
|    loss                 | 1.54e+04      |
|    n_updates            | 1760          |
|    policy_gradient_loss | -1.18e-06     |
|    value_loss           | 5.59e+04      |
-------------------------------------------
Eval num_timesteps=159500, episode_reward=1227.31 +/- 611.83
Episode length: 36.30 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 156      |
|    time_elapsed    | 559      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=1272.62 +/- 679.18
Episode length: 35.08 +/- 6.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.1        |
|    mean_reward          | 1.27e+03    |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 8.20728e-09 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000251   |
|    explained_variance   | 0.291       |
|    learning_rate        | 0.001       |
|    loss                 | 1.3e+04     |
|    n_updates            | 1770        |
|    policy_gradient_loss | -6.27e-06   |
|    value_loss           | 5.61e+04    |
-----------------------------------------
Eval num_timesteps=160500, episode_reward=1188.52 +/- 563.42
Episode length: 35.92 +/- 5.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 157      |
|    time_elapsed    | 563      |
|    total_timesteps | 160768   |
---------------------------------
Eval num_timesteps=161000, episode_reward=1179.21 +/- 643.32
Episode length: 34.54 +/- 6.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 1.18e+03      |
| time/                   |               |
|    total_timesteps      | 161000        |
| train/                  |               |
|    approx_kl            | 2.8521754e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000154     |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.001         |
|    loss                 | 1.07e+04      |
|    n_updates            | 1780          |
|    policy_gradient_loss | -4.23e-06     |
|    value_loss           | 5.08e+04      |
-------------------------------------------
Eval num_timesteps=161500, episode_reward=1281.20 +/- 639.95
Episode length: 36.32 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 158      |
|    time_elapsed    | 566      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=1176.54 +/- 644.66
Episode length: 34.44 +/- 6.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 1.18e+03      |
| time/                   |               |
|    total_timesteps      | 162000        |
| train/                  |               |
|    approx_kl            | 1.1641532e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000121     |
|    explained_variance   | 0.00194       |
|    learning_rate        | 0.001         |
|    loss                 | 1.16e+04      |
|    n_updates            | 1790          |
|    policy_gradient_loss | -3.74e-06     |
|    value_loss           | 5.44e+04      |
-------------------------------------------
Eval num_timesteps=162500, episode_reward=1138.23 +/- 564.41
Episode length: 35.04 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 159      |
|    time_elapsed    | 570      |
|    total_timesteps | 162816   |
---------------------------------
Eval num_timesteps=163000, episode_reward=1339.59 +/- 698.11
Episode length: 36.44 +/- 7.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 1.34e+03      |
| time/                   |               |
|    total_timesteps      | 163000        |
| train/                  |               |
|    approx_kl            | 1.5716068e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000116     |
|    explained_variance   | 0.429         |
|    learning_rate        | 0.001         |
|    loss                 | 1.58e+04      |
|    n_updates            | 1800          |
|    policy_gradient_loss | -2.14e-06     |
|    value_loss           | 4.26e+04      |
-------------------------------------------
Eval num_timesteps=163500, episode_reward=1266.82 +/- 687.30
Episode length: 35.08 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 160      |
|    time_elapsed    | 573      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=1189.13 +/- 640.82
Episode length: 34.94 +/- 7.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 1.19e+03      |
| time/                   |               |
|    total_timesteps      | 164000        |
| train/                  |               |
|    approx_kl            | 6.4028427e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000125     |
|    explained_variance   | 0.302         |
|    learning_rate        | 0.001         |
|    loss                 | 1.76e+04      |
|    n_updates            | 1810          |
|    policy_gradient_loss | -2.35e-06     |
|    value_loss           | 5.34e+04      |
-------------------------------------------
Eval num_timesteps=164500, episode_reward=1248.03 +/- 660.72
Episode length: 35.14 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 161      |
|    time_elapsed    | 577      |
|    total_timesteps | 164864   |
---------------------------------
Eval num_timesteps=165000, episode_reward=1184.86 +/- 635.94
Episode length: 34.42 +/- 6.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 1.18e+03     |
| time/                   |              |
|    total_timesteps      | 165000       |
| train/                  |              |
|    approx_kl            | 1.344597e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000435    |
|    explained_variance   | 0.0442       |
|    learning_rate        | 0.001        |
|    loss                 | 1.71e+04     |
|    n_updates            | 1820         |
|    policy_gradient_loss | -1.2e-05     |
|    value_loss           | 5.56e+04     |
------------------------------------------
Eval num_timesteps=165500, episode_reward=1258.60 +/- 666.25
Episode length: 35.74 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 162      |
|    time_elapsed    | 580      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=1272.07 +/- 646.74
Episode length: 35.92 +/- 5.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 1.27e+03      |
| time/                   |               |
|    total_timesteps      | 166000        |
| train/                  |               |
|    approx_kl            | 2.3108441e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000296     |
|    explained_variance   | 0.198         |
|    learning_rate        | 0.001         |
|    loss                 | 1.7e+04       |
|    n_updates            | 1830          |
|    policy_gradient_loss | -2.24e-05     |
|    value_loss           | 6.22e+04      |
-------------------------------------------
Eval num_timesteps=166500, episode_reward=1191.41 +/- 601.15
Episode length: 35.56 +/- 5.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 163      |
|    time_elapsed    | 584      |
|    total_timesteps | 166912   |
---------------------------------
Eval num_timesteps=167000, episode_reward=1205.79 +/- 670.61
Episode length: 34.62 +/- 7.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 1.21e+03      |
| time/                   |               |
|    total_timesteps      | 167000        |
| train/                  |               |
|    approx_kl            | 1.2805685e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00023      |
|    explained_variance   | 0.243         |
|    learning_rate        | 0.001         |
|    loss                 | 1.82e+04      |
|    n_updates            | 1840          |
|    policy_gradient_loss | -7.2e-06      |
|    value_loss           | 5.55e+04      |
-------------------------------------------
Eval num_timesteps=167500, episode_reward=1076.39 +/- 560.82
Episode length: 33.98 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 164      |
|    time_elapsed    | 587      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=1215.64 +/- 624.10
Episode length: 35.86 +/- 6.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 1.22e+03      |
| time/                   |               |
|    total_timesteps      | 168000        |
| train/                  |               |
|    approx_kl            | 2.9685907e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00028      |
|    explained_variance   | 0.286         |
|    learning_rate        | 0.001         |
|    loss                 | 1.37e+04      |
|    n_updates            | 1850          |
|    policy_gradient_loss | -1.06e-05     |
|    value_loss           | 4.58e+04      |
-------------------------------------------
Eval num_timesteps=168500, episode_reward=1205.59 +/- 604.62
Episode length: 35.70 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 165      |
|    time_elapsed    | 591      |
|    total_timesteps | 168960   |
---------------------------------
Eval num_timesteps=169000, episode_reward=1261.47 +/- 687.80
Episode length: 34.74 +/- 7.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 1.26e+03     |
| time/                   |              |
|    total_timesteps      | 169000       |
| train/                  |              |
|    approx_kl            | 8.032657e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000287    |
|    explained_variance   | 0.18         |
|    learning_rate        | 0.001        |
|    loss                 | 1.71e+04     |
|    n_updates            | 1860         |
|    policy_gradient_loss | -8.51e-06    |
|    value_loss           | 5.05e+04     |
------------------------------------------
Eval num_timesteps=169500, episode_reward=1221.67 +/- 615.54
Episode length: 35.74 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 166      |
|    time_elapsed    | 595      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=1193.23 +/- 629.56
Episode length: 34.98 +/- 6.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 1.19e+03     |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 9.895302e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000489    |
|    explained_variance   | 0.316        |
|    learning_rate        | 0.001        |
|    loss                 | 1.48e+04     |
|    n_updates            | 1870         |
|    policy_gradient_loss | -9.84e-06    |
|    value_loss           | 5.4e+04      |
------------------------------------------
Eval num_timesteps=170500, episode_reward=1411.72 +/- 696.33
Episode length: 36.42 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=1375.77 +/- 693.09
Episode length: 36.04 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 167      |
|    time_elapsed    | 599      |
|    total_timesteps | 171008   |
---------------------------------
Eval num_timesteps=171500, episode_reward=1200.56 +/- 594.58
Episode length: 35.58 +/- 5.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 1.2e+03       |
| time/                   |               |
|    total_timesteps      | 171500        |
| train/                  |               |
|    approx_kl            | 1.0477379e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000331     |
|    explained_variance   | 0.404         |
|    learning_rate        | 0.001         |
|    loss                 | 1.05e+04      |
|    n_updates            | 1880          |
|    policy_gradient_loss | -5.28e-06     |
|    value_loss           | 4.01e+04      |
-------------------------------------------
Eval num_timesteps=172000, episode_reward=1006.52 +/- 506.32
Episode length: 33.52 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 168      |
|    time_elapsed    | 603      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=1217.21 +/- 620.61
Episode length: 35.56 +/- 6.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 1.22e+03      |
| time/                   |               |
|    total_timesteps      | 172500        |
| train/                  |               |
|    approx_kl            | 1.7811544e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000625     |
|    explained_variance   | 0.215         |
|    learning_rate        | 0.001         |
|    loss                 | 1.04e+04      |
|    n_updates            | 1890          |
|    policy_gradient_loss | -1.49e-05     |
|    value_loss           | 4.8e+04       |
-------------------------------------------
Eval num_timesteps=173000, episode_reward=1139.24 +/- 676.68
Episode length: 33.60 +/- 7.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 169      |
|    time_elapsed    | 606      |
|    total_timesteps | 173056   |
---------------------------------
Eval num_timesteps=173500, episode_reward=1276.17 +/- 685.72
Episode length: 35.48 +/- 7.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.5         |
|    mean_reward          | 1.28e+03     |
| time/                   |              |
|    total_timesteps      | 173500       |
| train/                  |              |
|    approx_kl            | 3.958121e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000422    |
|    explained_variance   | 0.277        |
|    learning_rate        | 0.001        |
|    loss                 | 1.7e+04      |
|    n_updates            | 1900         |
|    policy_gradient_loss | -1.04e-05    |
|    value_loss           | 5.6e+04      |
------------------------------------------
Eval num_timesteps=174000, episode_reward=1304.45 +/- 657.46
Episode length: 36.52 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 170      |
|    time_elapsed    | 610      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=1278.99 +/- 691.09
Episode length: 35.58 +/- 7.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 1.28e+03      |
| time/                   |               |
|    total_timesteps      | 174500        |
| train/                  |               |
|    approx_kl            | 3.8999133e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000348     |
|    explained_variance   | 0.214         |
|    learning_rate        | 0.001         |
|    loss                 | 1.46e+04      |
|    n_updates            | 1910          |
|    policy_gradient_loss | -1.06e-05     |
|    value_loss           | 5.67e+04      |
-------------------------------------------
Eval num_timesteps=175000, episode_reward=1400.22 +/- 702.97
Episode length: 36.14 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 171      |
|    time_elapsed    | 614      |
|    total_timesteps | 175104   |
---------------------------------
Eval num_timesteps=175500, episode_reward=1337.36 +/- 701.99
Episode length: 36.12 +/- 7.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 1.34e+03     |
| time/                   |              |
|    total_timesteps      | 175500       |
| train/                  |              |
|    approx_kl            | 1.169974e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000464    |
|    explained_variance   | 0.212        |
|    learning_rate        | 0.001        |
|    loss                 | 1.66e+04     |
|    n_updates            | 1920         |
|    policy_gradient_loss | -1.11e-05    |
|    value_loss           | 6.14e+04     |
------------------------------------------
Eval num_timesteps=176000, episode_reward=1248.42 +/- 645.59
Episode length: 35.60 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 172      |
|    time_elapsed    | 617      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=1359.61 +/- 749.12
Episode length: 35.22 +/- 8.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 1.36e+03      |
| time/                   |               |
|    total_timesteps      | 176500        |
| train/                  |               |
|    approx_kl            | 2.0430889e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000642     |
|    explained_variance   | 0.172         |
|    learning_rate        | 0.001         |
|    loss                 | 1.34e+04      |
|    n_updates            | 1930          |
|    policy_gradient_loss | -1.49e-05     |
|    value_loss           | 5.33e+04      |
-------------------------------------------
Eval num_timesteps=177000, episode_reward=1220.54 +/- 545.59
Episode length: 36.62 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 173      |
|    time_elapsed    | 621      |
|    total_timesteps | 177152   |
---------------------------------
Eval num_timesteps=177500, episode_reward=1362.35 +/- 709.88
Episode length: 35.92 +/- 6.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 1.36e+03     |
| time/                   |              |
|    total_timesteps      | 177500       |
| train/                  |              |
|    approx_kl            | 5.806796e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000864    |
|    explained_variance   | 0.15         |
|    learning_rate        | 0.001        |
|    loss                 | 1.43e+04     |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.000102    |
|    value_loss           | 4.96e+04     |
------------------------------------------
Eval num_timesteps=178000, episode_reward=1340.68 +/- 698.30
Episode length: 36.00 +/- 6.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 174      |
|    time_elapsed    | 624      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=1300.82 +/- 630.91
Episode length: 36.72 +/- 5.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.7          |
|    mean_reward          | 1.3e+03       |
| time/                   |               |
|    total_timesteps      | 178500        |
| train/                  |               |
|    approx_kl            | 1.2503006e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000275     |
|    explained_variance   | -0.0544       |
|    learning_rate        | 0.001         |
|    loss                 | 1.66e+04      |
|    n_updates            | 1950          |
|    policy_gradient_loss | -4.03e-05     |
|    value_loss           | 6.95e+04      |
-------------------------------------------
Eval num_timesteps=179000, episode_reward=1127.27 +/- 641.33
Episode length: 33.48 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 175      |
|    time_elapsed    | 628      |
|    total_timesteps | 179200   |
---------------------------------
Eval num_timesteps=179500, episode_reward=1237.09 +/- 637.78
Episode length: 35.50 +/- 6.56
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 1.24e+03      |
| time/                   |               |
|    total_timesteps      | 179500        |
| train/                  |               |
|    approx_kl            | -4.656613e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000154     |
|    explained_variance   | 0.215         |
|    learning_rate        | 0.001         |
|    loss                 | 2.46e+04      |
|    n_updates            | 1960          |
|    policy_gradient_loss | -5.15e-06     |
|    value_loss           | 6.43e+04      |
-------------------------------------------
Eval num_timesteps=180000, episode_reward=1238.02 +/- 671.94
Episode length: 34.64 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 176      |
|    time_elapsed    | 631      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=1303.36 +/- 689.43
Episode length: 35.64 +/- 6.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 1.3e+03      |
| time/                   |              |
|    total_timesteps      | 180500       |
| train/                  |              |
|    approx_kl            | 5.005859e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000254    |
|    explained_variance   | 0.258        |
|    learning_rate        | 0.001        |
|    loss                 | 2.24e+04     |
|    n_updates            | 1970         |
|    policy_gradient_loss | -5.41e-06    |
|    value_loss           | 6.25e+04     |
------------------------------------------
Eval num_timesteps=181000, episode_reward=1290.49 +/- 697.23
Episode length: 35.56 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 177      |
|    time_elapsed    | 635      |
|    total_timesteps | 181248   |
---------------------------------
Eval num_timesteps=181500, episode_reward=1356.58 +/- 711.62
Episode length: 35.78 +/- 6.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 1.36e+03      |
| time/                   |               |
|    total_timesteps      | 181500        |
| train/                  |               |
|    approx_kl            | 2.5785994e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000789     |
|    explained_variance   | 0.284         |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+04      |
|    n_updates            | 1980          |
|    policy_gradient_loss | -1.47e-05     |
|    value_loss           | 5.15e+04      |
-------------------------------------------
Eval num_timesteps=182000, episode_reward=1239.88 +/- 640.24
Episode length: 35.58 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 178      |
|    time_elapsed    | 638      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=1164.22 +/- 552.41
Episode length: 35.62 +/- 5.86
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 1.16e+03     |
| time/                   |              |
|    total_timesteps      | 182500       |
| train/                  |              |
|    approx_kl            | 8.090865e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000433    |
|    explained_variance   | 0.284        |
|    learning_rate        | 0.001        |
|    loss                 | 1.53e+04     |
|    n_updates            | 1990         |
|    policy_gradient_loss | -1.69e-05    |
|    value_loss           | 5.37e+04     |
------------------------------------------
Eval num_timesteps=183000, episode_reward=1139.12 +/- 556.83
Episode length: 34.92 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 179      |
|    time_elapsed    | 642      |
|    total_timesteps | 183296   |
---------------------------------
Eval num_timesteps=183500, episode_reward=1327.59 +/- 643.06
Episode length: 37.04 +/- 5.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37            |
|    mean_reward          | 1.33e+03      |
| time/                   |               |
|    total_timesteps      | 183500        |
| train/                  |               |
|    approx_kl            | 2.4330802e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000832     |
|    explained_variance   | 0.159         |
|    learning_rate        | 0.001         |
|    loss                 | 1.46e+04      |
|    n_updates            | 2000          |
|    policy_gradient_loss | -1.41e-05     |
|    value_loss           | 6.52e+04      |
-------------------------------------------
Eval num_timesteps=184000, episode_reward=1121.16 +/- 610.38
Episode length: 33.92 +/- 6.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 180      |
|    time_elapsed    | 646      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=1284.39 +/- 606.62
Episode length: 36.80 +/- 5.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.8          |
|    mean_reward          | 1.28e+03      |
| time/                   |               |
|    total_timesteps      | 184500        |
| train/                  |               |
|    approx_kl            | 1.3562385e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000951     |
|    explained_variance   | 0.303         |
|    learning_rate        | 0.001         |
|    loss                 | 1.53e+04      |
|    n_updates            | 2010          |
|    policy_gradient_loss | -2.75e-05     |
|    value_loss           | 5.05e+04      |
-------------------------------------------
Eval num_timesteps=185000, episode_reward=1186.80 +/- 607.18
Episode length: 35.48 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 181      |
|    time_elapsed    | 649      |
|    total_timesteps | 185344   |
---------------------------------
Eval num_timesteps=185500, episode_reward=1220.47 +/- 649.89
Episode length: 34.88 +/- 6.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 1.22e+03     |
| time/                   |              |
|    total_timesteps      | 185500       |
| train/                  |              |
|    approx_kl            | 7.625204e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000722    |
|    explained_variance   | 0.264        |
|    learning_rate        | 0.001        |
|    loss                 | 2.27e+04     |
|    n_updates            | 2020         |
|    policy_gradient_loss | -1.65e-05    |
|    value_loss           | 6.59e+04     |
------------------------------------------
Eval num_timesteps=186000, episode_reward=1241.82 +/- 647.14
Episode length: 35.38 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 182      |
|    time_elapsed    | 653      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=1195.72 +/- 633.02
Episode length: 35.16 +/- 6.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 1.2e+03      |
| time/                   |              |
|    total_timesteps      | 186500       |
| train/                  |              |
|    approx_kl            | 3.213063e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000948    |
|    explained_variance   | 0.272        |
|    learning_rate        | 0.001        |
|    loss                 | 7.92e+03     |
|    n_updates            | 2030         |
|    policy_gradient_loss | -1.61e-05    |
|    value_loss           | 5.47e+04     |
------------------------------------------
Eval num_timesteps=187000, episode_reward=1119.37 +/- 570.34
Episode length: 34.68 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 183      |
|    time_elapsed    | 656      |
|    total_timesteps | 187392   |
---------------------------------
Eval num_timesteps=187500, episode_reward=1230.03 +/- 645.49
Episode length: 35.42 +/- 6.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 1.23e+03     |
| time/                   |              |
|    total_timesteps      | 187500       |
| train/                  |              |
|    approx_kl            | 6.170012e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00061     |
|    explained_variance   | 0.22         |
|    learning_rate        | 0.001        |
|    loss                 | 1.63e+04     |
|    n_updates            | 2040         |
|    policy_gradient_loss | -1.48e-05    |
|    value_loss           | 5.74e+04     |
------------------------------------------
Eval num_timesteps=188000, episode_reward=1152.35 +/- 631.48
Episode length: 34.40 +/- 7.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 184      |
|    time_elapsed    | 660      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=1144.10 +/- 601.39
Episode length: 34.76 +/- 6.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 1.14e+03      |
| time/                   |               |
|    total_timesteps      | 188500        |
| train/                  |               |
|    approx_kl            | 2.4505425e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00123      |
|    explained_variance   | 0.0676        |
|    learning_rate        | 0.001         |
|    loss                 | 1.89e+04      |
|    n_updates            | 2050          |
|    policy_gradient_loss | -2.93e-05     |
|    value_loss           | 6.11e+04      |
-------------------------------------------
Eval num_timesteps=189000, episode_reward=1258.14 +/- 660.95
Episode length: 35.36 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 185      |
|    time_elapsed    | 663      |
|    total_timesteps | 189440   |
---------------------------------
Eval num_timesteps=189500, episode_reward=1161.17 +/- 642.97
Episode length: 33.86 +/- 6.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.9          |
|    mean_reward          | 1.16e+03      |
| time/                   |               |
|    total_timesteps      | 189500        |
| train/                  |               |
|    approx_kl            | 2.1769665e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0012       |
|    explained_variance   | 0.151         |
|    learning_rate        | 0.001         |
|    loss                 | 2.43e+04      |
|    n_updates            | 2060          |
|    policy_gradient_loss | -4.09e-05     |
|    value_loss           | 5.85e+04      |
-------------------------------------------
Eval num_timesteps=190000, episode_reward=1318.74 +/- 650.69
Episode length: 36.24 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 186      |
|    time_elapsed    | 667      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=1122.56 +/- 576.46
Episode length: 34.42 +/- 6.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 1.12e+03     |
| time/                   |              |
|    total_timesteps      | 190500       |
| train/                  |              |
|    approx_kl            | 1.344597e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000943    |
|    explained_variance   | 0.297        |
|    learning_rate        | 0.001        |
|    loss                 | 2.41e+04     |
|    n_updates            | 2070         |
|    policy_gradient_loss | -2.05e-05    |
|    value_loss           | 6.7e+04      |
------------------------------------------
Eval num_timesteps=191000, episode_reward=1273.32 +/- 681.61
Episode length: 35.14 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 187      |
|    time_elapsed    | 670      |
|    total_timesteps | 191488   |
---------------------------------
Eval num_timesteps=191500, episode_reward=1174.12 +/- 649.45
Episode length: 34.46 +/- 7.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 1.17e+03      |
| time/                   |               |
|    total_timesteps      | 191500        |
| train/                  |               |
|    approx_kl            | 3.8184226e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00163      |
|    explained_variance   | 0.272         |
|    learning_rate        | 0.001         |
|    loss                 | 1.7e+04       |
|    n_updates            | 2080          |
|    policy_gradient_loss | -6.89e-05     |
|    value_loss           | 6.45e+04      |
-------------------------------------------
Eval num_timesteps=192000, episode_reward=1330.96 +/- 703.58
Episode length: 35.70 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=1294.82 +/- 663.06
Episode length: 35.44 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 188      |
|    time_elapsed    | 675      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=1168.61 +/- 573.12
Episode length: 35.34 +/- 5.51
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.3           |
|    mean_reward          | 1.17e+03       |
| time/                   |                |
|    total_timesteps      | 193000         |
| train/                  |                |
|    approx_kl            | 1.48429535e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00123       |
|    explained_variance   | 0.357          |
|    learning_rate        | 0.001          |
|    loss                 | 1.59e+04       |
|    n_updates            | 2090           |
|    policy_gradient_loss | -4.07e-05      |
|    value_loss           | 5.26e+04       |
--------------------------------------------
Eval num_timesteps=193500, episode_reward=1310.21 +/- 654.25
Episode length: 36.40 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 189      |
|    time_elapsed    | 678      |
|    total_timesteps | 193536   |
---------------------------------
Eval num_timesteps=194000, episode_reward=1314.87 +/- 679.06
Episode length: 35.86 +/- 6.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 1.31e+03      |
| time/                   |               |
|    total_timesteps      | 194000        |
| train/                  |               |
|    approx_kl            | 0.00069450313 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00103      |
|    explained_variance   | 0.276         |
|    learning_rate        | 0.001         |
|    loss                 | 1.14e+04      |
|    n_updates            | 2100          |
|    policy_gradient_loss | -6.84e-05     |
|    value_loss           | 5.05e+04      |
-------------------------------------------
Eval num_timesteps=194500, episode_reward=1276.08 +/- 681.12
Episode length: 35.32 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 190      |
|    time_elapsed    | 682      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=1155.17 +/- 618.94
Episode length: 34.40 +/- 6.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 1.16e+03      |
| time/                   |               |
|    total_timesteps      | 195000        |
| train/                  |               |
|    approx_kl            | 6.7520887e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000488     |
|    explained_variance   | 0.206         |
|    learning_rate        | 0.001         |
|    loss                 | 2.17e+04      |
|    n_updates            | 2110          |
|    policy_gradient_loss | -1.12e-05     |
|    value_loss           | 5.77e+04      |
-------------------------------------------
Eval num_timesteps=195500, episode_reward=1271.26 +/- 617.39
Episode length: 36.34 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 191      |
|    time_elapsed    | 685      |
|    total_timesteps | 195584   |
---------------------------------
Eval num_timesteps=196000, episode_reward=1243.02 +/- 667.36
Episode length: 35.24 +/- 7.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 1.24e+03      |
| time/                   |               |
|    total_timesteps      | 196000        |
| train/                  |               |
|    approx_kl            | 0.00056058314 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000155     |
|    explained_variance   | 0.203         |
|    learning_rate        | 0.001         |
|    loss                 | 1.85e+04      |
|    n_updates            | 2120          |
|    policy_gradient_loss | -0.000126     |
|    value_loss           | 7.4e+04       |
-------------------------------------------
Eval num_timesteps=196500, episode_reward=1218.72 +/- 678.26
Episode length: 34.28 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 192      |
|    time_elapsed    | 689      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=1143.45 +/- 626.78
Episode length: 33.92 +/- 6.65
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.9           |
|    mean_reward          | 1.14e+03       |
| time/                   |                |
|    total_timesteps      | 197000         |
| train/                  |                |
|    approx_kl            | -3.3178367e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000127      |
|    explained_variance   | 0.291          |
|    learning_rate        | 0.001          |
|    loss                 | 2.15e+04       |
|    n_updates            | 2130           |
|    policy_gradient_loss | -2.36e-06      |
|    value_loss           | 5.56e+04       |
--------------------------------------------
Eval num_timesteps=197500, episode_reward=1291.60 +/- 664.24
Episode length: 35.74 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 193      |
|    time_elapsed    | 692      |
|    total_timesteps | 197632   |
---------------------------------
Eval num_timesteps=198000, episode_reward=1221.12 +/- 615.17
Episode length: 35.46 +/- 6.13
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.5           |
|    mean_reward          | 1.22e+03       |
| time/                   |                |
|    total_timesteps      | 198000         |
| train/                  |                |
|    approx_kl            | -1.7462298e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.1e-05       |
|    explained_variance   | 0.369          |
|    learning_rate        | 0.001          |
|    loss                 | 2.16e+04       |
|    n_updates            | 2140           |
|    policy_gradient_loss | -1.27e-06      |
|    value_loss           | 5.93e+04       |
--------------------------------------------
Eval num_timesteps=198500, episode_reward=1238.12 +/- 629.15
Episode length: 35.68 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 194      |
|    time_elapsed    | 696      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=1392.87 +/- 648.10
Episode length: 37.18 +/- 4.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.2         |
|    mean_reward          | 1.39e+03     |
| time/                   |              |
|    total_timesteps      | 199000       |
| train/                  |              |
|    approx_kl            | 5.820766e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000177    |
|    explained_variance   | 0.185        |
|    learning_rate        | 0.001        |
|    loss                 | 1.68e+04     |
|    n_updates            | 2150         |
|    policy_gradient_loss | -1.86e-06    |
|    value_loss           | 5.9e+04      |
------------------------------------------
Eval num_timesteps=199500, episode_reward=1257.07 +/- 656.76
Episode length: 35.88 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 195      |
|    time_elapsed    | 700      |
|    total_timesteps | 199680   |
---------------------------------
Eval num_timesteps=200000, episode_reward=1239.52 +/- 675.84
Episode length: 35.02 +/- 7.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 1.24e+03      |
| time/                   |               |
|    total_timesteps      | 200000        |
| train/                  |               |
|    approx_kl            | 1.6682316e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000208     |
|    explained_variance   | 0.25          |
|    learning_rate        | 0.001         |
|    loss                 | 1.45e+04      |
|    n_updates            | 2160          |
|    policy_gradient_loss | -2.32e-05     |
|    value_loss           | 5.84e+04      |
-------------------------------------------
Eval num_timesteps=200500, episode_reward=1290.21 +/- 667.77
Episode length: 35.76 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 196      |
|    time_elapsed    | 703      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=1309.31 +/- 648.06
Episode length: 36.16 +/- 5.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.2          |
|    mean_reward          | 1.31e+03      |
| time/                   |               |
|    total_timesteps      | 201000        |
| train/                  |               |
|    approx_kl            | 1.6298145e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000126     |
|    explained_variance   | 0.325         |
|    learning_rate        | 0.001         |
|    loss                 | 2.62e+04      |
|    n_updates            | 2170          |
|    policy_gradient_loss | -2.5e-06      |
|    value_loss           | 6.95e+04      |
-------------------------------------------
Eval num_timesteps=201500, episode_reward=1143.09 +/- 593.91
Episode length: 35.20 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 197      |
|    time_elapsed    | 707      |
|    total_timesteps | 201728   |
---------------------------------
Eval num_timesteps=202000, episode_reward=1360.35 +/- 650.36
Episode length: 37.02 +/- 5.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37            |
|    mean_reward          | 1.36e+03      |
| time/                   |               |
|    total_timesteps      | 202000        |
| train/                  |               |
|    approx_kl            | 8.3236955e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00034      |
|    explained_variance   | 0.242         |
|    learning_rate        | 0.001         |
|    loss                 | 2.11e+04      |
|    n_updates            | 2180          |
|    policy_gradient_loss | -1.83e-06     |
|    value_loss           | 5.91e+04      |
-------------------------------------------
Eval num_timesteps=202500, episode_reward=1399.74 +/- 705.69
Episode length: 36.38 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 198      |
|    time_elapsed    | 710      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=1353.63 +/- 681.13
Episode length: 36.04 +/- 6.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 1.35e+03     |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 2.561137e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000264    |
|    explained_variance   | 0.399        |
|    learning_rate        | 0.001        |
|    loss                 | 1.84e+04     |
|    n_updates            | 2190         |
|    policy_gradient_loss | -6e-06       |
|    value_loss           | 5.08e+04     |
------------------------------------------
Eval num_timesteps=203500, episode_reward=1282.91 +/- 640.85
Episode length: 35.86 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 199      |
|    time_elapsed    | 714      |
|    total_timesteps | 203776   |
---------------------------------
Eval num_timesteps=204000, episode_reward=1112.49 +/- 573.36
Episode length: 34.44 +/- 6.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 1.11e+03      |
| time/                   |               |
|    total_timesteps      | 204000        |
| train/                  |               |
|    approx_kl            | 1.3387762e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000201     |
|    explained_variance   | 0.392         |
|    learning_rate        | 0.001         |
|    loss                 | 2.34e+04      |
|    n_updates            | 2200          |
|    policy_gradient_loss | -2.74e-06     |
|    value_loss           | 6.43e+04      |
-------------------------------------------
Eval num_timesteps=204500, episode_reward=1253.00 +/- 701.19
Episode length: 34.76 +/- 7.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 200      |
|    time_elapsed    | 717      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=1150.71 +/- 625.63
Episode length: 34.58 +/- 7.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 1.15e+03      |
| time/                   |               |
|    total_timesteps      | 205000        |
| train/                  |               |
|    approx_kl            | 2.8521754e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000176     |
|    explained_variance   | 0.102         |
|    learning_rate        | 0.001         |
|    loss                 | 2.48e+04      |
|    n_updates            | 2210          |
|    policy_gradient_loss | -5.57e-06     |
|    value_loss           | 7.13e+04      |
-------------------------------------------
Eval num_timesteps=205500, episode_reward=1329.08 +/- 674.03
Episode length: 36.24 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 201      |
|    time_elapsed    | 721      |
|    total_timesteps | 205824   |
---------------------------------
Eval num_timesteps=206000, episode_reward=1428.53 +/- 709.60
Episode length: 36.46 +/- 6.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.5          |
|    mean_reward          | 1.43e+03      |
| time/                   |               |
|    total_timesteps      | 206000        |
| train/                  |               |
|    approx_kl            | 7.3341653e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00032      |
|    explained_variance   | 0.429         |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+04      |
|    n_updates            | 2220          |
|    policy_gradient_loss | -6.96e-06     |
|    value_loss           | 6.03e+04      |
-------------------------------------------
Eval num_timesteps=206500, episode_reward=1160.04 +/- 612.68
Episode length: 34.52 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 202      |
|    time_elapsed    | 724      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=1175.08 +/- 566.16
Episode length: 35.72 +/- 5.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 1.18e+03      |
| time/                   |               |
|    total_timesteps      | 207000        |
| train/                  |               |
|    approx_kl            | 5.4133125e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000274     |
|    explained_variance   | 0.208         |
|    learning_rate        | 0.001         |
|    loss                 | 1.72e+04      |
|    n_updates            | 2230          |
|    policy_gradient_loss | -6.33e-06     |
|    value_loss           | 6.1e+04       |
-------------------------------------------
Eval num_timesteps=207500, episode_reward=1193.12 +/- 621.14
Episode length: 35.14 +/- 7.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 203      |
|    time_elapsed    | 728      |
|    total_timesteps | 207872   |
---------------------------------
Eval num_timesteps=208000, episode_reward=1187.72 +/- 638.86
Episode length: 34.66 +/- 6.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 1.19e+03      |
| time/                   |               |
|    total_timesteps      | 208000        |
| train/                  |               |
|    approx_kl            | 2.7765054e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00115      |
|    explained_variance   | -0.0379       |
|    learning_rate        | 0.001         |
|    loss                 | 1.59e+04      |
|    n_updates            | 2240          |
|    policy_gradient_loss | -2.27e-05     |
|    value_loss           | 5.34e+04      |
-------------------------------------------
Eval num_timesteps=208500, episode_reward=1229.39 +/- 579.28
Episode length: 36.76 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 204      |
|    time_elapsed    | 732      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=1288.24 +/- 680.84
Episode length: 35.56 +/- 7.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 1.29e+03      |
| time/                   |               |
|    total_timesteps      | 209000        |
| train/                  |               |
|    approx_kl            | 4.5011984e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000979     |
|    explained_variance   | 0.251         |
|    learning_rate        | 0.001         |
|    loss                 | 1.01e+04      |
|    n_updates            | 2250          |
|    policy_gradient_loss | -4.89e-05     |
|    value_loss           | 4.03e+04      |
-------------------------------------------
Eval num_timesteps=209500, episode_reward=1240.92 +/- 636.38
Episode length: 35.70 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 205      |
|    time_elapsed    | 735      |
|    total_timesteps | 209920   |
---------------------------------
Eval num_timesteps=210000, episode_reward=1128.77 +/- 564.30
Episode length: 35.18 +/- 5.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 1.13e+03     |
| time/                   |              |
|    total_timesteps      | 210000       |
| train/                  |              |
|    approx_kl            | 7.019844e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000704    |
|    explained_variance   | 0.267        |
|    learning_rate        | 0.001        |
|    loss                 | 2.15e+04     |
|    n_updates            | 2260         |
|    policy_gradient_loss | -2.48e-05    |
|    value_loss           | 5.67e+04     |
------------------------------------------
Eval num_timesteps=210500, episode_reward=1232.27 +/- 643.80
Episode length: 35.48 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 206      |
|    time_elapsed    | 739      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=1440.56 +/- 729.46
Episode length: 36.20 +/- 6.76
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.2          |
|    mean_reward          | 1.44e+03      |
| time/                   |               |
|    total_timesteps      | 211000        |
| train/                  |               |
|    approx_kl            | 1.1689845e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000618     |
|    explained_variance   | 0.183         |
|    learning_rate        | 0.001         |
|    loss                 | 1.73e+04      |
|    n_updates            | 2270          |
|    policy_gradient_loss | -4.3e-05      |
|    value_loss           | 5.63e+04      |
-------------------------------------------
Eval num_timesteps=211500, episode_reward=1363.88 +/- 678.80
Episode length: 36.90 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 207      |
|    time_elapsed    | 742      |
|    total_timesteps | 211968   |
---------------------------------
Eval num_timesteps=212000, episode_reward=1230.28 +/- 574.29
Episode length: 36.36 +/- 5.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 1.23e+03      |
| time/                   |               |
|    total_timesteps      | 212000        |
| train/                  |               |
|    approx_kl            | 2.2118911e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000634     |
|    explained_variance   | 0.204         |
|    learning_rate        | 0.001         |
|    loss                 | 2.17e+04      |
|    n_updates            | 2280          |
|    policy_gradient_loss | -1.79e-05     |
|    value_loss           | 5.34e+04      |
-------------------------------------------
Eval num_timesteps=212500, episode_reward=1276.09 +/- 681.95
Episode length: 35.26 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 208      |
|    time_elapsed    | 746      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=1367.22 +/- 672.57
Episode length: 36.56 +/- 6.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 1.37e+03      |
| time/                   |               |
|    total_timesteps      | 213000        |
| train/                  |               |
|    approx_kl            | 1.6472768e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000549     |
|    explained_variance   | 0.328         |
|    learning_rate        | 0.001         |
|    loss                 | 1.94e+04      |
|    n_updates            | 2290          |
|    policy_gradient_loss | -1.08e-05     |
|    value_loss           | 5.43e+04      |
-------------------------------------------
Eval num_timesteps=213500, episode_reward=1167.88 +/- 611.67
Episode length: 34.92 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=1377.73 +/- 723.42
Episode length: 35.86 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 209      |
|    time_elapsed    | 751      |
|    total_timesteps | 214016   |
---------------------------------
Eval num_timesteps=214500, episode_reward=1281.27 +/- 680.27
Episode length: 35.38 +/- 6.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 1.28e+03      |
| time/                   |               |
|    total_timesteps      | 214500        |
| train/                  |               |
|    approx_kl            | 2.8579962e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00119      |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.001         |
|    loss                 | 1.79e+04      |
|    n_updates            | 2300          |
|    policy_gradient_loss | -1.4e-05      |
|    value_loss           | 5.93e+04      |
-------------------------------------------
Eval num_timesteps=215000, episode_reward=1306.50 +/- 689.37
Episode length: 35.90 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 210      |
|    time_elapsed    | 754      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=1412.63 +/- 687.64
Episode length: 37.02 +/- 5.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37            |
|    mean_reward          | 1.41e+03      |
| time/                   |               |
|    total_timesteps      | 215500        |
| train/                  |               |
|    approx_kl            | 0.00085202185 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000533     |
|    explained_variance   | 0.299         |
|    learning_rate        | 0.001         |
|    loss                 | 1.83e+04      |
|    n_updates            | 2310          |
|    policy_gradient_loss | -0.00015      |
|    value_loss           | 5.74e+04      |
-------------------------------------------
Eval num_timesteps=216000, episode_reward=1251.05 +/- 635.45
Episode length: 35.94 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 211      |
|    time_elapsed    | 758      |
|    total_timesteps | 216064   |
---------------------------------
Eval num_timesteps=216500, episode_reward=1204.13 +/- 626.77
Episode length: 35.50 +/- 6.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.5        |
|    mean_reward          | 1.2e+03     |
| time/                   |             |
|    total_timesteps      | 216500      |
| train/                  |             |
|    approx_kl            | 1.36788e-08 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000749   |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.001       |
|    loss                 | 1.46e+04    |
|    n_updates            | 2320        |
|    policy_gradient_loss | -1.15e-05   |
|    value_loss           | 3.93e+04    |
-----------------------------------------
Eval num_timesteps=217000, episode_reward=1170.00 +/- 645.46
Episode length: 34.68 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 212      |
|    time_elapsed    | 761      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=1303.32 +/- 624.31
Episode length: 36.96 +/- 5.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37            |
|    mean_reward          | 1.3e+03       |
| time/                   |               |
|    total_timesteps      | 217500        |
| train/                  |               |
|    approx_kl            | 1.5133992e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0005       |
|    explained_variance   | 0.136         |
|    learning_rate        | 0.001         |
|    loss                 | 2.11e+04      |
|    n_updates            | 2330          |
|    policy_gradient_loss | -9.69e-06     |
|    value_loss           | 5.96e+04      |
-------------------------------------------
Eval num_timesteps=218000, episode_reward=1194.04 +/- 662.17
Episode length: 34.18 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 213      |
|    time_elapsed    | 765      |
|    total_timesteps | 218112   |
---------------------------------
Eval num_timesteps=218500, episode_reward=1260.58 +/- 656.89
Episode length: 35.36 +/- 6.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | 1.26e+03    |
| time/                   |             |
|    total_timesteps      | 218500      |
| train/                  |             |
|    approx_kl            | 8.20728e-09 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000348   |
|    explained_variance   | 0.0985      |
|    learning_rate        | 0.001       |
|    loss                 | 2.07e+04    |
|    n_updates            | 2340        |
|    policy_gradient_loss | -1.74e-05   |
|    value_loss           | 6.98e+04    |
-----------------------------------------
Eval num_timesteps=219000, episode_reward=1373.37 +/- 692.01
Episode length: 36.46 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 214      |
|    time_elapsed    | 769      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=1194.64 +/- 637.09
Episode length: 35.00 +/- 6.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 1.19e+03      |
| time/                   |               |
|    total_timesteps      | 219500        |
| train/                  |               |
|    approx_kl            | 1.3969839e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000681     |
|    explained_variance   | 0.286         |
|    learning_rate        | 0.001         |
|    loss                 | 1.16e+04      |
|    n_updates            | 2350          |
|    policy_gradient_loss | -1.02e-05     |
|    value_loss           | 4.72e+04      |
-------------------------------------------
Eval num_timesteps=220000, episode_reward=1159.90 +/- 579.16
Episode length: 35.40 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 284      |
|    iterations      | 215      |
|    time_elapsed    | 772      |
|    total_timesteps | 220160   |
---------------------------------
Eval num_timesteps=220500, episode_reward=1200.98 +/- 623.08
Episode length: 35.00 +/- 5.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 1.2e+03       |
| time/                   |               |
|    total_timesteps      | 220500        |
| train/                  |               |
|    approx_kl            | 7.9744495e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000427     |
|    explained_variance   | 0.17          |
|    learning_rate        | 0.001         |
|    loss                 | 1.78e+04      |
|    n_updates            | 2360          |
|    policy_gradient_loss | -1.65e-05     |
|    value_loss           | 4.63e+04      |
-------------------------------------------
Eval num_timesteps=221000, episode_reward=1272.16 +/- 680.81
Episode length: 35.04 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 216      |
|    time_elapsed    | 776      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=1051.58 +/- 525.07
Episode length: 33.84 +/- 6.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 1.05e+03      |
| time/                   |               |
|    total_timesteps      | 221500        |
| train/                  |               |
|    approx_kl            | 3.2014214e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000231     |
|    explained_variance   | 0.331         |
|    learning_rate        | 0.001         |
|    loss                 | 1.65e+04      |
|    n_updates            | 2370          |
|    policy_gradient_loss | -1.3e-05      |
|    value_loss           | 6.42e+04      |
-------------------------------------------
Eval num_timesteps=222000, episode_reward=1151.68 +/- 592.86
Episode length: 34.92 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 217      |
|    time_elapsed    | 779      |
|    total_timesteps | 222208   |
---------------------------------
Eval num_timesteps=222500, episode_reward=1232.94 +/- 642.27
Episode length: 35.58 +/- 6.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 1.23e+03      |
| time/                   |               |
|    total_timesteps      | 222500        |
| train/                  |               |
|    approx_kl            | 1.8510036e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000557     |
|    explained_variance   | 0.273         |
|    learning_rate        | 0.001         |
|    loss                 | 8.63e+03      |
|    n_updates            | 2380          |
|    policy_gradient_loss | -8.7e-06      |
|    value_loss           | 4.92e+04      |
-------------------------------------------
Eval num_timesteps=223000, episode_reward=1081.52 +/- 556.48
Episode length: 34.20 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 218      |
|    time_elapsed    | 783      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=1307.17 +/- 690.63
Episode length: 35.72 +/- 7.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 1.31e+03     |
| time/                   |              |
|    total_timesteps      | 223500       |
| train/                  |              |
|    approx_kl            | 1.717126e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000774    |
|    explained_variance   | 0.223        |
|    learning_rate        | 0.001        |
|    loss                 | 1.1e+04      |
|    n_updates            | 2390         |
|    policy_gradient_loss | -1.04e-05    |
|    value_loss           | 5.16e+04     |
------------------------------------------
Eval num_timesteps=224000, episode_reward=1132.22 +/- 634.32
Episode length: 33.76 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 219      |
|    time_elapsed    | 786      |
|    total_timesteps | 224256   |
---------------------------------
Eval num_timesteps=224500, episode_reward=1132.21 +/- 601.99
Episode length: 34.40 +/- 6.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 1.13e+03      |
| time/                   |               |
|    total_timesteps      | 224500        |
| train/                  |               |
|    approx_kl            | 4.0745363e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000472     |
|    explained_variance   | 0.211         |
|    learning_rate        | 0.001         |
|    loss                 | 1.26e+04      |
|    n_updates            | 2400          |
|    policy_gradient_loss | -1.17e-05     |
|    value_loss           | 5.89e+04      |
-------------------------------------------
Eval num_timesteps=225000, episode_reward=1288.82 +/- 669.42
Episode length: 35.80 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 220      |
|    time_elapsed    | 790      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=1120.43 +/- 643.74
Episode length: 33.14 +/- 7.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.1        |
|    mean_reward          | 1.12e+03    |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 7.21775e-09 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000335   |
|    explained_variance   | 0.0878      |
|    learning_rate        | 0.001       |
|    loss                 | 2.19e+04    |
|    n_updates            | 2410        |
|    policy_gradient_loss | -8.67e-06   |
|    value_loss           | 6.76e+04    |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=1291.85 +/- 672.38
Episode length: 36.08 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 221      |
|    time_elapsed    | 793      |
|    total_timesteps | 226304   |
---------------------------------
Eval num_timesteps=226500, episode_reward=1327.50 +/- 697.63
Episode length: 35.62 +/- 6.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 1.33e+03    |
| time/                   |             |
|    total_timesteps      | 226500      |
| train/                  |             |
|    approx_kl            | 9.95351e-09 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000454   |
|    explained_variance   | 0.0262      |
|    learning_rate        | 0.001       |
|    loss                 | 1.32e+04    |
|    n_updates            | 2420        |
|    policy_gradient_loss | -8.42e-06   |
|    value_loss           | 6.12e+04    |
-----------------------------------------
Eval num_timesteps=227000, episode_reward=1246.82 +/- 674.37
Episode length: 35.02 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 222      |
|    time_elapsed    | 797      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=1293.77 +/- 662.51
Episode length: 36.00 +/- 6.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 1.29e+03      |
| time/                   |               |
|    total_timesteps      | 227500        |
| train/                  |               |
|    approx_kl            | 7.3341653e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000519     |
|    explained_variance   | 0.304         |
|    learning_rate        | 0.001         |
|    loss                 | 2.18e+04      |
|    n_updates            | 2430          |
|    policy_gradient_loss | -8.99e-06     |
|    value_loss           | 5.74e+04      |
-------------------------------------------
Eval num_timesteps=228000, episode_reward=1380.80 +/- 664.79
Episode length: 37.40 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.4     |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 223      |
|    time_elapsed    | 800      |
|    total_timesteps | 228352   |
---------------------------------
Eval num_timesteps=228500, episode_reward=1180.18 +/- 656.20
Episode length: 34.68 +/- 7.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 1.18e+03     |
| time/                   |              |
|    total_timesteps      | 228500       |
| train/                  |              |
|    approx_kl            | 2.066372e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00103     |
|    explained_variance   | -0.0367      |
|    learning_rate        | 0.001        |
|    loss                 | 2.09e+04     |
|    n_updates            | 2440         |
|    policy_gradient_loss | -4.46e-06    |
|    value_loss           | 6.71e+04     |
------------------------------------------
Eval num_timesteps=229000, episode_reward=1371.06 +/- 702.38
Episode length: 36.12 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 224      |
|    time_elapsed    | 804      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=1384.13 +/- 715.56
Episode length: 35.90 +/- 6.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 1.38e+03      |
| time/                   |               |
|    total_timesteps      | 229500        |
| train/                  |               |
|    approx_kl            | 1.1641532e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000284     |
|    explained_variance   | 0.306         |
|    learning_rate        | 0.001         |
|    loss                 | 1.72e+04      |
|    n_updates            | 2450          |
|    policy_gradient_loss | -5.18e-06     |
|    value_loss           | 5.37e+04      |
-------------------------------------------
Eval num_timesteps=230000, episode_reward=1259.71 +/- 620.29
Episode length: 36.36 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 225      |
|    time_elapsed    | 808      |
|    total_timesteps | 230400   |
---------------------------------
Eval num_timesteps=230500, episode_reward=1072.04 +/- 553.26
Episode length: 34.10 +/- 6.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.1         |
|    mean_reward          | 1.07e+03     |
| time/                   |              |
|    total_timesteps      | 230500       |
| train/                  |              |
|    approx_kl            | 7.392373e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00039     |
|    explained_variance   | 0.225        |
|    learning_rate        | 0.001        |
|    loss                 | 3.05e+04     |
|    n_updates            | 2460         |
|    policy_gradient_loss | -6.84e-06    |
|    value_loss           | 5.88e+04     |
------------------------------------------
Eval num_timesteps=231000, episode_reward=1153.73 +/- 622.90
Episode length: 34.38 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 226      |
|    time_elapsed    | 811      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=1053.62 +/- 525.28
Episode length: 34.22 +/- 6.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 1.05e+03      |
| time/                   |               |
|    total_timesteps      | 231500        |
| train/                  |               |
|    approx_kl            | 7.3341653e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000406     |
|    explained_variance   | 0.302         |
|    learning_rate        | 0.001         |
|    loss                 | 2.03e+04      |
|    n_updates            | 2470          |
|    policy_gradient_loss | -1.01e-05     |
|    value_loss           | 5.41e+04      |
-------------------------------------------
Eval num_timesteps=232000, episode_reward=1295.00 +/- 696.97
Episode length: 35.44 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 227      |
|    time_elapsed    | 815      |
|    total_timesteps | 232448   |
---------------------------------
Eval num_timesteps=232500, episode_reward=1167.01 +/- 613.67
Episode length: 34.56 +/- 6.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 1.17e+03      |
| time/                   |               |
|    total_timesteps      | 232500        |
| train/                  |               |
|    approx_kl            | 2.2351742e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000905     |
|    explained_variance   | 0.285         |
|    learning_rate        | 0.001         |
|    loss                 | 1.91e+04      |
|    n_updates            | 2480          |
|    policy_gradient_loss | -1.08e-05     |
|    value_loss           | 5.74e+04      |
-------------------------------------------
Eval num_timesteps=233000, episode_reward=1231.77 +/- 646.90
Episode length: 35.34 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 228      |
|    time_elapsed    | 818      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=1112.70 +/- 573.57
Episode length: 34.40 +/- 6.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 1.11e+03     |
| time/                   |              |
|    total_timesteps      | 233500       |
| train/                  |              |
|    approx_kl            | 3.488385e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00174     |
|    explained_variance   | 0.302        |
|    learning_rate        | 0.001        |
|    loss                 | 1.02e+04     |
|    n_updates            | 2490         |
|    policy_gradient_loss | -8.89e-05    |
|    value_loss           | 3.93e+04     |
------------------------------------------
Eval num_timesteps=234000, episode_reward=1154.21 +/- 553.51
Episode length: 35.82 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 229      |
|    time_elapsed    | 822      |
|    total_timesteps | 234496   |
---------------------------------
Eval num_timesteps=234500, episode_reward=1348.82 +/- 617.56
Episode length: 37.72 +/- 4.98
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.7         |
|    mean_reward          | 1.35e+03     |
| time/                   |              |
|    total_timesteps      | 234500       |
| train/                  |              |
|    approx_kl            | 5.122274e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000511    |
|    explained_variance   | 0.159        |
|    learning_rate        | 0.001        |
|    loss                 | 1.95e+04     |
|    n_updates            | 2500         |
|    policy_gradient_loss | -1.13e-05    |
|    value_loss           | 5.99e+04     |
------------------------------------------
Eval num_timesteps=235000, episode_reward=1200.46 +/- 640.52
Episode length: 35.00 +/- 7.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=1216.86 +/- 618.59
Episode length: 35.30 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 230      |
|    time_elapsed    | 826      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=1347.66 +/- 655.63
Episode length: 36.78 +/- 5.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.8          |
|    mean_reward          | 1.35e+03      |
| time/                   |               |
|    total_timesteps      | 236000        |
| train/                  |               |
|    approx_kl            | 7.1013346e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000384     |
|    explained_variance   | 0.13          |
|    learning_rate        | 0.001         |
|    loss                 | 3.11e+04      |
|    n_updates            | 2510          |
|    policy_gradient_loss | -5.56e-06     |
|    value_loss           | 7.42e+04      |
-------------------------------------------
Eval num_timesteps=236500, episode_reward=1036.03 +/- 493.53
Episode length: 34.40 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 231      |
|    time_elapsed    | 830      |
|    total_timesteps | 236544   |
---------------------------------
Eval num_timesteps=237000, episode_reward=1149.87 +/- 549.09
Episode length: 35.72 +/- 5.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 1.15e+03      |
| time/                   |               |
|    total_timesteps      | 237000        |
| train/                  |               |
|    approx_kl            | 2.0372681e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000242     |
|    explained_variance   | 0.256         |
|    learning_rate        | 0.001         |
|    loss                 | 2.28e+04      |
|    n_updates            | 2520          |
|    policy_gradient_loss | -9.13e-06     |
|    value_loss           | 5.96e+04      |
-------------------------------------------
Eval num_timesteps=237500, episode_reward=1257.58 +/- 687.94
Episode length: 34.60 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 284      |
|    iterations      | 232      |
|    time_elapsed    | 834      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=1256.03 +/- 628.04
Episode length: 36.06 +/- 6.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 1.26e+03     |
| time/                   |              |
|    total_timesteps      | 238000       |
| train/                  |              |
|    approx_kl            | 4.307367e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000326    |
|    explained_variance   | 0.406        |
|    learning_rate        | 0.001        |
|    loss                 | 1.73e+04     |
|    n_updates            | 2530         |
|    policy_gradient_loss | -6.6e-06     |
|    value_loss           | 6.09e+04     |
------------------------------------------
Eval num_timesteps=238500, episode_reward=1244.35 +/- 706.00
Episode length: 34.54 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 233      |
|    time_elapsed    | 837      |
|    total_timesteps | 238592   |
---------------------------------
Eval num_timesteps=239000, episode_reward=1138.74 +/- 570.27
Episode length: 35.26 +/- 6.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 1.14e+03      |
| time/                   |               |
|    total_timesteps      | 239000        |
| train/                  |               |
|    approx_kl            | 4.8882794e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0033       |
|    explained_variance   | 0.339         |
|    learning_rate        | 0.001         |
|    loss                 | 1.35e+04      |
|    n_updates            | 2540          |
|    policy_gradient_loss | -2.09e-05     |
|    value_loss           | 4.8e+04       |
-------------------------------------------
Eval num_timesteps=239500, episode_reward=1194.21 +/- 601.56
Episode length: 35.82 +/- 6.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    fps             | 284      |
|    iterations      | 234      |
|    time_elapsed    | 841      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=1267.01 +/- 687.81
Episode length: 35.20 +/- 7.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 1.27e+03     |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0002906283 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00319     |
|    explained_variance   | 0.0153       |
|    learning_rate        | 0.001        |
|    loss                 | 1.18e+04     |
|    n_updates            | 2550         |
|    policy_gradient_loss | -4.44e-05    |
|    value_loss           | 5.79e+04     |
------------------------------------------
Eval num_timesteps=240500, episode_reward=1200.65 +/- 629.13
Episode length: 34.92 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 235      |
|    time_elapsed    | 844      |
|    total_timesteps | 240640   |
---------------------------------
Eval num_timesteps=241000, episode_reward=1170.50 +/- 646.68
Episode length: 34.26 +/- 7.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 1.17e+03      |
| time/                   |               |
|    total_timesteps      | 241000        |
| train/                  |               |
|    approx_kl            | 3.5041012e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00112      |
|    explained_variance   | 0.123         |
|    learning_rate        | 0.001         |
|    loss                 | 1.38e+04      |
|    n_updates            | 2560          |
|    policy_gradient_loss | -4.94e-05     |
|    value_loss           | 5.23e+04      |
-------------------------------------------
Eval num_timesteps=241500, episode_reward=1143.81 +/- 589.96
Episode length: 34.76 +/- 5.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 236      |
|    time_elapsed    | 848      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=1181.37 +/- 606.34
Episode length: 34.88 +/- 6.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 1.18e+03      |
| time/                   |               |
|    total_timesteps      | 242000        |
| train/                  |               |
|    approx_kl            | 4.0745363e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000443     |
|    explained_variance   | 0.327         |
|    learning_rate        | 0.001         |
|    loss                 | 2.55e+04      |
|    n_updates            | 2570          |
|    policy_gradient_loss | -1.74e-05     |
|    value_loss           | 6.49e+04      |
-------------------------------------------
Eval num_timesteps=242500, episode_reward=1183.27 +/- 601.18
Episode length: 35.38 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 237      |
|    time_elapsed    | 851      |
|    total_timesteps | 242688   |
---------------------------------
Eval num_timesteps=243000, episode_reward=1035.76 +/- 530.65
Episode length: 33.26 +/- 6.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.3          |
|    mean_reward          | 1.04e+03      |
| time/                   |               |
|    total_timesteps      | 243000        |
| train/                  |               |
|    approx_kl            | 7.5262506e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000991     |
|    explained_variance   | 0.23          |
|    learning_rate        | 0.001         |
|    loss                 | 1.77e+04      |
|    n_updates            | 2580          |
|    policy_gradient_loss | -3.87e-05     |
|    value_loss           | 6.49e+04      |
-------------------------------------------
Eval num_timesteps=243500, episode_reward=1406.74 +/- 701.53
Episode length: 36.60 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 238      |
|    time_elapsed    | 855      |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=1231.92 +/- 653.80
Episode length: 35.44 +/- 7.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 1.23e+03      |
| time/                   |               |
|    total_timesteps      | 244000        |
| train/                  |               |
|    approx_kl            | 1.0617077e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00188      |
|    explained_variance   | 0.35          |
|    learning_rate        | 0.001         |
|    loss                 | 1.65e+04      |
|    n_updates            | 2590          |
|    policy_gradient_loss | -2.23e-05     |
|    value_loss           | 5.84e+04      |
-------------------------------------------
Eval num_timesteps=244500, episode_reward=1205.88 +/- 661.21
Episode length: 34.44 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 239      |
|    time_elapsed    | 858      |
|    total_timesteps | 244736   |
---------------------------------
Eval num_timesteps=245000, episode_reward=1502.04 +/- 693.36
Episode length: 37.78 +/- 5.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.8         |
|    mean_reward          | 1.5e+03      |
| time/                   |              |
|    total_timesteps      | 245000       |
| train/                  |              |
|    approx_kl            | 0.0016026852 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00412     |
|    explained_variance   | 0.214        |
|    learning_rate        | 0.001        |
|    loss                 | 1.4e+04      |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.000367    |
|    value_loss           | 4.84e+04     |
------------------------------------------
New best mean reward!
Eval num_timesteps=245500, episode_reward=1103.83 +/- 582.15
Episode length: 34.52 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 240      |
|    time_elapsed    | 862      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=1348.83 +/- 687.53
Episode length: 36.02 +/- 6.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 1.35e+03      |
| time/                   |               |
|    total_timesteps      | 246000        |
| train/                  |               |
|    approx_kl            | 0.00041585945 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.003        |
|    explained_variance   | 0.422         |
|    learning_rate        | 0.001         |
|    loss                 | 1.68e+04      |
|    n_updates            | 2610          |
|    policy_gradient_loss | 0.000153      |
|    value_loss           | 4.56e+04      |
-------------------------------------------
Eval num_timesteps=246500, episode_reward=1207.93 +/- 624.30
Episode length: 35.42 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 241      |
|    time_elapsed    | 865      |
|    total_timesteps | 246784   |
---------------------------------
Eval num_timesteps=247000, episode_reward=1320.05 +/- 683.78
Episode length: 36.02 +/- 6.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 1.32e+03      |
| time/                   |               |
|    total_timesteps      | 247000        |
| train/                  |               |
|    approx_kl            | 1.8277206e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00105      |
|    explained_variance   | 0.146         |
|    learning_rate        | 0.001         |
|    loss                 | 2.15e+04      |
|    n_updates            | 2620          |
|    policy_gradient_loss | -4.48e-05     |
|    value_loss           | 6.5e+04       |
-------------------------------------------
Eval num_timesteps=247500, episode_reward=1189.82 +/- 608.80
Episode length: 35.50 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 242      |
|    time_elapsed    | 869      |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=1183.42 +/- 569.64
Episode length: 35.20 +/- 5.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 1.18e+03      |
| time/                   |               |
|    total_timesteps      | 248000        |
| train/                  |               |
|    approx_kl            | 2.2177119e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000979     |
|    explained_variance   | 0.283         |
|    learning_rate        | 0.001         |
|    loss                 | 1.89e+04      |
|    n_updates            | 2630          |
|    policy_gradient_loss | -2.74e-05     |
|    value_loss           | 5.22e+04      |
-------------------------------------------
Eval num_timesteps=248500, episode_reward=1193.70 +/- 632.22
Episode length: 35.12 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 285      |
|    iterations      | 243      |
|    time_elapsed    | 873      |
|    total_timesteps | 248832   |
---------------------------------
Eval num_timesteps=249000, episode_reward=1263.76 +/- 684.44
Episode length: 35.06 +/- 6.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 1.26e+03      |
| time/                   |               |
|    total_timesteps      | 249000        |
| train/                  |               |
|    approx_kl            | 8.6729415e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00122      |
|    explained_variance   | 0.281         |
|    learning_rate        | 0.001         |
|    loss                 | 1.6e+04       |
|    n_updates            | 2640          |
|    policy_gradient_loss | -1.63e-05     |
|    value_loss           | 5.46e+04      |
-------------------------------------------
Eval num_timesteps=249500, episode_reward=1298.35 +/- 707.66
Episode length: 35.58 +/- 8.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 244      |
|    time_elapsed    | 876      |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=1316.37 +/- 671.53
Episode length: 35.72 +/- 5.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 1.32e+03     |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 8.440111e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000947    |
|    explained_variance   | 0.154        |
|    learning_rate        | 0.001        |
|    loss                 | 1.81e+04     |
|    n_updates            | 2650         |
|    policy_gradient_loss | -1.11e-05    |
|    value_loss           | 6.43e+04     |
------------------------------------------
Eval num_timesteps=250500, episode_reward=1255.98 +/- 660.61
Episode length: 35.68 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 245      |
|    time_elapsed    | 880      |
|    total_timesteps | 250880   |
---------------------------------
Eval num_timesteps=251000, episode_reward=1180.79 +/- 614.51
Episode length: 34.98 +/- 6.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 1.18e+03      |
| time/                   |               |
|    total_timesteps      | 251000        |
| train/                  |               |
|    approx_kl            | 5.0640665e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000418     |
|    explained_variance   | 0.349         |
|    learning_rate        | 0.001         |
|    loss                 | 1.81e+04      |
|    n_updates            | 2660          |
|    policy_gradient_loss | -8.67e-06     |
|    value_loss           | 7.09e+04      |
-------------------------------------------
Eval num_timesteps=251500, episode_reward=1324.25 +/- 647.08
Episode length: 36.80 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 246      |
|    time_elapsed    | 883      |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=1120.71 +/- 565.17
Episode length: 34.98 +/- 5.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 1.12e+03     |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 4.284084e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00147     |
|    explained_variance   | 0.064        |
|    learning_rate        | 0.001        |
|    loss                 | 1.99e+04     |
|    n_updates            | 2670         |
|    policy_gradient_loss | -1.97e-05    |
|    value_loss           | 6.49e+04     |
------------------------------------------
Eval num_timesteps=252500, episode_reward=1105.99 +/- 541.71
Episode length: 34.62 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 247      |
|    time_elapsed    | 887      |
|    total_timesteps | 252928   |
---------------------------------
Eval num_timesteps=253000, episode_reward=1128.60 +/- 637.36
Episode length: 33.36 +/- 6.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 1.13e+03      |
| time/                   |               |
|    total_timesteps      | 253000        |
| train/                  |               |
|    approx_kl            | 3.7660357e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00173      |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.001         |
|    loss                 | 1.83e+04      |
|    n_updates            | 2680          |
|    policy_gradient_loss | -5.05e-05     |
|    value_loss           | 5.94e+04      |
-------------------------------------------
Eval num_timesteps=253500, episode_reward=1169.28 +/- 642.09
Episode length: 33.68 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 248      |
|    time_elapsed    | 890      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=1272.76 +/- 717.70
Episode length: 34.96 +/- 7.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 1.27e+03      |
| time/                   |               |
|    total_timesteps      | 254000        |
| train/                  |               |
|    approx_kl            | 2.6915222e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00186      |
|    explained_variance   | 0.139         |
|    learning_rate        | 0.001         |
|    loss                 | 2.32e+04      |
|    n_updates            | 2690          |
|    policy_gradient_loss | -5.49e-05     |
|    value_loss           | 6.38e+04      |
-------------------------------------------
Eval num_timesteps=254500, episode_reward=1218.79 +/- 617.04
Episode length: 35.78 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 249      |
|    time_elapsed    | 894      |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=255000, episode_reward=1226.78 +/- 613.60
Episode length: 35.72 +/- 5.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 1.23e+03     |
| time/                   |              |
|    total_timesteps      | 255000       |
| train/                  |              |
|    approx_kl            | 2.188608e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00135     |
|    explained_variance   | 0.202        |
|    learning_rate        | 0.001        |
|    loss                 | 1.6e+04      |
|    n_updates            | 2700         |
|    policy_gradient_loss | -2.68e-05    |
|    value_loss           | 7.16e+04     |
------------------------------------------
Eval num_timesteps=255500, episode_reward=1094.87 +/- 548.38
Episode length: 34.62 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=1198.79 +/- 672.74
Episode length: 34.56 +/- 7.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 250      |
|    time_elapsed    | 899      |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=1234.30 +/- 683.96
Episode length: 35.06 +/- 7.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 1.23e+03      |
| time/                   |               |
|    total_timesteps      | 256500        |
| train/                  |               |
|    approx_kl            | 1.6880222e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00143      |
|    explained_variance   | 0.24          |
|    learning_rate        | 0.001         |
|    loss                 | 3.41e+04      |
|    n_updates            | 2710          |
|    policy_gradient_loss | -1.75e-05     |
|    value_loss           | 8.2e+04       |
-------------------------------------------
Eval num_timesteps=257000, episode_reward=1313.01 +/- 684.89
Episode length: 36.06 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 251      |
|    time_elapsed    | 902      |
|    total_timesteps | 257024   |
---------------------------------
Eval num_timesteps=257500, episode_reward=1208.30 +/- 635.13
Episode length: 35.16 +/- 6.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 1.21e+03      |
| time/                   |               |
|    total_timesteps      | 257500        |
| train/                  |               |
|    approx_kl            | 7.4040145e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00291      |
|    explained_variance   | 0.105         |
|    learning_rate        | 0.001         |
|    loss                 | 1.73e+04      |
|    n_updates            | 2720          |
|    policy_gradient_loss | -8.28e-05     |
|    value_loss           | 7.42e+04      |
-------------------------------------------
Eval num_timesteps=258000, episode_reward=1118.06 +/- 601.20
Episode length: 33.82 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 252      |
|    time_elapsed    | 906      |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=1226.49 +/- 619.10
Episode length: 36.00 +/- 6.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 1.23e+03     |
| time/                   |              |
|    total_timesteps      | 258500       |
| train/                  |              |
|    approx_kl            | 7.735798e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00195     |
|    explained_variance   | 0.317        |
|    learning_rate        | 0.001        |
|    loss                 | 2.04e+04     |
|    n_updates            | 2730         |
|    policy_gradient_loss | -7.39e-05    |
|    value_loss           | 6.18e+04     |
------------------------------------------
Eval num_timesteps=259000, episode_reward=1264.43 +/- 686.94
Episode length: 34.78 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 253      |
|    time_elapsed    | 909      |
|    total_timesteps | 259072   |
---------------------------------
Eval num_timesteps=259500, episode_reward=1104.87 +/- 550.26
Episode length: 35.16 +/- 6.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 1.1e+03       |
| time/                   |               |
|    total_timesteps      | 259500        |
| train/                  |               |
|    approx_kl            | 0.00038927735 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0028       |
|    explained_variance   | 0.108         |
|    learning_rate        | 0.001         |
|    loss                 | 1.66e+04      |
|    n_updates            | 2740          |
|    policy_gradient_loss | 0.00134       |
|    value_loss           | 6.17e+04      |
-------------------------------------------
Eval num_timesteps=260000, episode_reward=1195.05 +/- 643.15
Episode length: 34.70 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 254      |
|    time_elapsed    | 913      |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=1288.00 +/- 636.38
Episode length: 36.40 +/- 5.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.4          |
|    mean_reward          | 1.29e+03      |
| time/                   |               |
|    total_timesteps      | 260500        |
| train/                  |               |
|    approx_kl            | 1.0367366e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00601      |
|    explained_variance   | 0.316         |
|    learning_rate        | 0.001         |
|    loss                 | 1.93e+04      |
|    n_updates            | 2750          |
|    policy_gradient_loss | -0.000102     |
|    value_loss           | 6.09e+04      |
-------------------------------------------
Eval num_timesteps=261000, episode_reward=1289.43 +/- 637.90
Episode length: 36.32 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 255      |
|    time_elapsed    | 916      |
|    total_timesteps | 261120   |
---------------------------------
Eval num_timesteps=261500, episode_reward=1177.95 +/- 600.45
Episode length: 34.92 +/- 5.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 1.18e+03      |
| time/                   |               |
|    total_timesteps      | 261500        |
| train/                  |               |
|    approx_kl            | 2.2408785e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00685      |
|    explained_variance   | 0.252         |
|    learning_rate        | 0.001         |
|    loss                 | 1.44e+04      |
|    n_updates            | 2760          |
|    policy_gradient_loss | -0.000259     |
|    value_loss           | 6.29e+04      |
-------------------------------------------
Eval num_timesteps=262000, episode_reward=1194.31 +/- 612.33
Episode length: 35.68 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 256      |
|    time_elapsed    | 920      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=1294.98 +/- 699.84
Episode length: 34.86 +/- 6.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 1.29e+03      |
| time/                   |               |
|    total_timesteps      | 262500        |
| train/                  |               |
|    approx_kl            | 1.4103716e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00122      |
|    explained_variance   | 0.28          |
|    learning_rate        | 0.001         |
|    loss                 | 1.89e+04      |
|    n_updates            | 2770          |
|    policy_gradient_loss | -8.91e-05     |
|    value_loss           | 6.07e+04      |
-------------------------------------------
Eval num_timesteps=263000, episode_reward=1244.55 +/- 707.93
Episode length: 34.60 +/- 8.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 257      |
|    time_elapsed    | 923      |
|    total_timesteps | 263168   |
---------------------------------
Eval num_timesteps=263500, episode_reward=1240.40 +/- 618.06
Episode length: 36.18 +/- 6.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.2         |
|    mean_reward          | 1.24e+03     |
| time/                   |              |
|    total_timesteps      | 263500       |
| train/                  |              |
|    approx_kl            | 5.500624e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00335     |
|    explained_variance   | 0.224        |
|    learning_rate        | 0.001        |
|    loss                 | 1.68e+04     |
|    n_updates            | 2780         |
|    policy_gradient_loss | -3.84e-05    |
|    value_loss           | 5.23e+04     |
------------------------------------------
Eval num_timesteps=264000, episode_reward=1265.34 +/- 658.55
Episode length: 35.90 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 258      |
|    time_elapsed    | 927      |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=1404.98 +/- 698.86
Episode length: 36.40 +/- 5.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.4         |
|    mean_reward          | 1.4e+03      |
| time/                   |              |
|    total_timesteps      | 264500       |
| train/                  |              |
|    approx_kl            | 7.043127e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00387     |
|    explained_variance   | 0.288        |
|    learning_rate        | 0.001        |
|    loss                 | 2.13e+04     |
|    n_updates            | 2790         |
|    policy_gradient_loss | -5.04e-05    |
|    value_loss           | 5.3e+04      |
------------------------------------------
Eval num_timesteps=265000, episode_reward=1289.87 +/- 670.49
Episode length: 36.20 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 259      |
|    time_elapsed    | 930      |
|    total_timesteps | 265216   |
---------------------------------
Eval num_timesteps=265500, episode_reward=1385.58 +/- 691.40
Episode length: 36.62 +/- 6.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 1.39e+03      |
| time/                   |               |
|    total_timesteps      | 265500        |
| train/                  |               |
|    approx_kl            | 2.6274938e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00659      |
|    explained_variance   | 0.257         |
|    learning_rate        | 0.001         |
|    loss                 | 1.31e+04      |
|    n_updates            | 2800          |
|    policy_gradient_loss | -9.1e-05      |
|    value_loss           | 4.94e+04      |
-------------------------------------------
Eval num_timesteps=266000, episode_reward=1337.43 +/- 657.01
Episode length: 36.64 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 260      |
|    time_elapsed    | 934      |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=1100.26 +/- 508.49
Episode length: 35.30 +/- 5.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 1.1e+03      |
| time/                   |              |
|    total_timesteps      | 266500       |
| train/                  |              |
|    approx_kl            | 0.0003089545 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00644     |
|    explained_variance   | 0.26         |
|    learning_rate        | 0.001        |
|    loss                 | 2.17e+04     |
|    n_updates            | 2810         |
|    policy_gradient_loss | 0.000157     |
|    value_loss           | 6.12e+04     |
------------------------------------------
Eval num_timesteps=267000, episode_reward=1379.10 +/- 723.54
Episode length: 35.94 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 261      |
|    time_elapsed    | 938      |
|    total_timesteps | 267264   |
---------------------------------
Eval num_timesteps=267500, episode_reward=1138.17 +/- 609.85
Episode length: 34.46 +/- 7.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.5         |
|    mean_reward          | 1.14e+03     |
| time/                   |              |
|    total_timesteps      | 267500       |
| train/                  |              |
|    approx_kl            | 9.911455e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0084      |
|    explained_variance   | 0.216        |
|    learning_rate        | 0.001        |
|    loss                 | 2.97e+04     |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.000237    |
|    value_loss           | 7.6e+04      |
------------------------------------------
Eval num_timesteps=268000, episode_reward=1133.57 +/- 602.92
Episode length: 34.46 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 262      |
|    time_elapsed    | 941      |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=1136.16 +/- 634.89
Episode length: 33.82 +/- 7.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.8         |
|    mean_reward          | 1.14e+03     |
| time/                   |              |
|    total_timesteps      | 268500       |
| train/                  |              |
|    approx_kl            | 7.127068e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00379     |
|    explained_variance   | 0.265        |
|    learning_rate        | 0.001        |
|    loss                 | 3.26e+04     |
|    n_updates            | 2830         |
|    policy_gradient_loss | 4e-05        |
|    value_loss           | 7.19e+04     |
------------------------------------------
Eval num_timesteps=269000, episode_reward=1242.88 +/- 634.83
Episode length: 35.74 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 263      |
|    time_elapsed    | 945      |
|    total_timesteps | 269312   |
---------------------------------
Eval num_timesteps=269500, episode_reward=1103.12 +/- 537.71
Episode length: 34.64 +/- 5.44
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.6           |
|    mean_reward          | 1.1e+03        |
| time/                   |                |
|    total_timesteps      | 269500         |
| train/                  |                |
|    approx_kl            | 1.00873876e-07 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00393       |
|    explained_variance   | 0.355          |
|    learning_rate        | 0.001          |
|    loss                 | 2.06e+04       |
|    n_updates            | 2840           |
|    policy_gradient_loss | -0.000107      |
|    value_loss           | 5.66e+04       |
--------------------------------------------
Eval num_timesteps=270000, episode_reward=1041.86 +/- 502.52
Episode length: 34.34 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 264      |
|    time_elapsed    | 948      |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=1212.12 +/- 684.61
Episode length: 34.10 +/- 7.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.1          |
|    mean_reward          | 1.21e+03      |
| time/                   |               |
|    total_timesteps      | 270500        |
| train/                  |               |
|    approx_kl            | 2.9329909e-05 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00831      |
|    explained_variance   | 0.29          |
|    learning_rate        | 0.001         |
|    loss                 | 1.54e+04      |
|    n_updates            | 2850          |
|    policy_gradient_loss | 1.81e-05      |
|    value_loss           | 5.69e+04      |
-------------------------------------------
Eval num_timesteps=271000, episode_reward=1187.68 +/- 666.18
Episode length: 34.14 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 265      |
|    time_elapsed    | 952      |
|    total_timesteps | 271360   |
---------------------------------
Eval num_timesteps=271500, episode_reward=1380.61 +/- 717.09
Episode length: 35.98 +/- 6.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36          |
|    mean_reward          | 1.38e+03    |
| time/                   |             |
|    total_timesteps      | 271500      |
| train/                  |             |
|    approx_kl            | 0.000828811 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00278    |
|    explained_variance   | 0.226       |
|    learning_rate        | 0.001       |
|    loss                 | 2.11e+04    |
|    n_updates            | 2860        |
|    policy_gradient_loss | -0.000251   |
|    value_loss           | 6.47e+04    |
-----------------------------------------
Eval num_timesteps=272000, episode_reward=1234.86 +/- 676.53
Episode length: 34.96 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 266      |
|    time_elapsed    | 955      |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=1212.89 +/- 623.73
Episode length: 35.46 +/- 5.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.5        |
|    mean_reward          | 1.21e+03    |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 7.69638e-05 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00337    |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.001       |
|    loss                 | 1.27e+04    |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.000117   |
|    value_loss           | 5.13e+04    |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=1172.88 +/- 649.25
Episode length: 34.22 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 267      |
|    time_elapsed    | 959      |
|    total_timesteps | 273408   |
---------------------------------
Eval num_timesteps=273500, episode_reward=1118.78 +/- 535.60
Episode length: 35.20 +/- 6.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 1.12e+03     |
| time/                   |              |
|    total_timesteps      | 273500       |
| train/                  |              |
|    approx_kl            | 0.0040137265 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00113     |
|    explained_variance   | 0.113        |
|    learning_rate        | 0.001        |
|    loss                 | 2.44e+04     |
|    n_updates            | 2880         |
|    policy_gradient_loss | 0.00196      |
|    value_loss           | 7.26e+04     |
------------------------------------------
Eval num_timesteps=274000, episode_reward=1168.12 +/- 619.92
Episode length: 34.60 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 268      |
|    time_elapsed    | 962      |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=1303.38 +/- 657.58
Episode length: 36.18 +/- 6.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.2          |
|    mean_reward          | 1.3e+03       |
| time/                   |               |
|    total_timesteps      | 274500        |
| train/                  |               |
|    approx_kl            | 3.4924597e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000363     |
|    explained_variance   | 0.308         |
|    learning_rate        | 0.001         |
|    loss                 | 1.37e+04      |
|    n_updates            | 2890          |
|    policy_gradient_loss | -1e-05        |
|    value_loss           | 5.97e+04      |
-------------------------------------------
Eval num_timesteps=275000, episode_reward=1233.32 +/- 634.86
Episode length: 35.54 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 269      |
|    time_elapsed    | 966      |
|    total_timesteps | 275456   |
---------------------------------
Eval num_timesteps=275500, episode_reward=1260.20 +/- 701.46
Episode length: 35.14 +/- 7.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 1.26e+03      |
| time/                   |               |
|    total_timesteps      | 275500        |
| train/                  |               |
|    approx_kl            | 2.5203917e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000493     |
|    explained_variance   | 0.248         |
|    learning_rate        | 0.001         |
|    loss                 | 2.11e+04      |
|    n_updates            | 2900          |
|    policy_gradient_loss | -2.16e-05     |
|    value_loss           | 6.38e+04      |
-------------------------------------------
Eval num_timesteps=276000, episode_reward=1248.81 +/- 634.16
Episode length: 35.60 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 270      |
|    time_elapsed    | 969      |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=1289.23 +/- 637.12
Episode length: 36.68 +/- 6.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.7          |
|    mean_reward          | 1.29e+03      |
| time/                   |               |
|    total_timesteps      | 276500        |
| train/                  |               |
|    approx_kl            | 2.1827873e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000966     |
|    explained_variance   | 0.376         |
|    learning_rate        | 0.001         |
|    loss                 | 2.57e+04      |
|    n_updates            | 2910          |
|    policy_gradient_loss | -1.7e-05      |
|    value_loss           | 5.52e+04      |
-------------------------------------------
Eval num_timesteps=277000, episode_reward=1185.52 +/- 638.30
Episode length: 34.58 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=1217.14 +/- 654.54
Episode length: 34.64 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 271      |
|    time_elapsed    | 974      |
|    total_timesteps | 277504   |
---------------------------------
Eval num_timesteps=278000, episode_reward=1334.07 +/- 666.53
Episode length: 36.32 +/- 5.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 1.33e+03      |
| time/                   |               |
|    total_timesteps      | 278000        |
| train/                  |               |
|    approx_kl            | 3.3003744e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00119      |
|    explained_variance   | 0.16          |
|    learning_rate        | 0.001         |
|    loss                 | 1.93e+04      |
|    n_updates            | 2920          |
|    policy_gradient_loss | -2.97e-05     |
|    value_loss           | 6.65e+04      |
-------------------------------------------
Eval num_timesteps=278500, episode_reward=1188.61 +/- 606.50
Episode length: 35.62 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 272      |
|    time_elapsed    | 978      |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=1239.73 +/- 608.13
Episode length: 36.28 +/- 5.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 1.24e+03      |
| time/                   |               |
|    total_timesteps      | 279000        |
| train/                  |               |
|    approx_kl            | 4.6333298e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000875     |
|    explained_variance   | 0.385         |
|    learning_rate        | 0.001         |
|    loss                 | 1.58e+04      |
|    n_updates            | 2930          |
|    policy_gradient_loss | -3.44e-05     |
|    value_loss           | 5.16e+04      |
-------------------------------------------
Eval num_timesteps=279500, episode_reward=1416.08 +/- 692.32
Episode length: 36.86 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 273      |
|    time_elapsed    | 981      |
|    total_timesteps | 279552   |
---------------------------------
Eval num_timesteps=280000, episode_reward=1138.57 +/- 671.57
Episode length: 33.40 +/- 7.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.4          |
|    mean_reward          | 1.14e+03      |
| time/                   |               |
|    total_timesteps      | 280000        |
| train/                  |               |
|    approx_kl            | 5.0407834e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00082      |
|    explained_variance   | 0.16          |
|    learning_rate        | 0.001         |
|    loss                 | 2.25e+04      |
|    n_updates            | 2940          |
|    policy_gradient_loss | -2.55e-05     |
|    value_loss           | 6.12e+04      |
-------------------------------------------
Eval num_timesteps=280500, episode_reward=1315.87 +/- 682.40
Episode length: 35.64 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 274      |
|    time_elapsed    | 985      |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=1209.63 +/- 626.66
Episode length: 35.42 +/- 6.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 1.21e+03     |
| time/                   |              |
|    total_timesteps      | 281000       |
| train/                  |              |
|    approx_kl            | 9.586802e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0019      |
|    explained_variance   | 0.242        |
|    learning_rate        | 0.001        |
|    loss                 | 1.86e+04     |
|    n_updates            | 2950         |
|    policy_gradient_loss | -1.96e-05    |
|    value_loss           | 6.28e+04     |
------------------------------------------
Eval num_timesteps=281500, episode_reward=1216.01 +/- 623.65
Episode length: 35.46 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 275      |
|    time_elapsed    | 988      |
|    total_timesteps | 281600   |
---------------------------------
Eval num_timesteps=282000, episode_reward=1353.55 +/- 711.36
Episode length: 35.62 +/- 6.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 1.35e+03      |
| time/                   |               |
|    total_timesteps      | 282000        |
| train/                  |               |
|    approx_kl            | 3.4377445e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00384      |
|    explained_variance   | 0.358         |
|    learning_rate        | 0.001         |
|    loss                 | 1.58e+04      |
|    n_updates            | 2960          |
|    policy_gradient_loss | -3.89e-05     |
|    value_loss           | 5.33e+04      |
-------------------------------------------
Eval num_timesteps=282500, episode_reward=1075.02 +/- 597.80
Episode length: 33.36 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 276      |
|    time_elapsed    | 992      |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=1160.22 +/- 667.46
Episode length: 33.78 +/- 7.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 1.16e+03      |
| time/                   |               |
|    total_timesteps      | 283000        |
| train/                  |               |
|    approx_kl            | 6.0854247e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00324      |
|    explained_variance   | 0.243         |
|    learning_rate        | 0.001         |
|    loss                 | 1.33e+04      |
|    n_updates            | 2970          |
|    policy_gradient_loss | 0.000415      |
|    value_loss           | 4.47e+04      |
-------------------------------------------
Eval num_timesteps=283500, episode_reward=1176.45 +/- 648.23
Episode length: 34.24 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 277      |
|    time_elapsed    | 995      |
|    total_timesteps | 283648   |
---------------------------------
Eval num_timesteps=284000, episode_reward=1251.83 +/- 675.30
Episode length: 35.40 +/- 7.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 1.25e+03     |
| time/                   |              |
|    total_timesteps      | 284000       |
| train/                  |              |
|    approx_kl            | 1.215958e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00102     |
|    explained_variance   | 0.314        |
|    learning_rate        | 0.001        |
|    loss                 | 2e+04        |
|    n_updates            | 2980         |
|    policy_gradient_loss | -5.45e-05    |
|    value_loss           | 5.84e+04     |
------------------------------------------
Eval num_timesteps=284500, episode_reward=1349.62 +/- 651.75
Episode length: 37.04 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 278      |
|    time_elapsed    | 999      |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=1151.18 +/- 631.05
Episode length: 34.40 +/- 7.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 1.15e+03     |
| time/                   |              |
|    total_timesteps      | 285000       |
| train/                  |              |
|    approx_kl            | 9.778887e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000351    |
|    explained_variance   | 0.159        |
|    learning_rate        | 0.001        |
|    loss                 | 2.42e+04     |
|    n_updates            | 2990         |
|    policy_gradient_loss | -2.07e-05    |
|    value_loss           | 7.25e+04     |
------------------------------------------
Eval num_timesteps=285500, episode_reward=1213.88 +/- 617.47
Episode length: 35.56 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 279      |
|    time_elapsed    | 1002     |
|    total_timesteps | 285696   |
---------------------------------
Eval num_timesteps=286000, episode_reward=1289.60 +/- 679.71
Episode length: 35.98 +/- 7.70
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36             |
|    mean_reward          | 1.29e+03       |
| time/                   |                |
|    total_timesteps      | 286000         |
| train/                  |                |
|    approx_kl            | 1.21071935e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000426      |
|    explained_variance   | 0.272          |
|    learning_rate        | 0.001          |
|    loss                 | 2.96e+04       |
|    n_updates            | 3000           |
|    policy_gradient_loss | -1.33e-05      |
|    value_loss           | 6.76e+04       |
--------------------------------------------
Eval num_timesteps=286500, episode_reward=1236.35 +/- 671.60
Episode length: 34.96 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 280      |
|    time_elapsed    | 1006     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=1172.77 +/- 614.04
Episode length: 35.20 +/- 6.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.2        |
|    mean_reward          | 1.17e+03    |
| time/                   |             |
|    total_timesteps      | 287000      |
| train/                  |             |
|    approx_kl            | 4.80155e-07 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00223    |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.001       |
|    loss                 | 2.37e+04    |
|    n_updates            | 3010        |
|    policy_gradient_loss | -3.32e-05   |
|    value_loss           | 5.8e+04     |
-----------------------------------------
Eval num_timesteps=287500, episode_reward=1245.65 +/- 674.42
Episode length: 35.04 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 284      |
|    iterations      | 281      |
|    time_elapsed    | 1009     |
|    total_timesteps | 287744   |
---------------------------------
Eval num_timesteps=288000, episode_reward=1088.43 +/- 653.36
Episode length: 32.56 +/- 7.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.6         |
|    mean_reward          | 1.09e+03     |
| time/                   |              |
|    total_timesteps      | 288000       |
| train/                  |              |
|    approx_kl            | 6.111397e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00598     |
|    explained_variance   | 0.158        |
|    learning_rate        | 0.001        |
|    loss                 | 2.23e+04     |
|    n_updates            | 3020         |
|    policy_gradient_loss | -0.000236    |
|    value_loss           | 5.8e+04      |
------------------------------------------
Eval num_timesteps=288500, episode_reward=1342.58 +/- 666.94
Episode length: 36.70 +/- 6.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 282      |
|    time_elapsed    | 1013     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=1137.15 +/- 602.48
Episode length: 34.52 +/- 6.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 1.14e+03      |
| time/                   |               |
|    total_timesteps      | 289000        |
| train/                  |               |
|    approx_kl            | 1.0028016e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00138      |
|    explained_variance   | 0.286         |
|    learning_rate        | 0.001         |
|    loss                 | 1.6e+04       |
|    n_updates            | 3030          |
|    policy_gradient_loss | -0.000175     |
|    value_loss           | 5.63e+04      |
-------------------------------------------
Eval num_timesteps=289500, episode_reward=1214.42 +/- 657.68
Episode length: 35.02 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 283      |
|    time_elapsed    | 1016     |
|    total_timesteps | 289792   |
---------------------------------
Eval num_timesteps=290000, episode_reward=1151.80 +/- 591.86
Episode length: 35.02 +/- 6.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 1.15e+03      |
| time/                   |               |
|    total_timesteps      | 290000        |
| train/                  |               |
|    approx_kl            | 0.00075940904 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00197      |
|    explained_variance   | 0.316         |
|    learning_rate        | 0.001         |
|    loss                 | 2.3e+04       |
|    n_updates            | 3040          |
|    policy_gradient_loss | 0.000414      |
|    value_loss           | 6.28e+04      |
-------------------------------------------
Eval num_timesteps=290500, episode_reward=1248.99 +/- 669.75
Episode length: 35.26 +/- 7.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 284      |
|    time_elapsed    | 1020     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=1372.46 +/- 733.06
Episode length: 35.68 +/- 7.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 1.37e+03      |
| time/                   |               |
|    total_timesteps      | 291000        |
| train/                  |               |
|    approx_kl            | 0.00074431946 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000742     |
|    explained_variance   | 0.315         |
|    learning_rate        | 0.001         |
|    loss                 | 2.07e+04      |
|    n_updates            | 3050          |
|    policy_gradient_loss | -6.57e-05     |
|    value_loss           | 6.89e+04      |
-------------------------------------------
Eval num_timesteps=291500, episode_reward=1353.77 +/- 681.49
Episode length: 36.40 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 285      |
|    time_elapsed    | 1023     |
|    total_timesteps | 291840   |
---------------------------------
Eval num_timesteps=292000, episode_reward=1450.10 +/- 718.35
Episode length: 36.74 +/- 6.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.7          |
|    mean_reward          | 1.45e+03      |
| time/                   |               |
|    total_timesteps      | 292000        |
| train/                  |               |
|    approx_kl            | 1.3719546e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00193      |
|    explained_variance   | 0.199         |
|    learning_rate        | 0.001         |
|    loss                 | 3.17e+04      |
|    n_updates            | 3060          |
|    policy_gradient_loss | -5.45e-05     |
|    value_loss           | 7.53e+04      |
-------------------------------------------
Eval num_timesteps=292500, episode_reward=1049.69 +/- 577.73
Episode length: 33.20 +/- 7.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.2     |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 286      |
|    time_elapsed    | 1027     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=1192.29 +/- 641.96
Episode length: 34.90 +/- 7.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 1.19e+03      |
| time/                   |               |
|    total_timesteps      | 293000        |
| train/                  |               |
|    approx_kl            | 3.8184226e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000724     |
|    explained_variance   | 0.316         |
|    learning_rate        | 0.001         |
|    loss                 | 1.62e+04      |
|    n_updates            | 3070          |
|    policy_gradient_loss | -4.19e-05     |
|    value_loss           | 6.07e+04      |
-------------------------------------------
Eval num_timesteps=293500, episode_reward=1269.57 +/- 640.27
Episode length: 35.88 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 287      |
|    time_elapsed    | 1031     |
|    total_timesteps | 293888   |
---------------------------------
Eval num_timesteps=294000, episode_reward=1164.42 +/- 584.39
Episode length: 35.28 +/- 6.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 1.16e+03     |
| time/                   |              |
|    total_timesteps      | 294000       |
| train/                  |              |
|    approx_kl            | 7.450581e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00067     |
|    explained_variance   | 0.429        |
|    learning_rate        | 0.001        |
|    loss                 | 1.35e+04     |
|    n_updates            | 3080         |
|    policy_gradient_loss | -1.21e-05    |
|    value_loss           | 5.46e+04     |
------------------------------------------
Eval num_timesteps=294500, episode_reward=1208.59 +/- 660.33
Episode length: 34.82 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 288      |
|    time_elapsed    | 1034     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=1236.94 +/- 640.81
Episode length: 35.48 +/- 6.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 1.24e+03      |
| time/                   |               |
|    total_timesteps      | 295000        |
| train/                  |               |
|    approx_kl            | 7.3341653e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000483     |
|    explained_variance   | 0.217         |
|    learning_rate        | 0.001         |
|    loss                 | 2.55e+04      |
|    n_updates            | 3090          |
|    policy_gradient_loss | -1.36e-05     |
|    value_loss           | 6.94e+04      |
-------------------------------------------
Eval num_timesteps=295500, episode_reward=1166.03 +/- 548.53
Episode length: 36.00 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 289      |
|    time_elapsed    | 1038     |
|    total_timesteps | 295936   |
---------------------------------
Eval num_timesteps=296000, episode_reward=1404.13 +/- 698.87
Episode length: 36.32 +/- 6.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.3          |
|    mean_reward          | 1.4e+03       |
| time/                   |               |
|    total_timesteps      | 296000        |
| train/                  |               |
|    approx_kl            | 0.00025089225 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00155      |
|    explained_variance   | 0.435         |
|    learning_rate        | 0.001         |
|    loss                 | 2.7e+04       |
|    n_updates            | 3100          |
|    policy_gradient_loss | 7.52e-06      |
|    value_loss           | 5e+04         |
-------------------------------------------
Eval num_timesteps=296500, episode_reward=1212.38 +/- 643.47
Episode length: 35.80 +/- 7.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 290      |
|    time_elapsed    | 1041     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=1142.71 +/- 670.09
Episode length: 33.54 +/- 7.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.5         |
|    mean_reward          | 1.14e+03     |
| time/                   |              |
|    total_timesteps      | 297000       |
| train/                  |              |
|    approx_kl            | 6.047776e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000857    |
|    explained_variance   | 0.334        |
|    learning_rate        | 0.001        |
|    loss                 | 2.69e+04     |
|    n_updates            | 3110         |
|    policy_gradient_loss | -2.89e-05    |
|    value_loss           | 6.7e+04      |
------------------------------------------
Eval num_timesteps=297500, episode_reward=1260.01 +/- 648.87
Episode length: 35.82 +/- 5.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 285      |
|    iterations      | 291      |
|    time_elapsed    | 1045     |
|    total_timesteps | 297984   |
---------------------------------
Eval num_timesteps=298000, episode_reward=1122.56 +/- 576.19
Episode length: 34.80 +/- 6.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 1.12e+03      |
| time/                   |               |
|    total_timesteps      | 298000        |
| train/                  |               |
|    approx_kl            | 1.8678838e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000886     |
|    explained_variance   | 0.228         |
|    learning_rate        | 0.001         |
|    loss                 | 1.97e+04      |
|    n_updates            | 3120          |
|    policy_gradient_loss | -3.93e-05     |
|    value_loss           | 6.19e+04      |
-------------------------------------------
Eval num_timesteps=298500, episode_reward=1216.96 +/- 616.86
Episode length: 35.70 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=1096.51 +/- 578.91
Episode length: 34.04 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 292      |
|    time_elapsed    | 1049     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=1205.49 +/- 656.88
Episode length: 34.70 +/- 6.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 1.21e+03     |
| time/                   |              |
|    total_timesteps      | 299500       |
| train/                  |              |
|    approx_kl            | 2.561137e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000729    |
|    explained_variance   | 0.343        |
|    learning_rate        | 0.001        |
|    loss                 | 3.4e+04      |
|    n_updates            | 3130         |
|    policy_gradient_loss | -1.68e-05    |
|    value_loss           | 6.75e+04     |
------------------------------------------
Eval num_timesteps=300000, episode_reward=1356.38 +/- 683.36
Episode length: 36.40 +/- 6.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 284      |
|    iterations      | 293      |
|    time_elapsed    | 1053     |
|    total_timesteps | 300032   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-8/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 1024, 'batch_size': 64, 'learning_rate': 0.001, 'gamma': 0.9635, 'gae_lambda': 0.879}
Training steps: 300000
Frame skip: 4
