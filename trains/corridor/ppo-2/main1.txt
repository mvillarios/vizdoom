/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=-708.96 +/- 181.32
Episode length: 62.04 +/- 18.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62       |
|    mean_reward     | -709     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-776.94 +/- 175.01
Episode length: 58.06 +/- 17.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.1     |
|    mean_reward     | -777     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 377      |
|    ep_rew_mean     | -571     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 1        |
|    time_elapsed    | 6        |
|    total_timesteps | 1024     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-1016.69 +/- 76.90
Episode length: 99.74 +/- 27.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 99.7      |
|    mean_reward          | -1.02e+03 |
| time/                   |           |
|    total_timesteps      | 1500      |
| train/                  |           |
|    approx_kl            | 0.747915  |
|    clip_fraction        | 0.558     |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.97     |
|    explained_variance   | -2.5e-06  |
|    learning_rate        | 0.001     |
|    loss                 | 758       |
|    n_updates            | 10        |
|    policy_gradient_loss | 0.0483    |
|    value_loss           | 3.37e+03  |
---------------------------------------
Eval num_timesteps=2000, episode_reward=-1024.70 +/- 65.11
Episode length: 102.26 +/- 25.48
----------------------------------
| eval/              |           |
|    mean_ep_length  | 102       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 2000      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 225      |
|    ep_rew_mean     | -775     |
| time/              |          |
|    fps             | 115      |
|    iterations      | 2        |
|    time_elapsed    | 17       |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=-1017.20 +/- 67.34
Episode length: 97.76 +/- 29.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.8       |
|    mean_reward          | -1.02e+03  |
| time/                   |            |
|    total_timesteps      | 2500       |
| train/                  |            |
|    approx_kl            | 0.27102864 |
|    clip_fraction        | 0.765      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.17      |
|    explained_variance   | -0.0101    |
|    learning_rate        | 0.001      |
|    loss                 | 5.39e+03   |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.137      |
|    value_loss           | 1.21e+04   |
----------------------------------------
Eval num_timesteps=3000, episode_reward=-1013.48 +/- 76.25
Episode length: 99.14 +/- 29.28
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.1      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 3000      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 205      |
|    ep_rew_mean     | -648     |
| time/              |          |
|    fps             | 108      |
|    iterations      | 3        |
|    time_elapsed    | 28       |
|    total_timesteps | 3072     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-1034.00 +/- 63.28
Episode length: 100.14 +/- 24.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 100        |
|    mean_reward          | -1.03e+03  |
| time/                   |            |
|    total_timesteps      | 3500       |
| train/                  |            |
|    approx_kl            | 0.18745854 |
|    clip_fraction        | 0.456      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.0785     |
|    learning_rate        | 0.001      |
|    loss                 | 4.59e+03   |
|    n_updates            | 30         |
|    policy_gradient_loss | 0.0435     |
|    value_loss           | 1.43e+04   |
----------------------------------------
Eval num_timesteps=4000, episode_reward=-1030.00 +/- 71.92
Episode length: 97.84 +/- 24.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.8      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 4000      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 157      |
|    ep_rew_mean     | -807     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 4        |
|    time_elapsed    | 38       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=-1016.80 +/- 60.92
Episode length: 98.34 +/- 22.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 98.3      |
|    mean_reward          | -1.02e+03 |
| time/                   |           |
|    total_timesteps      | 4500      |
| train/                  |           |
|    approx_kl            | 7.008396  |
|    clip_fraction        | 0.933     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.106    |
|    explained_variance   | 0.0556    |
|    learning_rate        | 0.001     |
|    loss                 | 2.15e+03  |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.16      |
|    value_loss           | 7.3e+03   |
---------------------------------------
Eval num_timesteps=5000, episode_reward=-1026.40 +/- 52.19
Episode length: 99.76 +/- 21.39
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.8      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 5000      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | -861     |
| time/              |          |
|    fps             | 103      |
|    iterations      | 5        |
|    time_elapsed    | 49       |
|    total_timesteps | 5120     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-1016.40 +/- 64.31
Episode length: 99.44 +/- 24.84
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 99.4           |
|    mean_reward          | -1.02e+03      |
| time/                   |                |
|    total_timesteps      | 5500           |
| train/                  |                |
|    approx_kl            | -1.9208528e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6e-06         |
|    explained_variance   | 0.341          |
|    learning_rate        | 0.001          |
|    loss                 | 2.36e+03       |
|    n_updates            | 50             |
|    policy_gradient_loss | 4.33e-07       |
|    value_loss           | 6.3e+03        |
--------------------------------------------
Eval num_timesteps=6000, episode_reward=-1029.60 +/- 65.90
Episode length: 100.96 +/- 31.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 6000      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | -896     |
| time/              |          |
|    fps             | 101      |
|    iterations      | 6        |
|    time_elapsed    | 60       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=-998.40 +/- 77.22
Episode length: 96.30 +/- 25.75
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 96.3           |
|    mean_reward          | -998           |
| time/                   |                |
|    total_timesteps      | 6500           |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.9e-06       |
|    explained_variance   | 0.675          |
|    learning_rate        | 0.001          |
|    loss                 | 1.68e+03       |
|    n_updates            | 60             |
|    policy_gradient_loss | 8.59e-08       |
|    value_loss           | 4.67e+03       |
--------------------------------------------
Eval num_timesteps=7000, episode_reward=-1015.20 +/- 69.85
Episode length: 94.30 +/- 19.78
----------------------------------
| eval/              |           |
|    mean_ep_length  | 94.3      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 7000      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | -914     |
| time/              |          |
|    fps             | 101      |
|    iterations      | 7        |
|    time_elapsed    | 70       |
|    total_timesteps | 7168     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-1002.40 +/- 73.26
Episode length: 99.62 +/- 26.01
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 99.6           |
|    mean_reward          | -1e+03         |
| time/                   |                |
|    total_timesteps      | 7500           |
| train/                  |                |
|    approx_kl            | -1.8044375e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2.92e-05      |
|    explained_variance   | 0.885          |
|    learning_rate        | 0.001          |
|    loss                 | 1.1e+03        |
|    n_updates            | 70             |
|    policy_gradient_loss | -1.47e-07      |
|    value_loss           | 2.62e+03       |
--------------------------------------------
Eval num_timesteps=8000, episode_reward=-1013.20 +/- 74.62
Episode length: 94.72 +/- 26.68
----------------------------------
| eval/              |           |
|    mean_ep_length  | 94.7      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 8000      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | -930     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 8        |
|    time_elapsed    | 81       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=-1031.60 +/- 56.04
Episode length: 102.08 +/- 24.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | -1.03e+03 |
| time/                   |           |
|    total_timesteps      | 8500      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.01e-05 |
|    explained_variance   | 0.884     |
|    learning_rate        | 0.001     |
|    loss                 | 1.04e+03  |
|    n_updates            | 80        |
|    policy_gradient_loss | 5.94e-08  |
|    value_loss           | 3.38e+03  |
---------------------------------------
Eval num_timesteps=9000, episode_reward=-1018.00 +/- 67.56
Episode length: 98.66 +/- 24.02
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.7      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 9000      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | -940     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 9        |
|    time_elapsed    | 91       |
|    total_timesteps | 9216     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-1027.93 +/- 59.79
Episode length: 98.68 +/- 20.82
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 98.7           |
|    mean_reward          | -1.03e+03      |
| time/                   |                |
|    total_timesteps      | 9500           |
| train/                  |                |
|    approx_kl            | -2.2700988e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -2e-05         |
|    explained_variance   | 0.917          |
|    learning_rate        | 0.001          |
|    loss                 | 1.29e+03       |
|    n_updates            | 90             |
|    policy_gradient_loss | -1.37e-07      |
|    value_loss           | 2.41e+03       |
--------------------------------------------
Eval num_timesteps=10000, episode_reward=-1013.60 +/- 63.83
Episode length: 91.42 +/- 21.11
----------------------------------
| eval/              |           |
|    mean_ep_length  | 91.4      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 10000     |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | -946     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 10       |
|    time_elapsed    | 102      |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=-1028.40 +/- 55.84
Episode length: 100.08 +/- 23.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 100           |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 10500         |
| train/                  |               |
|    approx_kl            | -9.313226e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.47e-05     |
|    explained_variance   | 0.907         |
|    learning_rate        | 0.001         |
|    loss                 | 1.11e+03      |
|    n_updates            | 100           |
|    policy_gradient_loss | 8.8e-08       |
|    value_loss           | 2.34e+03      |
-------------------------------------------
Eval num_timesteps=11000, episode_reward=-1007.60 +/- 65.49
Episode length: 99.26 +/- 25.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.3      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 11000     |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 110      |
|    ep_rew_mean     | -957     |
| time/              |          |
|    fps             | 99       |
|    iterations      | 11       |
|    time_elapsed    | 112      |
|    total_timesteps | 11264    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-1015.20 +/- 54.98
Episode length: 97.12 +/- 21.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.1         |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 11500        |
| train/                  |              |
|    approx_kl            | 7.858034e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000268    |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.001        |
|    loss                 | 627          |
|    n_updates            | 110          |
|    policy_gradient_loss | -5.7e-06     |
|    value_loss           | 2.1e+03      |
------------------------------------------
Eval num_timesteps=12000, episode_reward=-1023.20 +/- 63.09
Episode length: 99.74 +/- 20.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.7      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 12000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.6      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 99        |
|    iterations      | 12        |
|    time_elapsed    | 123       |
|    total_timesteps | 12288     |
----------------------------------
Eval num_timesteps=12500, episode_reward=-1028.40 +/- 73.42
Episode length: 102.84 +/- 22.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 103           |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 12500         |
| train/                  |               |
|    approx_kl            | 5.2386895e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.33e-05     |
|    explained_variance   | 0.913         |
|    learning_rate        | 0.001         |
|    loss                 | 1.15e+03      |
|    n_updates            | 120           |
|    policy_gradient_loss | 4.45e-06      |
|    value_loss           | 2.58e+03      |
-------------------------------------------
Eval num_timesteps=13000, episode_reward=-1035.60 +/- 53.89
Episode length: 103.02 +/- 26.13
----------------------------------
| eval/              |           |
|    mean_ep_length  | 103       |
|    mean_reward     | -1.04e+03 |
| time/              |           |
|    total_timesteps | 13000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.6      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 99        |
|    iterations      | 13        |
|    time_elapsed    | 134       |
|    total_timesteps | 13312     |
----------------------------------
Eval num_timesteps=13500, episode_reward=-1024.00 +/- 56.71
Episode length: 97.70 +/- 26.42
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 97.7           |
|    mean_reward          | -1.02e+03      |
| time/                   |                |
|    total_timesteps      | 13500          |
| train/                  |                |
|    approx_kl            | -2.4447218e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.68e-05      |
|    explained_variance   | 0.926          |
|    learning_rate        | 0.001          |
|    loss                 | 1.24e+03       |
|    n_updates            | 130            |
|    policy_gradient_loss | 7.6e-07        |
|    value_loss           | 2.26e+03       |
--------------------------------------------
Eval num_timesteps=14000, episode_reward=-1025.15 +/- 69.37
Episode length: 103.40 +/- 27.20
----------------------------------
| eval/              |           |
|    mean_ep_length  | 103       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 14000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.5      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 98        |
|    iterations      | 14        |
|    time_elapsed    | 144       |
|    total_timesteps | 14336     |
----------------------------------
Eval num_timesteps=14500, episode_reward=-1035.08 +/- 58.36
Episode length: 102.40 +/- 28.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 102       |
|    mean_reward          | -1.04e+03 |
| time/                   |           |
|    total_timesteps      | 14500     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.25e-05 |
|    explained_variance   | 0.935     |
|    learning_rate        | 0.001     |
|    loss                 | 936       |
|    n_updates            | 140       |
|    policy_gradient_loss | 3.92e-08  |
|    value_loss           | 2.08e+03  |
---------------------------------------
Eval num_timesteps=15000, episode_reward=-1019.91 +/- 64.06
Episode length: 96.66 +/- 26.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.7      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 15000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 91.8      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 98        |
|    iterations      | 15        |
|    time_elapsed    | 155       |
|    total_timesteps | 15360     |
----------------------------------
Eval num_timesteps=15500, episode_reward=-1030.00 +/- 60.30
Episode length: 98.34 +/- 19.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 98.3          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 15500         |
| train/                  |               |
|    approx_kl            | 2.7939677e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.66e-05     |
|    explained_variance   | 0.945         |
|    learning_rate        | 0.001         |
|    loss                 | 935           |
|    n_updates            | 150           |
|    policy_gradient_loss | 1.8e-06       |
|    value_loss           | 1.83e+03      |
-------------------------------------------
Eval num_timesteps=16000, episode_reward=-1016.80 +/- 71.88
Episode length: 94.22 +/- 22.62
----------------------------------
| eval/              |           |
|    mean_ep_length  | 94.2      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 16000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 92.3      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 98        |
|    iterations      | 16        |
|    time_elapsed    | 165       |
|    total_timesteps | 16384     |
----------------------------------
Eval num_timesteps=16500, episode_reward=-1021.17 +/- 64.58
Episode length: 100.38 +/- 25.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 100           |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 16500         |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.22e-05     |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 419           |
|    n_updates            | 160           |
|    policy_gradient_loss | 1.16e-06      |
|    value_loss           | 1.12e+03      |
-------------------------------------------
Eval num_timesteps=17000, episode_reward=-1015.20 +/- 62.86
Episode length: 101.20 +/- 25.68
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 17000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.4      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 98        |
|    iterations      | 17        |
|    time_elapsed    | 176       |
|    total_timesteps | 17408     |
----------------------------------
Eval num_timesteps=17500, episode_reward=-1032.80 +/- 57.04
Episode length: 99.72 +/- 22.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.7         |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 17500        |
| train/                  |              |
|    approx_kl            | 2.910383e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.5e-05     |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 675          |
|    n_updates            | 170          |
|    policy_gradient_loss | -1.15e-06    |
|    value_loss           | 1.39e+03     |
------------------------------------------
Eval num_timesteps=18000, episode_reward=-1025.60 +/- 64.78
Episode length: 103.88 +/- 26.59
----------------------------------
| eval/              |           |
|    mean_ep_length  | 104       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 18000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.1      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 98        |
|    iterations      | 18        |
|    time_elapsed    | 187       |
|    total_timesteps | 18432     |
----------------------------------
Eval num_timesteps=18500, episode_reward=-1020.80 +/- 67.71
Episode length: 92.04 +/- 23.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 92            |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 18500         |
| train/                  |               |
|    approx_kl            | -9.895302e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.74e-05     |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 493           |
|    n_updates            | 180           |
|    policy_gradient_loss | 6.25e-07      |
|    value_loss           | 1.14e+03      |
-------------------------------------------
Eval num_timesteps=19000, episode_reward=-1023.20 +/- 66.54
Episode length: 102.60 +/- 25.84
----------------------------------
| eval/              |           |
|    mean_ep_length  | 103       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 19000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.1      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 98        |
|    iterations      | 19        |
|    time_elapsed    | 197       |
|    total_timesteps | 19456     |
----------------------------------
Eval num_timesteps=19500, episode_reward=-1006.40 +/- 71.47
Episode length: 87.94 +/- 20.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 87.9          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 19500         |
| train/                  |               |
|    approx_kl            | 2.2700988e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000217     |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 573           |
|    n_updates            | 190           |
|    policy_gradient_loss | -5.17e-06     |
|    value_loss           | 1.58e+03      |
-------------------------------------------
Eval num_timesteps=20000, episode_reward=-1012.23 +/- 74.57
Episode length: 98.76 +/- 26.40
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.8      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 20000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 98        |
|    iterations      | 20        |
|    time_elapsed    | 207       |
|    total_timesteps | 20480     |
----------------------------------
Eval num_timesteps=20500, episode_reward=-1026.73 +/- 67.76
Episode length: 93.08 +/- 20.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 93.1          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 20500         |
| train/                  |               |
|    approx_kl            | 1.0477379e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000322     |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.001         |
|    loss                 | 332           |
|    n_updates            | 200           |
|    policy_gradient_loss | -5.64e-06     |
|    value_loss           | 1.17e+03      |
-------------------------------------------
Eval num_timesteps=21000, episode_reward=-1008.80 +/- 68.44
Episode length: 99.88 +/- 26.90
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.9      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 21000     |
----------------------------------
Eval num_timesteps=21500, episode_reward=-1027.51 +/- 68.51
Episode length: 101.44 +/- 22.21
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 21500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 21        |
|    time_elapsed    | 222       |
|    total_timesteps | 21504     |
----------------------------------
Eval num_timesteps=22000, episode_reward=-1032.40 +/- 58.34
Episode length: 94.42 +/- 21.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 94.4          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 22000         |
| train/                  |               |
|    approx_kl            | 6.9849193e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.42e-05     |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 438           |
|    n_updates            | 210           |
|    policy_gradient_loss | 2.5e-06       |
|    value_loss           | 1.42e+03      |
-------------------------------------------
Eval num_timesteps=22500, episode_reward=-1032.40 +/- 64.71
Episode length: 96.96 +/- 23.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97        |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 22500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.1      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 22        |
|    time_elapsed    | 233       |
|    total_timesteps | 22528     |
----------------------------------
Eval num_timesteps=23000, episode_reward=-1023.60 +/- 74.07
Episode length: 97.52 +/- 19.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.5          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 23000         |
| train/                  |               |
|    approx_kl            | 3.8417056e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000126     |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.001         |
|    loss                 | 583           |
|    n_updates            | 220           |
|    policy_gradient_loss | -2.72e-07     |
|    value_loss           | 1.21e+03      |
-------------------------------------------
Eval num_timesteps=23500, episode_reward=-1019.05 +/- 60.33
Episode length: 98.70 +/- 18.99
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.7      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 23500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 23        |
|    time_elapsed    | 243       |
|    total_timesteps | 23552     |
----------------------------------
Eval num_timesteps=24000, episode_reward=-1026.80 +/- 58.68
Episode length: 96.72 +/- 22.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96.7          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 24000         |
| train/                  |               |
|    approx_kl            | 2.1536835e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000189     |
|    explained_variance   | 0.977         |
|    learning_rate        | 0.001         |
|    loss                 | 612           |
|    n_updates            | 230           |
|    policy_gradient_loss | -9.43e-07     |
|    value_loss           | 968           |
-------------------------------------------
Eval num_timesteps=24500, episode_reward=-1020.80 +/- 67.95
Episode length: 97.68 +/- 30.11
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.7      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 24500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.9      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 24        |
|    time_elapsed    | 254       |
|    total_timesteps | 24576     |
----------------------------------
Eval num_timesteps=25000, episode_reward=-1010.80 +/- 68.45
Episode length: 98.62 +/- 26.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.6         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 2.910383e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000187    |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 492          |
|    n_updates            | 240          |
|    policy_gradient_loss | -1.12e-06    |
|    value_loss           | 1.26e+03     |
------------------------------------------
Eval num_timesteps=25500, episode_reward=-1029.20 +/- 73.50
Episode length: 97.94 +/- 26.60
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.9      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 25500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99        |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 25        |
|    time_elapsed    | 265       |
|    total_timesteps | 25600     |
----------------------------------
Eval num_timesteps=26000, episode_reward=-1008.00 +/- 72.00
Episode length: 94.64 +/- 27.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.6         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 26000        |
| train/                  |              |
|    approx_kl            | 1.816079e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000495    |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 806          |
|    n_updates            | 250          |
|    policy_gradient_loss | -1.38e-05    |
|    value_loss           | 1.64e+03     |
------------------------------------------
Eval num_timesteps=26500, episode_reward=-1020.00 +/- 71.78
Episode length: 98.48 +/- 25.84
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.5      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 26500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 26        |
|    time_elapsed    | 275       |
|    total_timesteps | 26624     |
----------------------------------
Eval num_timesteps=27000, episode_reward=-1006.80 +/- 69.08
Episode length: 98.26 +/- 21.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.3         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 4.132744e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000531    |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 543          |
|    n_updates            | 260          |
|    policy_gradient_loss | -1.65e-06    |
|    value_loss           | 1.37e+03     |
------------------------------------------
Eval num_timesteps=27500, episode_reward=-1006.25 +/- 73.95
Episode length: 99.66 +/- 23.03
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.7      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 27500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97        |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 27        |
|    time_elapsed    | 286       |
|    total_timesteps | 27648     |
----------------------------------
Eval num_timesteps=28000, episode_reward=-1033.20 +/- 55.14
Episode length: 102.56 +/- 26.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 103         |
|    mean_reward          | -1.03e+03   |
| time/                   |             |
|    total_timesteps      | 28000       |
| train/                  |             |
|    approx_kl            | 9.19681e-09 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00057    |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.001       |
|    loss                 | 630         |
|    n_updates            | 270         |
|    policy_gradient_loss | 8.32e-06    |
|    value_loss           | 1.69e+03    |
-----------------------------------------
Eval num_timesteps=28500, episode_reward=-1008.40 +/- 67.40
Episode length: 92.76 +/- 25.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 92.8      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 28500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.2      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 28        |
|    time_elapsed    | 296       |
|    total_timesteps | 28672     |
----------------------------------
Eval num_timesteps=29000, episode_reward=-1032.80 +/- 64.05
Episode length: 100.86 +/- 24.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 101          |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 2.963352e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00227     |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 629          |
|    n_updates            | 280          |
|    policy_gradient_loss | -5.17e-05    |
|    value_loss           | 1.47e+03     |
------------------------------------------
Eval num_timesteps=29500, episode_reward=-1008.40 +/- 69.50
Episode length: 96.38 +/- 24.24
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.4      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 29500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.8      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 29        |
|    time_elapsed    | 307       |
|    total_timesteps | 29696     |
----------------------------------
Eval num_timesteps=30000, episode_reward=-1008.73 +/- 79.42
Episode length: 100.86 +/- 27.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 101           |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 30000         |
| train/                  |               |
|    approx_kl            | 5.7292054e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00405      |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.001         |
|    loss                 | 632           |
|    n_updates            | 290           |
|    policy_gradient_loss | -2.96e-05     |
|    value_loss           | 1.14e+03      |
-------------------------------------------
Eval num_timesteps=30500, episode_reward=-1017.84 +/- 58.27
Episode length: 100.56 +/- 27.85
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 30500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.9      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 30        |
|    time_elapsed    | 318       |
|    total_timesteps | 30720     |
----------------------------------
Eval num_timesteps=31000, episode_reward=-1012.64 +/- 65.60
Episode length: 95.70 +/- 20.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.7        |
|    mean_reward          | -1.01e+03   |
| time/                   |             |
|    total_timesteps      | 31000       |
| train/                  |             |
|    approx_kl            | 0.003391002 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0644     |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 569         |
|    n_updates            | 300         |
|    policy_gradient_loss | 6.2e-05     |
|    value_loss           | 1.14e+03    |
-----------------------------------------
Eval num_timesteps=31500, episode_reward=-1011.60 +/- 87.93
Episode length: 95.66 +/- 23.08
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.7      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 31500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.6      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 31        |
|    time_elapsed    | 328       |
|    total_timesteps | 31744     |
----------------------------------
Eval num_timesteps=32000, episode_reward=-1025.20 +/- 66.00
Episode length: 99.52 +/- 25.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 99.5       |
|    mean_reward          | -1.03e+03  |
| time/                   |            |
|    total_timesteps      | 32000      |
| train/                  |            |
|    approx_kl            | 0.14270115 |
|    clip_fraction        | 0.0132     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0327    |
|    explained_variance   | 0.966      |
|    learning_rate        | 0.001      |
|    loss                 | 720        |
|    n_updates            | 310        |
|    policy_gradient_loss | 0.00238    |
|    value_loss           | 1.14e+03   |
----------------------------------------
Eval num_timesteps=32500, episode_reward=-1030.36 +/- 59.64
Episode length: 99.68 +/- 22.99
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.7      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 32500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97        |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 32        |
|    time_elapsed    | 339       |
|    total_timesteps | 32768     |
----------------------------------
Eval num_timesteps=33000, episode_reward=-1025.20 +/- 55.31
Episode length: 93.82 +/- 24.68
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 93.8           |
|    mean_reward          | -1.03e+03      |
| time/                   |                |
|    total_timesteps      | 33000          |
| train/                  |                |
|    approx_kl            | -5.2386895e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.23e-05      |
|    explained_variance   | 0.957          |
|    learning_rate        | 0.001          |
|    loss                 | 700            |
|    n_updates            | 320            |
|    policy_gradient_loss | 9.39e-08       |
|    value_loss           | 1.39e+03       |
--------------------------------------------
Eval num_timesteps=33500, episode_reward=-1016.94 +/- 61.35
Episode length: 94.72 +/- 25.80
----------------------------------
| eval/              |           |
|    mean_ep_length  | 94.7      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 33500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.9      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 33        |
|    time_elapsed    | 349       |
|    total_timesteps | 33792     |
----------------------------------
Eval num_timesteps=34000, episode_reward=-1009.93 +/- 63.04
Episode length: 100.90 +/- 26.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 101           |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 34000         |
| train/                  |               |
|    approx_kl            | 3.4924597e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.08e-05     |
|    explained_variance   | 0.948         |
|    learning_rate        | 0.001         |
|    loss                 | 406           |
|    n_updates            | 330           |
|    policy_gradient_loss | 1.09e-08      |
|    value_loss           | 1.27e+03      |
-------------------------------------------
Eval num_timesteps=34500, episode_reward=-1030.70 +/- 54.08
Episode length: 99.58 +/- 24.85
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.6      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 34500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.6      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 34        |
|    time_elapsed    | 359       |
|    total_timesteps | 34816     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-1026.00 +/- 67.32
Episode length: 101.26 +/- 20.40
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 101            |
|    mean_reward          | -1.03e+03      |
| time/                   |                |
|    total_timesteps      | 35000          |
| train/                  |                |
|    approx_kl            | -3.4924597e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.18e-06      |
|    explained_variance   | 0.962          |
|    learning_rate        | 0.001          |
|    loss                 | 502            |
|    n_updates            | 340            |
|    policy_gradient_loss | 2.57e-08       |
|    value_loss           | 1.3e+03        |
--------------------------------------------
Eval num_timesteps=35500, episode_reward=-1032.40 +/- 45.55
Episode length: 102.32 +/- 25.19
----------------------------------
| eval/              |           |
|    mean_ep_length  | 102       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 35500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 35        |
|    time_elapsed    | 370       |
|    total_timesteps | 35840     |
----------------------------------
Eval num_timesteps=36000, episode_reward=-1009.60 +/- 70.71
Episode length: 96.04 +/- 27.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96            |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 36000         |
| train/                  |               |
|    approx_kl            | 2.3283064e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.67e-06     |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 634           |
|    n_updates            | 350           |
|    policy_gradient_loss | 6.34e-09      |
|    value_loss           | 1.24e+03      |
-------------------------------------------
Eval num_timesteps=36500, episode_reward=-1029.96 +/- 58.29
Episode length: 106.60 +/- 22.63
----------------------------------
| eval/              |           |
|    mean_ep_length  | 107       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 36500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 36        |
|    time_elapsed    | 381       |
|    total_timesteps | 36864     |
----------------------------------
Eval num_timesteps=37000, episode_reward=-1009.20 +/- 75.11
Episode length: 98.16 +/- 23.59
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 98.2           |
|    mean_reward          | -1.01e+03      |
| time/                   |                |
|    total_timesteps      | 37000          |
| train/                  |                |
|    approx_kl            | -6.9849193e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.22e-05      |
|    explained_variance   | 0.958          |
|    learning_rate        | 0.001          |
|    loss                 | 439            |
|    n_updates            | 360            |
|    policy_gradient_loss | -9.85e-08      |
|    value_loss           | 1.32e+03       |
--------------------------------------------
Eval num_timesteps=37500, episode_reward=-1031.38 +/- 60.26
Episode length: 101.16 +/- 23.51
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 37500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99        |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 37        |
|    time_elapsed    | 392       |
|    total_timesteps | 37888     |
----------------------------------
Eval num_timesteps=38000, episode_reward=-1017.97 +/- 58.56
Episode length: 96.96 +/- 24.18
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 97        |
|    mean_reward          | -1.02e+03 |
| time/                   |           |
|    total_timesteps      | 38000     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -1.32e-05 |
|    explained_variance   | 0.948     |
|    learning_rate        | 0.001     |
|    loss                 | 870       |
|    n_updates            | 370       |
|    policy_gradient_loss | 1.32e-07  |
|    value_loss           | 1.76e+03  |
---------------------------------------
Eval num_timesteps=38500, episode_reward=-1021.60 +/- 62.10
Episode length: 98.78 +/- 24.35
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.8      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 38500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.4      |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 38        |
|    time_elapsed    | 403       |
|    total_timesteps | 38912     |
----------------------------------
Eval num_timesteps=39000, episode_reward=-1028.40 +/- 60.65
Episode length: 99.60 +/- 23.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.6         |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 2.910383e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.37e-05    |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.001        |
|    loss                 | 479          |
|    n_updates            | 380          |
|    policy_gradient_loss | 2.31e-07     |
|    value_loss           | 1.51e+03     |
------------------------------------------
Eval num_timesteps=39500, episode_reward=-1023.20 +/- 73.84
Episode length: 96.70 +/- 24.12
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.7      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 39500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.2      |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 39        |
|    time_elapsed    | 413       |
|    total_timesteps | 39936     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-1010.00 +/- 80.42
Episode length: 87.42 +/- 20.66
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 87.4           |
|    mean_reward          | -1.01e+03      |
| time/                   |                |
|    total_timesteps      | 40000          |
| train/                  |                |
|    approx_kl            | -1.7462298e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -1.48e-05      |
|    explained_variance   | 0.954          |
|    learning_rate        | 0.001          |
|    loss                 | 420            |
|    n_updates            | 390            |
|    policy_gradient_loss | 1.47e-07       |
|    value_loss           | 1.33e+03       |
--------------------------------------------
Eval num_timesteps=40500, episode_reward=-1013.60 +/- 83.77
Episode length: 94.86 +/- 26.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 94.9      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 40500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 40        |
|    time_elapsed    | 423       |
|    total_timesteps | 40960     |
----------------------------------
Eval num_timesteps=41000, episode_reward=-1008.33 +/- 70.97
Episode length: 100.64 +/- 25.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 101           |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 41000         |
| train/                  |               |
|    approx_kl            | 3.4924597e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.97e-05     |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 287           |
|    n_updates            | 400           |
|    policy_gradient_loss | 2.49e-07      |
|    value_loss           | 783           |
-------------------------------------------
Eval num_timesteps=41500, episode_reward=-1023.60 +/- 67.75
Episode length: 100.10 +/- 27.07
----------------------------------
| eval/              |           |
|    mean_ep_length  | 100       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 41500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 41        |
|    time_elapsed    | 434       |
|    total_timesteps | 41984     |
----------------------------------
Eval num_timesteps=42000, episode_reward=-1022.72 +/- 65.67
Episode length: 100.10 +/- 23.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 100           |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 42000         |
| train/                  |               |
|    approx_kl            | 1.5133992e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.01e-05     |
|    explained_variance   | 0.956         |
|    learning_rate        | 0.001         |
|    loss                 | 625           |
|    n_updates            | 410           |
|    policy_gradient_loss | 6.68e-06      |
|    value_loss           | 1.31e+03      |
-------------------------------------------
Eval num_timesteps=42500, episode_reward=-1003.50 +/- 67.15
Episode length: 95.44 +/- 26.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.4     |
|    mean_reward     | -1e+03   |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-1010.80 +/- 75.24
Episode length: 91.80 +/- 23.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 91.8      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 43000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.8      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 42        |
|    time_elapsed    | 449       |
|    total_timesteps | 43008     |
----------------------------------
Eval num_timesteps=43500, episode_reward=-1025.20 +/- 67.08
Episode length: 97.12 +/- 21.80
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 97.1           |
|    mean_reward          | -1.03e+03      |
| time/                   |                |
|    total_timesteps      | 43500          |
| train/                  |                |
|    approx_kl            | -1.7462298e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.19e-06      |
|    explained_variance   | 0.964          |
|    learning_rate        | 0.001          |
|    loss                 | 378            |
|    n_updates            | 420            |
|    policy_gradient_loss | 2.91e-08       |
|    value_loss           | 1.16e+03       |
--------------------------------------------
Eval num_timesteps=44000, episode_reward=-1004.00 +/- 70.43
Episode length: 93.14 +/- 24.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | -1e+03   |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.8      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 43        |
|    time_elapsed    | 459       |
|    total_timesteps | 44032     |
----------------------------------
Eval num_timesteps=44500, episode_reward=-1034.40 +/- 59.04
Episode length: 102.50 +/- 26.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 102           |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 44500         |
| train/                  |               |
|    approx_kl            | 2.5029294e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.78e-05     |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 424           |
|    n_updates            | 430           |
|    policy_gradient_loss | -2.2e-06      |
|    value_loss           | 1.02e+03      |
-------------------------------------------
Eval num_timesteps=45000, episode_reward=-1021.20 +/- 56.40
Episode length: 94.62 +/- 26.61
----------------------------------
| eval/              |           |
|    mean_ep_length  | 94.6      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 45000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96        |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 44        |
|    time_elapsed    | 470       |
|    total_timesteps | 45056     |
----------------------------------
Eval num_timesteps=45500, episode_reward=-1008.80 +/- 83.11
Episode length: 95.94 +/- 25.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.9         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 45500        |
| train/                  |              |
|    approx_kl            | 9.313226e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000117    |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 650          |
|    n_updates            | 440          |
|    policy_gradient_loss | 7.65e-07     |
|    value_loss           | 1.28e+03     |
------------------------------------------
Eval num_timesteps=46000, episode_reward=-1013.60 +/- 81.65
Episode length: 95.14 +/- 24.03
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.1      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 46000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 45        |
|    time_elapsed    | 480       |
|    total_timesteps | 46080     |
----------------------------------
Eval num_timesteps=46500, episode_reward=-1021.93 +/- 69.03
Episode length: 99.48 +/- 21.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99.5          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 46500         |
| train/                  |               |
|    approx_kl            | 2.5029294e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000105     |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 699           |
|    n_updates            | 450           |
|    policy_gradient_loss | -2.28e-08     |
|    value_loss           | 1.18e+03      |
-------------------------------------------
Eval num_timesteps=47000, episode_reward=-1030.00 +/- 59.90
Episode length: 94.52 +/- 20.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 94.5      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 47000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.6      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 46        |
|    time_elapsed    | 490       |
|    total_timesteps | 47104     |
----------------------------------
Eval num_timesteps=47500, episode_reward=-1018.40 +/- 67.25
Episode length: 95.64 +/- 19.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 95.6          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 47500         |
| train/                  |               |
|    approx_kl            | 4.0745363e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.06e-05     |
|    explained_variance   | 0.973         |
|    learning_rate        | 0.001         |
|    loss                 | 669           |
|    n_updates            | 460           |
|    policy_gradient_loss | 3.97e-07      |
|    value_loss           | 833           |
-------------------------------------------
Eval num_timesteps=48000, episode_reward=-1032.00 +/- 65.24
Episode length: 101.26 +/- 22.30
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 48000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 47        |
|    time_elapsed    | 501       |
|    total_timesteps | 48128     |
----------------------------------
Eval num_timesteps=48500, episode_reward=-1028.00 +/- 62.74
Episode length: 95.18 +/- 29.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 95.2          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 48500         |
| train/                  |               |
|    approx_kl            | 6.4028427e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -3.92e-05     |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 295           |
|    n_updates            | 470           |
|    policy_gradient_loss | 3.33e-07      |
|    value_loss           | 1.16e+03      |
-------------------------------------------
Eval num_timesteps=49000, episode_reward=-1018.00 +/- 66.72
Episode length: 95.18 +/- 21.76
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.2      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 49000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 48        |
|    time_elapsed    | 512       |
|    total_timesteps | 49152     |
----------------------------------
Eval num_timesteps=49500, episode_reward=-1021.60 +/- 64.38
Episode length: 98.70 +/- 26.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 98.7          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 49500         |
| train/                  |               |
|    approx_kl            | 1.6298145e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000119     |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 576           |
|    n_updates            | 480           |
|    policy_gradient_loss | -1.88e-06     |
|    value_loss           | 1.18e+03      |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=-1019.20 +/- 73.36
Episode length: 99.84 +/- 20.78
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.8      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 50000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.5      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 49        |
|    time_elapsed    | 522       |
|    total_timesteps | 50176     |
----------------------------------
Eval num_timesteps=50500, episode_reward=-1011.20 +/- 76.16
Episode length: 98.70 +/- 28.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 98.7          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 50500         |
| train/                  |               |
|    approx_kl            | 2.2118911e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000179     |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.001         |
|    loss                 | 726           |
|    n_updates            | 490           |
|    policy_gradient_loss | 1.58e-06      |
|    value_loss           | 1.1e+03       |
-------------------------------------------
Eval num_timesteps=51000, episode_reward=-1039.60 +/- 55.96
Episode length: 96.46 +/- 22.66
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.5      |
|    mean_reward     | -1.04e+03 |
| time/              |           |
|    total_timesteps | 51000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.5      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 50        |
|    time_elapsed    | 533       |
|    total_timesteps | 51200     |
----------------------------------
Eval num_timesteps=51500, episode_reward=-995.20 +/- 95.66
Episode length: 92.02 +/- 22.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92          |
|    mean_reward          | -995        |
| time/                   |             |
|    total_timesteps      | 51500       |
| train/                  |             |
|    approx_kl            | 5.47152e-09 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000344   |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.001       |
|    loss                 | 847         |
|    n_updates            | 500         |
|    policy_gradient_loss | 5.17e-06    |
|    value_loss           | 1.29e+03    |
-----------------------------------------
Eval num_timesteps=52000, episode_reward=-1035.54 +/- 48.93
Episode length: 104.64 +/- 26.92
----------------------------------
| eval/              |           |
|    mean_ep_length  | 105       |
|    mean_reward     | -1.04e+03 |
| time/              |           |
|    total_timesteps | 52000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 51        |
|    time_elapsed    | 543       |
|    total_timesteps | 52224     |
----------------------------------
Eval num_timesteps=52500, episode_reward=-1010.00 +/- 61.74
Episode length: 99.72 +/- 26.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99.7          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 52500         |
| train/                  |               |
|    approx_kl            | 1.2805685e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000274     |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.001         |
|    loss                 | 653           |
|    n_updates            | 510           |
|    policy_gradient_loss | 1.97e-06      |
|    value_loss           | 1.27e+03      |
-------------------------------------------
Eval num_timesteps=53000, episode_reward=-1012.40 +/- 74.26
Episode length: 99.08 +/- 23.03
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.1      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 53000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 52        |
|    time_elapsed    | 554       |
|    total_timesteps | 53248     |
----------------------------------
Eval num_timesteps=53500, episode_reward=-1020.35 +/- 71.33
Episode length: 97.52 +/- 28.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.5          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 53500         |
| train/                  |               |
|    approx_kl            | 7.9744495e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000162     |
|    explained_variance   | 0.943         |
|    learning_rate        | 0.001         |
|    loss                 | 357           |
|    n_updates            | 520           |
|    policy_gradient_loss | 1.32e-05      |
|    value_loss           | 1.43e+03      |
-------------------------------------------
Eval num_timesteps=54000, episode_reward=-1004.40 +/- 71.79
Episode length: 97.70 +/- 23.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.7     |
|    mean_reward     | -1e+03   |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.5      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 53        |
|    time_elapsed    | 564       |
|    total_timesteps | 54272     |
----------------------------------
Eval num_timesteps=54500, episode_reward=-1032.00 +/- 60.13
Episode length: 99.02 +/- 22.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99            |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 54500         |
| train/                  |               |
|    approx_kl            | 1.1059456e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000127     |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 510           |
|    n_updates            | 530           |
|    policy_gradient_loss | -4.84e-07     |
|    value_loss           | 909           |
-------------------------------------------
Eval num_timesteps=55000, episode_reward=-1029.94 +/- 72.39
Episode length: 96.46 +/- 24.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.5      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 55000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 54        |
|    time_elapsed    | 575       |
|    total_timesteps | 55296     |
----------------------------------
Eval num_timesteps=55500, episode_reward=-1020.00 +/- 76.94
Episode length: 93.44 +/- 26.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 93.4         |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 55500        |
| train/                  |              |
|    approx_kl            | 8.381903e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000141    |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 440          |
|    n_updates            | 540          |
|    policy_gradient_loss | 7.54e-06     |
|    value_loss           | 1.15e+03     |
------------------------------------------
Eval num_timesteps=56000, episode_reward=-1023.05 +/- 66.77
Episode length: 97.92 +/- 25.88
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.9      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 56000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.6      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 55        |
|    time_elapsed    | 585       |
|    total_timesteps | 56320     |
----------------------------------
Eval num_timesteps=56500, episode_reward=-1014.80 +/- 64.43
Episode length: 99.52 +/- 23.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.5         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 56500        |
| train/                  |              |
|    approx_kl            | 4.598405e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000445    |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 539          |
|    n_updates            | 550          |
|    policy_gradient_loss | 4.58e-06     |
|    value_loss           | 1e+03        |
------------------------------------------
Eval num_timesteps=57000, episode_reward=-1027.60 +/- 61.59
Episode length: 98.96 +/- 22.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99        |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 57000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 56        |
|    time_elapsed    | 596       |
|    total_timesteps | 57344     |
----------------------------------
Eval num_timesteps=57500, episode_reward=-1026.80 +/- 55.89
Episode length: 96.82 +/- 23.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96.8          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 57500         |
| train/                  |               |
|    approx_kl            | 2.7939677e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000189     |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.001         |
|    loss                 | 535           |
|    n_updates            | 560           |
|    policy_gradient_loss | 1.96e-06      |
|    value_loss           | 1.27e+03      |
-------------------------------------------
Eval num_timesteps=58000, episode_reward=-1008.40 +/- 81.97
Episode length: 97.64 +/- 23.55
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.6      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 58000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.2      |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 57        |
|    time_elapsed    | 606       |
|    total_timesteps | 58368     |
----------------------------------
Eval num_timesteps=58500, episode_reward=-1030.00 +/- 65.02
Episode length: 93.58 +/- 21.81
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 93.6          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 58500         |
| train/                  |               |
|    approx_kl            | 2.7939677e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000166     |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.001         |
|    loss                 | 732           |
|    n_updates            | 570           |
|    policy_gradient_loss | 4.92e-06      |
|    value_loss           | 1.36e+03      |
-------------------------------------------
Eval num_timesteps=59000, episode_reward=-1010.00 +/- 70.91
Episode length: 97.86 +/- 25.85
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.9      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 59000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93        |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 58        |
|    time_elapsed    | 616       |
|    total_timesteps | 59392     |
----------------------------------
Eval num_timesteps=59500, episode_reward=-1026.67 +/- 56.11
Episode length: 89.26 +/- 22.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 89.3         |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 2.165325e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.38e-05    |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 467          |
|    n_updates            | 580          |
|    policy_gradient_loss | 1.31e-05     |
|    value_loss           | 1.21e+03     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-1031.60 +/- 52.20
Episode length: 98.20 +/- 28.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.2      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 60000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.6      |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 59        |
|    time_elapsed    | 626       |
|    total_timesteps | 60416     |
----------------------------------
Eval num_timesteps=60500, episode_reward=-1027.20 +/- 47.02
Episode length: 99.14 +/- 28.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99.1          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 60500         |
| train/                  |               |
|    approx_kl            | 2.9685907e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.47e-05     |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.001         |
|    loss                 | 442           |
|    n_updates            | 590           |
|    policy_gradient_loss | -5.74e-07     |
|    value_loss           | 1.07e+03      |
-------------------------------------------
Eval num_timesteps=61000, episode_reward=-1032.00 +/- 58.92
Episode length: 95.88 +/- 24.53
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.9      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 61000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.8      |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 60        |
|    time_elapsed    | 637       |
|    total_timesteps | 61440     |
----------------------------------
Eval num_timesteps=61500, episode_reward=-1013.60 +/- 65.44
Episode length: 94.28 +/- 20.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 94.3          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 61500         |
| train/                  |               |
|    approx_kl            | 3.4924597e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000173     |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.001         |
|    loss                 | 424           |
|    n_updates            | 600           |
|    policy_gradient_loss | -2.15e-06     |
|    value_loss           | 957           |
-------------------------------------------
Eval num_timesteps=62000, episode_reward=-1028.00 +/- 68.00
Episode length: 92.78 +/- 21.32
----------------------------------
| eval/              |           |
|    mean_ep_length  | 92.8      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 62000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 92.8      |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 61        |
|    time_elapsed    | 647       |
|    total_timesteps | 62464     |
----------------------------------
Eval num_timesteps=62500, episode_reward=-1015.60 +/- 75.10
Episode length: 99.92 +/- 22.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99.9          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 62500         |
| train/                  |               |
|    approx_kl            | 2.2700988e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000232     |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 349           |
|    n_updates            | 610           |
|    policy_gradient_loss | -7.47e-07     |
|    value_loss           | 1.13e+03      |
-------------------------------------------
Eval num_timesteps=63000, episode_reward=-1017.92 +/- 73.18
Episode length: 104.92 +/- 26.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 105       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 63000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.6      |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 62        |
|    time_elapsed    | 658       |
|    total_timesteps | 63488     |
----------------------------------
Eval num_timesteps=63500, episode_reward=-1027.20 +/- 60.42
Episode length: 98.80 +/- 22.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 98.8          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 63500         |
| train/                  |               |
|    approx_kl            | 1.0302756e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000333     |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 381           |
|    n_updates            | 620           |
|    policy_gradient_loss | 2.44e-06      |
|    value_loss           | 918           |
-------------------------------------------
Eval num_timesteps=64000, episode_reward=-1013.20 +/- 81.09
Episode length: 95.74 +/- 23.80
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.7      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 64000     |
----------------------------------
Eval num_timesteps=64500, episode_reward=-1025.94 +/- 68.79
Episode length: 97.16 +/- 20.02
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.2      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 64500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.5      |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 63        |
|    time_elapsed    | 672       |
|    total_timesteps | 64512     |
----------------------------------
Eval num_timesteps=65000, episode_reward=-1015.20 +/- 86.16
Episode length: 95.14 +/- 23.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 95.1          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 65000         |
| train/                  |               |
|    approx_kl            | 1.0477379e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000129     |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.001         |
|    loss                 | 326           |
|    n_updates            | 630           |
|    policy_gradient_loss | 3.68e-07      |
|    value_loss           | 1.01e+03      |
-------------------------------------------
Eval num_timesteps=65500, episode_reward=-1019.60 +/- 61.41
Episode length: 96.30 +/- 23.31
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.3      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 65500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.5      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 64        |
|    time_elapsed    | 682       |
|    total_timesteps | 65536     |
----------------------------------
Eval num_timesteps=66000, episode_reward=-1015.96 +/- 63.74
Episode length: 96.44 +/- 26.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.4         |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0001506969 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00018     |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 555          |
|    n_updates            | 640          |
|    policy_gradient_loss | -1.28e-06    |
|    value_loss           | 1.75e+03     |
------------------------------------------
Eval num_timesteps=66500, episode_reward=-1023.60 +/- 63.23
Episode length: 100.70 +/- 22.43
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 66500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 65        |
|    time_elapsed    | 693       |
|    total_timesteps | 66560     |
----------------------------------
Eval num_timesteps=67000, episode_reward=-1015.99 +/- 59.19
Episode length: 97.88 +/- 23.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.9          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 67000         |
| train/                  |               |
|    approx_kl            | 1.0069925e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000511     |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.001         |
|    loss                 | 319           |
|    n_updates            | 650           |
|    policy_gradient_loss | 6.64e-06      |
|    value_loss           | 1.18e+03      |
-------------------------------------------
Eval num_timesteps=67500, episode_reward=-1025.20 +/- 71.91
Episode length: 95.72 +/- 29.35
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.7      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 67500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.5      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 66        |
|    time_elapsed    | 703       |
|    total_timesteps | 67584     |
----------------------------------
Eval num_timesteps=68000, episode_reward=-1011.55 +/- 71.95
Episode length: 99.88 +/- 26.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99.9          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 68000         |
| train/                  |               |
|    approx_kl            | 2.0954758e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000358     |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.001         |
|    loss                 | 751           |
|    n_updates            | 660           |
|    policy_gradient_loss | 9.74e-06      |
|    value_loss           | 1.72e+03      |
-------------------------------------------
Eval num_timesteps=68500, episode_reward=-1016.40 +/- 53.29
Episode length: 97.32 +/- 22.24
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.3      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 68500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.9      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 67        |
|    time_elapsed    | 714       |
|    total_timesteps | 68608     |
----------------------------------
Eval num_timesteps=69000, episode_reward=-1011.20 +/- 65.31
Episode length: 96.32 +/- 22.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.3         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 69000        |
| train/                  |              |
|    approx_kl            | 2.386514e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000246    |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 222          |
|    n_updates            | 670          |
|    policy_gradient_loss | 7.66e-07     |
|    value_loss           | 760          |
------------------------------------------
Eval num_timesteps=69500, episode_reward=-1018.40 +/- 73.83
Episode length: 97.10 +/- 25.07
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.1      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 69500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99        |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 68        |
|    time_elapsed    | 724       |
|    total_timesteps | 69632     |
----------------------------------
Eval num_timesteps=70000, episode_reward=-1024.40 +/- 60.28
Episode length: 98.96 +/- 25.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99           |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 70000        |
| train/                  |              |
|    approx_kl            | 5.820766e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000346    |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.001        |
|    loss                 | 384          |
|    n_updates            | 680          |
|    policy_gradient_loss | 2.24e-06     |
|    value_loss           | 910          |
------------------------------------------
Eval num_timesteps=70500, episode_reward=-1021.40 +/- 66.96
Episode length: 99.76 +/- 30.66
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.8      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 70500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.8      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 69        |
|    time_elapsed    | 735       |
|    total_timesteps | 70656     |
----------------------------------
Eval num_timesteps=71000, episode_reward=-1010.80 +/- 67.15
Episode length: 90.74 +/- 20.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 90.7          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 71000         |
| train/                  |               |
|    approx_kl            | 5.9371814e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000355     |
|    explained_variance   | 0.955         |
|    learning_rate        | 0.001         |
|    loss                 | 535           |
|    n_updates            | 690           |
|    policy_gradient_loss | 4.67e-06      |
|    value_loss           | 1.35e+03      |
-------------------------------------------
Eval num_timesteps=71500, episode_reward=-1017.60 +/- 62.84
Episode length: 95.34 +/- 25.69
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.3      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 71500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.6      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 70        |
|    time_elapsed    | 745       |
|    total_timesteps | 71680     |
----------------------------------
Eval num_timesteps=72000, episode_reward=-1024.00 +/- 61.97
Episode length: 97.80 +/- 28.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.8          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 72000         |
| train/                  |               |
|    approx_kl            | 1.9208528e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000326     |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.001         |
|    loss                 | 638           |
|    n_updates            | 700           |
|    policy_gradient_loss | 2.99e-06      |
|    value_loss           | 1.29e+03      |
-------------------------------------------
Eval num_timesteps=72500, episode_reward=-1023.60 +/- 68.92
Episode length: 103.44 +/- 25.55
----------------------------------
| eval/              |           |
|    mean_ep_length  | 103       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 72500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.5      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 71        |
|    time_elapsed    | 755       |
|    total_timesteps | 72704     |
----------------------------------
Eval num_timesteps=73000, episode_reward=-1009.20 +/- 61.27
Episode length: 98.70 +/- 29.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 98.7          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 73000         |
| train/                  |               |
|    approx_kl            | 7.1013346e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00053      |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 339           |
|    n_updates            | 710           |
|    policy_gradient_loss | 2.79e-06      |
|    value_loss           | 971           |
-------------------------------------------
Eval num_timesteps=73500, episode_reward=-1010.70 +/- 81.71
Episode length: 101.48 +/- 29.17
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 73500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 72        |
|    time_elapsed    | 766       |
|    total_timesteps | 73728     |
----------------------------------
Eval num_timesteps=74000, episode_reward=-1015.99 +/- 66.57
Episode length: 105.96 +/- 28.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 106           |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 74000         |
| train/                  |               |
|    approx_kl            | 2.3923349e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000937     |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 650           |
|    n_updates            | 720           |
|    policy_gradient_loss | 6.16e-06      |
|    value_loss           | 1.07e+03      |
-------------------------------------------
Eval num_timesteps=74500, episode_reward=-1040.00 +/- 64.25
Episode length: 96.28 +/- 23.22
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.3      |
|    mean_reward     | -1.04e+03 |
| time/              |           |
|    total_timesteps | 74500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.8      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 73        |
|    time_elapsed    | 777       |
|    total_timesteps | 74752     |
----------------------------------
Eval num_timesteps=75000, episode_reward=-1024.59 +/- 69.56
Episode length: 95.50 +/- 24.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 95.5          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 75000         |
| train/                  |               |
|    approx_kl            | 1.8218998e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000574     |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.001         |
|    loss                 | 434           |
|    n_updates            | 730           |
|    policy_gradient_loss | -6.08e-06     |
|    value_loss           | 1.28e+03      |
-------------------------------------------
Eval num_timesteps=75500, episode_reward=-1038.40 +/- 50.22
Episode length: 106.22 +/- 31.56
----------------------------------
| eval/              |           |
|    mean_ep_length  | 106       |
|    mean_reward     | -1.04e+03 |
| time/              |           |
|    total_timesteps | 75500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.6      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 74        |
|    time_elapsed    | 787       |
|    total_timesteps | 75776     |
----------------------------------
Eval num_timesteps=76000, episode_reward=-1031.60 +/- 53.11
Episode length: 99.80 +/- 21.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.8         |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 9.080395e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000603    |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 631          |
|    n_updates            | 740          |
|    policy_gradient_loss | 1.05e-05     |
|    value_loss           | 1.21e+03     |
------------------------------------------
Eval num_timesteps=76500, episode_reward=-992.80 +/- 79.44
Episode length: 97.22 +/- 25.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.2     |
|    mean_reward     | -993     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 75        |
|    time_elapsed    | 798       |
|    total_timesteps | 76800     |
----------------------------------
Eval num_timesteps=77000, episode_reward=-1006.40 +/- 67.32
Episode length: 93.58 +/- 23.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 93.6          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 77000         |
| train/                  |               |
|    approx_kl            | 3.2014214e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000471     |
|    explained_variance   | 0.944         |
|    learning_rate        | 0.001         |
|    loss                 | 890           |
|    n_updates            | 750           |
|    policy_gradient_loss | -2.52e-07     |
|    value_loss           | 1.68e+03      |
-------------------------------------------
Eval num_timesteps=77500, episode_reward=-1033.60 +/- 50.86
Episode length: 104.96 +/- 25.63
----------------------------------
| eval/              |           |
|    mean_ep_length  | 105       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 77500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.8      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 76        |
|    time_elapsed    | 808       |
|    total_timesteps | 77824     |
----------------------------------
Eval num_timesteps=78000, episode_reward=-1020.40 +/- 64.54
Episode length: 99.68 +/- 26.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.7         |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 9.837095e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00035     |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 472          |
|    n_updates            | 760          |
|    policy_gradient_loss | 9.8e-06      |
|    value_loss           | 1.03e+03     |
------------------------------------------
Eval num_timesteps=78500, episode_reward=-1031.60 +/- 74.76
Episode length: 102.58 +/- 25.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 103       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 78500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 77        |
|    time_elapsed    | 819       |
|    total_timesteps | 78848     |
----------------------------------
Eval num_timesteps=79000, episode_reward=-1036.80 +/- 57.96
Episode length: 101.40 +/- 26.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 101           |
|    mean_reward          | -1.04e+03     |
| time/                   |               |
|    total_timesteps      | 79000         |
| train/                  |               |
|    approx_kl            | 1.3737008e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000427     |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.001         |
|    loss                 | 389           |
|    n_updates            | 770           |
|    policy_gradient_loss | 3.55e-06      |
|    value_loss           | 774           |
-------------------------------------------
Eval num_timesteps=79500, episode_reward=-1010.00 +/- 72.03
Episode length: 98.70 +/- 26.61
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.7      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 79500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 78        |
|    time_elapsed    | 829       |
|    total_timesteps | 79872     |
----------------------------------
Eval num_timesteps=80000, episode_reward=-1028.00 +/- 69.17
Episode length: 103.94 +/- 21.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 5.296897e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00105     |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 356          |
|    n_updates            | 780          |
|    policy_gradient_loss | 1.75e-05     |
|    value_loss           | 995          |
------------------------------------------
Eval num_timesteps=80500, episode_reward=-1006.00 +/- 57.59
Episode length: 96.36 +/- 24.95
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.4      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 80500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 79        |
|    time_elapsed    | 840       |
|    total_timesteps | 80896     |
----------------------------------
Eval num_timesteps=81000, episode_reward=-1012.39 +/- 75.14
Episode length: 97.90 +/- 28.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.9          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 81000         |
| train/                  |               |
|    approx_kl            | 3.9039878e-07 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000887     |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.001         |
|    loss                 | 559           |
|    n_updates            | 790           |
|    policy_gradient_loss | 1.49e-05      |
|    value_loss           | 1.14e+03      |
-------------------------------------------
Eval num_timesteps=81500, episode_reward=-1016.40 +/- 71.27
Episode length: 102.90 +/- 26.87
----------------------------------
| eval/              |           |
|    mean_ep_length  | 103       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 81500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.1      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 80        |
|    time_elapsed    | 850       |
|    total_timesteps | 81920     |
----------------------------------
Eval num_timesteps=82000, episode_reward=-1011.60 +/- 77.08
Episode length: 101.04 +/- 25.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 101           |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 82000         |
| train/                  |               |
|    approx_kl            | 1.7636921e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000954     |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.001         |
|    loss                 | 484           |
|    n_updates            | 800           |
|    policy_gradient_loss | -1.17e-06     |
|    value_loss           | 889           |
-------------------------------------------
Eval num_timesteps=82500, episode_reward=-1014.00 +/- 74.97
Episode length: 101.74 +/- 28.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 102       |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 82500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95        |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 81        |
|    time_elapsed    | 861       |
|    total_timesteps | 82944     |
----------------------------------
Eval num_timesteps=83000, episode_reward=-1035.56 +/- 56.02
Episode length: 97.28 +/- 20.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.3         |
|    mean_reward          | -1.04e+03    |
| time/                   |              |
|    total_timesteps      | 83000        |
| train/                  |              |
|    approx_kl            | 7.625204e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000652    |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 376          |
|    n_updates            | 810          |
|    policy_gradient_loss | 7.28e-06     |
|    value_loss           | 1.06e+03     |
------------------------------------------
Eval num_timesteps=83500, episode_reward=-1038.80 +/- 69.51
Episode length: 102.74 +/- 27.82
----------------------------------
| eval/              |           |
|    mean_ep_length  | 103       |
|    mean_reward     | -1.04e+03 |
| time/              |           |
|    total_timesteps | 83500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95        |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 82        |
|    time_elapsed    | 872       |
|    total_timesteps | 83968     |
----------------------------------
Eval num_timesteps=84000, episode_reward=-1023.60 +/- 67.28
Episode length: 94.24 +/- 24.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.2         |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 3.282912e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000956    |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 295          |
|    n_updates            | 820          |
|    policy_gradient_loss | 1.32e-05     |
|    value_loss           | 823          |
------------------------------------------
Eval num_timesteps=84500, episode_reward=-1028.80 +/- 61.42
Episode length: 96.20 +/- 22.15
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.2      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 84500     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.4      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 83        |
|    time_elapsed    | 882       |
|    total_timesteps | 84992     |
----------------------------------
Eval num_timesteps=85000, episode_reward=-1007.06 +/- 74.49
Episode length: 102.22 +/- 31.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 102           |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 85000         |
| train/                  |               |
|    approx_kl            | 1.9732397e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00055      |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 324           |
|    n_updates            | 830           |
|    policy_gradient_loss | 1.06e-05      |
|    value_loss           | 938           |
-------------------------------------------
Eval num_timesteps=85500, episode_reward=-1027.07 +/- 58.21
Episode length: 101.20 +/- 26.71
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 85500     |
----------------------------------
Eval num_timesteps=86000, episode_reward=-1023.60 +/- 68.69
Episode length: 99.80 +/- 22.96
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.8      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 86000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 84        |
|    time_elapsed    | 897       |
|    total_timesteps | 86016     |
----------------------------------
Eval num_timesteps=86500, episode_reward=-1037.12 +/- 57.12
Episode length: 96.70 +/- 28.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.7         |
|    mean_reward          | -1.04e+03    |
| time/                   |              |
|    total_timesteps      | 86500        |
| train/                  |              |
|    approx_kl            | 2.142042e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000859    |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 340          |
|    n_updates            | 840          |
|    policy_gradient_loss | 4.74e-06     |
|    value_loss           | 796          |
------------------------------------------
Eval num_timesteps=87000, episode_reward=-1017.20 +/- 79.64
Episode length: 103.28 +/- 25.88
----------------------------------
| eval/              |           |
|    mean_ep_length  | 103       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 87000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98        |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 85        |
|    time_elapsed    | 908       |
|    total_timesteps | 87040     |
----------------------------------
Eval num_timesteps=87500, episode_reward=-1026.00 +/- 70.68
Episode length: 102.00 +/- 25.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 102          |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 87500        |
| train/                  |              |
|    approx_kl            | 6.641494e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00202     |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.001        |
|    loss                 | 676          |
|    n_updates            | 850          |
|    policy_gradient_loss | 2.11e-05     |
|    value_loss           | 1.63e+03     |
------------------------------------------
Eval num_timesteps=88000, episode_reward=-1029.60 +/- 58.85
Episode length: 97.96 +/- 29.74
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98        |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 88000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.6      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 86        |
|    time_elapsed    | 918       |
|    total_timesteps | 88064     |
----------------------------------
Eval num_timesteps=88500, episode_reward=-1019.01 +/- 65.79
Episode length: 93.50 +/- 23.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.5        |
|    mean_reward          | -1.02e+03   |
| time/                   |             |
|    total_timesteps      | 88500       |
| train/                  |             |
|    approx_kl            | 5.92554e-08 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00232    |
|    explained_variance   | 0.945       |
|    learning_rate        | 0.001       |
|    loss                 | 527         |
|    n_updates            | 860         |
|    policy_gradient_loss | 5.26e-05    |
|    value_loss           | 1.37e+03    |
-----------------------------------------
Eval num_timesteps=89000, episode_reward=-1018.00 +/- 61.22
Episode length: 92.58 +/- 22.37
----------------------------------
| eval/              |           |
|    mean_ep_length  | 92.6      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 89000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.9      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 87        |
|    time_elapsed    | 928       |
|    total_timesteps | 89088     |
----------------------------------
Eval num_timesteps=89500, episode_reward=-1016.30 +/- 66.74
Episode length: 98.20 +/- 26.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.2         |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 89500        |
| train/                  |              |
|    approx_kl            | 1.291628e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00275     |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.001        |
|    loss                 | 498          |
|    n_updates            | 870          |
|    policy_gradient_loss | -1.46e-05    |
|    value_loss           | 1.23e+03     |
------------------------------------------
Eval num_timesteps=90000, episode_reward=-1033.20 +/- 58.65
Episode length: 100.68 +/- 24.61
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 90000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.1      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 88        |
|    time_elapsed    | 939       |
|    total_timesteps | 90112     |
----------------------------------
Eval num_timesteps=90500, episode_reward=-1027.56 +/- 59.20
Episode length: 98.22 +/- 25.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 98.2          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 90500         |
| train/                  |               |
|    approx_kl            | 8.4452215e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00477      |
|    explained_variance   | 0.947         |
|    learning_rate        | 0.001         |
|    loss                 | 361           |
|    n_updates            | 880           |
|    policy_gradient_loss | 1.64e-05      |
|    value_loss           | 1.11e+03      |
-------------------------------------------
Eval num_timesteps=91000, episode_reward=-1024.40 +/- 75.80
Episode length: 97.24 +/- 27.55
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.2      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 91000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99        |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 89        |
|    time_elapsed    | 949       |
|    total_timesteps | 91136     |
----------------------------------
Eval num_timesteps=91500, episode_reward=-1025.60 +/- 65.76
Episode length: 95.28 +/- 24.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 95.3          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 91500         |
| train/                  |               |
|    approx_kl            | 1.4134566e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00411      |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.001         |
|    loss                 | 680           |
|    n_updates            | 890           |
|    policy_gradient_loss | 5.77e-05      |
|    value_loss           | 1.27e+03      |
-------------------------------------------
Eval num_timesteps=92000, episode_reward=-1031.82 +/- 57.74
Episode length: 100.54 +/- 25.71
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 92000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.2      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 95        |
|    iterations      | 90        |
|    time_elapsed    | 960       |
|    total_timesteps | 92160     |
----------------------------------
Eval num_timesteps=92500, episode_reward=-1019.20 +/- 71.93
Episode length: 91.24 +/- 22.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 91.2          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 92500         |
| train/                  |               |
|    approx_kl            | 3.6915299e-07 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00349      |
|    explained_variance   | 0.943         |
|    learning_rate        | 0.001         |
|    loss                 | 728           |
|    n_updates            | 900           |
|    policy_gradient_loss | 9.38e-05      |
|    value_loss           | 1.45e+03      |
-------------------------------------------
Eval num_timesteps=93000, episode_reward=-1018.30 +/- 76.96
Episode length: 93.70 +/- 24.77
----------------------------------
| eval/              |           |
|    mean_ep_length  | 93.7      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 93000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98        |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 91        |
|    time_elapsed    | 969       |
|    total_timesteps | 93184     |
----------------------------------
Eval num_timesteps=93500, episode_reward=-1033.60 +/- 53.31
Episode length: 102.36 +/- 23.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 102          |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 93500        |
| train/                  |              |
|    approx_kl            | 0.0011010859 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00292     |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.001        |
|    loss                 | 387          |
|    n_updates            | 910          |
|    policy_gradient_loss | 5.67e-05     |
|    value_loss           | 1.36e+03     |
------------------------------------------
Eval num_timesteps=94000, episode_reward=-1007.20 +/- 69.42
Episode length: 95.06 +/- 24.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.1      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 94000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.5      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 92        |
|    time_elapsed    | 980       |
|    total_timesteps | 94208     |
----------------------------------
Eval num_timesteps=94500, episode_reward=-1038.00 +/- 54.15
Episode length: 98.08 +/- 23.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 98.1          |
|    mean_reward          | -1.04e+03     |
| time/                   |               |
|    total_timesteps      | 94500         |
| train/                  |               |
|    approx_kl            | 1.0340009e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00277      |
|    explained_variance   | 0.949         |
|    learning_rate        | 0.001         |
|    loss                 | 904           |
|    n_updates            | 920           |
|    policy_gradient_loss | 9.33e-06      |
|    value_loss           | 1.32e+03      |
-------------------------------------------
Eval num_timesteps=95000, episode_reward=-1014.00 +/- 69.88
Episode length: 97.42 +/- 23.55
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.4      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 95000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.6      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 93        |
|    time_elapsed    | 990       |
|    total_timesteps | 95232     |
----------------------------------
Eval num_timesteps=95500, episode_reward=-1002.80 +/- 80.36
Episode length: 94.86 +/- 27.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 94.9          |
|    mean_reward          | -1e+03        |
| time/                   |               |
|    total_timesteps      | 95500         |
| train/                  |               |
|    approx_kl            | 3.0646333e-06 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00285      |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.001         |
|    loss                 | 453           |
|    n_updates            | 930           |
|    policy_gradient_loss | 0.000118      |
|    value_loss           | 1.04e+03      |
-------------------------------------------
Eval num_timesteps=96000, episode_reward=-1003.60 +/- 84.46
Episode length: 93.76 +/- 23.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | -1e+03   |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.2      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 94        |
|    time_elapsed    | 1000      |
|    total_timesteps | 96256     |
----------------------------------
Eval num_timesteps=96500, episode_reward=-1018.40 +/- 61.66
Episode length: 97.26 +/- 22.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.3          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 96500         |
| train/                  |               |
|    approx_kl            | 0.00034668943 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00341      |
|    explained_variance   | 0.935         |
|    learning_rate        | 0.001         |
|    loss                 | 1.57e+03      |
|    n_updates            | 940           |
|    policy_gradient_loss | -3.67e-05     |
|    value_loss           | 1.83e+03      |
-------------------------------------------
Eval num_timesteps=97000, episode_reward=-1028.40 +/- 61.04
Episode length: 98.08 +/- 22.82
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.1      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 97000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.4      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 95        |
|    time_elapsed    | 1010      |
|    total_timesteps | 97280     |
----------------------------------
Eval num_timesteps=97500, episode_reward=-1019.20 +/- 65.77
Episode length: 105.56 +/- 28.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 106          |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 97500        |
| train/                  |              |
|    approx_kl            | 0.0007350537 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00365     |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 695          |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.0001      |
|    value_loss           | 909          |
------------------------------------------
Eval num_timesteps=98000, episode_reward=-1042.25 +/- 45.23
Episode length: 95.12 +/- 22.57
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.1      |
|    mean_reward     | -1.04e+03 |
| time/              |           |
|    total_timesteps | 98000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.8      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 96        |
|    time_elapsed    | 1021      |
|    total_timesteps | 98304     |
----------------------------------
Eval num_timesteps=98500, episode_reward=-1024.40 +/- 68.60
Episode length: 103.66 +/- 25.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 104           |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 98500         |
| train/                  |               |
|    approx_kl            | 1.7171085e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00799      |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 880           |
|    n_updates            | 960           |
|    policy_gradient_loss | -0.000208     |
|    value_loss           | 988           |
-------------------------------------------
Eval num_timesteps=99000, episode_reward=-1022.00 +/- 74.97
Episode length: 100.88 +/- 29.55
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 99000     |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.1      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 97        |
|    time_elapsed    | 1032      |
|    total_timesteps | 99328     |
----------------------------------
Eval num_timesteps=99500, episode_reward=-1014.74 +/- 69.78
Episode length: 97.84 +/- 25.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.8       |
|    mean_reward          | -1.01e+03  |
| time/                   |            |
|    total_timesteps      | 99500      |
| train/                  |            |
|    approx_kl            | 0.00384588 |
|    clip_fraction        | 0.00439    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0294    |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.001      |
|    loss                 | 676        |
|    n_updates            | 970        |
|    policy_gradient_loss | -0.000566  |
|    value_loss           | 1.18e+03   |
----------------------------------------
Eval num_timesteps=100000, episode_reward=-1023.46 +/- 75.09
Episode length: 92.60 +/- 21.20
----------------------------------
| eval/              |           |
|    mean_ep_length  | 92.6      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 100000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 98        |
|    time_elapsed    | 1042      |
|    total_timesteps | 100352    |
----------------------------------
Eval num_timesteps=100500, episode_reward=-1012.72 +/- 91.75
Episode length: 97.04 +/- 23.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97         |
|    mean_reward          | -1.01e+03  |
| time/                   |            |
|    total_timesteps      | 100500     |
| train/                  |            |
|    approx_kl            | 0.27528614 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.104     |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.001      |
|    loss                 | 694        |
|    n_updates            | 980        |
|    policy_gradient_loss | 0.0153     |
|    value_loss           | 1.36e+03   |
----------------------------------------
Eval num_timesteps=101000, episode_reward=-1002.40 +/- 74.34
Episode length: 101.10 +/- 29.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -1e+03   |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.8      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 99        |
|    time_elapsed    | 1052      |
|    total_timesteps | 101376    |
----------------------------------
Eval num_timesteps=101500, episode_reward=-1021.14 +/- 65.71
Episode length: 92.06 +/- 19.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.1        |
|    mean_reward          | -1.02e+03   |
| time/                   |             |
|    total_timesteps      | 101500      |
| train/                  |             |
|    approx_kl            | 0.019242592 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.157      |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 309         |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00282    |
|    value_loss           | 871         |
-----------------------------------------
Eval num_timesteps=102000, episode_reward=-1018.80 +/- 69.17
Episode length: 95.22 +/- 19.08
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.2      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 102000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 100       |
|    time_elapsed    | 1062      |
|    total_timesteps | 102400    |
----------------------------------
Eval num_timesteps=102500, episode_reward=-1014.40 +/- 59.17
Episode length: 99.26 +/- 28.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.3         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 102500       |
| train/                  |              |
|    approx_kl            | 0.0014362151 |
|    clip_fraction        | 0.00732      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0812      |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 287          |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.000332    |
|    value_loss           | 868          |
------------------------------------------
Eval num_timesteps=103000, episode_reward=-1033.89 +/- 57.76
Episode length: 98.84 +/- 30.21
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.8      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 103000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99.5      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 101       |
|    time_elapsed    | 1073      |
|    total_timesteps | 103424    |
----------------------------------
Eval num_timesteps=103500, episode_reward=-1016.00 +/- 78.38
Episode length: 102.66 +/- 24.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 103          |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 103500       |
| train/                  |              |
|    approx_kl            | 0.0017978129 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.061       |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.001        |
|    loss                 | 264          |
|    n_updates            | 1010         |
|    policy_gradient_loss | 0.000957     |
|    value_loss           | 942          |
------------------------------------------
Eval num_timesteps=104000, episode_reward=-1011.60 +/- 82.88
Episode length: 93.30 +/- 30.78
----------------------------------
| eval/              |           |
|    mean_ep_length  | 93.3      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 104000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 102       |
|    time_elapsed    | 1083      |
|    total_timesteps | 104448    |
----------------------------------
Eval num_timesteps=104500, episode_reward=-1017.20 +/- 66.62
Episode length: 99.00 +/- 27.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99          |
|    mean_reward          | -1.02e+03   |
| time/                   |             |
|    total_timesteps      | 104500      |
| train/                  |             |
|    approx_kl            | 0.006344164 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0627     |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.001       |
|    loss                 | 488         |
|    n_updates            | 1020        |
|    policy_gradient_loss | 0.000566    |
|    value_loss           | 1.15e+03    |
-----------------------------------------
Eval num_timesteps=105000, episode_reward=-1022.00 +/- 64.53
Episode length: 94.22 +/- 26.65
----------------------------------
| eval/              |           |
|    mean_ep_length  | 94.2      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 105000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.1      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 103       |
|    time_elapsed    | 1093      |
|    total_timesteps | 105472    |
----------------------------------
Eval num_timesteps=105500, episode_reward=-1005.91 +/- 64.40
Episode length: 95.80 +/- 25.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.8        |
|    mean_reward          | -1.01e+03   |
| time/                   |             |
|    total_timesteps      | 105500      |
| train/                  |             |
|    approx_kl            | 0.010014813 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.001       |
|    loss                 | 288         |
|    n_updates            | 1030        |
|    policy_gradient_loss | 0.000362    |
|    value_loss           | 854         |
-----------------------------------------
Eval num_timesteps=106000, episode_reward=-1007.60 +/- 78.29
Episode length: 98.92 +/- 25.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.9      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 106000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.4      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 104       |
|    time_elapsed    | 1104      |
|    total_timesteps | 106496    |
----------------------------------
Eval num_timesteps=106500, episode_reward=-1012.40 +/- 84.73
Episode length: 97.18 +/- 25.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.2         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0088023525 |
|    clip_fraction        | 0.039        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.123       |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 482          |
|    n_updates            | 1040         |
|    policy_gradient_loss | 0.00155      |
|    value_loss           | 969          |
------------------------------------------
Eval num_timesteps=107000, episode_reward=-1028.00 +/- 56.71
Episode length: 94.24 +/- 22.73
----------------------------------
| eval/              |           |
|    mean_ep_length  | 94.2      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 107000    |
----------------------------------
Eval num_timesteps=107500, episode_reward=-1020.00 +/- 74.19
Episode length: 98.58 +/- 22.70
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.6      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 107500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.4      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 105       |
|    time_elapsed    | 1118      |
|    total_timesteps | 107520    |
----------------------------------
Eval num_timesteps=108000, episode_reward=-1023.02 +/- 44.91
Episode length: 97.46 +/- 24.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.5        |
|    mean_reward          | -1.02e+03   |
| time/                   |             |
|    total_timesteps      | 108000      |
| train/                  |             |
|    approx_kl            | 0.015529798 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.2        |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 446         |
|    n_updates            | 1050        |
|    policy_gradient_loss | -0.00622    |
|    value_loss           | 916         |
-----------------------------------------
Eval num_timesteps=108500, episode_reward=-1011.41 +/- 60.98
Episode length: 96.64 +/- 22.84
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.6      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 108500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95        |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 106       |
|    time_elapsed    | 1129      |
|    total_timesteps | 108544    |
----------------------------------
Eval num_timesteps=109000, episode_reward=-989.10 +/- 62.33
Episode length: 93.32 +/- 18.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.3        |
|    mean_reward          | -989        |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.014200199 |
|    clip_fraction        | 0.0269      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0962     |
|    explained_variance   | 0.948       |
|    learning_rate        | 0.001       |
|    loss                 | 1.1e+03     |
|    n_updates            | 1060        |
|    policy_gradient_loss | 0.00486     |
|    value_loss           | 1.55e+03    |
-----------------------------------------
Eval num_timesteps=109500, episode_reward=-987.94 +/- 55.16
Episode length: 91.92 +/- 24.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.9     |
|    mean_reward     | -988     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.5     |
|    ep_rew_mean     | -998     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 107      |
|    time_elapsed    | 1139     |
|    total_timesteps | 109568   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-941.00 +/- 76.28
Episode length: 91.94 +/- 22.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 91.9         |
|    mean_reward          | -941         |
| time/                   |              |
|    total_timesteps      | 110000       |
| train/                  |              |
|    approx_kl            | 0.0041239504 |
|    clip_fraction        | 0.0513       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.107       |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.001        |
|    loss                 | 407          |
|    n_updates            | 1070         |
|    policy_gradient_loss | 0.00678      |
|    value_loss           | 1.12e+03     |
------------------------------------------
Eval num_timesteps=110500, episode_reward=-928.85 +/- 72.91
Episode length: 94.90 +/- 20.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | -929     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.5     |
|    ep_rew_mean     | -990     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 108      |
|    time_elapsed    | 1149     |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=-874.38 +/- 173.05
Episode length: 94.52 +/- 21.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.5        |
|    mean_reward          | -874        |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.025598543 |
|    clip_fraction        | 0.0748      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 543         |
|    n_updates            | 1080        |
|    policy_gradient_loss | 0.00548     |
|    value_loss           | 694         |
-----------------------------------------
Eval num_timesteps=111500, episode_reward=-895.16 +/- 159.24
Episode length: 96.36 +/- 25.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.4     |
|    mean_reward     | -895     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.3     |
|    ep_rew_mean     | -976     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 109      |
|    time_elapsed    | 1159     |
|    total_timesteps | 111616   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-904.86 +/- 105.49
Episode length: 92.88 +/- 20.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 92.9         |
|    mean_reward          | -905         |
| time/                   |              |
|    total_timesteps      | 112000       |
| train/                  |              |
|    approx_kl            | 0.0073687905 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0889      |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 273          |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00057     |
|    value_loss           | 1.04e+03     |
------------------------------------------
Eval num_timesteps=112500, episode_reward=-895.18 +/- 110.13
Episode length: 90.18 +/- 24.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.2     |
|    mean_reward     | -895     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.4     |
|    ep_rew_mean     | -959     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 110      |
|    time_elapsed    | 1169     |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=-999.38 +/- 67.64
Episode length: 97.82 +/- 24.53
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.8       |
|    mean_reward          | -999       |
| time/                   |            |
|    total_timesteps      | 113000     |
| train/                  |            |
|    approx_kl            | 0.30755368 |
|    clip_fraction        | 0.0884     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0438    |
|    explained_variance   | 0.949      |
|    learning_rate        | 0.001      |
|    loss                 | 540        |
|    n_updates            | 1100       |
|    policy_gradient_loss | 0.0224     |
|    value_loss           | 1.5e+03    |
----------------------------------------
Eval num_timesteps=113500, episode_reward=-1018.67 +/- 66.76
Episode length: 97.82 +/- 20.93
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.8      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 113500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.4     |
|    ep_rew_mean     | -955     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 111      |
|    time_elapsed    | 1179     |
|    total_timesteps | 113664   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-1011.25 +/- 91.71
Episode length: 92.96 +/- 26.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 93         |
|    mean_reward          | -1.01e+03  |
| time/                   |            |
|    total_timesteps      | 114000     |
| train/                  |            |
|    approx_kl            | 0.03594809 |
|    clip_fraction        | 0.0323     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.023     |
|    explained_variance   | 0.932      |
|    learning_rate        | 0.001      |
|    loss                 | 500        |
|    n_updates            | 1110       |
|    policy_gradient_loss | 0.00664    |
|    value_loss           | 1.13e+03   |
----------------------------------------
Eval num_timesteps=114500, episode_reward=-1012.00 +/- 72.45
Episode length: 98.88 +/- 29.69
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.9      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 114500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.8     |
|    ep_rew_mean     | -956     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 112      |
|    time_elapsed    | 1190     |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-1010.80 +/- 81.47
Episode length: 97.12 +/- 23.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.1        |
|    mean_reward          | -1.01e+03   |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.009332344 |
|    clip_fraction        | 0.00732     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.019      |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 332         |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.000443   |
|    value_loss           | 812         |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=-1019.60 +/- 79.57
Episode length: 97.86 +/- 22.17
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.9      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 115500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.5     |
|    ep_rew_mean     | -960     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 113      |
|    time_elapsed    | 1200     |
|    total_timesteps | 115712   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-1016.80 +/- 82.94
Episode length: 93.46 +/- 24.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.5        |
|    mean_reward          | -1.02e+03   |
| time/                   |             |
|    total_timesteps      | 116000      |
| train/                  |             |
|    approx_kl            | 7.72301e-05 |
|    clip_fraction        | 0.00186     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00439    |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.001       |
|    loss                 | 524         |
|    n_updates            | 1130        |
|    policy_gradient_loss | 0.00064     |
|    value_loss           | 948         |
-----------------------------------------
Eval num_timesteps=116500, episode_reward=-1028.00 +/- 60.93
Episode length: 96.86 +/- 20.97
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.9      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 116500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.5     |
|    ep_rew_mean     | -963     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 114      |
|    time_elapsed    | 1210     |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-1025.20 +/- 62.38
Episode length: 96.44 +/- 23.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96.4          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 117000        |
| train/                  |               |
|    approx_kl            | 0.00010684127 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00912      |
|    explained_variance   | 0.928         |
|    learning_rate        | 0.001         |
|    loss                 | 604           |
|    n_updates            | 1140          |
|    policy_gradient_loss | 0.000311      |
|    value_loss           | 1.79e+03      |
-------------------------------------------
Eval num_timesteps=117500, episode_reward=-1010.40 +/- 66.70
Episode length: 96.42 +/- 25.15
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.4      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 117500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.8     |
|    ep_rew_mean     | -968     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 115      |
|    time_elapsed    | 1220     |
|    total_timesteps | 117760   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-1028.00 +/- 75.26
Episode length: 93.84 +/- 26.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.8        |
|    mean_reward          | -1.03e+03   |
| time/                   |             |
|    total_timesteps      | 118000      |
| train/                  |             |
|    approx_kl            | 0.001186403 |
|    clip_fraction        | 0.00381     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0102     |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 917         |
|    n_updates            | 1150        |
|    policy_gradient_loss | 4.67e-05    |
|    value_loss           | 1.34e+03    |
-----------------------------------------
Eval num_timesteps=118500, episode_reward=-1021.52 +/- 66.78
Episode length: 103.54 +/- 28.22
----------------------------------
| eval/              |           |
|    mean_ep_length  | 104       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 118500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.2     |
|    ep_rew_mean     | -978     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 116      |
|    time_elapsed    | 1231     |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=-998.00 +/- 79.22
Episode length: 99.34 +/- 27.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.3         |
|    mean_reward          | -998         |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 0.0017575393 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00781     |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 373          |
|    n_updates            | 1160         |
|    policy_gradient_loss | -4.51e-05    |
|    value_loss           | 994          |
------------------------------------------
Eval num_timesteps=119500, episode_reward=-1006.40 +/- 82.97
Episode length: 99.38 +/- 24.09
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.4      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 119500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.7     |
|    ep_rew_mean     | -984     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 117      |
|    time_elapsed    | 1241     |
|    total_timesteps | 119808   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-1021.52 +/- 63.29
Episode length: 98.88 +/- 25.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 98.9          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 120000        |
| train/                  |               |
|    approx_kl            | 0.00077093835 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00996      |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 311           |
|    n_updates            | 1170          |
|    policy_gradient_loss | 2.06e-05      |
|    value_loss           | 873           |
-------------------------------------------
Eval num_timesteps=120500, episode_reward=-1018.35 +/- 69.70
Episode length: 99.90 +/- 26.96
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.9      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 120500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.3     |
|    ep_rew_mean     | -992     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 118      |
|    time_elapsed    | 1252     |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=-998.66 +/- 86.21
Episode length: 98.08 +/- 27.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.1         |
|    mean_reward          | -999         |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 6.284146e-05 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00542     |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 363          |
|    n_updates            | 1180         |
|    policy_gradient_loss | -9.8e-05     |
|    value_loss           | 1.16e+03     |
------------------------------------------
Eval num_timesteps=121500, episode_reward=-1034.00 +/- 54.88
Episode length: 96.98 +/- 22.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97        |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 121500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.9      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 119       |
|    time_elapsed    | 1263      |
|    total_timesteps | 121856    |
----------------------------------
Eval num_timesteps=122000, episode_reward=-1014.40 +/- 67.51
Episode length: 99.20 +/- 23.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.2         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 122000       |
| train/                  |              |
|    approx_kl            | 0.0010605277 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0387      |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 365          |
|    n_updates            | 1190         |
|    policy_gradient_loss | 0.000455     |
|    value_loss           | 891          |
------------------------------------------
Eval num_timesteps=122500, episode_reward=-1026.40 +/- 68.73
Episode length: 96.98 +/- 23.51
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97        |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 122500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.5      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 120       |
|    time_elapsed    | 1273      |
|    total_timesteps | 122880    |
----------------------------------
Eval num_timesteps=123000, episode_reward=-1009.15 +/- 78.15
Episode length: 103.50 +/- 30.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0014185109 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0105      |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 497          |
|    n_updates            | 1200         |
|    policy_gradient_loss | 0.000656     |
|    value_loss           | 1.06e+03     |
------------------------------------------
Eval num_timesteps=123500, episode_reward=-1027.82 +/- 66.69
Episode length: 96.64 +/- 25.02
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.6      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 123500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.1      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 121       |
|    time_elapsed    | 1284      |
|    total_timesteps | 123904    |
----------------------------------
Eval num_timesteps=124000, episode_reward=-1029.60 +/- 56.77
Episode length: 100.66 +/- 27.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 101          |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 124000       |
| train/                  |              |
|    approx_kl            | 6.259256e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0152      |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 359          |
|    n_updates            | 1210         |
|    policy_gradient_loss | 0.000106     |
|    value_loss           | 1.21e+03     |
------------------------------------------
Eval num_timesteps=124500, episode_reward=-1022.80 +/- 71.17
Episode length: 100.22 +/- 23.69
----------------------------------
| eval/              |           |
|    mean_ep_length  | 100       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 124500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.6      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 122       |
|    time_elapsed    | 1295      |
|    total_timesteps | 124928    |
----------------------------------
Eval num_timesteps=125000, episode_reward=-1028.66 +/- 60.48
Episode length: 99.14 +/- 23.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99.1          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 125000        |
| train/                  |               |
|    approx_kl            | 0.00031549862 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00756      |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.001         |
|    loss                 | 276           |
|    n_updates            | 1220          |
|    policy_gradient_loss | 0.000126      |
|    value_loss           | 899           |
-------------------------------------------
Eval num_timesteps=125500, episode_reward=-1003.34 +/- 72.24
Episode length: 93.50 +/- 22.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.5     |
|    mean_reward     | -1e+03   |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.9      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 123       |
|    time_elapsed    | 1305      |
|    total_timesteps | 125952    |
----------------------------------
Eval num_timesteps=126000, episode_reward=-1015.44 +/- 72.59
Episode length: 96.96 +/- 24.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97           |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 126000       |
| train/                  |              |
|    approx_kl            | 0.0050629936 |
|    clip_fraction        | 0.00693      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0396      |
|    explained_variance   | 0.959        |
|    learning_rate        | 0.001        |
|    loss                 | 959          |
|    n_updates            | 1230         |
|    policy_gradient_loss | 0.000321     |
|    value_loss           | 1.48e+03     |
------------------------------------------
Eval num_timesteps=126500, episode_reward=-1021.04 +/- 58.49
Episode length: 98.08 +/- 25.08
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.1      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 126500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.6      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 124       |
|    time_elapsed    | 1315      |
|    total_timesteps | 126976    |
----------------------------------
Eval num_timesteps=127000, episode_reward=-1004.80 +/- 73.99
Episode length: 97.34 +/- 24.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.3        |
|    mean_reward          | -1e+03      |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.016954422 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00952    |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.001       |
|    loss                 | 511         |
|    n_updates            | 1240        |
|    policy_gradient_loss | 0.00294     |
|    value_loss           | 989         |
-----------------------------------------
Eval num_timesteps=127500, episode_reward=-1025.60 +/- 67.08
Episode length: 96.60 +/- 23.29
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.6      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 127500    |
----------------------------------
Eval num_timesteps=128000, episode_reward=-1023.60 +/- 69.50
Episode length: 92.78 +/- 23.35
----------------------------------
| eval/              |           |
|    mean_ep_length  | 92.8      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 128000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99.4      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 125       |
|    time_elapsed    | 1330      |
|    total_timesteps | 128000    |
----------------------------------
Eval num_timesteps=128500, episode_reward=-1019.60 +/- 62.95
Episode length: 100.78 +/- 22.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 101          |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 128500       |
| train/                  |              |
|    approx_kl            | 0.0017845333 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00547     |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.001        |
|    loss                 | 383          |
|    n_updates            | 1250         |
|    policy_gradient_loss | 6.03e-05     |
|    value_loss           | 922          |
------------------------------------------
Eval num_timesteps=129000, episode_reward=-1023.20 +/- 72.08
Episode length: 105.38 +/- 26.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 105       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 129000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99.4      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 126       |
|    time_elapsed    | 1341      |
|    total_timesteps | 129024    |
----------------------------------
Eval num_timesteps=129500, episode_reward=-1015.16 +/- 73.74
Episode length: 97.24 +/- 24.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.2          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 129500        |
| train/                  |               |
|    approx_kl            | 0.00076269114 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00218      |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.001         |
|    loss                 | 358           |
|    n_updates            | 1260          |
|    policy_gradient_loss | 2.75e-05      |
|    value_loss           | 1.21e+03      |
-------------------------------------------
Eval num_timesteps=130000, episode_reward=-1019.94 +/- 70.34
Episode length: 97.44 +/- 23.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.4      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 130000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99.8      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 127       |
|    time_elapsed    | 1352      |
|    total_timesteps | 130048    |
----------------------------------
Eval num_timesteps=130500, episode_reward=-1020.00 +/- 84.00
Episode length: 98.34 +/- 25.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 98.3          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 130500        |
| train/                  |               |
|    approx_kl            | 4.5791967e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0019       |
|    explained_variance   | 0.949         |
|    learning_rate        | 0.001         |
|    loss                 | 570           |
|    n_updates            | 1270          |
|    policy_gradient_loss | -9.31e-06     |
|    value_loss           | 1.25e+03      |
-------------------------------------------
Eval num_timesteps=131000, episode_reward=-1010.80 +/- 73.30
Episode length: 100.72 +/- 25.98
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 131000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.2     |
|    ep_rew_mean     | -1e+03   |
| time/              |          |
|    fps             | 96       |
|    iterations      | 128      |
|    time_elapsed    | 1362     |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=-1003.98 +/- 78.06
Episode length: 97.56 +/- 27.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.6         |
|    mean_reward          | -1e+03       |
| time/                   |              |
|    total_timesteps      | 131500       |
| train/                  |              |
|    approx_kl            | 0.0034777033 |
|    clip_fraction        | 0.00439      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0154      |
|    explained_variance   | 0.941        |
|    learning_rate        | 0.001        |
|    loss                 | 977          |
|    n_updates            | 1280         |
|    policy_gradient_loss | 0.000354     |
|    value_loss           | 1.62e+03     |
------------------------------------------
Eval num_timesteps=132000, episode_reward=-1024.02 +/- 54.55
Episode length: 96.36 +/- 18.50
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.4      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 132000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.4     |
|    ep_rew_mean     | -1e+03   |
| time/              |          |
|    fps             | 96       |
|    iterations      | 129      |
|    time_elapsed    | 1373     |
|    total_timesteps | 132096   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-1019.10 +/- 66.87
Episode length: 94.34 +/- 22.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.3        |
|    mean_reward          | -1.02e+03   |
| time/                   |             |
|    total_timesteps      | 132500      |
| train/                  |             |
|    approx_kl            | 0.002883701 |
|    clip_fraction        | 0.00498     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0224     |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.001       |
|    loss                 | 541         |
|    n_updates            | 1290        |
|    policy_gradient_loss | 0.000252    |
|    value_loss           | 1.17e+03    |
-----------------------------------------
Eval num_timesteps=133000, episode_reward=-1031.01 +/- 64.34
Episode length: 93.92 +/- 19.32
----------------------------------
| eval/              |           |
|    mean_ep_length  | 93.9      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 133000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.9     |
|    ep_rew_mean     | -1e+03   |
| time/              |          |
|    fps             | 96       |
|    iterations      | 130      |
|    time_elapsed    | 1383     |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=-1020.00 +/- 62.87
Episode length: 88.32 +/- 24.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 88.3         |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 133500       |
| train/                  |              |
|    approx_kl            | 0.0010025931 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0309      |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.001        |
|    loss                 | 459          |
|    n_updates            | 1300         |
|    policy_gradient_loss | 0.000264     |
|    value_loss           | 1.27e+03     |
------------------------------------------
Eval num_timesteps=134000, episode_reward=-1026.69 +/- 57.04
Episode length: 100.42 +/- 25.31
----------------------------------
| eval/              |           |
|    mean_ep_length  | 100       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 134000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.6     |
|    ep_rew_mean     | -1e+03   |
| time/              |          |
|    fps             | 96       |
|    iterations      | 131      |
|    time_elapsed    | 1393     |
|    total_timesteps | 134144   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-1008.81 +/- 71.13
Episode length: 101.14 +/- 24.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 101          |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 134500       |
| train/                  |              |
|    approx_kl            | 0.0019960129 |
|    clip_fraction        | 0.00713      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0333      |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 549          |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.000702    |
|    value_loss           | 1.19e+03     |
------------------------------------------
Eval num_timesteps=135000, episode_reward=-1010.94 +/- 59.86
Episode length: 100.36 +/- 26.12
----------------------------------
| eval/              |           |
|    mean_ep_length  | 100       |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 135000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.4     |
|    ep_rew_mean     | -998     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 132      |
|    time_elapsed    | 1404     |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=-1014.69 +/- 63.93
Episode length: 93.18 +/- 24.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 93.2         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 135500       |
| train/                  |              |
|    approx_kl            | 0.0019865194 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0335      |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 513          |
|    n_updates            | 1320         |
|    policy_gradient_loss | 0.00297      |
|    value_loss           | 1.21e+03     |
------------------------------------------
Eval num_timesteps=136000, episode_reward=-1017.26 +/- 62.72
Episode length: 98.30 +/- 26.32
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.3      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 136000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.6     |
|    ep_rew_mean     | -1e+03   |
| time/              |          |
|    fps             | 96       |
|    iterations      | 133      |
|    time_elapsed    | 1414     |
|    total_timesteps | 136192   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-1017.99 +/- 77.19
Episode length: 96.62 +/- 28.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.6        |
|    mean_reward          | -1.02e+03   |
| time/                   |             |
|    total_timesteps      | 136500      |
| train/                  |             |
|    approx_kl            | 0.008685635 |
|    clip_fraction        | 0.0223      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0447     |
|    explained_variance   | 0.965       |
|    learning_rate        | 0.001       |
|    loss                 | 186         |
|    n_updates            | 1330        |
|    policy_gradient_loss | 0.00218     |
|    value_loss           | 1e+03       |
-----------------------------------------
Eval num_timesteps=137000, episode_reward=-1016.03 +/- 73.09
Episode length: 99.30 +/- 20.34
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.3      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 137000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.7     |
|    ep_rew_mean     | -1e+03   |
| time/              |          |
|    fps             | 96       |
|    iterations      | 134      |
|    time_elapsed    | 1425     |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=-991.50 +/- 79.90
Episode length: 95.82 +/- 23.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.8         |
|    mean_reward          | -992         |
| time/                   |              |
|    total_timesteps      | 137500       |
| train/                  |              |
|    approx_kl            | 0.0034472712 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0291      |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 493          |
|    n_updates            | 1340         |
|    policy_gradient_loss | 0.000665     |
|    value_loss           | 1.1e+03      |
------------------------------------------
Eval num_timesteps=138000, episode_reward=-980.31 +/- 65.91
Episode length: 90.64 +/- 26.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.6     |
|    mean_reward     | -980     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.7     |
|    ep_rew_mean     | -999     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 135      |
|    time_elapsed    | 1435     |
|    total_timesteps | 138240   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-982.68 +/- 117.40
Episode length: 99.26 +/- 25.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.3         |
|    mean_reward          | -983         |
| time/                   |              |
|    total_timesteps      | 138500       |
| train/                  |              |
|    approx_kl            | 0.0034473136 |
|    clip_fraction        | 0.0307       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0451      |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 462          |
|    n_updates            | 1350         |
|    policy_gradient_loss | 0.00346      |
|    value_loss           | 1.18e+03     |
------------------------------------------
Eval num_timesteps=139000, episode_reward=-980.62 +/- 63.93
Episode length: 97.00 +/- 22.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97       |
|    mean_reward     | -981     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.3     |
|    ep_rew_mean     | -995     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 136      |
|    time_elapsed    | 1446     |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=-932.46 +/- 110.35
Episode length: 94.12 +/- 19.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.1        |
|    mean_reward          | -932        |
| time/                   |             |
|    total_timesteps      | 139500      |
| train/                  |             |
|    approx_kl            | 0.009724409 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.054      |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 606         |
|    n_updates            | 1360        |
|    policy_gradient_loss | 0.00281     |
|    value_loss           | 1.41e+03    |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=-935.13 +/- 100.70
Episode length: 103.10 +/- 28.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -935     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.9     |
|    ep_rew_mean     | -984     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 137      |
|    time_elapsed    | 1456     |
|    total_timesteps | 140288   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-938.72 +/- 66.22
Episode length: 94.36 +/- 21.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 94.4       |
|    mean_reward          | -939       |
| time/                   |            |
|    total_timesteps      | 140500     |
| train/                  |            |
|    approx_kl            | 0.02942976 |
|    clip_fraction        | 0.0754     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0919    |
|    explained_variance   | 0.956      |
|    learning_rate        | 0.001      |
|    loss                 | 510        |
|    n_updates            | 1370       |
|    policy_gradient_loss | 0.0187     |
|    value_loss           | 1.38e+03   |
----------------------------------------
Eval num_timesteps=141000, episode_reward=-966.51 +/- 63.99
Episode length: 96.50 +/- 18.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.5     |
|    mean_reward     | -967     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.7     |
|    ep_rew_mean     | -979     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 138      |
|    time_elapsed    | 1467     |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=-937.15 +/- 66.16
Episode length: 95.00 +/- 23.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95          |
|    mean_reward          | -937        |
| time/                   |             |
|    total_timesteps      | 141500      |
| train/                  |             |
|    approx_kl            | 0.021418307 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0848     |
|    explained_variance   | 0.947       |
|    learning_rate        | 0.001       |
|    loss                 | 336         |
|    n_updates            | 1380        |
|    policy_gradient_loss | 0.00458     |
|    value_loss           | 1.18e+03    |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=-943.97 +/- 71.95
Episode length: 92.54 +/- 25.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | -944     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.3     |
|    ep_rew_mean     | -967     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 139      |
|    time_elapsed    | 1477     |
|    total_timesteps | 142336   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-967.12 +/- 49.37
Episode length: 96.70 +/- 23.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.7        |
|    mean_reward          | -967        |
| time/                   |             |
|    total_timesteps      | 142500      |
| train/                  |             |
|    approx_kl            | 0.011414526 |
|    clip_fraction        | 0.0456      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0838     |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.001       |
|    loss                 | 502         |
|    n_updates            | 1390        |
|    policy_gradient_loss | 0.00377     |
|    value_loss           | 839         |
-----------------------------------------
Eval num_timesteps=143000, episode_reward=-965.01 +/- 66.09
Episode length: 93.90 +/- 18.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.9     |
|    mean_reward     | -965     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.5     |
|    ep_rew_mean     | -962     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 140      |
|    time_elapsed    | 1487     |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=-956.51 +/- 82.20
Episode length: 96.74 +/- 23.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.7        |
|    mean_reward          | -957        |
| time/                   |             |
|    total_timesteps      | 143500      |
| train/                  |             |
|    approx_kl            | 0.009061407 |
|    clip_fraction        | 0.0405      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0997     |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 384         |
|    n_updates            | 1400        |
|    policy_gradient_loss | 0.00425     |
|    value_loss           | 827         |
-----------------------------------------
Eval num_timesteps=144000, episode_reward=-941.96 +/- 84.56
Episode length: 99.94 +/- 20.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.9     |
|    mean_reward     | -942     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.9     |
|    ep_rew_mean     | -942     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 141      |
|    time_elapsed    | 1498     |
|    total_timesteps | 144384   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-971.60 +/- 73.82
Episode length: 95.86 +/- 23.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.9        |
|    mean_reward          | -972        |
| time/                   |             |
|    total_timesteps      | 144500      |
| train/                  |             |
|    approx_kl            | 0.021772198 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 470         |
|    n_updates            | 1410        |
|    policy_gradient_loss | 0.0146      |
|    value_loss           | 1.14e+03    |
-----------------------------------------
Eval num_timesteps=145000, episode_reward=-1007.77 +/- 60.33
Episode length: 99.96 +/- 29.92
----------------------------------
| eval/              |           |
|    mean_ep_length  | 100       |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 145000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.8     |
|    ep_rew_mean     | -929     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 142      |
|    time_elapsed    | 1508     |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=-1016.79 +/- 55.27
Episode length: 99.22 +/- 29.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.2        |
|    mean_reward          | -1.02e+03   |
| time/                   |             |
|    total_timesteps      | 145500      |
| train/                  |             |
|    approx_kl            | 0.003520912 |
|    clip_fraction        | 0.0569      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0945     |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 260         |
|    n_updates            | 1420        |
|    policy_gradient_loss | 0.00711     |
|    value_loss           | 830         |
-----------------------------------------
Eval num_timesteps=146000, episode_reward=-1012.78 +/- 74.64
Episode length: 97.26 +/- 27.81
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.3      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 146000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.2     |
|    ep_rew_mean     | -924     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 143      |
|    time_elapsed    | 1519     |
|    total_timesteps | 146432   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-1012.80 +/- 85.92
Episode length: 96.04 +/- 26.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 96         |
|    mean_reward          | -1.01e+03  |
| time/                   |            |
|    total_timesteps      | 146500     |
| train/                  |            |
|    approx_kl            | 0.43204802 |
|    clip_fraction        | 0.0926     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0129    |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.001      |
|    loss                 | 269        |
|    n_updates            | 1430       |
|    policy_gradient_loss | 0.0285     |
|    value_loss           | 959        |
----------------------------------------
Eval num_timesteps=147000, episode_reward=-1018.40 +/- 73.06
Episode length: 100.64 +/- 23.21
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 147000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.3     |
|    ep_rew_mean     | -926     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 144      |
|    time_elapsed    | 1529     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=-1027.60 +/- 70.55
Episode length: 97.68 +/- 26.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.7          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 147500        |
| train/                  |               |
|    approx_kl            | 1.9324943e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.56e-05     |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.001         |
|    loss                 | 432           |
|    n_updates            | 1440          |
|    policy_gradient_loss | 3.19e-05      |
|    value_loss           | 1.12e+03      |
-------------------------------------------
Eval num_timesteps=148000, episode_reward=-1022.40 +/- 72.38
Episode length: 96.92 +/- 22.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.9      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 148000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.9     |
|    ep_rew_mean     | -931     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 145      |
|    time_elapsed    | 1540     |
|    total_timesteps | 148480   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-1025.20 +/- 59.63
Episode length: 93.80 +/- 22.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 93.8          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 148500        |
| train/                  |               |
|    approx_kl            | 4.0745363e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -1.72e-05     |
|    explained_variance   | 0.948         |
|    learning_rate        | 0.001         |
|    loss                 | 299           |
|    n_updates            | 1450          |
|    policy_gradient_loss | -2.21e-07     |
|    value_loss           | 748           |
-------------------------------------------
Eval num_timesteps=149000, episode_reward=-1033.92 +/- 58.82
Episode length: 98.98 +/- 22.37
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99        |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 149000    |
----------------------------------
Eval num_timesteps=149500, episode_reward=-1023.20 +/- 63.47
Episode length: 95.82 +/- 24.29
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.8      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 149500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.4     |
|    ep_rew_mean     | -942     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 146      |
|    time_elapsed    | 1555     |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-1005.60 +/- 78.41
Episode length: 91.64 +/- 21.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 91.6          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 150000        |
| train/                  |               |
|    approx_kl            | 1.7462298e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.22e-05     |
|    explained_variance   | 0.94          |
|    learning_rate        | 0.001         |
|    loss                 | 808           |
|    n_updates            | 1460          |
|    policy_gradient_loss | -2.53e-07     |
|    value_loss           | 1.4e+03       |
-------------------------------------------
Eval num_timesteps=150500, episode_reward=-1019.60 +/- 59.02
Episode length: 100.60 +/- 25.13
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 150500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96.4     |
|    ep_rew_mean     | -955     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 147      |
|    time_elapsed    | 1565     |
|    total_timesteps | 150528   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-1020.40 +/- 57.59
Episode length: 96.72 +/- 21.35
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 96.7      |
|    mean_reward          | -1.02e+03 |
| time/                   |           |
|    total_timesteps      | 151000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.54e-06 |
|    explained_variance   | 0.941     |
|    learning_rate        | 0.001     |
|    loss                 | 545       |
|    n_updates            | 1470      |
|    policy_gradient_loss | 3.16e-08  |
|    value_loss           | 1.37e+03  |
---------------------------------------
Eval num_timesteps=151500, episode_reward=-1020.80 +/- 67.36
Episode length: 97.04 +/- 24.50
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97        |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 151500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.6     |
|    ep_rew_mean     | -962     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 148      |
|    time_elapsed    | 1575     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-994.40 +/- 81.47
Episode length: 99.82 +/- 26.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99.8          |
|    mean_reward          | -994          |
| time/                   |               |
|    total_timesteps      | 152000        |
| train/                  |               |
|    approx_kl            | 1.1641532e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.91e-06     |
|    explained_variance   | 0.937         |
|    learning_rate        | 0.001         |
|    loss                 | 1.28e+03      |
|    n_updates            | 1480          |
|    policy_gradient_loss | -1.69e-08     |
|    value_loss           | 1.43e+03      |
-------------------------------------------
Eval num_timesteps=152500, episode_reward=-1028.00 +/- 62.23
Episode length: 96.18 +/- 25.52
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.2      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 152500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.2     |
|    ep_rew_mean     | -973     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 149      |
|    time_elapsed    | 1586     |
|    total_timesteps | 152576   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-1027.13 +/- 63.13
Episode length: 97.34 +/- 25.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.3          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 153000        |
| train/                  |               |
|    approx_kl            | 3.4924597e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.22e-06     |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 295           |
|    n_updates            | 1490          |
|    policy_gradient_loss | 9.4e-08       |
|    value_loss           | 806           |
-------------------------------------------
Eval num_timesteps=153500, episode_reward=-1024.00 +/- 55.71
Episode length: 98.40 +/- 24.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.4      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 153500    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.9     |
|    ep_rew_mean     | -985     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 150      |
|    time_elapsed    | 1597     |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=-1006.40 +/- 89.02
Episode length: 93.54 +/- 24.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 93.5          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 154000        |
| train/                  |               |
|    approx_kl            | 3.4924597e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.07e-05     |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.001         |
|    loss                 | 398           |
|    n_updates            | 1500          |
|    policy_gradient_loss | 8.49e-07      |
|    value_loss           | 992           |
-------------------------------------------
Eval num_timesteps=154500, episode_reward=-1030.33 +/- 56.36
Episode length: 101.08 +/- 19.28
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 154500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 101       |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 151       |
|    time_elapsed    | 1607      |
|    total_timesteps | 154624    |
----------------------------------
Eval num_timesteps=155000, episode_reward=-1023.60 +/- 66.32
Episode length: 98.42 +/- 23.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 98.4          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 155000        |
| train/                  |               |
|    approx_kl            | 1.6298145e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.25e-05     |
|    explained_variance   | 0.958         |
|    learning_rate        | 0.001         |
|    loss                 | 223           |
|    n_updates            | 1510          |
|    policy_gradient_loss | 4.5e-07       |
|    value_loss           | 991           |
-------------------------------------------
Eval num_timesteps=155500, episode_reward=-1024.00 +/- 63.75
Episode length: 102.02 +/- 25.46
----------------------------------
| eval/              |           |
|    mean_ep_length  | 102       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 155500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 100       |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 152       |
|    time_elapsed    | 1618      |
|    total_timesteps | 155648    |
----------------------------------
Eval num_timesteps=156000, episode_reward=-1013.40 +/- 80.26
Episode length: 96.34 +/- 23.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96.3          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 156000        |
| train/                  |               |
|    approx_kl            | 5.2386895e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -2.3e-05      |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.001         |
|    loss                 | 326           |
|    n_updates            | 1520          |
|    policy_gradient_loss | 1.67e-07      |
|    value_loss           | 1.11e+03      |
-------------------------------------------
Eval num_timesteps=156500, episode_reward=-1027.20 +/- 49.66
Episode length: 97.36 +/- 22.12
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.4      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 156500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.1      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 153       |
|    time_elapsed    | 1628      |
|    total_timesteps | 156672    |
----------------------------------
Eval num_timesteps=157000, episode_reward=-1021.60 +/- 59.47
Episode length: 96.00 +/- 20.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96           |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 157000       |
| train/                  |              |
|    approx_kl            | 9.313226e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -2.22e-05    |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 377          |
|    n_updates            | 1530         |
|    policy_gradient_loss | 6.13e-07     |
|    value_loss           | 1.14e+03     |
------------------------------------------
Eval num_timesteps=157500, episode_reward=-1018.56 +/- 67.09
Episode length: 100.64 +/- 26.76
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 157500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98        |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 154       |
|    time_elapsed    | 1638      |
|    total_timesteps | 157696    |
----------------------------------
Eval num_timesteps=158000, episode_reward=-1027.06 +/- 67.66
Episode length: 98.34 +/- 22.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.3         |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 2.910383e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.81e-05    |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 297          |
|    n_updates            | 1540         |
|    policy_gradient_loss | -8.69e-07    |
|    value_loss           | 847          |
------------------------------------------
Eval num_timesteps=158500, episode_reward=-1023.07 +/- 65.76
Episode length: 98.30 +/- 23.81
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.3      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 158500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97        |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 155       |
|    time_elapsed    | 1649      |
|    total_timesteps | 158720    |
----------------------------------
Eval num_timesteps=159000, episode_reward=-1031.60 +/- 62.01
Episode length: 100.34 +/- 22.63
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 159000       |
| train/                  |              |
|    approx_kl            | 9.313226e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.05e-05    |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 794          |
|    n_updates            | 1550         |
|    policy_gradient_loss | -4.12e-07    |
|    value_loss           | 1.45e+03     |
------------------------------------------
Eval num_timesteps=159500, episode_reward=-1022.00 +/- 65.88
Episode length: 94.24 +/- 22.80
----------------------------------
| eval/              |           |
|    mean_ep_length  | 94.2      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 159500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 156       |
|    time_elapsed    | 1659      |
|    total_timesteps | 159744    |
----------------------------------
Eval num_timesteps=160000, episode_reward=-1021.20 +/- 65.70
Episode length: 94.82 +/- 25.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 94.8          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 160000        |
| train/                  |               |
|    approx_kl            | 1.3213139e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000186     |
|    explained_variance   | 0.95          |
|    learning_rate        | 0.001         |
|    loss                 | 489           |
|    n_updates            | 1560          |
|    policy_gradient_loss | 7.31e-06      |
|    value_loss           | 1.22e+03      |
-------------------------------------------
Eval num_timesteps=160500, episode_reward=-1022.69 +/- 65.54
Episode length: 89.08 +/- 20.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 89.1      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 160500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 157       |
|    time_elapsed    | 1669      |
|    total_timesteps | 160768    |
----------------------------------
Eval num_timesteps=161000, episode_reward=-1027.95 +/- 60.08
Episode length: 98.68 +/- 29.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.7         |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 161000       |
| train/                  |              |
|    approx_kl            | 4.947651e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000194    |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 593          |
|    n_updates            | 1570         |
|    policy_gradient_loss | 8.37e-07     |
|    value_loss           | 1.09e+03     |
------------------------------------------
Eval num_timesteps=161500, episode_reward=-1022.80 +/- 60.48
Episode length: 99.46 +/- 23.37
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.5      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 161500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.8      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 158       |
|    time_elapsed    | 1680      |
|    total_timesteps | 161792    |
----------------------------------
Eval num_timesteps=162000, episode_reward=-1028.40 +/- 59.31
Episode length: 104.64 +/- 24.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 105          |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 162000       |
| train/                  |              |
|    approx_kl            | 0.0021305883 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00222     |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 549          |
|    n_updates            | 1580         |
|    policy_gradient_loss | -2.02e-05    |
|    value_loss           | 1.07e+03     |
------------------------------------------
Eval num_timesteps=162500, episode_reward=-1012.40 +/- 71.74
Episode length: 93.40 +/- 27.34
----------------------------------
| eval/              |           |
|    mean_ep_length  | 93.4      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 162500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 92        |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 159       |
|    time_elapsed    | 1690      |
|    total_timesteps | 162816    |
----------------------------------
Eval num_timesteps=163000, episode_reward=-1025.20 +/- 70.68
Episode length: 102.04 +/- 26.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | -1.03e+03   |
| time/                   |             |
|    total_timesteps      | 163000      |
| train/                  |             |
|    approx_kl            | 6.26198e-07 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000206   |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.001       |
|    loss                 | 279         |
|    n_updates            | 1590        |
|    policy_gradient_loss | 4.68e-05    |
|    value_loss           | 1e+03       |
-----------------------------------------
Eval num_timesteps=163500, episode_reward=-1026.00 +/- 65.27
Episode length: 96.02 +/- 22.14
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96        |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 163500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 91.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 160       |
|    time_elapsed    | 1701      |
|    total_timesteps | 163840    |
----------------------------------
Eval num_timesteps=164000, episode_reward=-1017.87 +/- 57.18
Episode length: 99.40 +/- 27.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99.4          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 164000        |
| train/                  |               |
|    approx_kl            | 2.2411114e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000311     |
|    explained_variance   | 0.953         |
|    learning_rate        | 0.001         |
|    loss                 | 753           |
|    n_updates            | 1600          |
|    policy_gradient_loss | 1.79e-05      |
|    value_loss           | 1.3e+03       |
-------------------------------------------
Eval num_timesteps=164500, episode_reward=-1011.20 +/- 56.65
Episode length: 92.86 +/- 21.77
----------------------------------
| eval/              |           |
|    mean_ep_length  | 92.9      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 164500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 92.6      |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 161       |
|    time_elapsed    | 1711      |
|    total_timesteps | 164864    |
----------------------------------
Eval num_timesteps=165000, episode_reward=-1031.91 +/- 62.43
Episode length: 101.96 +/- 25.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 102           |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 165000        |
| train/                  |               |
|    approx_kl            | 2.7631177e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000485     |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.001         |
|    loss                 | 317           |
|    n_updates            | 1610          |
|    policy_gradient_loss | 3.56e-06      |
|    value_loss           | 849           |
-------------------------------------------
Eval num_timesteps=165500, episode_reward=-998.40 +/- 85.57
Episode length: 100.18 +/- 28.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | -998     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.3      |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 162       |
|    time_elapsed    | 1722      |
|    total_timesteps | 165888    |
----------------------------------
Eval num_timesteps=166000, episode_reward=-1030.80 +/- 62.07
Episode length: 99.66 +/- 20.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99.7          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 166000        |
| train/                  |               |
|    approx_kl            | 1.2847595e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00106      |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 535           |
|    n_updates            | 1620          |
|    policy_gradient_loss | -1.91e-05     |
|    value_loss           | 1.21e+03      |
-------------------------------------------
Eval num_timesteps=166500, episode_reward=-1018.36 +/- 77.29
Episode length: 97.58 +/- 25.78
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.6      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 166500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.9      |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 163       |
|    time_elapsed    | 1732      |
|    total_timesteps | 166912    |
----------------------------------
Eval num_timesteps=167000, episode_reward=-1026.40 +/- 58.55
Episode length: 99.70 +/- 21.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99.7          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 167000        |
| train/                  |               |
|    approx_kl            | 2.2933818e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00088      |
|    explained_variance   | 0.954         |
|    learning_rate        | 0.001         |
|    loss                 | 597           |
|    n_updates            | 1630          |
|    policy_gradient_loss | -1.91e-06     |
|    value_loss           | 1.19e+03      |
-------------------------------------------
Eval num_timesteps=167500, episode_reward=-1035.13 +/- 53.82
Episode length: 104.04 +/- 29.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 104       |
|    mean_reward     | -1.04e+03 |
| time/              |           |
|    total_timesteps | 167500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 164       |
|    time_elapsed    | 1743      |
|    total_timesteps | 167936    |
----------------------------------
Eval num_timesteps=168000, episode_reward=-1020.40 +/- 58.01
Episode length: 105.30 +/- 34.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 105           |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 168000        |
| train/                  |               |
|    approx_kl            | 1.5640398e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000398     |
|    explained_variance   | 0.969         |
|    learning_rate        | 0.001         |
|    loss                 | 320           |
|    n_updates            | 1640          |
|    policy_gradient_loss | 1.13e-06      |
|    value_loss           | 728           |
-------------------------------------------
Eval num_timesteps=168500, episode_reward=-1023.13 +/- 66.24
Episode length: 97.24 +/- 20.68
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.2      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 168500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.1      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 165       |
|    time_elapsed    | 1754      |
|    total_timesteps | 168960    |
----------------------------------
Eval num_timesteps=169000, episode_reward=-1001.20 +/- 81.76
Episode length: 90.00 +/- 25.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 90            |
|    mean_reward          | -1e+03        |
| time/                   |               |
|    total_timesteps      | 169000        |
| train/                  |               |
|    approx_kl            | 2.3795874e-06 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00185      |
|    explained_variance   | 0.971         |
|    learning_rate        | 0.001         |
|    loss                 | 412           |
|    n_updates            | 1650          |
|    policy_gradient_loss | -0.000158     |
|    value_loss           | 767           |
-------------------------------------------
Eval num_timesteps=169500, episode_reward=-1021.63 +/- 67.11
Episode length: 98.50 +/- 31.19
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.5      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 169500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 166       |
|    time_elapsed    | 1764      |
|    total_timesteps | 169984    |
----------------------------------
Eval num_timesteps=170000, episode_reward=-1010.19 +/- 65.10
Episode length: 103.84 +/- 30.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0026673893 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00825     |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.001        |
|    loss                 | 386          |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.000237    |
|    value_loss           | 769          |
------------------------------------------
Eval num_timesteps=170500, episode_reward=-1033.66 +/- 66.42
Episode length: 104.46 +/- 23.32
----------------------------------
| eval/              |           |
|    mean_ep_length  | 104       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 170500    |
----------------------------------
Eval num_timesteps=171000, episode_reward=-1003.99 +/- 73.98
Episode length: 101.48 +/- 28.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -1e+03   |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 92.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 167       |
|    time_elapsed    | 1779      |
|    total_timesteps | 171008    |
----------------------------------
Eval num_timesteps=171500, episode_reward=-1005.43 +/- 79.72
Episode length: 98.84 +/- 24.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 98.8          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 171500        |
| train/                  |               |
|    approx_kl            | 0.00060423213 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00336      |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.001         |
|    loss                 | 399           |
|    n_updates            | 1670          |
|    policy_gradient_loss | 1.71e-05      |
|    value_loss           | 870           |
-------------------------------------------
Eval num_timesteps=172000, episode_reward=-1020.26 +/- 64.24
Episode length: 98.80 +/- 22.95
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.8      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 172000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 92.1      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 168       |
|    time_elapsed    | 1790      |
|    total_timesteps | 172032    |
----------------------------------
Eval num_timesteps=172500, episode_reward=-1008.36 +/- 76.84
Episode length: 101.24 +/- 28.71
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | -1.01e+03   |
| time/                   |             |
|    total_timesteps      | 172500      |
| train/                  |             |
|    approx_kl            | 0.012189871 |
|    clip_fraction        | 0.0136      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0134     |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.001       |
|    loss                 | 274         |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.000679   |
|    value_loss           | 768         |
-----------------------------------------
Eval num_timesteps=173000, episode_reward=-1020.81 +/- 68.86
Episode length: 98.50 +/- 21.09
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.5      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 173000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.1     |
|    ep_rew_mean     | -1e+03   |
| time/              |          |
|    fps             | 96       |
|    iterations      | 169      |
|    time_elapsed    | 1801     |
|    total_timesteps | 173056   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-1022.97 +/- 56.01
Episode length: 94.88 +/- 23.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 94.9          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 173500        |
| train/                  |               |
|    approx_kl            | 0.00052648643 |
|    clip_fraction        | 0.00654       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00938      |
|    explained_variance   | 0.96          |
|    learning_rate        | 0.001         |
|    loss                 | 388           |
|    n_updates            | 1690          |
|    policy_gradient_loss | 6.28e-06      |
|    value_loss           | 1.06e+03      |
-------------------------------------------
Eval num_timesteps=174000, episode_reward=-1015.74 +/- 66.86
Episode length: 99.04 +/- 23.78
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99        |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 174000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.4     |
|    ep_rew_mean     | -1e+03   |
| time/              |          |
|    fps             | 96       |
|    iterations      | 170      |
|    time_elapsed    | 1811     |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=-1005.76 +/- 74.45
Episode length: 97.74 +/- 21.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.7         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 174500       |
| train/                  |              |
|    approx_kl            | 0.0010725094 |
|    clip_fraction        | 0.00625      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0185      |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 278          |
|    n_updates            | 1700         |
|    policy_gradient_loss | 0.000359     |
|    value_loss           | 779          |
------------------------------------------
Eval num_timesteps=175000, episode_reward=-992.70 +/- 80.91
Episode length: 99.32 +/- 27.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.3     |
|    mean_reward     | -993     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.5     |
|    ep_rew_mean     | -997     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 171      |
|    time_elapsed    | 1822     |
|    total_timesteps | 175104   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-995.60 +/- 79.86
Episode length: 97.74 +/- 27.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.7        |
|    mean_reward          | -996        |
| time/                   |             |
|    total_timesteps      | 175500      |
| train/                  |             |
|    approx_kl            | 0.003947626 |
|    clip_fraction        | 0.00996     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0174     |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.001       |
|    loss                 | 389         |
|    n_updates            | 1710        |
|    policy_gradient_loss | 0.00168     |
|    value_loss           | 968         |
-----------------------------------------
Eval num_timesteps=176000, episode_reward=-1000.95 +/- 77.34
Episode length: 92.78 +/- 23.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.8     |
|    mean_reward     | -1e+03   |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.3     |
|    ep_rew_mean     | -993     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 172      |
|    time_elapsed    | 1832     |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=-1005.89 +/- 75.86
Episode length: 97.66 +/- 27.74
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.7         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 176500       |
| train/                  |              |
|    approx_kl            | 0.0046004085 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0239      |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 640          |
|    n_updates            | 1720         |
|    policy_gradient_loss | 0.00238      |
|    value_loss           | 741          |
------------------------------------------
Eval num_timesteps=177000, episode_reward=-1013.99 +/- 68.82
Episode length: 90.64 +/- 27.19
----------------------------------
| eval/              |           |
|    mean_ep_length  | 90.6      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 177000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.5     |
|    ep_rew_mean     | -994     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 173      |
|    time_elapsed    | 1842     |
|    total_timesteps | 177152   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-1018.31 +/- 77.20
Episode length: 101.48 +/- 24.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 101         |
|    mean_reward          | -1.02e+03   |
| time/                   |             |
|    total_timesteps      | 177500      |
| train/                  |             |
|    approx_kl            | 0.020430055 |
|    clip_fraction        | 0.0207      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0179     |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.001       |
|    loss                 | 532         |
|    n_updates            | 1730        |
|    policy_gradient_loss | 0.00224     |
|    value_loss           | 1.24e+03    |
-----------------------------------------
Eval num_timesteps=178000, episode_reward=-1017.20 +/- 71.82
Episode length: 95.70 +/- 25.95
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.7      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 178000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.8     |
|    ep_rew_mean     | -994     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 174      |
|    time_elapsed    | 1852     |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=-1022.19 +/- 63.12
Episode length: 90.24 +/- 24.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 90.2         |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0004975488 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00833     |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.001        |
|    loss                 | 400          |
|    n_updates            | 1740         |
|    policy_gradient_loss | 0.000289     |
|    value_loss           | 768          |
------------------------------------------
Eval num_timesteps=179000, episode_reward=-1002.60 +/- 80.80
Episode length: 95.26 +/- 24.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.3     |
|    mean_reward     | -1e+03   |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.1     |
|    ep_rew_mean     | -998     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 175      |
|    time_elapsed    | 1862     |
|    total_timesteps | 179200   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-1026.06 +/- 63.42
Episode length: 90.76 +/- 24.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 90.8         |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 179500       |
| train/                  |              |
|    approx_kl            | 0.0013951676 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0206      |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.001        |
|    loss                 | 403          |
|    n_updates            | 1750         |
|    policy_gradient_loss | 0.000602     |
|    value_loss           | 903          |
------------------------------------------
Eval num_timesteps=180000, episode_reward=-1019.39 +/- 70.04
Episode length: 96.40 +/- 23.08
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.4      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 180000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.5     |
|    ep_rew_mean     | -996     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 176      |
|    time_elapsed    | 1872     |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=-1037.91 +/- 57.64
Episode length: 96.44 +/- 20.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.4        |
|    mean_reward          | -1.04e+03   |
| time/                   |             |
|    total_timesteps      | 180500      |
| train/                  |             |
|    approx_kl            | 0.013305996 |
|    clip_fraction        | 0.0103      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00863    |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.001       |
|    loss                 | 409         |
|    n_updates            | 1760        |
|    policy_gradient_loss | 0.00202     |
|    value_loss           | 1.09e+03    |
-----------------------------------------
Eval num_timesteps=181000, episode_reward=-993.20 +/- 81.09
Episode length: 90.16 +/- 22.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.2     |
|    mean_reward     | -993     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.4     |
|    ep_rew_mean     | -1e+03   |
| time/              |          |
|    fps             | 96       |
|    iterations      | 177      |
|    time_elapsed    | 1882     |
|    total_timesteps | 181248   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-1006.00 +/- 67.20
Episode length: 91.08 +/- 21.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 91.1         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 181500       |
| train/                  |              |
|    approx_kl            | 0.0005733938 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000993    |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.001        |
|    loss                 | 271          |
|    n_updates            | 1770         |
|    policy_gradient_loss | 0.000755     |
|    value_loss           | 1.04e+03     |
------------------------------------------
Eval num_timesteps=182000, episode_reward=-1022.00 +/- 61.87
Episode length: 101.02 +/- 28.30
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 182000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.1     |
|    ep_rew_mean     | -1e+03   |
| time/              |          |
|    fps             | 96       |
|    iterations      | 178      |
|    time_elapsed    | 1893     |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=-1031.92 +/- 78.28
Episode length: 97.54 +/- 23.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.5         |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 182500       |
| train/                  |              |
|    approx_kl            | 1.565786e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000584    |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.001        |
|    loss                 | 407          |
|    n_updates            | 1780         |
|    policy_gradient_loss | -2.55e-07    |
|    value_loss           | 1.01e+03     |
------------------------------------------
Eval num_timesteps=183000, episode_reward=-1019.07 +/- 57.72
Episode length: 97.92 +/- 24.83
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.9      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 183000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.8      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 179       |
|    time_elapsed    | 1903      |
|    total_timesteps | 183296    |
----------------------------------
Eval num_timesteps=183500, episode_reward=-1030.00 +/- 54.59
Episode length: 101.42 +/- 25.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 101          |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 183500       |
| train/                  |              |
|    approx_kl            | 3.342051e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000369    |
|    explained_variance   | 0.953        |
|    learning_rate        | 0.001        |
|    loss                 | 355          |
|    n_updates            | 1790         |
|    policy_gradient_loss | 0.000187     |
|    value_loss           | 792          |
------------------------------------------
Eval num_timesteps=184000, episode_reward=-1022.80 +/- 64.45
Episode length: 101.72 +/- 25.73
----------------------------------
| eval/              |           |
|    mean_ep_length  | 102       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 184000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.5      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 180       |
|    time_elapsed    | 1914      |
|    total_timesteps | 184320    |
----------------------------------
Eval num_timesteps=184500, episode_reward=-1019.60 +/- 72.74
Episode length: 97.30 +/- 22.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.3          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 184500        |
| train/                  |               |
|    approx_kl            | 3.7020072e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000269     |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.001         |
|    loss                 | 494           |
|    n_updates            | 1800          |
|    policy_gradient_loss | 1.93e-05      |
|    value_loss           | 994           |
-------------------------------------------
Eval num_timesteps=185000, episode_reward=-1029.60 +/- 62.15
Episode length: 96.16 +/- 21.02
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.2      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 185000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96        |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 181       |
|    time_elapsed    | 1924      |
|    total_timesteps | 185344    |
----------------------------------
Eval num_timesteps=185500, episode_reward=-1025.55 +/- 62.25
Episode length: 99.58 +/- 23.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.6         |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 185500       |
| train/                  |              |
|    approx_kl            | 2.910383e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -9.52e-05    |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.001        |
|    loss                 | 609          |
|    n_updates            | 1810         |
|    policy_gradient_loss | -1.06e-06    |
|    value_loss           | 1.44e+03     |
------------------------------------------
Eval num_timesteps=186000, episode_reward=-1018.40 +/- 62.95
Episode length: 93.68 +/- 19.91
----------------------------------
| eval/              |           |
|    mean_ep_length  | 93.7      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 186000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.7      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 182       |
|    time_elapsed    | 1935      |
|    total_timesteps | 186368    |
----------------------------------
Eval num_timesteps=186500, episode_reward=-1022.40 +/- 63.81
Episode length: 99.76 +/- 24.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99.8          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 186500        |
| train/                  |               |
|    approx_kl            | 4.3021282e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00141      |
|    explained_variance   | 0.959         |
|    learning_rate        | 0.001         |
|    loss                 | 292           |
|    n_updates            | 1820          |
|    policy_gradient_loss | -2.44e-05     |
|    value_loss           | 866           |
-------------------------------------------
Eval num_timesteps=187000, episode_reward=-1018.40 +/- 62.69
Episode length: 92.24 +/- 19.04
----------------------------------
| eval/              |           |
|    mean_ep_length  | 92.2      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 187000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.2      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 183       |
|    time_elapsed    | 1945      |
|    total_timesteps | 187392    |
----------------------------------
Eval num_timesteps=187500, episode_reward=-1021.20 +/- 66.07
Episode length: 97.74 +/- 27.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.7          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 187500        |
| train/                  |               |
|    approx_kl            | 5.6810677e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000372     |
|    explained_variance   | 0.951         |
|    learning_rate        | 0.001         |
|    loss                 | 592           |
|    n_updates            | 1830          |
|    policy_gradient_loss | -5.85e-06     |
|    value_loss           | 1.34e+03      |
-------------------------------------------
Eval num_timesteps=188000, episode_reward=-1012.33 +/- 66.62
Episode length: 96.30 +/- 26.68
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.3      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 188000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98        |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 184       |
|    time_elapsed    | 1956      |
|    total_timesteps | 188416    |
----------------------------------
Eval num_timesteps=188500, episode_reward=-1022.40 +/- 75.09
Episode length: 95.78 +/- 22.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 95.8          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 188500        |
| train/                  |               |
|    approx_kl            | 1.2526289e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000227     |
|    explained_variance   | 0.946         |
|    learning_rate        | 0.001         |
|    loss                 | 381           |
|    n_updates            | 1840          |
|    policy_gradient_loss | 1.33e-05      |
|    value_loss           | 1.42e+03      |
-------------------------------------------
Eval num_timesteps=189000, episode_reward=-1010.65 +/- 89.20
Episode length: 97.14 +/- 29.47
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.1      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 189000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99.5      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 185       |
|    time_elapsed    | 1966      |
|    total_timesteps | 189440    |
----------------------------------
Eval num_timesteps=189500, episode_reward=-1031.20 +/- 63.19
Episode length: 97.52 +/- 23.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 97.5          |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 189500        |
| train/                  |               |
|    approx_kl            | 1.9540312e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000409     |
|    explained_variance   | 0.949         |
|    learning_rate        | 0.001         |
|    loss                 | 736           |
|    n_updates            | 1850          |
|    policy_gradient_loss | 3.93e-05      |
|    value_loss           | 1.06e+03      |
-------------------------------------------
Eval num_timesteps=190000, episode_reward=-1020.00 +/- 64.37
Episode length: 93.88 +/- 24.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 93.9      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 190000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.8      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 186       |
|    time_elapsed    | 1976      |
|    total_timesteps | 190464    |
----------------------------------
Eval num_timesteps=190500, episode_reward=-1017.20 +/- 70.13
Episode length: 98.88 +/- 26.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.9         |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 190500       |
| train/                  |              |
|    approx_kl            | 6.274786e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00141     |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.001        |
|    loss                 | 277          |
|    n_updates            | 1860         |
|    policy_gradient_loss | 2.34e-05     |
|    value_loss           | 1.09e+03     |
------------------------------------------
Eval num_timesteps=191000, episode_reward=-1021.20 +/- 66.07
Episode length: 98.24 +/- 22.73
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.2      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 191000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99.4      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 187       |
|    time_elapsed    | 1986      |
|    total_timesteps | 191488    |
----------------------------------
Eval num_timesteps=191500, episode_reward=-1027.60 +/- 58.25
Episode length: 105.74 +/- 28.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 106           |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 191500        |
| train/                  |               |
|    approx_kl            | 1.8277206e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00102      |
|    explained_variance   | 0.961         |
|    learning_rate        | 0.001         |
|    loss                 | 460           |
|    n_updates            | 1870          |
|    policy_gradient_loss | -1.04e-05     |
|    value_loss           | 1.04e+03      |
-------------------------------------------
Eval num_timesteps=192000, episode_reward=-1030.80 +/- 47.63
Episode length: 96.76 +/- 27.51
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.8      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 192000    |
----------------------------------
Eval num_timesteps=192500, episode_reward=-1012.79 +/- 57.60
Episode length: 94.08 +/- 22.29
----------------------------------
| eval/              |           |
|    mean_ep_length  | 94.1      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 192500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.5      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 188       |
|    time_elapsed    | 2001      |
|    total_timesteps | 192512    |
----------------------------------
Eval num_timesteps=193000, episode_reward=-1006.00 +/- 85.81
Episode length: 94.80 +/- 25.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94.8         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 6.996561e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000938    |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 260          |
|    n_updates            | 1880         |
|    policy_gradient_loss | 1.56e-07     |
|    value_loss           | 995          |
------------------------------------------
Eval num_timesteps=193500, episode_reward=-1001.60 +/- 70.32
Episode length: 100.94 +/- 30.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | -1e+03   |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 189       |
|    time_elapsed    | 2012      |
|    total_timesteps | 193536    |
----------------------------------
Eval num_timesteps=194000, episode_reward=-1007.28 +/- 70.92
Episode length: 91.36 +/- 23.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 91.4          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 194000        |
| train/                  |               |
|    approx_kl            | 3.1583477e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000469     |
|    explained_variance   | 0.963         |
|    learning_rate        | 0.001         |
|    loss                 | 345           |
|    n_updates            | 1890          |
|    policy_gradient_loss | 3.1e-05       |
|    value_loss           | 988           |
-------------------------------------------
Eval num_timesteps=194500, episode_reward=-1026.80 +/- 61.61
Episode length: 106.32 +/- 29.83
----------------------------------
| eval/              |           |
|    mean_ep_length  | 106       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 194500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.8      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 190       |
|    time_elapsed    | 2022      |
|    total_timesteps | 194560    |
----------------------------------
Eval num_timesteps=195000, episode_reward=-1016.40 +/- 70.71
Episode length: 105.06 +/- 28.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 105           |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 195000        |
| train/                  |               |
|    approx_kl            | 0.00084225234 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000698     |
|    explained_variance   | 0.967         |
|    learning_rate        | 0.001         |
|    loss                 | 441           |
|    n_updates            | 1900          |
|    policy_gradient_loss | -5.53e-05     |
|    value_loss           | 966           |
-------------------------------------------
Eval num_timesteps=195500, episode_reward=-1028.80 +/- 62.32
Episode length: 97.76 +/- 19.72
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.8      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 195500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.4      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 191       |
|    time_elapsed    | 2033      |
|    total_timesteps | 195584    |
----------------------------------
Eval num_timesteps=196000, episode_reward=-1041.88 +/- 56.24
Episode length: 99.82 +/- 29.10
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 99.8          |
|    mean_reward          | -1.04e+03     |
| time/                   |               |
|    total_timesteps      | 196000        |
| train/                  |               |
|    approx_kl            | 2.2817403e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000143     |
|    explained_variance   | 0.962         |
|    learning_rate        | 0.001         |
|    loss                 | 252           |
|    n_updates            | 1910          |
|    policy_gradient_loss | 1.7e-05       |
|    value_loss           | 815           |
-------------------------------------------
Eval num_timesteps=196500, episode_reward=-1011.85 +/- 79.30
Episode length: 100.80 +/- 25.43
----------------------------------
| eval/              |           |
|    mean_ep_length  | 101       |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 196500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 192       |
|    time_elapsed    | 2044      |
|    total_timesteps | 196608    |
----------------------------------
Eval num_timesteps=197000, episode_reward=-1014.27 +/- 62.47
Episode length: 94.44 +/- 24.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 94.4          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 197000        |
| train/                  |               |
|    approx_kl            | 6.4086635e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000272     |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 824           |
|    n_updates            | 1920          |
|    policy_gradient_loss | 2.08e-05      |
|    value_loss           | 1.35e+03      |
-------------------------------------------
Eval num_timesteps=197500, episode_reward=-1017.20 +/- 61.37
Episode length: 96.28 +/- 23.85
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.3      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 197500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 193       |
|    time_elapsed    | 2054      |
|    total_timesteps | 197632    |
----------------------------------
Eval num_timesteps=198000, episode_reward=-997.20 +/- 79.54
Episode length: 91.60 +/- 24.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 91.6         |
|    mean_reward          | -997         |
| time/                   |              |
|    total_timesteps      | 198000       |
| train/                  |              |
|    approx_kl            | 2.386514e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000204    |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 458          |
|    n_updates            | 1930         |
|    policy_gradient_loss | 8.23e-07     |
|    value_loss           | 896          |
------------------------------------------
Eval num_timesteps=198500, episode_reward=-1033.18 +/- 53.39
Episode length: 104.74 +/- 23.15
----------------------------------
| eval/              |           |
|    mean_ep_length  | 105       |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 198500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 194       |
|    time_elapsed    | 2064      |
|    total_timesteps | 198656    |
----------------------------------
Eval num_timesteps=199000, episode_reward=-1009.60 +/- 79.34
Episode length: 96.76 +/- 25.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96.8          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 199000        |
| train/                  |               |
|    approx_kl            | 4.7148205e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000156     |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 569           |
|    n_updates            | 1940          |
|    policy_gradient_loss | -2.27e-06     |
|    value_loss           | 1.22e+03      |
-------------------------------------------
Eval num_timesteps=199500, episode_reward=-1024.00 +/- 81.29
Episode length: 97.84 +/- 24.53
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.8      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 199500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 195       |
|    time_elapsed    | 2075      |
|    total_timesteps | 199680    |
----------------------------------
Eval num_timesteps=200000, episode_reward=-1030.40 +/- 43.09
Episode length: 97.86 +/- 22.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 97.9         |
|    mean_reward          | -1.03e+03    |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 9.662472e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00016     |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 408          |
|    n_updates            | 1950         |
|    policy_gradient_loss | -9.97e-07    |
|    value_loss           | 831          |
------------------------------------------
Eval num_timesteps=200500, episode_reward=-1034.00 +/- 62.00
Episode length: 95.62 +/- 27.53
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.6      |
|    mean_reward     | -1.03e+03 |
| time/              |           |
|    total_timesteps | 200500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 196       |
|    time_elapsed    | 2085      |
|    total_timesteps | 200704    |
----------------------------------
Eval num_timesteps=201000, episode_reward=-1024.00 +/- 56.28
Episode length: 93.60 +/- 21.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 93.6          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 201000        |
| train/                  |               |
|    approx_kl            | 2.4447218e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -9.33e-05     |
|    explained_variance   | 0.937         |
|    learning_rate        | 0.001         |
|    loss                 | 737           |
|    n_updates            | 1960          |
|    policy_gradient_loss | 2.05e-07      |
|    value_loss           | 1.67e+03      |
-------------------------------------------
Eval num_timesteps=201500, episode_reward=-1011.08 +/- 80.53
Episode length: 96.84 +/- 21.79
----------------------------------
| eval/              |           |
|    mean_ep_length  | 96.8      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 201500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.5      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 197       |
|    time_elapsed    | 2095      |
|    total_timesteps | 201728    |
----------------------------------
Eval num_timesteps=202000, episode_reward=-1017.92 +/- 76.07
Episode length: 93.96 +/- 20.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 94           |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 202000       |
| train/                  |              |
|    approx_kl            | 3.632158e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -8.96e-05    |
|    explained_variance   | 0.952        |
|    learning_rate        | 0.001        |
|    loss                 | 588          |
|    n_updates            | 1970         |
|    policy_gradient_loss | -8.92e-07    |
|    value_loss           | 1.15e+03     |
------------------------------------------
Eval num_timesteps=202500, episode_reward=-1012.80 +/- 71.37
Episode length: 95.74 +/- 28.01
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.7      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 202500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.1      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 198       |
|    time_elapsed    | 2105      |
|    total_timesteps | 202752    |
----------------------------------
Eval num_timesteps=203000, episode_reward=-1014.00 +/- 70.34
Episode length: 93.62 +/- 22.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 93.6          |
|    mean_reward          | -1.01e+03     |
| time/                   |               |
|    total_timesteps      | 203000        |
| train/                  |               |
|    approx_kl            | 1.2805685e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000222     |
|    explained_variance   | 0.957         |
|    learning_rate        | 0.001         |
|    loss                 | 396           |
|    n_updates            | 1980          |
|    policy_gradient_loss | 7.29e-07      |
|    value_loss           | 994           |
-------------------------------------------
Eval num_timesteps=203500, episode_reward=-1017.60 +/- 60.11
Episode length: 97.18 +/- 24.44
----------------------------------
| eval/              |           |
|    mean_ep_length  | 97.2      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 203500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 199       |
|    time_elapsed    | 2115      |
|    total_timesteps | 203776    |
----------------------------------
Eval num_timesteps=204000, episode_reward=-1018.00 +/- 71.25
Episode length: 102.02 +/- 24.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 102           |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 204000        |
| train/                  |               |
|    approx_kl            | 4.9942173e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000129     |
|    explained_variance   | 0.966         |
|    learning_rate        | 0.001         |
|    loss                 | 325           |
|    n_updates            | 1990          |
|    policy_gradient_loss | 4.22e-06      |
|    value_loss           | 912           |
-------------------------------------------
Eval num_timesteps=204500, episode_reward=-1023.20 +/- 58.76
Episode length: 99.02 +/- 21.26
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99        |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 204500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96        |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 200       |
|    time_elapsed    | 2126      |
|    total_timesteps | 204800    |
----------------------------------
Eval num_timesteps=205000, episode_reward=-1020.70 +/- 75.38
Episode length: 96.68 +/- 24.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.7         |
|    mean_reward          | -1.02e+03    |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 3.207242e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000273    |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.001        |
|    loss                 | 166          |
|    n_updates            | 2000         |
|    policy_gradient_loss | 2.31e-07     |
|    value_loss           | 631          |
------------------------------------------
Eval num_timesteps=205500, episode_reward=-1021.60 +/- 61.32
Episode length: 95.64 +/- 20.61
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.6      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 205500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96.7      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 201       |
|    time_elapsed    | 2136      |
|    total_timesteps | 205824    |
----------------------------------
Eval num_timesteps=206000, episode_reward=-1017.20 +/- 68.63
Episode length: 104.66 +/- 26.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 105           |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 206000        |
| train/                  |               |
|    approx_kl            | 1.5925616e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00056      |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.001         |
|    loss                 | 468           |
|    n_updates            | 2010          |
|    policy_gradient_loss | -2.13e-06     |
|    value_loss           | 1.01e+03      |
-------------------------------------------
Eval num_timesteps=206500, episode_reward=-1012.80 +/- 73.36
Episode length: 101.54 +/- 23.68
----------------------------------
| eval/              |           |
|    mean_ep_length  | 102       |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 206500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 95.4      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 202       |
|    time_elapsed    | 2148      |
|    total_timesteps | 206848    |
----------------------------------
Eval num_timesteps=207000, episode_reward=-1009.20 +/- 80.36
Episode length: 102.88 +/- 23.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 103          |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 4.476169e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000922    |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.001        |
|    loss                 | 539          |
|    n_updates            | 2020         |
|    policy_gradient_loss | 1.17e-05     |
|    value_loss           | 1.23e+03     |
------------------------------------------
Eval num_timesteps=207500, episode_reward=-1006.40 +/- 73.67
Episode length: 99.18 +/- 25.30
----------------------------------
| eval/              |           |
|    mean_ep_length  | 99.2      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 207500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 94.6      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 203       |
|    time_elapsed    | 2158      |
|    total_timesteps | 207872    |
----------------------------------
Eval num_timesteps=208000, episode_reward=-1023.48 +/- 58.91
Episode length: 96.24 +/- 26.99
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 96.2          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 208000        |
| train/                  |               |
|    approx_kl            | 4.0221494e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00036      |
|    explained_variance   | 0.956         |
|    learning_rate        | 0.001         |
|    loss                 | 465           |
|    n_updates            | 2030          |
|    policy_gradient_loss | -1.01e-05     |
|    value_loss           | 1.33e+03      |
-------------------------------------------
Eval num_timesteps=208500, episode_reward=-1013.16 +/- 71.46
Episode length: 94.00 +/- 18.98
----------------------------------
| eval/              |           |
|    mean_ep_length  | 94        |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 208500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 93.8      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 204       |
|    time_elapsed    | 2168      |
|    total_timesteps | 208896    |
----------------------------------
Eval num_timesteps=209000, episode_reward=-1021.48 +/- 68.22
Episode length: 94.44 +/- 25.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 94.4          |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 209000        |
| train/                  |               |
|    approx_kl            | 1.3038516e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000388     |
|    explained_variance   | 0.952         |
|    learning_rate        | 0.001         |
|    loss                 | 622           |
|    n_updates            | 2040          |
|    policy_gradient_loss | 1.46e-05      |
|    value_loss           | 1.55e+03      |
-------------------------------------------
Eval num_timesteps=209500, episode_reward=-1005.60 +/- 76.76
Episode length: 91.98 +/- 22.25
----------------------------------
| eval/              |           |
|    mean_ep_length  | 92        |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 209500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 96        |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 205       |
|    time_elapsed    | 2179      |
|    total_timesteps | 209920    |
----------------------------------
Eval num_timesteps=210000, episode_reward=-1022.40 +/- 70.48
Episode length: 100.38 +/- 23.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 100           |
|    mean_reward          | -1.02e+03     |
| time/                   |               |
|    total_timesteps      | 210000        |
| train/                  |               |
|    approx_kl            | 2.7648639e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000415     |
|    explained_variance   | 0.943         |
|    learning_rate        | 0.001         |
|    loss                 | 517           |
|    n_updates            | 2050          |
|    policy_gradient_loss | -1.36e-05     |
|    value_loss           | 1.04e+03      |
-------------------------------------------
Eval num_timesteps=210500, episode_reward=-1016.40 +/- 71.72
Episode length: 98.16 +/- 29.01
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.2      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 210500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.3      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 206       |
|    time_elapsed    | 2189      |
|    total_timesteps | 210944    |
----------------------------------
Eval num_timesteps=211000, episode_reward=-1028.80 +/- 62.32
Episode length: 101.42 +/- 23.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 101           |
|    mean_reward          | -1.03e+03     |
| time/                   |               |
|    total_timesteps      | 211000        |
| train/                  |               |
|    approx_kl            | 0.00030887994 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00102      |
|    explained_variance   | 0.939         |
|    learning_rate        | 0.001         |
|    loss                 | 462           |
|    n_updates            | 2060          |
|    policy_gradient_loss | 5.9e-05       |
|    value_loss           | 1.27e+03      |
-------------------------------------------
Eval num_timesteps=211500, episode_reward=-1021.20 +/- 72.20
Episode length: 100.42 +/- 28.05
----------------------------------
| eval/              |           |
|    mean_ep_length  | 100       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 211500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 97.7      |
|    ep_rew_mean     | -1.03e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 207       |
|    time_elapsed    | 2200      |
|    total_timesteps | 211968    |
----------------------------------
Eval num_timesteps=212000, episode_reward=-1013.20 +/- 74.41
Episode length: 95.68 +/- 28.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 95.7         |
|    mean_reward          | -1.01e+03    |
| time/                   |              |
|    total_timesteps      | 212000       |
| train/                  |              |
|    approx_kl            | 0.0013342233 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00232     |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.001        |
|    loss                 | 305          |
|    n_updates            | 2070         |
|    policy_gradient_loss | 3.14e-05     |
|    value_loss           | 839          |
------------------------------------------
Eval num_timesteps=212500, episode_reward=-1022.40 +/- 61.64
Episode length: 91.70 +/- 18.50
----------------------------------
| eval/              |           |
|    mean_ep_length  | 91.7      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 212500    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 98.5      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 208       |
|    time_elapsed    | 2210      |
|    total_timesteps | 212992    |
----------------------------------
Eval num_timesteps=213000, episode_reward=-1035.46 +/- 44.09
Episode length: 104.70 +/- 25.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 105          |
|    mean_reward          | -1.04e+03    |
| time/                   |              |
|    total_timesteps      | 213000       |
| train/                  |              |
|    approx_kl            | 4.703179e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000397    |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.001        |
|    loss                 | 496          |
|    n_updates            | 2080         |
|    policy_gradient_loss | 8.25e-06     |
|    value_loss           | 899          |
------------------------------------------
Eval num_timesteps=213500, episode_reward=-1020.40 +/- 70.35
Episode length: 91.52 +/- 24.42
----------------------------------
| eval/              |           |
|    mean_ep_length  | 91.5      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 213500    |
----------------------------------
Eval num_timesteps=214000, episode_reward=-986.80 +/- 82.87
Episode length: 97.26 +/- 20.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | -987     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99.5      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 209       |
|    time_elapsed    | 2225      |
|    total_timesteps | 214016    |
----------------------------------
Eval num_timesteps=214500, episode_reward=-994.23 +/- 70.56
Episode length: 91.26 +/- 24.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.3        |
|    mean_reward          | -994        |
| time/                   |             |
|    total_timesteps      | 214500      |
| train/                  |             |
|    approx_kl            | 0.010576743 |
|    clip_fraction        | 0.034       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0407     |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.001       |
|    loss                 | 331         |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00468    |
|    value_loss           | 965         |
-----------------------------------------
Eval num_timesteps=215000, episode_reward=-1015.48 +/- 57.11
Episode length: 101.70 +/- 28.88
----------------------------------
| eval/              |           |
|    mean_ep_length  | 102       |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 215000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99.2      |
|    ep_rew_mean     | -1.02e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 210       |
|    time_elapsed    | 2235      |
|    total_timesteps | 215040    |
----------------------------------
Eval num_timesteps=215500, episode_reward=-990.54 +/- 77.73
Episode length: 95.04 +/- 24.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95          |
|    mean_reward          | -991        |
| time/                   |             |
|    total_timesteps      | 215500      |
| train/                  |             |
|    approx_kl            | 0.004476312 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0161     |
|    explained_variance   | 0.941       |
|    learning_rate        | 0.001       |
|    loss                 | 362         |
|    n_updates            | 2100        |
|    policy_gradient_loss | 0.00146     |
|    value_loss           | 1.16e+03    |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=-1007.71 +/- 59.14
Episode length: 95.52 +/- 22.61
----------------------------------
| eval/              |           |
|    mean_ep_length  | 95.5      |
|    mean_reward     | -1.01e+03 |
| time/              |           |
|    total_timesteps | 216000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 99.3      |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 211       |
|    time_elapsed    | 2245      |
|    total_timesteps | 216064    |
----------------------------------
Eval num_timesteps=216500, episode_reward=-989.11 +/- 72.39
Episode length: 93.06 +/- 23.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 93.1         |
|    mean_reward          | -989         |
| time/                   |              |
|    total_timesteps      | 216500       |
| train/                  |              |
|    approx_kl            | 0.0021737486 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0241      |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.001        |
|    loss                 | 667          |
|    n_updates            | 2110         |
|    policy_gradient_loss | 0.00021      |
|    value_loss           | 1.24e+03     |
------------------------------------------
Eval num_timesteps=217000, episode_reward=-994.34 +/- 68.39
Episode length: 95.50 +/- 21.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.5     |
|    mean_reward     | -994     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 100       |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 212       |
|    time_elapsed    | 2255      |
|    total_timesteps | 217088    |
----------------------------------
Eval num_timesteps=217500, episode_reward=-1012.24 +/- 61.51
Episode length: 99.04 +/- 27.21
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99          |
|    mean_reward          | -1.01e+03   |
| time/                   |             |
|    total_timesteps      | 217500      |
| train/                  |             |
|    approx_kl            | 0.009961633 |
|    clip_fraction        | 0.0324      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.068      |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.001       |
|    loss                 | 197         |
|    n_updates            | 2120        |
|    policy_gradient_loss | 0.0029      |
|    value_loss           | 741         |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=-1015.25 +/- 45.90
Episode length: 98.50 +/- 23.96
----------------------------------
| eval/              |           |
|    mean_ep_length  | 98.5      |
|    mean_reward     | -1.02e+03 |
| time/              |           |
|    total_timesteps | 218000    |
----------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 102       |
|    ep_rew_mean     | -1.01e+03 |
| time/              |           |
|    fps             | 96        |
|    iterations      | 213       |
|    time_elapsed    | 2266      |
|    total_timesteps | 218112    |
----------------------------------
Eval num_timesteps=218500, episode_reward=-981.24 +/- 58.77
Episode length: 97.30 +/- 25.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.3        |
|    mean_reward          | -981        |
| time/                   |             |
|    total_timesteps      | 218500      |
| train/                  |             |
|    approx_kl            | 0.011834525 |
|    clip_fraction        | 0.0275      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0515     |
|    explained_variance   | 0.958       |
|    learning_rate        | 0.001       |
|    loss                 | 335         |
|    n_updates            | 2130        |
|    policy_gradient_loss | 0.00106     |
|    value_loss           | 1.02e+03    |
-----------------------------------------
Eval num_timesteps=219000, episode_reward=-976.51 +/- 66.78
Episode length: 95.44 +/- 20.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.4     |
|    mean_reward     | -977     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | -997     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 214      |
|    time_elapsed    | 2276     |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=-1000.73 +/- 45.80
Episode length: 98.98 +/- 26.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99          |
|    mean_reward          | -1e+03      |
| time/                   |             |
|    total_timesteps      | 219500      |
| train/                  |             |
|    approx_kl            | 0.006226909 |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0482     |
|    explained_variance   | 0.952       |
|    learning_rate        | 0.001       |
|    loss                 | 361         |
|    n_updates            | 2140        |
|    policy_gradient_loss | 0.00344     |
|    value_loss           | 1.08e+03    |
-----------------------------------------
Eval num_timesteps=220000, episode_reward=-996.74 +/- 55.12
Episode length: 103.46 +/- 26.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | -997     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 102      |
|    ep_rew_mean     | -988     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 215      |
|    time_elapsed    | 2287     |
|    total_timesteps | 220160   |
---------------------------------
Eval num_timesteps=220500, episode_reward=-992.63 +/- 65.61
Episode length: 96.16 +/- 26.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 96.2        |
|    mean_reward          | -993        |
| time/                   |             |
|    total_timesteps      | 220500      |
| train/                  |             |
|    approx_kl            | 0.004783384 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0826     |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.001       |
|    loss                 | 385         |
|    n_updates            | 2150        |
|    policy_gradient_loss | 0.00276     |
|    value_loss           | 902         |
-----------------------------------------
Eval num_timesteps=221000, episode_reward=-999.06 +/- 58.39
Episode length: 97.32 +/- 20.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | -999     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | -981     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 216      |
|    time_elapsed    | 2297     |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=-986.26 +/- 60.18
Episode length: 96.10 +/- 24.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 96.1         |
|    mean_reward          | -986         |
| time/                   |              |
|    total_timesteps      | 221500       |
| train/                  |              |
|    approx_kl            | 0.0017910571 |
|    clip_fraction        | 0.0201       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0573      |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.001        |
|    loss                 | 583          |
|    n_updates            | 2160         |
|    policy_gradient_loss | 0.00151      |
|    value_loss           | 1.09e+03     |
------------------------------------------
Eval num_timesteps=222000, episode_reward=-997.92 +/- 59.88
Episode length: 99.00 +/- 25.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | -998     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | -975     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 217      |
|    time_elapsed    | 2307     |
|    total_timesteps | 222208   |
---------------------------------
Eval num_timesteps=222500, episode_reward=-920.12 +/- 82.15
Episode length: 94.44 +/- 25.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.4        |
|    mean_reward          | -920        |
| time/                   |             |
|    total_timesteps      | 222500      |
| train/                  |             |
|    approx_kl            | 0.002791256 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0781     |
|    explained_variance   | 0.956       |
|    learning_rate        | 0.001       |
|    loss                 | 352         |
|    n_updates            | 2170        |
|    policy_gradient_loss | 0.00156     |
|    value_loss           | 959         |
-----------------------------------------
Eval num_timesteps=223000, episode_reward=-914.93 +/- 73.07
Episode length: 99.38 +/- 22.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.4     |
|    mean_reward     | -915     |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.1     |
|    ep_rew_mean     | -966     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 218      |
|    time_elapsed    | 2318     |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=-946.69 +/- 75.38
Episode length: 93.00 +/- 23.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 93           |
|    mean_reward          | -947         |
| time/                   |              |
|    total_timesteps      | 223500       |
| train/                  |              |
|    approx_kl            | 0.0027639442 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0691      |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.001        |
|    loss                 | 229          |
|    n_updates            | 2180         |
|    policy_gradient_loss | 0.00365      |
|    value_loss           | 882          |
------------------------------------------
Eval num_timesteps=224000, episode_reward=-967.04 +/- 81.48
Episode length: 99.16 +/- 24.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99.2     |
|    mean_reward     | -967     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.8     |
|    ep_rew_mean     | -952     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 219      |
|    time_elapsed    | 2328     |
|    total_timesteps | 224256   |
---------------------------------
Eval num_timesteps=224500, episode_reward=-884.29 +/- 167.94
Episode length: 99.16 +/- 23.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.2        |
|    mean_reward          | -884        |
| time/                   |             |
|    total_timesteps      | 224500      |
| train/                  |             |
|    approx_kl            | 0.003185509 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0987     |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.001       |
|    loss                 | 113         |
|    n_updates            | 2190        |
|    policy_gradient_loss | 0.00673     |
|    value_loss           | 651         |
-----------------------------------------
Eval num_timesteps=225000, episode_reward=-859.98 +/- 143.45
Episode length: 97.88 +/- 22.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.9     |
|    mean_reward     | -860     |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 100      |
|    ep_rew_mean     | -940     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 220      |
|    time_elapsed    | 2339     |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=-910.26 +/- 86.38
Episode length: 97.52 +/- 23.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 97.5        |
|    mean_reward          | -910        |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 0.010201981 |
|    clip_fraction        | 0.0486      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0702     |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.001       |
|    loss                 | 706         |
|    n_updates            | 2200        |
|    policy_gradient_loss | 0.00492     |
|    value_loss           | 1.3e+03     |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=-858.71 +/- 158.23
Episode length: 92.54 +/- 21.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | -859     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 98.8     |
|    ep_rew_mean     | -927     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 221      |
|    time_elapsed    | 2349     |
|    total_timesteps | 226304   |
---------------------------------
Eval num_timesteps=226500, episode_reward=-838.95 +/- 138.61
Episode length: 91.72 +/- 24.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.7        |
|    mean_reward          | -839        |
| time/                   |             |
|    total_timesteps      | 226500      |
| train/                  |             |
|    approx_kl            | 0.006787678 |
|    clip_fraction        | 0.0511      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0737     |
|    explained_variance   | 0.964       |
|    learning_rate        | 0.001       |
|    loss                 | 373         |
|    n_updates            | 2210        |
|    policy_gradient_loss | 0.00702     |
|    value_loss           | 850         |
-----------------------------------------
Eval num_timesteps=227000, episode_reward=-876.92 +/- 90.42
Episode length: 93.48 +/- 22.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.5     |
|    mean_reward     | -877     |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 99.6     |
|    ep_rew_mean     | -913     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 222      |
|    time_elapsed    | 2359     |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=-766.28 +/- 213.37
Episode length: 98.40 +/- 20.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 98.4        |
|    mean_reward          | -766        |
| time/                   |             |
|    total_timesteps      | 227500      |
| train/                  |             |
|    approx_kl            | 0.017815981 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0752     |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 254         |
|    n_updates            | 2220        |
|    policy_gradient_loss | 0.0039      |
|    value_loss           | 1.01e+03    |
-----------------------------------------
Eval num_timesteps=228000, episode_reward=-768.70 +/- 215.16
Episode length: 96.92 +/- 24.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.9     |
|    mean_reward     | -769     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.2     |
|    ep_rew_mean     | -900     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 223      |
|    time_elapsed    | 2369     |
|    total_timesteps | 228352   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-814.55 +/- 170.38
Episode length: 93.34 +/- 20.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 93.3       |
|    mean_reward          | -815       |
| time/                   |            |
|    total_timesteps      | 228500     |
| train/                  |            |
|    approx_kl            | 0.01875368 |
|    clip_fraction        | 0.05       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0845    |
|    explained_variance   | 0.944      |
|    learning_rate        | 0.001      |
|    loss                 | 251        |
|    n_updates            | 2230       |
|    policy_gradient_loss | 0.0138     |
|    value_loss           | 1.08e+03   |
----------------------------------------
Eval num_timesteps=229000, episode_reward=-756.61 +/- 175.00
Episode length: 93.82 +/- 22.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | -757     |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 96       |
|    ep_rew_mean     | -889     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 224      |
|    time_elapsed    | 2379     |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=-839.42 +/- 144.57
Episode length: 86.16 +/- 25.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 86.2        |
|    mean_reward          | -839        |
| time/                   |             |
|    total_timesteps      | 229500      |
| train/                  |             |
|    approx_kl            | 0.013950052 |
|    clip_fraction        | 0.0499      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0822     |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.001       |
|    loss                 | 259         |
|    n_updates            | 2240        |
|    policy_gradient_loss | 0.0112      |
|    value_loss           | 1.14e+03    |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=-774.49 +/- 329.12
Episode length: 88.54 +/- 22.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.5     |
|    mean_reward     | -774     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93.7     |
|    ep_rew_mean     | -876     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 225      |
|    time_elapsed    | 2389     |
|    total_timesteps | 230400   |
---------------------------------
Eval num_timesteps=230500, episode_reward=-817.28 +/- 158.90
Episode length: 92.60 +/- 22.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 92.6       |
|    mean_reward          | -817       |
| time/                   |            |
|    total_timesteps      | 230500     |
| train/                  |            |
|    approx_kl            | 0.04235638 |
|    clip_fraction        | 0.0628     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0786    |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.001      |
|    loss                 | 464        |
|    n_updates            | 2250       |
|    policy_gradient_loss | 0.0133     |
|    value_loss           | 1.79e+03   |
----------------------------------------
Eval num_timesteps=231000, episode_reward=-858.91 +/- 102.00
Episode length: 94.86 +/- 28.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 94.9     |
|    mean_reward     | -859     |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 95.4     |
|    ep_rew_mean     | -864     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 226      |
|    time_elapsed    | 2399     |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=-769.04 +/- 202.97
Episode length: 99.84 +/- 20.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 99.8       |
|    mean_reward          | -769       |
| time/                   |            |
|    total_timesteps      | 231500     |
| train/                  |            |
|    approx_kl            | 0.01592519 |
|    clip_fraction        | 0.0479     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0592    |
|    explained_variance   | 0.945      |
|    learning_rate        | 0.001      |
|    loss                 | 377        |
|    n_updates            | 2260       |
|    policy_gradient_loss | 0.00647    |
|    value_loss           | 1.13e+03   |
----------------------------------------
Eval num_timesteps=232000, episode_reward=-772.01 +/- 209.88
Episode length: 93.76 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.8     |
|    mean_reward     | -772     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94.6     |
|    ep_rew_mean     | -854     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 227      |
|    time_elapsed    | 2409     |
|    total_timesteps | 232448   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-706.76 +/- 255.78
Episode length: 94.84 +/- 24.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.8        |
|    mean_reward          | -707        |
| time/                   |             |
|    total_timesteps      | 232500      |
| train/                  |             |
|    approx_kl            | 0.010135874 |
|    clip_fraction        | 0.0456      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.074      |
|    explained_variance   | 0.925       |
|    learning_rate        | 0.001       |
|    loss                 | 395         |
|    n_updates            | 2270        |
|    policy_gradient_loss | 0.00976     |
|    value_loss           | 1.9e+03     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=233000, episode_reward=-702.66 +/- 271.56
Episode length: 98.46 +/- 27.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 98.5     |
|    mean_reward     | -703     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 94       |
|    ep_rew_mean     | -836     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 228      |
|    time_elapsed    | 2419     |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=-643.94 +/- 274.30
Episode length: 91.44 +/- 25.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.4        |
|    mean_reward          | -644        |
| time/                   |             |
|    total_timesteps      | 233500      |
| train/                  |             |
|    approx_kl            | 0.029623039 |
|    clip_fraction        | 0.074       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.871       |
|    learning_rate        | 0.001       |
|    loss                 | 844         |
|    n_updates            | 2280        |
|    policy_gradient_loss | 0.0124      |
|    value_loss           | 3e+03       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=234000, episode_reward=-658.34 +/- 346.11
Episode length: 89.54 +/- 19.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.5     |
|    mean_reward     | -658     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.2     |
|    ep_rew_mean     | -815     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 229      |
|    time_elapsed    | 2429     |
|    total_timesteps | 234496   |
---------------------------------
Eval num_timesteps=234500, episode_reward=-769.71 +/- 190.88
Episode length: 90.26 +/- 23.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 90.3       |
|    mean_reward          | -770       |
| time/                   |            |
|    total_timesteps      | 234500     |
| train/                  |            |
|    approx_kl            | 0.07359856 |
|    clip_fraction        | 0.0732     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0808    |
|    explained_variance   | 0.898      |
|    learning_rate        | 0.001      |
|    loss                 | 596        |
|    n_updates            | 2290       |
|    policy_gradient_loss | 0.0133     |
|    value_loss           | 2.35e+03   |
----------------------------------------
Eval num_timesteps=235000, episode_reward=-716.84 +/- 198.04
Episode length: 101.50 +/- 28.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | -717     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=-734.03 +/- 257.85
Episode length: 93.40 +/- 19.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.4     |
|    mean_reward     | -734     |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.2     |
|    ep_rew_mean     | -811     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 230      |
|    time_elapsed    | 2444     |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=-810.85 +/- 153.63
Episode length: 84.18 +/- 19.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.2        |
|    mean_reward          | -811        |
| time/                   |             |
|    total_timesteps      | 236000      |
| train/                  |             |
|    approx_kl            | 0.009712159 |
|    clip_fraction        | 0.0395      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0487     |
|    explained_variance   | 0.893       |
|    learning_rate        | 0.001       |
|    loss                 | 277         |
|    n_updates            | 2300        |
|    policy_gradient_loss | 0.00531     |
|    value_loss           | 1.17e+03    |
-----------------------------------------
Eval num_timesteps=236500, episode_reward=-806.35 +/- 176.56
Episode length: 98.98 +/- 21.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 99       |
|    mean_reward     | -806     |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.5     |
|    ep_rew_mean     | -800     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 231      |
|    time_elapsed    | 2453     |
|    total_timesteps | 236544   |
---------------------------------
Eval num_timesteps=237000, episode_reward=-696.78 +/- 244.13
Episode length: 88.82 +/- 19.83
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 88.8       |
|    mean_reward          | -697       |
| time/                   |            |
|    total_timesteps      | 237000     |
| train/                  |            |
|    approx_kl            | 0.03224145 |
|    clip_fraction        | 0.0866     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.117     |
|    explained_variance   | 0.901      |
|    learning_rate        | 0.001      |
|    loss                 | 625        |
|    n_updates            | 2310       |
|    policy_gradient_loss | 0.014      |
|    value_loss           | 2.21e+03   |
----------------------------------------
Eval num_timesteps=237500, episode_reward=-564.12 +/- 410.73
Episode length: 92.86 +/- 23.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.9     |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.6     |
|    ep_rew_mean     | -784     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 232      |
|    time_elapsed    | 2463     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=-622.29 +/- 232.42
Episode length: 91.40 +/- 22.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.4        |
|    mean_reward          | -622        |
| time/                   |             |
|    total_timesteps      | 238000      |
| train/                  |             |
|    approx_kl            | 0.032559436 |
|    clip_fraction        | 0.0888      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0972     |
|    explained_variance   | 0.898       |
|    learning_rate        | 0.001       |
|    loss                 | 426         |
|    n_updates            | 2320        |
|    policy_gradient_loss | 0.0264      |
|    value_loss           | 2.06e+03    |
-----------------------------------------
Eval num_timesteps=238500, episode_reward=-589.71 +/- 253.54
Episode length: 90.32 +/- 22.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.3     |
|    mean_reward     | -590     |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.6     |
|    ep_rew_mean     | -757     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 233      |
|    time_elapsed    | 2473     |
|    total_timesteps | 238592   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-524.80 +/- 468.50
Episode length: 95.56 +/- 23.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 95.6        |
|    mean_reward          | -525        |
| time/                   |             |
|    total_timesteps      | 239000      |
| train/                  |             |
|    approx_kl            | 0.053857636 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.096      |
|    explained_variance   | 0.879       |
|    learning_rate        | 0.001       |
|    loss                 | 1.08e+03    |
|    n_updates            | 2330        |
|    policy_gradient_loss | 0.0359      |
|    value_loss           | 3.03e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=239500, episode_reward=-607.12 +/- 200.96
Episode length: 97.84 +/- 26.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.8     |
|    mean_reward     | -607     |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.1     |
|    ep_rew_mean     | -736     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 234      |
|    time_elapsed    | 2483     |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-630.36 +/- 205.99
Episode length: 92.06 +/- 23.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 92.1        |
|    mean_reward          | -630        |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.022377193 |
|    clip_fraction        | 0.0672      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0717     |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.001       |
|    loss                 | 598         |
|    n_updates            | 2340        |
|    policy_gradient_loss | 0.0173      |
|    value_loss           | 2.76e+03    |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=-605.26 +/- 250.27
Episode length: 92.48 +/- 25.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | -605     |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.6     |
|    ep_rew_mean     | -691     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 235      |
|    time_elapsed    | 2493     |
|    total_timesteps | 240640   |
---------------------------------
Eval num_timesteps=241000, episode_reward=-647.95 +/- 206.66
Episode length: 89.76 +/- 22.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 89.8       |
|    mean_reward          | -648       |
| time/                   |            |
|    total_timesteps      | 241000     |
| train/                  |            |
|    approx_kl            | 0.11787658 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.13      |
|    explained_variance   | 0.855      |
|    learning_rate        | 0.001      |
|    loss                 | 1.18e+03   |
|    n_updates            | 2350       |
|    policy_gradient_loss | 0.027      |
|    value_loss           | 4.58e+03   |
----------------------------------------
Eval num_timesteps=241500, episode_reward=-640.64 +/- 346.89
Episode length: 92.34 +/- 21.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.3     |
|    mean_reward     | -641     |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.2     |
|    ep_rew_mean     | -688     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 236      |
|    time_elapsed    | 2503     |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=-459.02 +/- 382.85
Episode length: 89.82 +/- 25.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 89.8       |
|    mean_reward          | -459       |
| time/                   |            |
|    total_timesteps      | 242000     |
| train/                  |            |
|    approx_kl            | 0.14831248 |
|    clip_fraction        | 0.0871     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0752    |
|    explained_variance   | 0.837      |
|    learning_rate        | 0.001      |
|    loss                 | 990        |
|    n_updates            | 2360       |
|    policy_gradient_loss | 0.033      |
|    value_loss           | 3.08e+03   |
----------------------------------------
New best mean reward!
Eval num_timesteps=242500, episode_reward=-538.78 +/- 253.82
Episode length: 85.70 +/- 20.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.7     |
|    mean_reward     | -539     |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88       |
|    ep_rew_mean     | -660     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 237      |
|    time_elapsed    | 2513     |
|    total_timesteps | 242688   |
---------------------------------
Eval num_timesteps=243000, episode_reward=-603.04 +/- 224.24
Episode length: 82.58 +/- 19.35
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 82.6       |
|    mean_reward          | -603       |
| time/                   |            |
|    total_timesteps      | 243000     |
| train/                  |            |
|    approx_kl            | 0.07551865 |
|    clip_fraction        | 0.0771     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0776    |
|    explained_variance   | 0.851      |
|    learning_rate        | 0.001      |
|    loss                 | 802        |
|    n_updates            | 2370       |
|    policy_gradient_loss | 0.0336     |
|    value_loss           | 3.68e+03   |
----------------------------------------
Eval num_timesteps=243500, episode_reward=-588.58 +/- 223.24
Episode length: 89.20 +/- 24.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.2     |
|    mean_reward     | -589     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.8     |
|    ep_rew_mean     | -648     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 238      |
|    time_elapsed    | 2522     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=-399.18 +/- 340.46
Episode length: 82.56 +/- 26.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 82.6      |
|    mean_reward          | -399      |
| time/                   |           |
|    total_timesteps      | 244000    |
| train/                  |           |
|    approx_kl            | 0.0788016 |
|    clip_fraction        | 0.117     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.126    |
|    explained_variance   | 0.877     |
|    learning_rate        | 0.001     |
|    loss                 | 550       |
|    n_updates            | 2380      |
|    policy_gradient_loss | 0.0484    |
|    value_loss           | 2.66e+03  |
---------------------------------------
New best mean reward!
Eval num_timesteps=244500, episode_reward=-310.83 +/- 444.05
Episode length: 86.98 +/- 22.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87       |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91       |
|    ep_rew_mean     | -602     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 239      |
|    time_elapsed    | 2531     |
|    total_timesteps | 244736   |
---------------------------------
Eval num_timesteps=245000, episode_reward=-378.47 +/- 311.12
Episode length: 84.14 +/- 17.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.1        |
|    mean_reward          | -378        |
| time/                   |             |
|    total_timesteps      | 245000      |
| train/                  |             |
|    approx_kl            | 0.038417686 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.001       |
|    loss                 | 612         |
|    n_updates            | 2390        |
|    policy_gradient_loss | 0.0312      |
|    value_loss           | 3.21e+03    |
-----------------------------------------
Eval num_timesteps=245500, episode_reward=-235.66 +/- 479.09
Episode length: 85.44 +/- 19.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.4     |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.3     |
|    ep_rew_mean     | -563     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 240      |
|    time_elapsed    | 2540     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=-36.74 +/- 614.88
Episode length: 88.94 +/- 21.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 88.9        |
|    mean_reward          | -36.7       |
| time/                   |             |
|    total_timesteps      | 246000      |
| train/                  |             |
|    approx_kl            | 0.027671583 |
|    clip_fraction        | 0.0733      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.001       |
|    loss                 | 1.2e+03     |
|    n_updates            | 2400        |
|    policy_gradient_loss | 0.0173      |
|    value_loss           | 3.87e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=246500, episode_reward=-202.69 +/- 627.22
Episode length: 78.26 +/- 19.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.3     |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 89.5     |
|    ep_rew_mean     | -505     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 241      |
|    time_elapsed    | 2549     |
|    total_timesteps | 246784   |
---------------------------------
Eval num_timesteps=247000, episode_reward=-359.29 +/- 363.47
Episode length: 87.52 +/- 24.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 87.5        |
|    mean_reward          | -359        |
| time/                   |             |
|    total_timesteps      | 247000      |
| train/                  |             |
|    approx_kl            | 0.028285384 |
|    clip_fraction        | 0.0841      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.001       |
|    loss                 | 1.98e+03    |
|    n_updates            | 2410        |
|    policy_gradient_loss | 0.0117      |
|    value_loss           | 1.16e+04    |
-----------------------------------------
Eval num_timesteps=247500, episode_reward=-379.46 +/- 328.37
Episode length: 88.04 +/- 19.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88       |
|    mean_reward     | -379     |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 87.6     |
|    ep_rew_mean     | -497     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 242      |
|    time_elapsed    | 2559     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=-355.04 +/- 269.31
Episode length: 83.24 +/- 19.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 83.2       |
|    mean_reward          | -355       |
| time/                   |            |
|    total_timesteps      | 248000     |
| train/                  |            |
|    approx_kl            | 0.03711737 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.156     |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.001      |
|    loss                 | 1.81e+03   |
|    n_updates            | 2420       |
|    policy_gradient_loss | 0.0176     |
|    value_loss           | 5.02e+03   |
----------------------------------------
Eval num_timesteps=248500, episode_reward=-329.33 +/- 340.11
Episode length: 80.36 +/- 21.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.4     |
|    mean_reward     | -329     |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 86.5     |
|    ep_rew_mean     | -470     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 243      |
|    time_elapsed    | 2568     |
|    total_timesteps | 248832   |
---------------------------------
Eval num_timesteps=249000, episode_reward=-299.69 +/- 395.47
Episode length: 82.60 +/- 20.31
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 82.6       |
|    mean_reward          | -300       |
| time/                   |            |
|    total_timesteps      | 249000     |
| train/                  |            |
|    approx_kl            | 0.05938427 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.14      |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.001      |
|    loss                 | 702        |
|    n_updates            | 2430       |
|    policy_gradient_loss | 0.0226     |
|    value_loss           | 6.24e+03   |
----------------------------------------
Eval num_timesteps=249500, episode_reward=-378.94 +/- 352.91
Episode length: 84.72 +/- 19.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.7     |
|    mean_reward     | -379     |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 85.9     |
|    ep_rew_mean     | -431     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 244      |
|    time_elapsed    | 2577     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-296.81 +/- 415.39
Episode length: 80.56 +/- 22.42
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.6        |
|    mean_reward          | -297        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.071677566 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.001       |
|    loss                 | 3.25e+03    |
|    n_updates            | 2440        |
|    policy_gradient_loss | 0.0103      |
|    value_loss           | 1.1e+04     |
-----------------------------------------
Eval num_timesteps=250500, episode_reward=-277.45 +/- 476.25
Episode length: 82.44 +/- 17.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.4     |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84.8     |
|    ep_rew_mean     | -385     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 245      |
|    time_elapsed    | 2586     |
|    total_timesteps | 250880   |
---------------------------------
Eval num_timesteps=251000, episode_reward=-256.49 +/- 539.97
Episode length: 81.48 +/- 22.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 81.5        |
|    mean_reward          | -256        |
| time/                   |             |
|    total_timesteps      | 251000      |
| train/                  |             |
|    approx_kl            | 0.033493772 |
|    clip_fraction        | 0.076       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.001       |
|    loss                 | 577         |
|    n_updates            | 2450        |
|    policy_gradient_loss | 0.0224      |
|    value_loss           | 4.48e+03    |
-----------------------------------------
Eval num_timesteps=251500, episode_reward=-165.69 +/- 511.23
Episode length: 89.30 +/- 21.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.3     |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.4     |
|    ep_rew_mean     | -360     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 246      |
|    time_elapsed    | 2595     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=-344.80 +/- 244.59
Episode length: 84.88 +/- 18.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 84.9       |
|    mean_reward          | -345       |
| time/                   |            |
|    total_timesteps      | 252000     |
| train/                  |            |
|    approx_kl            | 0.05338258 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.133     |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.001      |
|    loss                 | 1.19e+03   |
|    n_updates            | 2460       |
|    policy_gradient_loss | 0.00955    |
|    value_loss           | 5.93e+03   |
----------------------------------------
Eval num_timesteps=252500, episode_reward=-246.28 +/- 531.80
Episode length: 84.64 +/- 19.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.6     |
|    mean_reward     | -246     |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82       |
|    ep_rew_mean     | -336     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 247      |
|    time_elapsed    | 2605     |
|    total_timesteps | 252928   |
---------------------------------
Eval num_timesteps=253000, episode_reward=-508.73 +/- 329.08
Episode length: 79.68 +/- 19.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 79.7       |
|    mean_reward          | -509       |
| time/                   |            |
|    total_timesteps      | 253000     |
| train/                  |            |
|    approx_kl            | 0.17140734 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.168     |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.001      |
|    loss                 | 1.81e+03   |
|    n_updates            | 2470       |
|    policy_gradient_loss | 0.041      |
|    value_loss           | 8.29e+03   |
----------------------------------------
Eval num_timesteps=253500, episode_reward=-540.73 +/- 173.04
Episode length: 82.90 +/- 19.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 82.9     |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.1     |
|    ep_rew_mean     | -350     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 248      |
|    time_elapsed    | 2614     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=-422.33 +/- 331.44
Episode length: 82.28 +/- 19.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.3        |
|    mean_reward          | -422        |
| time/                   |             |
|    total_timesteps      | 254000      |
| train/                  |             |
|    approx_kl            | 0.048713155 |
|    clip_fraction        | 0.147       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.177      |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.001       |
|    loss                 | 393         |
|    n_updates            | 2480        |
|    policy_gradient_loss | 0.0336      |
|    value_loss           | 2.67e+03    |
-----------------------------------------
Eval num_timesteps=254500, episode_reward=-386.53 +/- 443.48
Episode length: 89.40 +/- 24.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.4     |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.3     |
|    ep_rew_mean     | -373     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 249      |
|    time_elapsed    | 2623     |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=255000, episode_reward=-419.36 +/- 403.56
Episode length: 86.20 +/- 20.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 86.2       |
|    mean_reward          | -419       |
| time/                   |            |
|    total_timesteps      | 255000     |
| train/                  |            |
|    approx_kl            | 0.05366534 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.179     |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.001      |
|    loss                 | 947        |
|    n_updates            | 2490       |
|    policy_gradient_loss | 0.0139     |
|    value_loss           | 3.94e+03   |
----------------------------------------
Eval num_timesteps=255500, episode_reward=-461.71 +/- 250.53
Episode length: 81.20 +/- 19.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 81.2     |
|    mean_reward     | -462     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-367.49 +/- 216.35
Episode length: 85.40 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.4     |
|    mean_reward     | -367     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 81.7     |
|    ep_rew_mean     | -371     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 250      |
|    time_elapsed    | 2636     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=-400.26 +/- 262.39
Episode length: 87.72 +/- 24.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 87.7       |
|    mean_reward          | -400       |
| time/                   |            |
|    total_timesteps      | 256500     |
| train/                  |            |
|    approx_kl            | 0.05146873 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.206     |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.001      |
|    loss                 | 731        |
|    n_updates            | 2500       |
|    policy_gradient_loss | 0.0255     |
|    value_loss           | 4.51e+03   |
----------------------------------------
Eval num_timesteps=257000, episode_reward=-329.60 +/- 421.23
Episode length: 88.12 +/- 21.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.1     |
|    mean_reward     | -330     |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.5     |
|    ep_rew_mean     | -404     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 251      |
|    time_elapsed    | 2646     |
|    total_timesteps | 257024   |
---------------------------------
Eval num_timesteps=257500, episode_reward=-212.56 +/- 400.82
Episode length: 80.06 +/- 19.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.1        |
|    mean_reward          | -213        |
| time/                   |             |
|    total_timesteps      | 257500      |
| train/                  |             |
|    approx_kl            | 0.056666676 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.759       |
|    learning_rate        | 0.001       |
|    loss                 | 554         |
|    n_updates            | 2510        |
|    policy_gradient_loss | 0.023       |
|    value_loss           | 2.59e+03    |
-----------------------------------------
Eval num_timesteps=258000, episode_reward=-207.07 +/- 511.72
Episode length: 78.16 +/- 22.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.2     |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.5     |
|    ep_rew_mean     | -372     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 252      |
|    time_elapsed    | 2655     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=223.50 +/- 696.72
Episode length: 63.00 +/- 10.37
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 63         |
|    mean_reward          | 224        |
| time/                   |            |
|    total_timesteps      | 258500     |
| train/                  |            |
|    approx_kl            | 0.19535427 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.306     |
|    explained_variance   | 0.573      |
|    learning_rate        | 0.001      |
|    loss                 | 888        |
|    n_updates            | 2520       |
|    policy_gradient_loss | 0.00848    |
|    value_loss           | 1.1e+04    |
----------------------------------------
New best mean reward!
Eval num_timesteps=259000, episode_reward=408.67 +/- 762.56
Episode length: 62.22 +/- 11.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.2     |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.2     |
|    ep_rew_mean     | -313     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 253      |
|    time_elapsed    | 2662     |
|    total_timesteps | 259072   |
---------------------------------
Eval num_timesteps=259500, episode_reward=710.71 +/- 801.26
Episode length: 48.76 +/- 6.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 48.8       |
|    mean_reward          | 711        |
| time/                   |            |
|    total_timesteps      | 259500     |
| train/                  |            |
|    approx_kl            | 0.09869335 |
|    clip_fraction        | 0.242      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.351     |
|    explained_variance   | 0.264      |
|    learning_rate        | 0.001      |
|    loss                 | 1.92e+03   |
|    n_updates            | 2530       |
|    policy_gradient_loss | 0.0198     |
|    value_loss           | 2.02e+04   |
----------------------------------------
New best mean reward!
Eval num_timesteps=260000, episode_reward=695.48 +/- 788.48
Episode length: 50.52 +/- 7.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 64.8     |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 254      |
|    time_elapsed    | 2668     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=977.31 +/- 780.44
Episode length: 46.72 +/- 5.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 46.7       |
|    mean_reward          | 977        |
| time/                   |            |
|    total_timesteps      | 260500     |
| train/                  |            |
|    approx_kl            | 0.08298878 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.285     |
|    explained_variance   | 0.132      |
|    learning_rate        | 0.001      |
|    loss                 | 6.39e+03   |
|    n_updates            | 2540       |
|    policy_gradient_loss | 0.0153     |
|    value_loss           | 4.28e+04   |
----------------------------------------
New best mean reward!
Eval num_timesteps=261000, episode_reward=654.59 +/- 797.94
Episode length: 46.18 +/- 10.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 46.2     |
|    mean_reward     | 655      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 56.6     |
|    ep_rew_mean     | 80.4     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 255      |
|    time_elapsed    | 2674     |
|    total_timesteps | 261120   |
---------------------------------
Eval num_timesteps=261500, episode_reward=769.07 +/- 787.64
Episode length: 47.54 +/- 8.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 47.5        |
|    mean_reward          | 769         |
| time/                   |             |
|    total_timesteps      | 261500      |
| train/                  |             |
|    approx_kl            | 0.030245673 |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.276      |
|    explained_variance   | 0.126       |
|    learning_rate        | 0.001       |
|    loss                 | 8.15e+03    |
|    n_updates            | 2550        |
|    policy_gradient_loss | 0.0128      |
|    value_loss           | 5.09e+04    |
-----------------------------------------
Eval num_timesteps=262000, episode_reward=967.92 +/- 760.49
Episode length: 50.08 +/- 7.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.1     |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.5     |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 97       |
|    iterations      | 256      |
|    time_elapsed    | 2680     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=1288.70 +/- 701.08
Episode length: 42.72 +/- 3.12
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.7       |
|    mean_reward          | 1.29e+03   |
| time/                   |            |
|    total_timesteps      | 262500     |
| train/                  |            |
|    approx_kl            | 0.16108862 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.229     |
|    explained_variance   | 0.0871     |
|    learning_rate        | 0.001      |
|    loss                 | 1.15e+04   |
|    n_updates            | 2560       |
|    policy_gradient_loss | 0.012      |
|    value_loss           | 5.34e+04   |
----------------------------------------
New best mean reward!
Eval num_timesteps=263000, episode_reward=1214.12 +/- 729.67
Episode length: 42.30 +/- 3.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 45.9     |
|    ep_rew_mean     | 594      |
| time/              |          |
|    fps             | 97       |
|    iterations      | 257      |
|    time_elapsed    | 2686     |
|    total_timesteps | 263168   |
---------------------------------
Eval num_timesteps=263500, episode_reward=1519.67 +/- 550.67
Episode length: 42.14 +/- 2.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.52e+03    |
| time/                   |             |
|    total_timesteps      | 263500      |
| train/                  |             |
|    approx_kl            | 0.021356225 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | -0.00616    |
|    learning_rate        | 0.001       |
|    loss                 | 1.17e+04    |
|    n_updates            | 2570        |
|    policy_gradient_loss | 0.0101      |
|    value_loss           | 5.32e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=264000, episode_reward=1503.75 +/- 544.02
Episode length: 42.12 +/- 2.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 44.8     |
|    ep_rew_mean     | 758      |
| time/              |          |
|    fps             | 98       |
|    iterations      | 258      |
|    time_elapsed    | 2691     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=1309.46 +/- 684.55
Episode length: 42.94 +/- 3.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.9        |
|    mean_reward          | 1.31e+03    |
| time/                   |             |
|    total_timesteps      | 264500      |
| train/                  |             |
|    approx_kl            | 0.015224001 |
|    clip_fraction        | 0.0914      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | -0.203      |
|    learning_rate        | 0.001       |
|    loss                 | 9.04e+03    |
|    n_updates            | 2580        |
|    policy_gradient_loss | 0.00183     |
|    value_loss           | 5.67e+04    |
-----------------------------------------
Eval num_timesteps=265000, episode_reward=1363.48 +/- 645.30
Episode length: 42.24 +/- 3.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.9     |
|    ep_rew_mean     | 840      |
| time/              |          |
|    fps             | 98       |
|    iterations      | 259      |
|    time_elapsed    | 2697     |
|    total_timesteps | 265216   |
---------------------------------
Eval num_timesteps=265500, episode_reward=1383.32 +/- 646.91
Episode length: 41.76 +/- 2.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.8        |
|    mean_reward          | 1.38e+03    |
| time/                   |             |
|    total_timesteps      | 265500      |
| train/                  |             |
|    approx_kl            | 0.023175996 |
|    clip_fraction        | 0.0834      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | -0.0809     |
|    learning_rate        | 0.001       |
|    loss                 | 7.12e+03    |
|    n_updates            | 2590        |
|    policy_gradient_loss | 0.00441     |
|    value_loss           | 6.2e+04     |
-----------------------------------------
Eval num_timesteps=266000, episode_reward=1315.43 +/- 649.32
Episode length: 41.42 +/- 3.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.7     |
|    ep_rew_mean     | 959      |
| time/              |          |
|    fps             | 98       |
|    iterations      | 260      |
|    time_elapsed    | 2702     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=1348.79 +/- 677.87
Episode length: 41.50 +/- 3.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.5         |
|    mean_reward          | 1.35e+03     |
| time/                   |              |
|    total_timesteps      | 266500       |
| train/                  |              |
|    approx_kl            | 0.0136196595 |
|    clip_fraction        | 0.088        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0882      |
|    explained_variance   | -0.268       |
|    learning_rate        | 0.001        |
|    loss                 | 1.59e+04     |
|    n_updates            | 2600         |
|    policy_gradient_loss | 0.00242      |
|    value_loss           | 6.5e+04      |
------------------------------------------
Eval num_timesteps=267000, episode_reward=1439.64 +/- 561.60
Episode length: 42.24 +/- 2.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 43.2     |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    fps             | 98       |
|    iterations      | 261      |
|    time_elapsed    | 2708     |
|    total_timesteps | 267264   |
---------------------------------
Eval num_timesteps=267500, episode_reward=1342.11 +/- 632.77
Episode length: 42.12 +/- 2.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 1.34e+03    |
| time/                   |             |
|    total_timesteps      | 267500      |
| train/                  |             |
|    approx_kl            | 0.025130803 |
|    clip_fraction        | 0.0311      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0249     |
|    explained_variance   | -0.357      |
|    learning_rate        | 0.001       |
|    loss                 | 9.3e+03     |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.00407    |
|    value_loss           | 5.05e+04    |
-----------------------------------------
Eval num_timesteps=268000, episode_reward=1357.83 +/- 686.46
Episode length: 41.44 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.7     |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 98       |
|    iterations      | 262      |
|    time_elapsed    | 2713     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=1479.78 +/- 586.95
Episode length: 41.92 +/- 1.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 1.48e+03     |
| time/                   |              |
|    total_timesteps      | 268500       |
| train/                  |              |
|    approx_kl            | 0.0015007588 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00938     |
|    explained_variance   | -0.0496      |
|    learning_rate        | 0.001        |
|    loss                 | 1.08e+04     |
|    n_updates            | 2620         |
|    policy_gradient_loss | -0.000512    |
|    value_loss           | 5.9e+04      |
------------------------------------------
Eval num_timesteps=269000, episode_reward=1523.21 +/- 559.12
Episode length: 42.20 +/- 2.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.8     |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 99       |
|    iterations      | 263      |
|    time_elapsed    | 2719     |
|    total_timesteps | 269312   |
---------------------------------
Eval num_timesteps=269500, episode_reward=1491.91 +/- 520.77
Episode length: 42.22 +/- 1.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.2        |
|    mean_reward          | 1.49e+03    |
| time/                   |             |
|    total_timesteps      | 269500      |
| train/                  |             |
|    approx_kl            | 0.005219969 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000815   |
|    explained_variance   | -1.02       |
|    learning_rate        | 0.001       |
|    loss                 | 5.99e+03    |
|    n_updates            | 2630        |
|    policy_gradient_loss | 0.00129     |
|    value_loss           | 2.02e+04    |
-----------------------------------------
Eval num_timesteps=270000, episode_reward=1485.89 +/- 534.01
Episode length: 42.24 +/- 1.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.5     |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    fps             | 99       |
|    iterations      | 264      |
|    time_elapsed    | 2724     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=1297.88 +/- 700.83
Episode length: 41.26 +/- 3.33
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 41.3           |
|    mean_reward          | 1.3e+03        |
| time/                   |                |
|    total_timesteps      | 270500         |
| train/                  |                |
|    approx_kl            | -3.2014214e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.58e-05      |
|    explained_variance   | 0.176          |
|    learning_rate        | 0.001          |
|    loss                 | 5.85e+03       |
|    n_updates            | 2640           |
|    policy_gradient_loss | -6e-06         |
|    value_loss           | 4.41e+04       |
--------------------------------------------
Eval num_timesteps=271000, episode_reward=1490.15 +/- 549.76
Episode length: 41.80 +/- 2.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 99       |
|    iterations      | 265      |
|    time_elapsed    | 2729     |
|    total_timesteps | 271360   |
---------------------------------
Eval num_timesteps=271500, episode_reward=1410.21 +/- 636.13
Episode length: 41.98 +/- 2.53
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42             |
|    mean_reward          | 1.41e+03       |
| time/                   |                |
|    total_timesteps      | 271500         |
| train/                  |                |
|    approx_kl            | -6.8102963e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.7e-05       |
|    explained_variance   | 0.0336         |
|    learning_rate        | 0.001          |
|    loss                 | 1.64e+04       |
|    n_updates            | 2650           |
|    policy_gradient_loss | -2.36e-07      |
|    value_loss           | 5.59e+04       |
--------------------------------------------
Eval num_timesteps=272000, episode_reward=1419.17 +/- 625.87
Episode length: 41.86 +/- 2.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    fps             | 99       |
|    iterations      | 266      |
|    time_elapsed    | 2735     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=1481.12 +/- 586.11
Episode length: 42.04 +/- 2.57
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42             |
|    mean_reward          | 1.48e+03       |
| time/                   |                |
|    total_timesteps      | 272500         |
| train/                  |                |
|    approx_kl            | -3.2014214e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.49e-05      |
|    explained_variance   | 0.00808        |
|    learning_rate        | 0.001          |
|    loss                 | 1.3e+04        |
|    n_updates            | 2660           |
|    policy_gradient_loss | 1.11e-07       |
|    value_loss           | 5.23e+04       |
--------------------------------------------
Eval num_timesteps=273000, episode_reward=1480.45 +/- 584.52
Episode length: 41.86 +/- 2.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.48e+03 |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 99       |
|    iterations      | 267      |
|    time_elapsed    | 2740     |
|    total_timesteps | 273408   |
---------------------------------
Eval num_timesteps=273500, episode_reward=1554.30 +/- 463.18
Episode length: 42.42 +/- 2.17
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42.4           |
|    mean_reward          | 1.55e+03       |
| time/                   |                |
|    total_timesteps      | 273500         |
| train/                  |                |
|    approx_kl            | -2.8521754e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000105      |
|    explained_variance   | -0.169         |
|    learning_rate        | 0.001          |
|    loss                 | 1.14e+04       |
|    n_updates            | 2670           |
|    policy_gradient_loss | -1.77e-06      |
|    value_loss           | 5.12e+04       |
--------------------------------------------
New best mean reward!
Eval num_timesteps=274000, episode_reward=1333.30 +/- 685.81
Episode length: 41.46 +/- 3.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.7     |
|    ep_rew_mean     | 1.39e+03 |
| time/              |          |
|    fps             | 99       |
|    iterations      | 268      |
|    time_elapsed    | 2746     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=1389.17 +/- 627.49
Episode length: 42.18 +/- 2.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.2          |
|    mean_reward          | 1.39e+03      |
| time/                   |               |
|    total_timesteps      | 274500        |
| train/                  |               |
|    approx_kl            | -2.386514e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000112     |
|    explained_variance   | -0.115        |
|    learning_rate        | 0.001         |
|    loss                 | 9.73e+03      |
|    n_updates            | 2680          |
|    policy_gradient_loss | -3.39e-07     |
|    value_loss           | 4.7e+04       |
-------------------------------------------
Eval num_timesteps=275000, episode_reward=1455.63 +/- 590.79
Episode length: 42.04 +/- 1.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    fps             | 100      |
|    iterations      | 269      |
|    time_elapsed    | 2752     |
|    total_timesteps | 275456   |
---------------------------------
Eval num_timesteps=275500, episode_reward=1432.85 +/- 575.21
Episode length: 42.30 +/- 1.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.3        |
|    mean_reward          | 1.43e+03    |
| time/                   |             |
|    total_timesteps      | 275500      |
| train/                  |             |
|    approx_kl            | 4.48199e-09 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000191   |
|    explained_variance   | -0.541      |
|    learning_rate        | 0.001       |
|    loss                 | 7.66e+03    |
|    n_updates            | 2690        |
|    policy_gradient_loss | -1.61e-06   |
|    value_loss           | 2.94e+04    |
-----------------------------------------
Eval num_timesteps=276000, episode_reward=1457.85 +/- 577.21
Episode length: 42.22 +/- 2.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 100      |
|    iterations      | 270      |
|    time_elapsed    | 2757     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=1411.45 +/- 630.32
Episode length: 42.30 +/- 1.73
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42.3           |
|    mean_reward          | 1.41e+03       |
| time/                   |                |
|    total_timesteps      | 276500         |
| train/                  |                |
|    approx_kl            | -1.2805685e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000136      |
|    explained_variance   | 0.0917         |
|    learning_rate        | 0.001          |
|    loss                 | 8.23e+03       |
|    n_updates            | 2700           |
|    policy_gradient_loss | -5.25e-07      |
|    value_loss           | 5.68e+04       |
--------------------------------------------
Eval num_timesteps=277000, episode_reward=1552.41 +/- 577.88
Episode length: 42.34 +/- 1.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=1414.74 +/- 597.20
Episode length: 42.20 +/- 1.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.9     |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    fps             | 100      |
|    iterations      | 271      |
|    time_elapsed    | 2764     |
|    total_timesteps | 277504   |
---------------------------------
Eval num_timesteps=278000, episode_reward=1549.06 +/- 514.58
Episode length: 41.96 +/- 1.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42            |
|    mean_reward          | 1.55e+03      |
| time/                   |               |
|    total_timesteps      | 278000        |
| train/                  |               |
|    approx_kl            | -4.307367e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -8.03e-05     |
|    explained_variance   | -0.161        |
|    learning_rate        | 0.001         |
|    loss                 | 9.97e+03      |
|    n_updates            | 2710          |
|    policy_gradient_loss | -5.54e-07     |
|    value_loss           | 3.68e+04      |
-------------------------------------------
Eval num_timesteps=278500, episode_reward=1426.26 +/- 545.90
Episode length: 42.02 +/- 1.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 1.49e+03 |
| time/              |          |
|    fps             | 100      |
|    iterations      | 272      |
|    time_elapsed    | 2770     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=1322.64 +/- 638.51
Episode length: 41.92 +/- 2.03
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 41.9           |
|    mean_reward          | 1.32e+03       |
| time/                   |                |
|    total_timesteps      | 279000         |
| train/                  |                |
|    approx_kl            | -1.8626451e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.3e-05       |
|    explained_variance   | -0.0354        |
|    learning_rate        | 0.001          |
|    loss                 | 6.89e+03       |
|    n_updates            | 2720           |
|    policy_gradient_loss | 2.52e-07       |
|    value_loss           | 3.09e+04       |
--------------------------------------------
Eval num_timesteps=279500, episode_reward=1490.28 +/- 546.55
Episode length: 41.94 +/- 2.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 1.48e+03 |
| time/              |          |
|    fps             | 100      |
|    iterations      | 273      |
|    time_elapsed    | 2775     |
|    total_timesteps | 279552   |
---------------------------------
Eval num_timesteps=280000, episode_reward=1604.84 +/- 489.19
Episode length: 42.38 +/- 1.70
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42.4           |
|    mean_reward          | 1.6e+03        |
| time/                   |                |
|    total_timesteps      | 280000         |
| train/                  |                |
|    approx_kl            | -2.0954758e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.0001        |
|    explained_variance   | -0.00877       |
|    learning_rate        | 0.001          |
|    loss                 | 1.04e+04       |
|    n_updates            | 2730           |
|    policy_gradient_loss | -3.65e-06      |
|    value_loss           | 2.61e+04       |
--------------------------------------------
New best mean reward!
Eval num_timesteps=280500, episode_reward=1323.21 +/- 608.25
Episode length: 42.16 +/- 1.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    fps             | 100      |
|    iterations      | 274      |
|    time_elapsed    | 2781     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=1538.88 +/- 478.05
Episode length: 42.66 +/- 1.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.7          |
|    mean_reward          | 1.54e+03      |
| time/                   |               |
|    total_timesteps      | 281000        |
| train/                  |               |
|    approx_kl            | -3.259629e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.73e-05     |
|    explained_variance   | -0.157        |
|    learning_rate        | 0.001         |
|    loss                 | 1e+04         |
|    n_updates            | 2740          |
|    policy_gradient_loss | -1.56e-06     |
|    value_loss           | 3.54e+04      |
-------------------------------------------
Eval num_timesteps=281500, episode_reward=1238.24 +/- 694.14
Episode length: 41.24 +/- 3.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 1.56e+03 |
| time/              |          |
|    fps             | 101      |
|    iterations      | 275      |
|    time_elapsed    | 2786     |
|    total_timesteps | 281600   |
---------------------------------
Eval num_timesteps=282000, episode_reward=1555.63 +/- 566.73
Episode length: 41.70 +/- 2.47
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 41.7           |
|    mean_reward          | 1.56e+03       |
| time/                   |                |
|    total_timesteps      | 282000         |
| train/                  |                |
|    approx_kl            | -5.4133125e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -5.61e-05      |
|    explained_variance   | -0.0218        |
|    learning_rate        | 0.001          |
|    loss                 | 8.15e+03       |
|    n_updates            | 2750           |
|    policy_gradient_loss | -9.06e-07      |
|    value_loss           | 3.69e+04       |
--------------------------------------------
Eval num_timesteps=282500, episode_reward=1404.98 +/- 623.12
Episode length: 41.92 +/- 2.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 1.52e+03 |
| time/              |          |
|    fps             | 101      |
|    iterations      | 276      |
|    time_elapsed    | 2792     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=1399.72 +/- 659.76
Episode length: 42.02 +/- 2.60
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42             |
|    mean_reward          | 1.4e+03        |
| time/                   |                |
|    total_timesteps      | 283000         |
| train/                  |                |
|    approx_kl            | -3.2014214e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -4.98e-05      |
|    explained_variance   | 0.154          |
|    learning_rate        | 0.001          |
|    loss                 | 8.52e+03       |
|    n_updates            | 2760           |
|    policy_gradient_loss | -2.36e-07      |
|    value_loss           | 3.38e+04       |
--------------------------------------------
Eval num_timesteps=283500, episode_reward=1470.81 +/- 552.43
Episode length: 41.76 +/- 2.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 1.46e+03 |
| time/              |          |
|    fps             | 101      |
|    iterations      | 277      |
|    time_elapsed    | 2797     |
|    total_timesteps | 283648   |
---------------------------------
Eval num_timesteps=284000, episode_reward=1450.78 +/- 566.39
Episode length: 42.20 +/- 2.47
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42.2           |
|    mean_reward          | 1.45e+03       |
| time/                   |                |
|    total_timesteps      | 284000         |
| train/                  |                |
|    approx_kl            | -4.8894435e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000101      |
|    explained_variance   | 0.0508         |
|    learning_rate        | 0.001          |
|    loss                 | 1.27e+04       |
|    n_updates            | 2770           |
|    policy_gradient_loss | -1.01e-06      |
|    value_loss           | 6.62e+04       |
--------------------------------------------
Eval num_timesteps=284500, episode_reward=1519.19 +/- 465.22
Episode length: 42.18 +/- 2.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    fps             | 101      |
|    iterations      | 278      |
|    time_elapsed    | 2803     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=1471.38 +/- 636.28
Episode length: 41.96 +/- 2.28
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42             |
|    mean_reward          | 1.47e+03       |
| time/                   |                |
|    total_timesteps      | 285000         |
| train/                  |                |
|    approx_kl            | -2.2700988e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000132      |
|    explained_variance   | -0.144         |
|    learning_rate        | 0.001          |
|    loss                 | 1.27e+04       |
|    n_updates            | 2780           |
|    policy_gradient_loss | 1.44e-06       |
|    value_loss           | 4.62e+04       |
--------------------------------------------
Eval num_timesteps=285500, episode_reward=1517.26 +/- 455.45
Episode length: 42.10 +/- 2.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 1.46e+03 |
| time/              |          |
|    fps             | 101      |
|    iterations      | 279      |
|    time_elapsed    | 2808     |
|    total_timesteps | 285696   |
---------------------------------
Eval num_timesteps=286000, episode_reward=1551.11 +/- 499.29
Episode length: 42.30 +/- 2.32
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42.3           |
|    mean_reward          | 1.55e+03       |
| time/                   |                |
|    total_timesteps      | 286000         |
| train/                  |                |
|    approx_kl            | -1.1641532e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000159      |
|    explained_variance   | -0.292         |
|    learning_rate        | 0.001          |
|    loss                 | 8.93e+03       |
|    n_updates            | 2790           |
|    policy_gradient_loss | -1.11e-06      |
|    value_loss           | 2.77e+04       |
--------------------------------------------
Eval num_timesteps=286500, episode_reward=1491.86 +/- 539.85
Episode length: 42.24 +/- 2.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.1     |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 101      |
|    iterations      | 280      |
|    time_elapsed    | 2813     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=1421.94 +/- 623.26
Episode length: 42.02 +/- 2.06
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42             |
|    mean_reward          | 1.42e+03       |
| time/                   |                |
|    total_timesteps      | 287000         |
| train/                  |                |
|    approx_kl            | -2.2700988e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000111      |
|    explained_variance   | 0.103          |
|    learning_rate        | 0.001          |
|    loss                 | 6.57e+03       |
|    n_updates            | 2800           |
|    policy_gradient_loss | -3.43e-06      |
|    value_loss           | 3.99e+04       |
--------------------------------------------
Eval num_timesteps=287500, episode_reward=1422.78 +/- 620.24
Episode length: 41.76 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 1.49e+03 |
| time/              |          |
|    fps             | 102      |
|    iterations      | 281      |
|    time_elapsed    | 2819     |
|    total_timesteps | 287744   |
---------------------------------
Eval num_timesteps=288000, episode_reward=1398.56 +/- 602.89
Episode length: 41.90 +/- 2.18
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 41.9           |
|    mean_reward          | 1.4e+03        |
| time/                   |                |
|    total_timesteps      | 288000         |
| train/                  |                |
|    approx_kl            | -2.2118911e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -7.21e-05      |
|    explained_variance   | 0.163          |
|    learning_rate        | 0.001          |
|    loss                 | 6.43e+03       |
|    n_updates            | 2810           |
|    policy_gradient_loss | -4.98e-06      |
|    value_loss           | 3.35e+04       |
--------------------------------------------
Eval num_timesteps=288500, episode_reward=1359.09 +/- 606.80
Episode length: 41.58 +/- 2.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.4     |
|    ep_rew_mean     | 1.48e+03 |
| time/              |          |
|    fps             | 102      |
|    iterations      | 282      |
|    time_elapsed    | 2824     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=1465.41 +/- 596.49
Episode length: 42.04 +/- 2.46
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42             |
|    mean_reward          | 1.47e+03       |
| time/                   |                |
|    total_timesteps      | 289000         |
| train/                  |                |
|    approx_kl            | -1.0477379e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000135      |
|    explained_variance   | -0.237         |
|    learning_rate        | 0.001          |
|    loss                 | 1.11e+04       |
|    n_updates            | 2820           |
|    policy_gradient_loss | -3.65e-06      |
|    value_loss           | 3.55e+04       |
--------------------------------------------
Eval num_timesteps=289500, episode_reward=1576.08 +/- 465.77
Episode length: 42.86 +/- 1.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.9     |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    fps             | 102      |
|    iterations      | 283      |
|    time_elapsed    | 2830     |
|    total_timesteps | 289792   |
---------------------------------
Eval num_timesteps=290000, episode_reward=1394.40 +/- 653.76
Episode length: 41.96 +/- 2.68
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42             |
|    mean_reward          | 1.39e+03       |
| time/                   |                |
|    total_timesteps      | 290000         |
| train/                  |                |
|    approx_kl            | -1.1641532e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000148      |
|    explained_variance   | 0.00815        |
|    learning_rate        | 0.001          |
|    loss                 | 9.68e+03       |
|    n_updates            | 2830           |
|    policy_gradient_loss | -1.38e-07      |
|    value_loss           | 4.99e+04       |
--------------------------------------------
Eval num_timesteps=290500, episode_reward=1465.66 +/- 545.35
Episode length: 42.22 +/- 2.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42.2     |
|    ep_rew_mean     | 1.46e+03 |
| time/              |          |
|    fps             | 102      |
|    iterations      | 284      |
|    time_elapsed    | 2835     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=1527.98 +/- 454.83
Episode length: 42.24 +/- 1.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 42.2      |
|    mean_reward          | 1.53e+03  |
| time/                   |           |
|    total_timesteps      | 291000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000233 |
|    explained_variance   | -0.489    |
|    learning_rate        | 0.001     |
|    loss                 | 5.73e+03  |
|    n_updates            | 2840      |
|    policy_gradient_loss | -8.22e-06 |
|    value_loss           | 2.81e+04  |
---------------------------------------
Eval num_timesteps=291500, episode_reward=1506.73 +/- 551.06
Episode length: 42.02 +/- 2.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.51e+03 |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.6     |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 102      |
|    iterations      | 285      |
|    time_elapsed    | 2841     |
|    total_timesteps | 291840   |
---------------------------------
Eval num_timesteps=292000, episode_reward=1362.39 +/- 671.70
Episode length: 41.60 +/- 2.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 41.6      |
|    mean_reward          | 1.36e+03  |
| time/                   |           |
|    total_timesteps      | 292000    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000174 |
|    explained_variance   | 0.0808    |
|    learning_rate        | 0.001     |
|    loss                 | 1.78e+04  |
|    n_updates            | 2850      |
|    policy_gradient_loss | 7.65e-07  |
|    value_loss           | 8.3e+04   |
---------------------------------------
Eval num_timesteps=292500, episode_reward=1343.89 +/- 659.94
Episode length: 41.84 +/- 2.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.3     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 102      |
|    iterations      | 286      |
|    time_elapsed    | 2846     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=1596.71 +/- 474.40
Episode length: 42.36 +/- 1.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.4         |
|    mean_reward          | 1.6e+03      |
| time/                   |              |
|    total_timesteps      | 293000       |
| train/                  |              |
|    approx_kl            | 6.868504e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000338    |
|    explained_variance   | -0.0596      |
|    learning_rate        | 0.001        |
|    loss                 | 9.06e+03     |
|    n_updates            | 2860         |
|    policy_gradient_loss | -1.25e-06    |
|    value_loss           | 5.85e+04     |
------------------------------------------
Eval num_timesteps=293500, episode_reward=1612.11 +/- 472.90
Episode length: 42.58 +/- 1.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.4     |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    fps             | 103      |
|    iterations      | 287      |
|    time_elapsed    | 2852     |
|    total_timesteps | 293888   |
---------------------------------
Eval num_timesteps=294000, episode_reward=1591.49 +/- 464.41
Episode length: 42.54 +/- 1.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.5         |
|    mean_reward          | 1.59e+03     |
| time/                   |              |
|    total_timesteps      | 294000       |
| train/                  |              |
|    approx_kl            | 4.831236e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00032     |
|    explained_variance   | -0.203       |
|    learning_rate        | 0.001        |
|    loss                 | 8.43e+03     |
|    n_updates            | 2870         |
|    policy_gradient_loss | 5.14e-07     |
|    value_loss           | 4.1e+04      |
------------------------------------------
Eval num_timesteps=294500, episode_reward=1537.77 +/- 537.79
Episode length: 42.30 +/- 2.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.3     |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 103      |
|    iterations      | 288      |
|    time_elapsed    | 2857     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=1403.18 +/- 684.76
Episode length: 41.86 +/- 2.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 41.9          |
|    mean_reward          | 1.4e+03       |
| time/                   |               |
|    total_timesteps      | 295000        |
| train/                  |               |
|    approx_kl            | 2.6193447e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0002       |
|    explained_variance   | -0.134        |
|    learning_rate        | 0.001         |
|    loss                 | 6.26e+03      |
|    n_updates            | 2880          |
|    policy_gradient_loss | -1.26e-06     |
|    value_loss           | 3.39e+04      |
-------------------------------------------
Eval num_timesteps=295500, episode_reward=1417.26 +/- 614.43
Episode length: 42.00 +/- 2.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    fps             | 103      |
|    iterations      | 289      |
|    time_elapsed    | 2863     |
|    total_timesteps | 295936   |
---------------------------------
Eval num_timesteps=296000, episode_reward=1506.12 +/- 515.83
Episode length: 42.32 +/- 1.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.3          |
|    mean_reward          | 1.51e+03      |
| time/                   |               |
|    total_timesteps      | 296000        |
| train/                  |               |
|    approx_kl            | 1.7462298e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000156     |
|    explained_variance   | -0.228        |
|    learning_rate        | 0.001         |
|    loss                 | 1.5e+04       |
|    n_updates            | 2890          |
|    policy_gradient_loss | -6.65e-07     |
|    value_loss           | 4.01e+04      |
-------------------------------------------
Eval num_timesteps=296500, episode_reward=1392.64 +/- 646.89
Episode length: 42.04 +/- 2.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 42       |
|    ep_rew_mean     | 1.43e+03 |
| time/              |          |
|    fps             | 103      |
|    iterations      | 290      |
|    time_elapsed    | 2868     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=1472.15 +/- 497.81
Episode length: 42.36 +/- 1.91
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 42.4           |
|    mean_reward          | 1.47e+03       |
| time/                   |                |
|    total_timesteps      | 297000         |
| train/                  |                |
|    approx_kl            | -2.1536835e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.000173      |
|    explained_variance   | 0.115          |
|    learning_rate        | 0.001          |
|    loss                 | 1.86e+04       |
|    n_updates            | 2900           |
|    policy_gradient_loss | -9.63e-07      |
|    value_loss           | 5.28e+04       |
--------------------------------------------
Eval num_timesteps=297500, episode_reward=1359.80 +/- 692.49
Episode length: 41.84 +/- 2.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 1.38e+03 |
| time/              |          |
|    fps             | 103      |
|    iterations      | 291      |
|    time_elapsed    | 2874     |
|    total_timesteps | 297984   |
---------------------------------
Eval num_timesteps=298000, episode_reward=1468.21 +/- 577.65
Episode length: 42.24 +/- 2.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.2          |
|    mean_reward          | 1.47e+03      |
| time/                   |               |
|    total_timesteps      | 298000        |
| train/                  |               |
|    approx_kl            | -2.561137e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000183     |
|    explained_variance   | -0.0254       |
|    learning_rate        | 0.001         |
|    loss                 | 9.64e+03      |
|    n_updates            | 2910          |
|    policy_gradient_loss | -1.8e-06      |
|    value_loss           | 5.94e+04      |
-------------------------------------------
Eval num_timesteps=298500, episode_reward=1360.48 +/- 673.58
Episode length: 41.72 +/- 2.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=1560.22 +/- 519.26
Episode length: 42.38 +/- 2.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 1.38e+03 |
| time/              |          |
|    fps             | 103      |
|    iterations      | 292      |
|    time_elapsed    | 2881     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=1519.74 +/- 494.02
Episode length: 42.22 +/- 1.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 42.2          |
|    mean_reward          | 1.52e+03      |
| time/                   |               |
|    total_timesteps      | 299500        |
| train/                  |               |
|    approx_kl            | 1.8626451e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000236     |
|    explained_variance   | -0.205        |
|    learning_rate        | 0.001         |
|    loss                 | 9.58e+03      |
|    n_updates            | 2920          |
|    policy_gradient_loss | 1.58e-06      |
|    value_loss           | 3.43e+04      |
-------------------------------------------
Eval num_timesteps=300000, episode_reward=1309.02 +/- 668.56
Episode length: 41.56 +/- 2.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 41.8     |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    fps             | 103      |
|    iterations      | 293      |
|    time_elapsed    | 2887     |
|    total_timesteps | 300032   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-2/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 1024, 'batch_size': 64, 'learning_rate': 0.001, 'gamma': 0.9635, 'gae_lambda': 0.879}
Training steps: 300000
Frame skip: 4
