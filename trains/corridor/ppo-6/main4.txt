/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Eval num_timesteps=500, episode_reward=-504.02 +/- 199.14
Episode length: 16.38 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=-502.13 +/- 149.91
Episode length: 15.88 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -502     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -509     |
| time/              |          |
|    fps             | 454      |
|    iterations      | 1        |
|    time_elapsed    | 2        |
|    total_timesteps | 1024     |
---------------------------------
Eval num_timesteps=1500, episode_reward=-530.82 +/- 146.88
Episode length: 16.56 +/- 4.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -531         |
| time/                   |              |
|    total_timesteps      | 1500         |
| train/                  |              |
|    approx_kl            | 0.0071050692 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00151     |
|    explained_variance   | -0.579       |
|    learning_rate        | 0.001        |
|    loss                 | 1.15e+04     |
|    n_updates            | 8330         |
|    policy_gradient_loss | 0.0012       |
|    value_loss           | 4.41e+04     |
------------------------------------------
Eval num_timesteps=2000, episode_reward=-542.51 +/- 177.33
Episode length: 16.76 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 427      |
|    iterations      | 2        |
|    time_elapsed    | 4        |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=-507.87 +/- 187.65
Episode length: 15.68 +/- 4.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.7          |
|    mean_reward          | -508          |
| time/                   |               |
|    total_timesteps      | 2500          |
| train/                  |               |
|    approx_kl            | 1.0221265e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00108      |
|    explained_variance   | 0.36          |
|    learning_rate        | 0.001         |
|    loss                 | 7.42e+03      |
|    n_updates            | 8340          |
|    policy_gradient_loss | -8.14e-06     |
|    value_loss           | 2.24e+04      |
-------------------------------------------
Eval num_timesteps=3000, episode_reward=-531.14 +/- 225.79
Episode length: 13.84 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -539     |
| time/              |          |
|    fps             | 434      |
|    iterations      | 3        |
|    time_elapsed    | 7        |
|    total_timesteps | 3072     |
---------------------------------
Eval num_timesteps=3500, episode_reward=-563.45 +/- 188.13
Episode length: 14.84 +/- 3.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.8        |
|    mean_reward          | -563        |
| time/                   |             |
|    total_timesteps      | 3500        |
| train/                  |             |
|    approx_kl            | 0.000482984 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00105    |
|    explained_variance   | 0.449       |
|    learning_rate        | 0.001       |
|    loss                 | 7.68e+03    |
|    n_updates            | 8350        |
|    policy_gradient_loss | 0.00195     |
|    value_loss           | 1.87e+04    |
-----------------------------------------
Eval num_timesteps=4000, episode_reward=-528.34 +/- 192.75
Episode length: 15.48 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 433      |
|    iterations      | 4        |
|    time_elapsed    | 9        |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=-482.31 +/- 165.93
Episode length: 15.28 +/- 4.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -482         |
| time/                   |              |
|    total_timesteps      | 4500         |
| train/                  |              |
|    approx_kl            | 2.834713e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000868    |
|    explained_variance   | 0.644        |
|    learning_rate        | 0.001        |
|    loss                 | 8.15e+03     |
|    n_updates            | 8360         |
|    policy_gradient_loss | -2.84e-05    |
|    value_loss           | 1.52e+04     |
------------------------------------------
New best mean reward!
Eval num_timesteps=5000, episode_reward=-530.87 +/- 164.88
Episode length: 16.10 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -552     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 5        |
|    time_elapsed    | 11       |
|    total_timesteps | 5120     |
---------------------------------
Eval num_timesteps=5500, episode_reward=-518.35 +/- 219.24
Episode length: 16.42 +/- 5.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.4        |
|    mean_reward          | -518        |
| time/                   |             |
|    total_timesteps      | 5500        |
| train/                  |             |
|    approx_kl            | 0.000991022 |
|    clip_fraction        | 0.000879    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000421   |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.001       |
|    loss                 | 6.92e+03    |
|    n_updates            | 8370        |
|    policy_gradient_loss | 9.57e-05    |
|    value_loss           | 1.54e+04    |
-----------------------------------------
Eval num_timesteps=6000, episode_reward=-524.63 +/- 170.51
Episode length: 15.36 +/- 4.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -555     |
| time/              |          |
|    fps             | 426      |
|    iterations      | 6        |
|    time_elapsed    | 14       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=-603.16 +/- 187.87
Episode length: 15.30 +/- 5.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.3          |
|    mean_reward          | -603          |
| time/                   |               |
|    total_timesteps      | 6500          |
| train/                  |               |
|    approx_kl            | 3.0267984e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000227     |
|    explained_variance   | 0.672         |
|    learning_rate        | 0.001         |
|    loss                 | 5.71e+03      |
|    n_updates            | 8380          |
|    policy_gradient_loss | -6.29e-06     |
|    value_loss           | 1.37e+04      |
-------------------------------------------
Eval num_timesteps=7000, episode_reward=-559.15 +/- 157.53
Episode length: 15.28 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -559     |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 427      |
|    iterations      | 7        |
|    time_elapsed    | 16       |
|    total_timesteps | 7168     |
---------------------------------
Eval num_timesteps=7500, episode_reward=-546.57 +/- 164.52
Episode length: 16.22 +/- 4.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -547          |
| time/                   |               |
|    total_timesteps      | 7500          |
| train/                  |               |
|    approx_kl            | 1.3620593e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000389     |
|    explained_variance   | 0.605         |
|    learning_rate        | 0.001         |
|    loss                 | 5.88e+03      |
|    n_updates            | 8390          |
|    policy_gradient_loss | -2.12e-05     |
|    value_loss           | 1.41e+04      |
-------------------------------------------
Eval num_timesteps=8000, episode_reward=-524.95 +/- 177.01
Episode length: 15.84 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -563     |
| time/              |          |
|    fps             | 427      |
|    iterations      | 8        |
|    time_elapsed    | 19       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=-515.47 +/- 155.24
Episode length: 14.46 +/- 4.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.5          |
|    mean_reward          | -515          |
| time/                   |               |
|    total_timesteps      | 8500          |
| train/                  |               |
|    approx_kl            | 6.0535967e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000256     |
|    explained_variance   | 0.665         |
|    learning_rate        | 0.001         |
|    loss                 | 5.06e+03      |
|    n_updates            | 8400          |
|    policy_gradient_loss | -2.62e-06     |
|    value_loss           | 1.34e+04      |
-------------------------------------------
Eval num_timesteps=9000, episode_reward=-535.90 +/- 166.30
Episode length: 14.54 +/- 3.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -551     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 9        |
|    time_elapsed    | 21       |
|    total_timesteps | 9216     |
---------------------------------
Eval num_timesteps=9500, episode_reward=-529.57 +/- 172.55
Episode length: 16.20 +/- 5.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -530          |
| time/                   |               |
|    total_timesteps      | 9500          |
| train/                  |               |
|    approx_kl            | 6.5194385e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000317     |
|    explained_variance   | 0.747         |
|    learning_rate        | 0.001         |
|    loss                 | 2.82e+03      |
|    n_updates            | 8410          |
|    policy_gradient_loss | -8.55e-05     |
|    value_loss           | 1.09e+04      |
-------------------------------------------
Eval num_timesteps=10000, episode_reward=-537.15 +/- 165.46
Episode length: 14.90 +/- 3.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -562     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 10       |
|    time_elapsed    | 23       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=-546.11 +/- 171.31
Episode length: 16.56 +/- 5.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | -546          |
| time/                   |               |
|    total_timesteps      | 10500         |
| train/                  |               |
|    approx_kl            | 9.8138116e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000473     |
|    explained_variance   | 0.724         |
|    learning_rate        | 0.001         |
|    loss                 | 5.41e+03      |
|    n_updates            | 8420          |
|    policy_gradient_loss | 1.83e-05      |
|    value_loss           | 1.19e+04      |
-------------------------------------------
Eval num_timesteps=11000, episode_reward=-508.97 +/- 136.82
Episode length: 15.62 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -554     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 11       |
|    time_elapsed    | 26       |
|    total_timesteps | 11264    |
---------------------------------
Eval num_timesteps=11500, episode_reward=-509.07 +/- 188.19
Episode length: 16.38 +/- 4.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.4          |
|    mean_reward          | -509          |
| time/                   |               |
|    total_timesteps      | 11500         |
| train/                  |               |
|    approx_kl            | 7.1595423e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00035      |
|    explained_variance   | 0.718         |
|    learning_rate        | 0.001         |
|    loss                 | 3.6e+03       |
|    n_updates            | 8430          |
|    policy_gradient_loss | -3.42e-06     |
|    value_loss           | 1.18e+04      |
-------------------------------------------
Eval num_timesteps=12000, episode_reward=-552.56 +/- 173.79
Episode length: 16.48 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -553     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 427      |
|    iterations      | 12       |
|    time_elapsed    | 28       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=-503.17 +/- 195.58
Episode length: 16.16 +/- 5.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -503          |
| time/                   |               |
|    total_timesteps      | 12500         |
| train/                  |               |
|    approx_kl            | 1.0011718e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000373     |
|    explained_variance   | 0.655         |
|    learning_rate        | 0.001         |
|    loss                 | 5.81e+03      |
|    n_updates            | 8440          |
|    policy_gradient_loss | -2.78e-06     |
|    value_loss           | 1.33e+04      |
-------------------------------------------
Eval num_timesteps=13000, episode_reward=-489.83 +/- 201.98
Episode length: 17.06 +/- 5.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -490     |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -533     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 13       |
|    time_elapsed    | 31       |
|    total_timesteps | 13312    |
---------------------------------
Eval num_timesteps=13500, episode_reward=-554.52 +/- 177.14
Episode length: 15.60 +/- 4.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | -555          |
| time/                   |               |
|    total_timesteps      | 13500         |
| train/                  |               |
|    approx_kl            | 2.5436748e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00038      |
|    explained_variance   | 0.732         |
|    learning_rate        | 0.001         |
|    loss                 | 5.09e+03      |
|    n_updates            | 8450          |
|    policy_gradient_loss | 2.37e-06      |
|    value_loss           | 1e+04         |
-------------------------------------------
Eval num_timesteps=14000, episode_reward=-474.27 +/- 172.87
Episode length: 16.18 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -474     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 14       |
|    time_elapsed    | 33       |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=-557.98 +/- 156.05
Episode length: 15.52 +/- 4.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -558         |
| time/                   |              |
|    total_timesteps      | 14500        |
| train/                  |              |
|    approx_kl            | 7.043127e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000469    |
|    explained_variance   | 0.68         |
|    learning_rate        | 0.001        |
|    loss                 | 6.72e+03     |
|    n_updates            | 8460         |
|    policy_gradient_loss | -4.32e-06    |
|    value_loss           | 1.24e+04     |
------------------------------------------
Eval num_timesteps=15000, episode_reward=-475.91 +/- 225.73
Episode length: 17.46 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -476     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 15       |
|    time_elapsed    | 35       |
|    total_timesteps | 15360    |
---------------------------------
Eval num_timesteps=15500, episode_reward=-521.98 +/- 183.44
Episode length: 15.46 +/- 5.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.5          |
|    mean_reward          | -522          |
| time/                   |               |
|    total_timesteps      | 15500         |
| train/                  |               |
|    approx_kl            | 1.0419171e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000632     |
|    explained_variance   | 0.658         |
|    learning_rate        | 0.001         |
|    loss                 | 7.39e+03      |
|    n_updates            | 8470          |
|    policy_gradient_loss | -1.07e-06     |
|    value_loss           | 1.25e+04      |
-------------------------------------------
Eval num_timesteps=16000, episode_reward=-539.17 +/- 164.82
Episode length: 15.94 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -539     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -511     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 16       |
|    time_elapsed    | 38       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=-539.74 +/- 184.98
Episode length: 16.86 +/- 5.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.9          |
|    mean_reward          | -540          |
| time/                   |               |
|    total_timesteps      | 16500         |
| train/                  |               |
|    approx_kl            | 0.00035164913 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000457     |
|    explained_variance   | 0.635         |
|    learning_rate        | 0.001         |
|    loss                 | 5.14e+03      |
|    n_updates            | 8480          |
|    policy_gradient_loss | -0.000121     |
|    value_loss           | 1.23e+04      |
-------------------------------------------
Eval num_timesteps=17000, episode_reward=-496.95 +/- 198.15
Episode length: 15.68 +/- 5.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -497     |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 17       |
|    time_elapsed    | 40       |
|    total_timesteps | 17408    |
---------------------------------
Eval num_timesteps=17500, episode_reward=-522.79 +/- 188.45
Episode length: 15.12 +/- 4.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.1          |
|    mean_reward          | -523          |
| time/                   |               |
|    total_timesteps      | 17500         |
| train/                  |               |
|    approx_kl            | 6.1118044e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00046      |
|    explained_variance   | 0.659         |
|    learning_rate        | 0.001         |
|    loss                 | 6.73e+03      |
|    n_updates            | 8490          |
|    policy_gradient_loss | -7.69e-06     |
|    value_loss           | 1.46e+04      |
-------------------------------------------
Eval num_timesteps=18000, episode_reward=-507.16 +/- 181.72
Episode length: 15.66 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | -557     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 18       |
|    time_elapsed    | 42       |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=-554.50 +/- 191.86
Episode length: 15.34 +/- 5.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.3          |
|    mean_reward          | -555          |
| time/                   |               |
|    total_timesteps      | 18500         |
| train/                  |               |
|    approx_kl            | 4.7148205e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00032      |
|    explained_variance   | 0.726         |
|    learning_rate        | 0.001         |
|    loss                 | 3.75e+03      |
|    n_updates            | 8500          |
|    policy_gradient_loss | 1.15e-06      |
|    value_loss           | 1.22e+04      |
-------------------------------------------
Eval num_timesteps=19000, episode_reward=-538.22 +/- 254.14
Episode length: 14.54 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.5     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -536     |
| time/              |          |
|    fps             | 431      |
|    iterations      | 19       |
|    time_elapsed    | 45       |
|    total_timesteps | 19456    |
---------------------------------
Eval num_timesteps=19500, episode_reward=-516.34 +/- 142.97
Episode length: 16.26 +/- 4.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.3          |
|    mean_reward          | -516          |
| time/                   |               |
|    total_timesteps      | 19500         |
| train/                  |               |
|    approx_kl            | 1.1990778e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000495     |
|    explained_variance   | 0.712         |
|    learning_rate        | 0.001         |
|    loss                 | 4.81e+03      |
|    n_updates            | 8510          |
|    policy_gradient_loss | -1.1e-05      |
|    value_loss           | 1.05e+04      |
-------------------------------------------
Eval num_timesteps=20000, episode_reward=-504.42 +/- 202.71
Episode length: 15.52 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -538     |
| time/              |          |
|    fps             | 432      |
|    iterations      | 20       |
|    time_elapsed    | 47       |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=-573.12 +/- 196.78
Episode length: 15.52 +/- 5.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.5          |
|    mean_reward          | -573          |
| time/                   |               |
|    total_timesteps      | 20500         |
| train/                  |               |
|    approx_kl            | 1.1001248e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000777     |
|    explained_variance   | 0.728         |
|    learning_rate        | 0.001         |
|    loss                 | 3.95e+03      |
|    n_updates            | 8520          |
|    policy_gradient_loss | -1.39e-05     |
|    value_loss           | 9.74e+03      |
-------------------------------------------
Eval num_timesteps=21000, episode_reward=-522.59 +/- 180.86
Episode length: 15.54 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=-552.12 +/- 193.01
Episode length: 14.16 +/- 4.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.2     |
|    mean_reward     | -552     |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 21       |
|    time_elapsed    | 50       |
|    total_timesteps | 21504    |
---------------------------------
Eval num_timesteps=22000, episode_reward=-494.74 +/- 179.05
Episode length: 16.80 +/- 4.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | -495          |
| time/                   |               |
|    total_timesteps      | 22000         |
| train/                  |               |
|    approx_kl            | 2.0954758e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000477     |
|    explained_variance   | 0.725         |
|    learning_rate        | 0.001         |
|    loss                 | 2.62e+03      |
|    n_updates            | 8530          |
|    policy_gradient_loss | -4.64e-06     |
|    value_loss           | 9.14e+03      |
-------------------------------------------
Eval num_timesteps=22500, episode_reward=-489.68 +/- 190.06
Episode length: 15.28 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -490     |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -551     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 22       |
|    time_elapsed    | 52       |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=-496.35 +/- 218.43
Episode length: 15.00 +/- 5.21
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15           |
|    mean_reward          | -496         |
| time/                   |              |
|    total_timesteps      | 23000        |
| train/                  |              |
|    approx_kl            | 6.170012e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000459    |
|    explained_variance   | 0.721        |
|    learning_rate        | 0.001        |
|    loss                 | 4.99e+03     |
|    n_updates            | 8540         |
|    policy_gradient_loss | -8.01e-06    |
|    value_loss           | 1.1e+04      |
------------------------------------------
Eval num_timesteps=23500, episode_reward=-511.38 +/- 187.37
Episode length: 15.94 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -546     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 23       |
|    time_elapsed    | 54       |
|    total_timesteps | 23552    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-540.39 +/- 169.30
Episode length: 16.12 +/- 4.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -540         |
| time/                   |              |
|    total_timesteps      | 24000        |
| train/                  |              |
|    approx_kl            | 5.296897e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000342    |
|    explained_variance   | 0.653        |
|    learning_rate        | 0.001        |
|    loss                 | 6.76e+03     |
|    n_updates            | 8550         |
|    policy_gradient_loss | -1.85e-05    |
|    value_loss           | 1.3e+04      |
------------------------------------------
Eval num_timesteps=24500, episode_reward=-512.21 +/- 181.90
Episode length: 16.28 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 24       |
|    time_elapsed    | 57       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-550.15 +/- 186.64
Episode length: 15.76 +/- 4.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -550          |
| time/                   |               |
|    total_timesteps      | 25000         |
| train/                  |               |
|    approx_kl            | 4.0745363e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000327     |
|    explained_variance   | 0.728         |
|    learning_rate        | 0.001         |
|    loss                 | 7.95e+03      |
|    n_updates            | 8560          |
|    policy_gradient_loss | -3.91e-06     |
|    value_loss           | 1.21e+04      |
-------------------------------------------
Eval num_timesteps=25500, episode_reward=-590.44 +/- 170.15
Episode length: 15.72 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -590     |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 25       |
|    time_elapsed    | 59       |
|    total_timesteps | 25600    |
---------------------------------
Eval num_timesteps=26000, episode_reward=-523.93 +/- 192.23
Episode length: 16.40 +/- 4.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -524         |
| time/                   |              |
|    total_timesteps      | 26000        |
| train/                  |              |
|    approx_kl            | 4.773028e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000375    |
|    explained_variance   | 0.666        |
|    learning_rate        | 0.001        |
|    loss                 | 5.65e+03     |
|    n_updates            | 8570         |
|    policy_gradient_loss | -2.27e-06    |
|    value_loss           | 1.33e+04     |
------------------------------------------
Eval num_timesteps=26500, episode_reward=-550.81 +/- 197.49
Episode length: 14.80 +/- 3.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -551     |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 26       |
|    time_elapsed    | 61       |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=-517.03 +/- 196.28
Episode length: 16.06 +/- 4.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -517         |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 2.910383e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000394    |
|    explained_variance   | 0.687        |
|    learning_rate        | 0.001        |
|    loss                 | 6.72e+03     |
|    n_updates            | 8580         |
|    policy_gradient_loss | -6.99e-07    |
|    value_loss           | 1.33e+04     |
------------------------------------------
Eval num_timesteps=27500, episode_reward=-482.25 +/- 193.91
Episode length: 16.30 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -482     |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 27       |
|    time_elapsed    | 64       |
|    total_timesteps | 27648    |
---------------------------------
Eval num_timesteps=28000, episode_reward=-513.97 +/- 214.36
Episode length: 16.10 +/- 5.75
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -514         |
| time/                   |              |
|    total_timesteps      | 28000        |
| train/                  |              |
|    approx_kl            | 9.778887e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000519    |
|    explained_variance   | 0.707        |
|    learning_rate        | 0.001        |
|    loss                 | 4.66e+03     |
|    n_updates            | 8590         |
|    policy_gradient_loss | -7.3e-06     |
|    value_loss           | 1.22e+04     |
------------------------------------------
Eval num_timesteps=28500, episode_reward=-549.38 +/- 168.31
Episode length: 16.02 +/- 4.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -549     |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -501     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 28       |
|    time_elapsed    | 66       |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=-566.10 +/- 158.80
Episode length: 16.76 +/- 4.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | -566          |
| time/                   |               |
|    total_timesteps      | 29000         |
| train/                  |               |
|    approx_kl            | 3.4400728e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000913     |
|    explained_variance   | 0.62          |
|    learning_rate        | 0.001         |
|    loss                 | 3.42e+03      |
|    n_updates            | 8600          |
|    policy_gradient_loss | 1.25e-05      |
|    value_loss           | 1.19e+04      |
-------------------------------------------
Eval num_timesteps=29500, episode_reward=-515.46 +/- 172.57
Episode length: 15.52 +/- 5.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -466     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 29       |
|    time_elapsed    | 69       |
|    total_timesteps | 29696    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-530.13 +/- 149.51
Episode length: 15.92 +/- 4.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -530         |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 4.773028e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000734    |
|    explained_variance   | 0.694        |
|    learning_rate        | 0.001        |
|    loss                 | 2.98e+03     |
|    n_updates            | 8610         |
|    policy_gradient_loss | 4.38e-07     |
|    value_loss           | 1.14e+04     |
------------------------------------------
Eval num_timesteps=30500, episode_reward=-558.38 +/- 177.42
Episode length: 15.22 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -558     |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 30       |
|    time_elapsed    | 71       |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=-540.11 +/- 185.19
Episode length: 15.10 +/- 4.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.1          |
|    mean_reward          | -540          |
| time/                   |               |
|    total_timesteps      | 31000         |
| train/                  |               |
|    approx_kl            | 0.00031495158 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000555     |
|    explained_variance   | 0.663         |
|    learning_rate        | 0.001         |
|    loss                 | 5.05e+03      |
|    n_updates            | 8620          |
|    policy_gradient_loss | -1.57e-05     |
|    value_loss           | 1.23e+04      |
-------------------------------------------
Eval num_timesteps=31500, episode_reward=-549.31 +/- 165.77
Episode length: 15.12 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -549     |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 31       |
|    time_elapsed    | 73       |
|    total_timesteps | 31744    |
---------------------------------
Eval num_timesteps=32000, episode_reward=-537.25 +/- 196.88
Episode length: 17.02 +/- 5.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | -537          |
| time/                   |               |
|    total_timesteps      | 32000         |
| train/                  |               |
|    approx_kl            | 6.0535967e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000484     |
|    explained_variance   | 0.652         |
|    learning_rate        | 0.001         |
|    loss                 | 5.41e+03      |
|    n_updates            | 8630          |
|    policy_gradient_loss | -7.83e-06     |
|    value_loss           | 1.24e+04      |
-------------------------------------------
Eval num_timesteps=32500, episode_reward=-526.20 +/- 181.46
Episode length: 16.96 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -529     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 32       |
|    time_elapsed    | 76       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=-535.22 +/- 168.51
Episode length: 15.08 +/- 4.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.1          |
|    mean_reward          | -535          |
| time/                   |               |
|    total_timesteps      | 33000         |
| train/                  |               |
|    approx_kl            | 1.1234079e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000615     |
|    explained_variance   | 0.721         |
|    learning_rate        | 0.001         |
|    loss                 | 4.97e+03      |
|    n_updates            | 8640          |
|    policy_gradient_loss | -2.68e-06     |
|    value_loss           | 1.12e+04      |
-------------------------------------------
Eval num_timesteps=33500, episode_reward=-522.47 +/- 194.68
Episode length: 14.94 +/- 4.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -571     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 33       |
|    time_elapsed    | 78       |
|    total_timesteps | 33792    |
---------------------------------
Eval num_timesteps=34000, episode_reward=-559.20 +/- 173.26
Episode length: 14.96 +/- 4.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | -559          |
| time/                   |               |
|    total_timesteps      | 34000         |
| train/                  |               |
|    approx_kl            | 2.6193447e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000354     |
|    explained_variance   | 0.696         |
|    learning_rate        | 0.001         |
|    loss                 | 6.03e+03      |
|    n_updates            | 8650          |
|    policy_gradient_loss | -5.26e-06     |
|    value_loss           | 1.4e+04       |
-------------------------------------------
Eval num_timesteps=34500, episode_reward=-546.15 +/- 187.30
Episode length: 15.78 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -546     |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 34       |
|    time_elapsed    | 80       |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-488.12 +/- 185.22
Episode length: 15.28 +/- 4.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -488         |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 7.799827e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000314    |
|    explained_variance   | 0.593        |
|    learning_rate        | 0.001        |
|    loss                 | 3.14e+03     |
|    n_updates            | 8660         |
|    policy_gradient_loss | -1.82e-05    |
|    value_loss           | 1.26e+04     |
------------------------------------------
Eval num_timesteps=35500, episode_reward=-538.38 +/- 173.29
Episode length: 16.00 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -487     |
| time/              |          |
|    fps             | 431      |
|    iterations      | 35       |
|    time_elapsed    | 83       |
|    total_timesteps | 35840    |
---------------------------------
Eval num_timesteps=36000, episode_reward=-496.64 +/- 191.55
Episode length: 17.30 +/- 5.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.3         |
|    mean_reward          | -497         |
| time/                   |              |
|    total_timesteps      | 36000        |
| train/                  |              |
|    approx_kl            | 7.683411e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000439    |
|    explained_variance   | 0.586        |
|    learning_rate        | 0.001        |
|    loss                 | 6.12e+03     |
|    n_updates            | 8670         |
|    policy_gradient_loss | -9.91e-06    |
|    value_loss           | 1.18e+04     |
------------------------------------------
Eval num_timesteps=36500, episode_reward=-525.80 +/- 173.59
Episode length: 16.52 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 431      |
|    iterations      | 36       |
|    time_elapsed    | 85       |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=-464.68 +/- 224.05
Episode length: 17.48 +/- 6.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.5         |
|    mean_reward          | -465         |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 5.355105e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000446    |
|    explained_variance   | 0.677        |
|    learning_rate        | 0.001        |
|    loss                 | 7.74e+03     |
|    n_updates            | 8680         |
|    policy_gradient_loss | 8.8e-07      |
|    value_loss           | 1.4e+04      |
------------------------------------------
New best mean reward!
Eval num_timesteps=37500, episode_reward=-507.34 +/- 172.48
Episode length: 16.42 +/- 4.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -533     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 37       |
|    time_elapsed    | 88       |
|    total_timesteps | 37888    |
---------------------------------
Eval num_timesteps=38000, episode_reward=-543.85 +/- 166.40
Episode length: 15.60 +/- 4.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | -544          |
| time/                   |               |
|    total_timesteps      | 38000         |
| train/                  |               |
|    approx_kl            | 8.8475645e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000529     |
|    explained_variance   | 0.722         |
|    learning_rate        | 0.001         |
|    loss                 | 4.16e+03      |
|    n_updates            | 8690          |
|    policy_gradient_loss | 1.02e-06      |
|    value_loss           | 9.77e+03      |
-------------------------------------------
Eval num_timesteps=38500, episode_reward=-499.18 +/- 213.50
Episode length: 15.46 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -499     |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 38       |
|    time_elapsed    | 90       |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=-522.81 +/- 202.34
Episode length: 15.12 +/- 4.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.1         |
|    mean_reward          | -523         |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 3.608875e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000538    |
|    explained_variance   | 0.663        |
|    learning_rate        | 0.001        |
|    loss                 | 5.36e+03     |
|    n_updates            | 8700         |
|    policy_gradient_loss | -3.65e-06    |
|    value_loss           | 1.33e+04     |
------------------------------------------
Eval num_timesteps=39500, episode_reward=-512.19 +/- 162.48
Episode length: 15.60 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 431      |
|    iterations      | 39       |
|    time_elapsed    | 92       |
|    total_timesteps | 39936    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-531.11 +/- 169.54
Episode length: 16.28 +/- 4.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -531         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 4.947651e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000359    |
|    explained_variance   | 0.663        |
|    learning_rate        | 0.001        |
|    loss                 | 6.24e+03     |
|    n_updates            | 8710         |
|    policy_gradient_loss | -4.27e-06    |
|    value_loss           | 1.39e+04     |
------------------------------------------
Eval num_timesteps=40500, episode_reward=-526.51 +/- 203.36
Episode length: 16.74 +/- 6.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -572     |
| time/              |          |
|    fps             | 431      |
|    iterations      | 40       |
|    time_elapsed    | 95       |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=-531.39 +/- 182.97
Episode length: 16.22 +/- 5.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -531         |
| time/                   |              |
|    total_timesteps      | 41000        |
| train/                  |              |
|    approx_kl            | 9.255018e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00065     |
|    explained_variance   | 0.65         |
|    learning_rate        | 0.001        |
|    loss                 | 5.3e+03      |
|    n_updates            | 8720         |
|    policy_gradient_loss | -7.12e-06    |
|    value_loss           | 1.29e+04     |
------------------------------------------
Eval num_timesteps=41500, episode_reward=-513.73 +/- 191.69
Episode length: 14.62 +/- 4.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -547     |
| time/              |          |
|    fps             | 431      |
|    iterations      | 41       |
|    time_elapsed    | 97       |
|    total_timesteps | 41984    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-509.61 +/- 157.18
Episode length: 16.02 +/- 5.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -510          |
| time/                   |               |
|    total_timesteps      | 42000         |
| train/                  |               |
|    approx_kl            | 1.2048986e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000666     |
|    explained_variance   | 0.707         |
|    learning_rate        | 0.001         |
|    loss                 | 3.02e+03      |
|    n_updates            | 8730          |
|    policy_gradient_loss | -6.85e-06     |
|    value_loss           | 9.27e+03      |
-------------------------------------------
Eval num_timesteps=42500, episode_reward=-482.10 +/- 202.01
Episode length: 15.66 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -482     |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-521.68 +/- 173.87
Episode length: 15.54 +/- 4.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -541     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 42       |
|    time_elapsed    | 100      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=-466.24 +/- 184.90
Episode length: 16.98 +/- 5.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | -466          |
| time/                   |               |
|    total_timesteps      | 43500         |
| train/                  |               |
|    approx_kl            | 4.3655746e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000605     |
|    explained_variance   | 0.746         |
|    learning_rate        | 0.001         |
|    loss                 | 4.7e+03       |
|    n_updates            | 8740          |
|    policy_gradient_loss | -2.86e-07     |
|    value_loss           | 1.01e+04      |
-------------------------------------------
Eval num_timesteps=44000, episode_reward=-513.96 +/- 180.86
Episode length: 15.36 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -546     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 43       |
|    time_elapsed    | 102      |
|    total_timesteps | 44032    |
---------------------------------
Eval num_timesteps=44500, episode_reward=-513.52 +/- 138.95
Episode length: 16.62 +/- 5.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | -514          |
| time/                   |               |
|    total_timesteps      | 44500         |
| train/                  |               |
|    approx_kl            | 6.5774657e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00054      |
|    explained_variance   | 0.725         |
|    learning_rate        | 0.001         |
|    loss                 | 4.94e+03      |
|    n_updates            | 8750          |
|    policy_gradient_loss | -6.73e-06     |
|    value_loss           | 1.15e+04      |
-------------------------------------------
Eval num_timesteps=45000, episode_reward=-498.67 +/- 196.54
Episode length: 17.22 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -499     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -539     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 44       |
|    time_elapsed    | 104      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=-489.32 +/- 200.82
Episode length: 15.16 +/- 4.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -489         |
| time/                   |              |
|    total_timesteps      | 45500        |
| train/                  |              |
|    approx_kl            | 7.858034e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000564    |
|    explained_variance   | 0.668        |
|    learning_rate        | 0.001        |
|    loss                 | 6.45e+03     |
|    n_updates            | 8760         |
|    policy_gradient_loss | -1.41e-05    |
|    value_loss           | 1.31e+04     |
------------------------------------------
Eval num_timesteps=46000, episode_reward=-476.31 +/- 191.14
Episode length: 15.70 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -476     |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -492     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 45       |
|    time_elapsed    | 107      |
|    total_timesteps | 46080    |
---------------------------------
Eval num_timesteps=46500, episode_reward=-516.47 +/- 151.33
Episode length: 16.28 +/- 4.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.3          |
|    mean_reward          | -516          |
| time/                   |               |
|    total_timesteps      | 46500         |
| train/                  |               |
|    approx_kl            | 1.2572855e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000855     |
|    explained_variance   | 0.712         |
|    learning_rate        | 0.001         |
|    loss                 | 2.4e+03       |
|    n_updates            | 8770          |
|    policy_gradient_loss | 1.06e-05      |
|    value_loss           | 8.91e+03      |
-------------------------------------------
Eval num_timesteps=47000, episode_reward=-518.88 +/- 196.78
Episode length: 16.02 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 46       |
|    time_elapsed    | 109      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=-541.34 +/- 189.27
Episode length: 14.94 +/- 4.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -541          |
| time/                   |               |
|    total_timesteps      | 47500         |
| train/                  |               |
|    approx_kl            | 2.6775524e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000672     |
|    explained_variance   | 0.715         |
|    learning_rate        | 0.001         |
|    loss                 | 5.21e+03      |
|    n_updates            | 8780          |
|    policy_gradient_loss | 1.88e-06      |
|    value_loss           | 1.13e+04      |
-------------------------------------------
Eval num_timesteps=48000, episode_reward=-450.55 +/- 199.13
Episode length: 15.78 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -451     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 47       |
|    time_elapsed    | 111      |
|    total_timesteps | 48128    |
---------------------------------
Eval num_timesteps=48500, episode_reward=-518.55 +/- 182.80
Episode length: 15.28 +/- 4.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.3          |
|    mean_reward          | -519          |
| time/                   |               |
|    total_timesteps      | 48500         |
| train/                  |               |
|    approx_kl            | 4.3425942e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000884     |
|    explained_variance   | 0.622         |
|    learning_rate        | 0.001         |
|    loss                 | 4.17e+03      |
|    n_updates            | 8790          |
|    policy_gradient_loss | -0.000332     |
|    value_loss           | 1.34e+04      |
-------------------------------------------
Eval num_timesteps=49000, episode_reward=-533.44 +/- 177.76
Episode length: 15.58 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 48       |
|    time_elapsed    | 114      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=-524.59 +/- 175.89
Episode length: 15.68 +/- 4.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.7          |
|    mean_reward          | -525          |
| time/                   |               |
|    total_timesteps      | 49500         |
| train/                  |               |
|    approx_kl            | 0.00032765378 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000718     |
|    explained_variance   | 0.594         |
|    learning_rate        | 0.001         |
|    loss                 | 4.58e+03      |
|    n_updates            | 8800          |
|    policy_gradient_loss | -0.000184     |
|    value_loss           | 1.51e+04      |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=-568.23 +/- 181.09
Episode length: 15.66 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -539     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 49       |
|    time_elapsed    | 116      |
|    total_timesteps | 50176    |
---------------------------------
Eval num_timesteps=50500, episode_reward=-514.05 +/- 160.92
Episode length: 15.22 +/- 4.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -514         |
| time/                   |              |
|    total_timesteps      | 50500        |
| train/                  |              |
|    approx_kl            | 5.646143e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000675    |
|    explained_variance   | 0.743        |
|    learning_rate        | 0.001        |
|    loss                 | 4.02e+03     |
|    n_updates            | 8810         |
|    policy_gradient_loss | -7.01e-06    |
|    value_loss           | 1.08e+04     |
------------------------------------------
Eval num_timesteps=51000, episode_reward=-530.38 +/- 156.29
Episode length: 15.46 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -541     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 50       |
|    time_elapsed    | 119      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=-509.48 +/- 208.26
Episode length: 16.66 +/- 6.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.7          |
|    mean_reward          | -509          |
| time/                   |               |
|    total_timesteps      | 51500         |
| train/                  |               |
|    approx_kl            | 1.4959369e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00102      |
|    explained_variance   | 0.743         |
|    learning_rate        | 0.001         |
|    loss                 | 6.54e+03      |
|    n_updates            | 8820          |
|    policy_gradient_loss | 1.94e-06      |
|    value_loss           | 1.02e+04      |
-------------------------------------------
Eval num_timesteps=52000, episode_reward=-555.98 +/- 185.18
Episode length: 16.36 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -556     |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 51       |
|    time_elapsed    | 121      |
|    total_timesteps | 52224    |
---------------------------------
Eval num_timesteps=52500, episode_reward=-530.50 +/- 177.54
Episode length: 15.12 +/- 4.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.1         |
|    mean_reward          | -530         |
| time/                   |              |
|    total_timesteps      | 52500        |
| train/                  |              |
|    approx_kl            | 8.090865e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00131     |
|    explained_variance   | 0.757        |
|    learning_rate        | 0.001        |
|    loss                 | 3.07e+03     |
|    n_updates            | 8830         |
|    policy_gradient_loss | 1.58e-06     |
|    value_loss           | 8.18e+03     |
------------------------------------------
Eval num_timesteps=53000, episode_reward=-528.52 +/- 179.04
Episode length: 17.00 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -553     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 52       |
|    time_elapsed    | 123      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=-523.66 +/- 177.97
Episode length: 15.78 +/- 4.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -524         |
| time/                   |              |
|    total_timesteps      | 53500        |
| train/                  |              |
|    approx_kl            | 9.662472e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000869    |
|    explained_variance   | 0.717        |
|    learning_rate        | 0.001        |
|    loss                 | 4.41e+03     |
|    n_updates            | 8840         |
|    policy_gradient_loss | -7.34e-06    |
|    value_loss           | 1.11e+04     |
------------------------------------------
Eval num_timesteps=54000, episode_reward=-509.67 +/- 187.45
Episode length: 16.68 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -542     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 53       |
|    time_elapsed    | 126      |
|    total_timesteps | 54272    |
---------------------------------
Eval num_timesteps=54500, episode_reward=-518.56 +/- 191.33
Episode length: 16.18 +/- 4.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -519          |
| time/                   |               |
|    total_timesteps      | 54500         |
| train/                  |               |
|    approx_kl            | 1.0884833e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00104      |
|    explained_variance   | 0.739         |
|    learning_rate        | 0.001         |
|    loss                 | 3.05e+03      |
|    n_updates            | 8850          |
|    policy_gradient_loss | -1.61e-05     |
|    value_loss           | 1.01e+04      |
-------------------------------------------
Eval num_timesteps=55000, episode_reward=-538.04 +/- 156.96
Episode length: 14.88 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 54       |
|    time_elapsed    | 128      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=-523.53 +/- 199.47
Episode length: 16.64 +/- 5.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | -524          |
| time/                   |               |
|    total_timesteps      | 55500         |
| train/                  |               |
|    approx_kl            | 1.9208528e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00124      |
|    explained_variance   | 0.744         |
|    learning_rate        | 0.001         |
|    loss                 | 4.64e+03      |
|    n_updates            | 8860          |
|    policy_gradient_loss | -1.34e-05     |
|    value_loss           | 8.64e+03      |
-------------------------------------------
Eval num_timesteps=56000, episode_reward=-541.37 +/- 150.28
Episode length: 16.76 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -504     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 55       |
|    time_elapsed    | 131      |
|    total_timesteps | 56320    |
---------------------------------
Eval num_timesteps=56500, episode_reward=-524.01 +/- 157.63
Episode length: 15.50 +/- 4.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.5          |
|    mean_reward          | -524          |
| time/                   |               |
|    total_timesteps      | 56500         |
| train/                  |               |
|    approx_kl            | 1.7869752e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00163      |
|    explained_variance   | 0.68          |
|    learning_rate        | 0.001         |
|    loss                 | 7.17e+03      |
|    n_updates            | 8870          |
|    policy_gradient_loss | -8.15e-06     |
|    value_loss           | 1.37e+04      |
-------------------------------------------
Eval num_timesteps=57000, episode_reward=-513.01 +/- 200.57
Episode length: 15.72 +/- 5.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -535     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 56       |
|    time_elapsed    | 133      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=-519.23 +/- 169.65
Episode length: 16.70 +/- 5.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.7          |
|    mean_reward          | -519          |
| time/                   |               |
|    total_timesteps      | 57500         |
| train/                  |               |
|    approx_kl            | 1.5948899e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00126      |
|    explained_variance   | 0.707         |
|    learning_rate        | 0.001         |
|    loss                 | 5.24e+03      |
|    n_updates            | 8880          |
|    policy_gradient_loss | 4.39e-07      |
|    value_loss           | 1.09e+04      |
-------------------------------------------
Eval num_timesteps=58000, episode_reward=-503.74 +/- 167.45
Episode length: 16.98 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 57       |
|    time_elapsed    | 135      |
|    total_timesteps | 58368    |
---------------------------------
Eval num_timesteps=58500, episode_reward=-513.91 +/- 157.10
Episode length: 16.00 +/- 4.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -514          |
| time/                   |               |
|    total_timesteps      | 58500         |
| train/                  |               |
|    approx_kl            | 3.1577947e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0012       |
|    explained_variance   | 0.661         |
|    learning_rate        | 0.001         |
|    loss                 | 5.48e+03      |
|    n_updates            | 8890          |
|    policy_gradient_loss | -0.000184     |
|    value_loss           | 1.16e+04      |
-------------------------------------------
Eval num_timesteps=59000, episode_reward=-455.56 +/- 172.53
Episode length: 15.78 +/- 5.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -456     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -502     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 58       |
|    time_elapsed    | 138      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=-522.90 +/- 200.66
Episode length: 16.10 +/- 5.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.1          |
|    mean_reward          | -523          |
| time/                   |               |
|    total_timesteps      | 59500         |
| train/                  |               |
|    approx_kl            | 2.3515895e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00111      |
|    explained_variance   | 0.567         |
|    learning_rate        | 0.001         |
|    loss                 | 8.37e+03      |
|    n_updates            | 8900          |
|    policy_gradient_loss | -3.87e-05     |
|    value_loss           | 1.57e+04      |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=-458.37 +/- 186.11
Episode length: 16.78 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -458     |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -541     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 59       |
|    time_elapsed    | 140      |
|    total_timesteps | 60416    |
---------------------------------
Eval num_timesteps=60500, episode_reward=-490.51 +/- 180.56
Episode length: 15.44 +/- 4.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -491         |
| time/                   |              |
|    total_timesteps      | 60500        |
| train/                  |              |
|    approx_kl            | 7.858034e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000702    |
|    explained_variance   | 0.675        |
|    learning_rate        | 0.001        |
|    loss                 | 6.98e+03     |
|    n_updates            | 8910         |
|    policy_gradient_loss | -5.62e-06    |
|    value_loss           | 1.37e+04     |
------------------------------------------
Eval num_timesteps=61000, episode_reward=-530.38 +/- 159.09
Episode length: 15.00 +/- 4.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 60       |
|    time_elapsed    | 143      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=-491.71 +/- 218.46
Episode length: 15.10 +/- 5.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.1          |
|    mean_reward          | -492          |
| time/                   |               |
|    total_timesteps      | 61500         |
| train/                  |               |
|    approx_kl            | 1.4901161e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000852     |
|    explained_variance   | 0.615         |
|    learning_rate        | 0.001         |
|    loss                 | 6.59e+03      |
|    n_updates            | 8920          |
|    policy_gradient_loss | -2.14e-05     |
|    value_loss           | 1.3e+04       |
-------------------------------------------
Eval num_timesteps=62000, episode_reward=-550.55 +/- 184.31
Episode length: 16.18 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -551     |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 61       |
|    time_elapsed    | 145      |
|    total_timesteps | 62464    |
---------------------------------
Eval num_timesteps=62500, episode_reward=-513.91 +/- 190.25
Episode length: 16.04 +/- 4.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -514          |
| time/                   |               |
|    total_timesteps      | 62500         |
| train/                  |               |
|    approx_kl            | 1.4493708e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000881     |
|    explained_variance   | 0.723         |
|    learning_rate        | 0.001         |
|    loss                 | 6.14e+03      |
|    n_updates            | 8930          |
|    policy_gradient_loss | -8.43e-06     |
|    value_loss           | 1.22e+04      |
-------------------------------------------
Eval num_timesteps=63000, episode_reward=-486.21 +/- 188.29
Episode length: 16.78 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -486     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -555     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 62       |
|    time_elapsed    | 147      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=-559.39 +/- 184.35
Episode length: 15.84 +/- 4.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -559         |
| time/                   |              |
|    total_timesteps      | 63500        |
| train/                  |              |
|    approx_kl            | 7.043127e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000695    |
|    explained_variance   | 0.695        |
|    learning_rate        | 0.001        |
|    loss                 | 6.42e+03     |
|    n_updates            | 8940         |
|    policy_gradient_loss | -1.9e-05     |
|    value_loss           | 1.21e+04     |
------------------------------------------
Eval num_timesteps=64000, episode_reward=-514.71 +/- 211.03
Episode length: 15.50 +/- 5.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=-547.14 +/- 140.41
Episode length: 16.32 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -547     |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -537     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 63       |
|    time_elapsed    | 150      |
|    total_timesteps | 64512    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-536.93 +/- 179.66
Episode length: 14.38 +/- 3.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.4          |
|    mean_reward          | -537          |
| time/                   |               |
|    total_timesteps      | 65000         |
| train/                  |               |
|    approx_kl            | 7.6405646e-05 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0011       |
|    explained_variance   | 0.667         |
|    learning_rate        | 0.001         |
|    loss                 | 5.25e+03      |
|    n_updates            | 8950          |
|    policy_gradient_loss | -5.36e-05     |
|    value_loss           | 1.47e+04      |
-------------------------------------------
Eval num_timesteps=65500, episode_reward=-510.57 +/- 162.92
Episode length: 16.60 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -487     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 64       |
|    time_elapsed    | 152      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=-549.70 +/- 174.07
Episode length: 15.10 +/- 4.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.1          |
|    mean_reward          | -550          |
| time/                   |               |
|    total_timesteps      | 66000         |
| train/                  |               |
|    approx_kl            | 1.8218998e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00129      |
|    explained_variance   | 0.591         |
|    learning_rate        | 0.001         |
|    loss                 | 6.7e+03       |
|    n_updates            | 8960          |
|    policy_gradient_loss | -7.54e-06     |
|    value_loss           | 1.23e+04      |
-------------------------------------------
Eval num_timesteps=66500, episode_reward=-570.13 +/- 151.62
Episode length: 15.68 +/- 4.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -498     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 65       |
|    time_elapsed    | 155      |
|    total_timesteps | 66560    |
---------------------------------
Eval num_timesteps=67000, episode_reward=-461.95 +/- 207.97
Episode length: 15.74 +/- 5.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.7          |
|    mean_reward          | -462          |
| time/                   |               |
|    total_timesteps      | 67000         |
| train/                  |               |
|    approx_kl            | 1.0302756e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000987     |
|    explained_variance   | 0.714         |
|    learning_rate        | 0.001         |
|    loss                 | 9.06e+03      |
|    n_updates            | 8970          |
|    policy_gradient_loss | -1.35e-05     |
|    value_loss           | 1.04e+04      |
-------------------------------------------
Eval num_timesteps=67500, episode_reward=-520.37 +/- 180.13
Episode length: 16.10 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 66       |
|    time_elapsed    | 157      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=-499.06 +/- 162.72
Episode length: 16.24 +/- 5.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -499         |
| time/                   |              |
|    total_timesteps      | 68000        |
| train/                  |              |
|    approx_kl            | 1.268927e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00118     |
|    explained_variance   | 0.71         |
|    learning_rate        | 0.001        |
|    loss                 | 4.54e+03     |
|    n_updates            | 8980         |
|    policy_gradient_loss | -1.26e-05    |
|    value_loss           | 9.88e+03     |
------------------------------------------
Eval num_timesteps=68500, episode_reward=-500.77 +/- 173.40
Episode length: 17.06 +/- 4.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -501     |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -527     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 67       |
|    time_elapsed    | 159      |
|    total_timesteps | 68608    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-571.38 +/- 181.01
Episode length: 15.62 +/- 4.39
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | -571          |
| time/                   |               |
|    total_timesteps      | 69000         |
| train/                  |               |
|    approx_kl            | 1.9848812e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0012       |
|    explained_variance   | 0.703         |
|    learning_rate        | 0.001         |
|    loss                 | 6.19e+03      |
|    n_updates            | 8990          |
|    policy_gradient_loss | -1.4e-05      |
|    value_loss           | 1.28e+04      |
-------------------------------------------
Eval num_timesteps=69500, episode_reward=-498.93 +/- 160.87
Episode length: 17.90 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.9     |
|    mean_reward     | -499     |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 68       |
|    time_elapsed    | 162      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-491.51 +/- 186.83
Episode length: 15.84 +/- 5.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -492         |
| time/                   |              |
|    total_timesteps      | 70000        |
| train/                  |              |
|    approx_kl            | 7.508788e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000775    |
|    explained_variance   | 0.673        |
|    learning_rate        | 0.001        |
|    loss                 | 3.32e+03     |
|    n_updates            | 9000         |
|    policy_gradient_loss | -1.23e-05    |
|    value_loss           | 1.13e+04     |
------------------------------------------
Eval num_timesteps=70500, episode_reward=-500.51 +/- 133.81
Episode length: 15.64 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -501     |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -539     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 69       |
|    time_elapsed    | 164      |
|    total_timesteps | 70656    |
---------------------------------
Eval num_timesteps=71000, episode_reward=-519.12 +/- 170.94
Episode length: 16.98 +/- 5.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | -519          |
| time/                   |               |
|    total_timesteps      | 71000         |
| train/                  |               |
|    approx_kl            | 6.8102963e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000787     |
|    explained_variance   | 0.724         |
|    learning_rate        | 0.001         |
|    loss                 | 6.07e+03      |
|    n_updates            | 9010          |
|    policy_gradient_loss | -9.03e-06     |
|    value_loss           | 1.1e+04       |
-------------------------------------------
Eval num_timesteps=71500, episode_reward=-565.35 +/- 161.72
Episode length: 16.14 +/- 5.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -565     |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 70       |
|    time_elapsed    | 167      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=-591.34 +/- 168.62
Episode length: 14.66 +/- 4.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | -591          |
| time/                   |               |
|    total_timesteps      | 72000         |
| train/                  |               |
|    approx_kl            | 2.5844201e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00169      |
|    explained_variance   | 0.571         |
|    learning_rate        | 0.001         |
|    loss                 | 6.55e+03      |
|    n_updates            | 9020          |
|    policy_gradient_loss | -2.22e-05     |
|    value_loss           | 1.65e+04      |
-------------------------------------------
Eval num_timesteps=72500, episode_reward=-513.36 +/- 166.85
Episode length: 17.52 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -549     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 71       |
|    time_elapsed    | 169      |
|    total_timesteps | 72704    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-543.70 +/- 167.96
Episode length: 16.16 +/- 4.96
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 16.2           |
|    mean_reward          | -544           |
| time/                   |                |
|    total_timesteps      | 73000          |
| train/                  |                |
|    approx_kl            | 1.14087015e-08 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00157       |
|    explained_variance   | 0.7            |
|    learning_rate        | 0.001          |
|    loss                 | 4.26e+03       |
|    n_updates            | 9030           |
|    policy_gradient_loss | -2.19e-05      |
|    value_loss           | 1.25e+04       |
--------------------------------------------
Eval num_timesteps=73500, episode_reward=-537.66 +/- 156.66
Episode length: 16.68 +/- 4.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -545     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 72       |
|    time_elapsed    | 171      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=-568.08 +/- 166.91
Episode length: 16.08 +/- 5.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -568         |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 9.604264e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00116     |
|    explained_variance   | 0.719        |
|    learning_rate        | 0.001        |
|    loss                 | 3.99e+03     |
|    n_updates            | 9040         |
|    policy_gradient_loss | -1.89e-05    |
|    value_loss           | 1.16e+04     |
------------------------------------------
Eval num_timesteps=74500, episode_reward=-527.20 +/- 172.73
Episode length: 14.80 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -501     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 73       |
|    time_elapsed    | 174      |
|    total_timesteps | 74752    |
---------------------------------
Eval num_timesteps=75000, episode_reward=-537.33 +/- 190.87
Episode length: 14.82 +/- 4.17
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | -537          |
| time/                   |               |
|    total_timesteps      | 75000         |
| train/                  |               |
|    approx_kl            | 2.5494955e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00169      |
|    explained_variance   | 0.626         |
|    learning_rate        | 0.001         |
|    loss                 | 4.27e+03      |
|    n_updates            | 9050          |
|    policy_gradient_loss | -1.02e-05     |
|    value_loss           | 1.23e+04      |
-------------------------------------------
Eval num_timesteps=75500, episode_reward=-535.52 +/- 188.67
Episode length: 16.64 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -543     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 74       |
|    time_elapsed    | 176      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=-523.69 +/- 188.50
Episode length: 16.08 +/- 5.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -524         |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 3.282912e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0013      |
|    explained_variance   | 0.722        |
|    learning_rate        | 0.001        |
|    loss                 | 4.08e+03     |
|    n_updates            | 9060         |
|    policy_gradient_loss | -1.7e-05     |
|    value_loss           | 1e+04        |
------------------------------------------
Eval num_timesteps=76500, episode_reward=-510.40 +/- 178.35
Episode length: 15.50 +/- 4.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -545     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 75       |
|    time_elapsed    | 178      |
|    total_timesteps | 76800    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-530.80 +/- 174.22
Episode length: 15.34 +/- 4.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.3          |
|    mean_reward          | -531          |
| time/                   |               |
|    total_timesteps      | 77000         |
| train/                  |               |
|    approx_kl            | 1.8859282e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00144      |
|    explained_variance   | 0.721         |
|    learning_rate        | 0.001         |
|    loss                 | 3.85e+03      |
|    n_updates            | 9070          |
|    policy_gradient_loss | -3.94e-05     |
|    value_loss           | 1.07e+04      |
-------------------------------------------
Eval num_timesteps=77500, episode_reward=-504.93 +/- 208.92
Episode length: 16.00 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -505     |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 76       |
|    time_elapsed    | 181      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=-538.18 +/- 149.98
Episode length: 15.20 +/- 4.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -538         |
| time/                   |              |
|    total_timesteps      | 78000        |
| train/                  |              |
|    approx_kl            | 1.693843e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00125     |
|    explained_variance   | 0.741        |
|    learning_rate        | 0.001        |
|    loss                 | 3.56e+03     |
|    n_updates            | 9080         |
|    policy_gradient_loss | -2.23e-05    |
|    value_loss           | 9.59e+03     |
------------------------------------------
Eval num_timesteps=78500, episode_reward=-486.36 +/- 206.03
Episode length: 17.68 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -486     |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -511     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 77       |
|    time_elapsed    | 183      |
|    total_timesteps | 78848    |
---------------------------------
Eval num_timesteps=79000, episode_reward=-542.69 +/- 147.92
Episode length: 15.72 +/- 4.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.7          |
|    mean_reward          | -543          |
| time/                   |               |
|    total_timesteps      | 79000         |
| train/                  |               |
|    approx_kl            | 1.6123522e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00109      |
|    explained_variance   | 0.631         |
|    learning_rate        | 0.001         |
|    loss                 | 5.11e+03      |
|    n_updates            | 9090          |
|    policy_gradient_loss | -1e-05        |
|    value_loss           | 1.34e+04      |
-------------------------------------------
Eval num_timesteps=79500, episode_reward=-509.04 +/- 163.63
Episode length: 15.32 +/- 3.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 78       |
|    time_elapsed    | 186      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-516.72 +/- 176.15
Episode length: 15.76 +/- 4.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -517          |
| time/                   |               |
|    total_timesteps      | 80000         |
| train/                  |               |
|    approx_kl            | 1.9033905e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00122      |
|    explained_variance   | 0.623         |
|    learning_rate        | 0.001         |
|    loss                 | 4.27e+03      |
|    n_updates            | 9100          |
|    policy_gradient_loss | -2.6e-05      |
|    value_loss           | 1.36e+04      |
-------------------------------------------
Eval num_timesteps=80500, episode_reward=-549.65 +/- 195.38
Episode length: 15.12 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -550     |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -544     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 79       |
|    time_elapsed    | 188      |
|    total_timesteps | 80896    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-532.91 +/- 169.52
Episode length: 16.74 +/- 5.74
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.7          |
|    mean_reward          | -533          |
| time/                   |               |
|    total_timesteps      | 81000         |
| train/                  |               |
|    approx_kl            | 1.2456439e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00117      |
|    explained_variance   | 0.766         |
|    learning_rate        | 0.001         |
|    loss                 | 3.42e+03      |
|    n_updates            | 9110          |
|    policy_gradient_loss | -1.4e-05      |
|    value_loss           | 9.08e+03      |
-------------------------------------------
Eval num_timesteps=81500, episode_reward=-524.15 +/- 172.56
Episode length: 16.86 +/- 5.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -549     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 80       |
|    time_elapsed    | 190      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=-526.94 +/- 151.44
Episode length: 16.56 +/- 5.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -527         |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 9.429641e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00109     |
|    explained_variance   | 0.732        |
|    learning_rate        | 0.001        |
|    loss                 | 5.75e+03     |
|    n_updates            | 9120         |
|    policy_gradient_loss | -2.45e-05    |
|    value_loss           | 1.09e+04     |
------------------------------------------
Eval num_timesteps=82500, episode_reward=-532.38 +/- 191.84
Episode length: 15.88 +/- 4.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | -538     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 81       |
|    time_elapsed    | 193      |
|    total_timesteps | 82944    |
---------------------------------
Eval num_timesteps=83000, episode_reward=-532.46 +/- 164.51
Episode length: 16.08 +/- 4.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.1          |
|    mean_reward          | -532          |
| time/                   |               |
|    total_timesteps      | 83000         |
| train/                  |               |
|    approx_kl            | 1.3504177e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00107      |
|    explained_variance   | 0.711         |
|    learning_rate        | 0.001         |
|    loss                 | 3.77e+03      |
|    n_updates            | 9130          |
|    policy_gradient_loss | -2.01e-05     |
|    value_loss           | 1.01e+04      |
-------------------------------------------
Eval num_timesteps=83500, episode_reward=-552.58 +/- 176.26
Episode length: 15.84 +/- 4.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -553     |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 82       |
|    time_elapsed    | 195      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=-540.92 +/- 184.89
Episode length: 16.84 +/- 4.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | -541          |
| time/                   |               |
|    total_timesteps      | 84000         |
| train/                  |               |
|    approx_kl            | 1.9615982e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00164      |
|    explained_variance   | 0.679         |
|    learning_rate        | 0.001         |
|    loss                 | 4e+03         |
|    n_updates            | 9140          |
|    policy_gradient_loss | -1.18e-05     |
|    value_loss           | 1.18e+04      |
-------------------------------------------
Eval num_timesteps=84500, episode_reward=-546.44 +/- 185.56
Episode length: 15.80 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -546     |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | -487     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 83       |
|    time_elapsed    | 197      |
|    total_timesteps | 84992    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-525.39 +/- 167.04
Episode length: 16.98 +/- 4.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | -525          |
| time/                   |               |
|    total_timesteps      | 85000         |
| train/                  |               |
|    approx_kl            | 1.7229468e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00189      |
|    explained_variance   | 0.72          |
|    learning_rate        | 0.001         |
|    loss                 | 2.15e+03      |
|    n_updates            | 9150          |
|    policy_gradient_loss | -7.6e-06      |
|    value_loss           | 1.02e+04      |
-------------------------------------------
Eval num_timesteps=85500, episode_reward=-507.74 +/- 158.07
Episode length: 14.86 +/- 4.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-562.50 +/- 193.77
Episode length: 15.78 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -563     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | -513     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 84       |
|    time_elapsed    | 200      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=-506.63 +/- 178.08
Episode length: 16.22 +/- 4.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -507          |
| time/                   |               |
|    total_timesteps      | 86500         |
| train/                  |               |
|    approx_kl            | 1.8859282e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00147      |
|    explained_variance   | 0.719         |
|    learning_rate        | 0.001         |
|    loss                 | 6.29e+03      |
|    n_updates            | 9160          |
|    policy_gradient_loss | -1.34e-05     |
|    value_loss           | 1.27e+04      |
-------------------------------------------
Eval num_timesteps=87000, episode_reward=-494.92 +/- 183.36
Episode length: 15.62 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -495     |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -536     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 85       |
|    time_elapsed    | 203      |
|    total_timesteps | 87040    |
---------------------------------
Eval num_timesteps=87500, episode_reward=-557.61 +/- 149.98
Episode length: 16.66 +/- 4.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -558         |
| time/                   |              |
|    total_timesteps      | 87500        |
| train/                  |              |
|    approx_kl            | 1.641456e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00102     |
|    explained_variance   | 0.747        |
|    learning_rate        | 0.001        |
|    loss                 | 3.57e+03     |
|    n_updates            | 9170         |
|    policy_gradient_loss | -1.88e-05    |
|    value_loss           | 1.04e+04     |
------------------------------------------
Eval num_timesteps=88000, episode_reward=-527.56 +/- 182.53
Episode length: 14.74 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -557     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 86       |
|    time_elapsed    | 205      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=-519.04 +/- 188.10
Episode length: 16.20 +/- 4.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -519          |
| time/                   |               |
|    total_timesteps      | 88500         |
| train/                  |               |
|    approx_kl            | 1.5366822e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00135      |
|    explained_variance   | 0.729         |
|    learning_rate        | 0.001         |
|    loss                 | 3.97e+03      |
|    n_updates            | 9180          |
|    policy_gradient_loss | -2.14e-05     |
|    value_loss           | 9.58e+03      |
-------------------------------------------
Eval num_timesteps=89000, episode_reward=-495.78 +/- 172.64
Episode length: 16.56 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -496     |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -544     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 87       |
|    time_elapsed    | 207      |
|    total_timesteps | 89088    |
---------------------------------
Eval num_timesteps=89500, episode_reward=-528.91 +/- 172.80
Episode length: 16.04 +/- 4.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -529         |
| time/                   |              |
|    total_timesteps      | 89500        |
| train/                  |              |
|    approx_kl            | 2.066372e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00162     |
|    explained_variance   | 0.714        |
|    learning_rate        | 0.001        |
|    loss                 | 3.62e+03     |
|    n_updates            | 9190         |
|    policy_gradient_loss | -2.1e-05     |
|    value_loss           | 9.85e+03     |
------------------------------------------
Eval num_timesteps=90000, episode_reward=-544.83 +/- 166.90
Episode length: 16.46 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -545     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -557     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 88       |
|    time_elapsed    | 210      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=-527.91 +/- 144.77
Episode length: 16.56 +/- 4.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | -528          |
| time/                   |               |
|    total_timesteps      | 90500         |
| train/                  |               |
|    approx_kl            | 1.3387762e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0016       |
|    explained_variance   | 0.714         |
|    learning_rate        | 0.001         |
|    loss                 | 3.63e+03      |
|    n_updates            | 9200          |
|    policy_gradient_loss | -1.7e-05      |
|    value_loss           | 1.04e+04      |
-------------------------------------------
Eval num_timesteps=91000, episode_reward=-508.95 +/- 193.27
Episode length: 15.86 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -568     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 89       |
|    time_elapsed    | 212      |
|    total_timesteps | 91136    |
---------------------------------
Eval num_timesteps=91500, episode_reward=-496.81 +/- 166.15
Episode length: 16.64 +/- 5.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | -497          |
| time/                   |               |
|    total_timesteps      | 91500         |
| train/                  |               |
|    approx_kl            | 0.00023271376 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00117      |
|    explained_variance   | 0.731         |
|    learning_rate        | 0.001         |
|    loss                 | 5.67e+03      |
|    n_updates            | 9210          |
|    policy_gradient_loss | -7.45e-06     |
|    value_loss           | 1.19e+04      |
-------------------------------------------
Eval num_timesteps=92000, episode_reward=-565.57 +/- 184.02
Episode length: 15.60 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -545     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 90       |
|    time_elapsed    | 214      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=-486.84 +/- 196.62
Episode length: 15.54 +/- 4.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.5          |
|    mean_reward          | -487          |
| time/                   |               |
|    total_timesteps      | 92500         |
| train/                  |               |
|    approx_kl            | 1.7636921e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00124      |
|    explained_variance   | 0.786         |
|    learning_rate        | 0.001         |
|    loss                 | 4.03e+03      |
|    n_updates            | 9220          |
|    policy_gradient_loss | -1.47e-05     |
|    value_loss           | 8.86e+03      |
-------------------------------------------
Eval num_timesteps=93000, episode_reward=-532.52 +/- 199.02
Episode length: 15.14 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 91       |
|    time_elapsed    | 217      |
|    total_timesteps | 93184    |
---------------------------------
Eval num_timesteps=93500, episode_reward=-493.63 +/- 206.16
Episode length: 18.02 +/- 5.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 18           |
|    mean_reward          | -494         |
| time/                   |              |
|    total_timesteps      | 93500        |
| train/                  |              |
|    approx_kl            | 1.816079e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00166     |
|    explained_variance   | 0.7          |
|    learning_rate        | 0.001        |
|    loss                 | 4.12e+03     |
|    n_updates            | 9230         |
|    policy_gradient_loss | -6.89e-06    |
|    value_loss           | 1.13e+04     |
------------------------------------------
Eval num_timesteps=94000, episode_reward=-531.50 +/- 165.52
Episode length: 15.08 +/- 3.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 92       |
|    time_elapsed    | 219      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=-514.83 +/- 174.14
Episode length: 16.28 +/- 4.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.3          |
|    mean_reward          | -515          |
| time/                   |               |
|    total_timesteps      | 94500         |
| train/                  |               |
|    approx_kl            | 1.4493708e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00134      |
|    explained_variance   | 0.711         |
|    learning_rate        | 0.001         |
|    loss                 | 8.64e+03      |
|    n_updates            | 9240          |
|    policy_gradient_loss | -7.67e-06     |
|    value_loss           | 1.21e+04      |
-------------------------------------------
Eval num_timesteps=95000, episode_reward=-508.51 +/- 177.40
Episode length: 16.14 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -550     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 93       |
|    time_elapsed    | 221      |
|    total_timesteps | 95232    |
---------------------------------
Eval num_timesteps=95500, episode_reward=-552.21 +/- 195.47
Episode length: 15.78 +/- 5.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -552          |
| time/                   |               |
|    total_timesteps      | 95500         |
| train/                  |               |
|    approx_kl            | 2.3457687e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00136      |
|    explained_variance   | 0.714         |
|    learning_rate        | 0.001         |
|    loss                 | 5.47e+03      |
|    n_updates            | 9250          |
|    policy_gradient_loss | -1.99e-05     |
|    value_loss           | 1.1e+04       |
-------------------------------------------
Eval num_timesteps=96000, episode_reward=-590.59 +/- 166.91
Episode length: 14.84 +/- 3.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -591     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -527     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 94       |
|    time_elapsed    | 224      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=-542.89 +/- 166.76
Episode length: 15.92 +/- 4.82
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -543         |
| time/                   |              |
|    total_timesteps      | 96500        |
| train/                  |              |
|    approx_kl            | 0.0006955831 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0015      |
|    explained_variance   | 0.7          |
|    learning_rate        | 0.001        |
|    loss                 | 4.92e+03     |
|    n_updates            | 9260         |
|    policy_gradient_loss | 0.000381     |
|    value_loss           | 1.03e+04     |
------------------------------------------
Eval num_timesteps=97000, episode_reward=-556.35 +/- 184.73
Episode length: 14.32 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.3     |
|    mean_reward     | -556     |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 95       |
|    time_elapsed    | 226      |
|    total_timesteps | 97280    |
---------------------------------
Eval num_timesteps=97500, episode_reward=-517.16 +/- 181.73
Episode length: 15.32 +/- 5.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.3          |
|    mean_reward          | -517          |
| time/                   |               |
|    total_timesteps      | 97500         |
| train/                  |               |
|    approx_kl            | 1.7287675e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00139      |
|    explained_variance   | 0.718         |
|    learning_rate        | 0.001         |
|    loss                 | 6.31e+03      |
|    n_updates            | 9270          |
|    policy_gradient_loss | -2.7e-05      |
|    value_loss           | 1.09e+04      |
-------------------------------------------
Eval num_timesteps=98000, episode_reward=-503.72 +/- 151.93
Episode length: 16.38 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 96       |
|    time_elapsed    | 229      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=-551.52 +/- 201.14
Episode length: 15.86 +/- 4.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -552          |
| time/                   |               |
|    total_timesteps      | 98500         |
| train/                  |               |
|    approx_kl            | 2.3166649e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00173      |
|    explained_variance   | 0.703         |
|    learning_rate        | 0.001         |
|    loss                 | 3.3e+03       |
|    n_updates            | 9280          |
|    policy_gradient_loss | -2.2e-05      |
|    value_loss           | 9.8e+03       |
-------------------------------------------
Eval num_timesteps=99000, episode_reward=-548.56 +/- 153.06
Episode length: 16.28 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -549     |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 97       |
|    time_elapsed    | 231      |
|    total_timesteps | 99328    |
---------------------------------
Eval num_timesteps=99500, episode_reward=-507.39 +/- 179.99
Episode length: 16.50 +/- 5.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.5          |
|    mean_reward          | -507          |
| time/                   |               |
|    total_timesteps      | 99500         |
| train/                  |               |
|    approx_kl            | 2.3748726e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0017       |
|    explained_variance   | 0.673         |
|    learning_rate        | 0.001         |
|    loss                 | 4.42e+03      |
|    n_updates            | 9290          |
|    policy_gradient_loss | -3.77e-05     |
|    value_loss           | 1.09e+04      |
-------------------------------------------
Eval num_timesteps=100000, episode_reward=-553.27 +/- 154.85
Episode length: 16.14 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -553     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -491     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 98       |
|    time_elapsed    | 233      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=-518.44 +/- 188.62
Episode length: 15.36 +/- 5.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | -518          |
| time/                   |               |
|    total_timesteps      | 100500        |
| train/                  |               |
|    approx_kl            | 2.5494955e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00194      |
|    explained_variance   | 0.636         |
|    learning_rate        | 0.001         |
|    loss                 | 6.58e+03      |
|    n_updates            | 9300          |
|    policy_gradient_loss | -4.05e-05     |
|    value_loss           | 1.2e+04       |
-------------------------------------------
Eval num_timesteps=101000, episode_reward=-547.68 +/- 160.64
Episode length: 16.46 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -548     |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -467     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 99       |
|    time_elapsed    | 236      |
|    total_timesteps | 101376   |
---------------------------------
Eval num_timesteps=101500, episode_reward=-536.90 +/- 160.24
Episode length: 16.04 +/- 4.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -537          |
| time/                   |               |
|    total_timesteps      | 101500        |
| train/                  |               |
|    approx_kl            | 3.3760443e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00223      |
|    explained_variance   | 0.635         |
|    learning_rate        | 0.001         |
|    loss                 | 8.95e+03      |
|    n_updates            | 9310          |
|    policy_gradient_loss | -1.02e-05     |
|    value_loss           | 1.33e+04      |
-------------------------------------------
Eval num_timesteps=102000, episode_reward=-537.40 +/- 170.38
Episode length: 16.24 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 100      |
|    time_elapsed    | 238      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=-493.04 +/- 203.44
Episode length: 16.16 +/- 5.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -493          |
| time/                   |               |
|    total_timesteps      | 102500        |
| train/                  |               |
|    approx_kl            | 4.7116773e-06 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00238      |
|    explained_variance   | 0.672         |
|    learning_rate        | 0.001         |
|    loss                 | 4.21e+03      |
|    n_updates            | 9320          |
|    policy_gradient_loss | 1.19e-05      |
|    value_loss           | 1.24e+04      |
-------------------------------------------
Eval num_timesteps=103000, episode_reward=-484.60 +/- 154.21
Episode length: 16.84 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -485     |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -537     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 101      |
|    time_elapsed    | 240      |
|    total_timesteps | 103424   |
---------------------------------
Eval num_timesteps=103500, episode_reward=-489.61 +/- 213.58
Episode length: 16.06 +/- 5.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.1          |
|    mean_reward          | -490          |
| time/                   |               |
|    total_timesteps      | 103500        |
| train/                  |               |
|    approx_kl            | 2.9802322e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00209      |
|    explained_variance   | 0.665         |
|    learning_rate        | 0.001         |
|    loss                 | 5.27e+03      |
|    n_updates            | 9330          |
|    policy_gradient_loss | -3.82e-05     |
|    value_loss           | 1.43e+04      |
-------------------------------------------
Eval num_timesteps=104000, episode_reward=-553.18 +/- 170.93
Episode length: 15.06 +/- 3.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -553     |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 102      |
|    time_elapsed    | 243      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=-519.71 +/- 194.66
Episode length: 15.78 +/- 5.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -520          |
| time/                   |               |
|    total_timesteps      | 104500        |
| train/                  |               |
|    approx_kl            | 2.2700988e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00227      |
|    explained_variance   | 0.749         |
|    learning_rate        | 0.001         |
|    loss                 | 4.52e+03      |
|    n_updates            | 9340          |
|    policy_gradient_loss | -2.88e-05     |
|    value_loss           | 9.82e+03      |
-------------------------------------------
Eval num_timesteps=105000, episode_reward=-533.67 +/- 158.74
Episode length: 15.38 +/- 4.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -549     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 103      |
|    time_elapsed    | 245      |
|    total_timesteps | 105472   |
---------------------------------
Eval num_timesteps=105500, episode_reward=-494.78 +/- 162.63
Episode length: 16.60 +/- 4.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | -495          |
| time/                   |               |
|    total_timesteps      | 105500        |
| train/                  |               |
|    approx_kl            | 2.1595042e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00204      |
|    explained_variance   | 0.794         |
|    learning_rate        | 0.001         |
|    loss                 | 4.13e+03      |
|    n_updates            | 9350          |
|    policy_gradient_loss | -2.39e-05     |
|    value_loss           | 8.78e+03      |
-------------------------------------------
Eval num_timesteps=106000, episode_reward=-478.21 +/- 188.31
Episode length: 15.86 +/- 4.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -478     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 104      |
|    time_elapsed    | 247      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=-510.42 +/- 201.06
Episode length: 15.54 +/- 5.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.5          |
|    mean_reward          | -510          |
| time/                   |               |
|    total_timesteps      | 106500        |
| train/                  |               |
|    approx_kl            | 2.1071173e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00264      |
|    explained_variance   | 0.707         |
|    learning_rate        | 0.001         |
|    loss                 | 5.47e+03      |
|    n_updates            | 9360          |
|    policy_gradient_loss | -1.26e-05     |
|    value_loss           | 1.13e+04      |
-------------------------------------------
Eval num_timesteps=107000, episode_reward=-545.39 +/- 172.59
Episode length: 15.36 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -545     |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=-550.71 +/- 188.16
Episode length: 16.56 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -551     |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 105      |
|    time_elapsed    | 250      |
|    total_timesteps | 107520   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-487.67 +/- 152.41
Episode length: 15.36 +/- 4.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | -488          |
| time/                   |               |
|    total_timesteps      | 108000        |
| train/                  |               |
|    approx_kl            | 2.0780135e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00252      |
|    explained_variance   | 0.717         |
|    learning_rate        | 0.001         |
|    loss                 | 4.15e+03      |
|    n_updates            | 9370          |
|    policy_gradient_loss | -3.39e-05     |
|    value_loss           | 1.13e+04      |
-------------------------------------------
Eval num_timesteps=108500, episode_reward=-544.47 +/- 209.31
Episode length: 15.68 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -544     |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -529     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 106      |
|    time_elapsed    | 253      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=-520.48 +/- 192.81
Episode length: 16.76 +/- 5.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -520         |
| time/                   |              |
|    total_timesteps      | 109000       |
| train/                  |              |
|    approx_kl            | 0.0031683438 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00227     |
|    explained_variance   | 0.685        |
|    learning_rate        | 0.001        |
|    loss                 | 3.58e+03     |
|    n_updates            | 9380         |
|    policy_gradient_loss | 0.002        |
|    value_loss           | 1e+04        |
------------------------------------------
Eval num_timesteps=109500, episode_reward=-521.22 +/- 152.52
Episode length: 14.76 +/- 3.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.8     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 107      |
|    time_elapsed    | 255      |
|    total_timesteps | 109568   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-546.92 +/- 167.84
Episode length: 15.94 +/- 5.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -547          |
| time/                   |               |
|    total_timesteps      | 110000        |
| train/                  |               |
|    approx_kl            | 4.4470653e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0022       |
|    explained_variance   | 0.685         |
|    learning_rate        | 0.001         |
|    loss                 | 3.25e+03      |
|    n_updates            | 9390          |
|    policy_gradient_loss | -2.33e-05     |
|    value_loss           | 1.11e+04      |
-------------------------------------------
Eval num_timesteps=110500, episode_reward=-536.71 +/- 172.74
Episode length: 16.18 +/- 4.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 108      |
|    time_elapsed    | 257      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=-492.46 +/- 164.75
Episode length: 16.18 +/- 4.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -492          |
| time/                   |               |
|    total_timesteps      | 111000        |
| train/                  |               |
|    approx_kl            | 7.0547685e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00185      |
|    explained_variance   | 0.591         |
|    learning_rate        | 0.001         |
|    loss                 | 9.31e+03      |
|    n_updates            | 9400          |
|    policy_gradient_loss | -4.25e-05     |
|    value_loss           | 1.55e+04      |
-------------------------------------------
Eval num_timesteps=111500, episode_reward=-511.21 +/- 172.56
Episode length: 16.32 +/- 4.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 428      |
|    iterations      | 109      |
|    time_elapsed    | 260      |
|    total_timesteps | 111616   |
---------------------------------
Eval num_timesteps=112000, episode_reward=-528.26 +/- 195.78
Episode length: 14.64 +/- 5.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.6         |
|    mean_reward          | -528         |
| time/                   |              |
|    total_timesteps      | 112000       |
| train/                  |              |
|    approx_kl            | 2.514571e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00216     |
|    explained_variance   | 0.723        |
|    learning_rate        | 0.001        |
|    loss                 | 3.86e+03     |
|    n_updates            | 9410         |
|    policy_gradient_loss | -3.59e-05    |
|    value_loss           | 8.42e+03     |
------------------------------------------
Eval num_timesteps=112500, episode_reward=-500.07 +/- 189.86
Episode length: 16.06 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -500     |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -503     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 110      |
|    time_elapsed    | 262      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=-494.66 +/- 193.82
Episode length: 16.38 +/- 5.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.4          |
|    mean_reward          | -495          |
| time/                   |               |
|    total_timesteps      | 113000        |
| train/                  |               |
|    approx_kl            | 3.9872248e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00323      |
|    explained_variance   | 0.682         |
|    learning_rate        | 0.001         |
|    loss                 | 3.18e+03      |
|    n_updates            | 9420          |
|    policy_gradient_loss | -3.92e-05     |
|    value_loss           | 1.03e+04      |
-------------------------------------------
Eval num_timesteps=113500, episode_reward=-522.18 +/- 185.62
Episode length: 17.12 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -495     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 111      |
|    time_elapsed    | 264      |
|    total_timesteps | 113664   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-496.99 +/- 212.34
Episode length: 16.78 +/- 6.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.8          |
|    mean_reward          | -497          |
| time/                   |               |
|    total_timesteps      | 114000        |
| train/                  |               |
|    approx_kl            | 0.00015164906 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00465      |
|    explained_variance   | 0.648         |
|    learning_rate        | 0.001         |
|    loss                 | 4.82e+03      |
|    n_updates            | 9430          |
|    policy_gradient_loss | -0.000469     |
|    value_loss           | 1.16e+04      |
-------------------------------------------
Eval num_timesteps=114500, episode_reward=-545.76 +/- 189.92
Episode length: 15.76 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -546     |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 112      |
|    time_elapsed    | 267      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-533.00 +/- 205.95
Episode length: 16.46 +/- 5.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.5        |
|    mean_reward          | -533        |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 8.03562e-05 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00335    |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.001       |
|    loss                 | 4.67e+03    |
|    n_updates            | 9440        |
|    policy_gradient_loss | 0.000118    |
|    value_loss           | 9.72e+03    |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=-555.80 +/- 164.07
Episode length: 15.74 +/- 4.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -556     |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -560     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 113      |
|    time_elapsed    | 269      |
|    total_timesteps | 115712   |
---------------------------------
Eval num_timesteps=116000, episode_reward=-506.92 +/- 169.68
Episode length: 15.86 +/- 4.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -507          |
| time/                   |               |
|    total_timesteps      | 116000        |
| train/                  |               |
|    approx_kl            | 4.2200554e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00267      |
|    explained_variance   | 0.685         |
|    learning_rate        | 0.001         |
|    loss                 | 6.43e+03      |
|    n_updates            | 9450          |
|    policy_gradient_loss | -4.28e-05     |
|    value_loss           | 1.34e+04      |
-------------------------------------------
Eval num_timesteps=116500, episode_reward=-514.90 +/- 199.67
Episode length: 15.60 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 114      |
|    time_elapsed    | 271      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-535.26 +/- 169.36
Episode length: 16.24 +/- 4.76
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -535        |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 7.52043e-08 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00345    |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.001       |
|    loss                 | 3.16e+03    |
|    n_updates            | 9460        |
|    policy_gradient_loss | -0.000105   |
|    value_loss           | 9.87e+03    |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=-494.08 +/- 187.80
Episode length: 15.70 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -494     |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -487     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 115      |
|    time_elapsed    | 274      |
|    total_timesteps | 117760   |
---------------------------------
Eval num_timesteps=118000, episode_reward=-528.60 +/- 180.91
Episode length: 16.74 +/- 5.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.7          |
|    mean_reward          | -529          |
| time/                   |               |
|    total_timesteps      | 118000        |
| train/                  |               |
|    approx_kl            | 1.0459917e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00472      |
|    explained_variance   | 0.682         |
|    learning_rate        | 0.001         |
|    loss                 | 4.85e+03      |
|    n_updates            | 9470          |
|    policy_gradient_loss | -2.74e-05     |
|    value_loss           | 1.12e+04      |
-------------------------------------------
Eval num_timesteps=118500, episode_reward=-467.87 +/- 153.18
Episode length: 16.10 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -468     |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -485     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 116      |
|    time_elapsed    | 276      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=-533.86 +/- 150.97
Episode length: 16.30 +/- 5.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -534         |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 7.933704e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00471     |
|    explained_variance   | 0.715        |
|    learning_rate        | 0.001        |
|    loss                 | 4.23e+03     |
|    n_updates            | 9480         |
|    policy_gradient_loss | -5.24e-05    |
|    value_loss           | 9.46e+03     |
------------------------------------------
Eval num_timesteps=119500, episode_reward=-546.99 +/- 204.37
Episode length: 16.36 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -547     |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -507     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 117      |
|    time_elapsed    | 278      |
|    total_timesteps | 119808   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-534.43 +/- 225.07
Episode length: 14.84 +/- 4.22
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.8       |
|    mean_reward          | -534       |
| time/                   |            |
|    total_timesteps      | 120000     |
| train/                  |            |
|    approx_kl            | 7.9046e-08 |
|    clip_fraction        | 0          |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.00444   |
|    explained_variance   | 0.657      |
|    learning_rate        | 0.001      |
|    loss                 | 5.65e+03   |
|    n_updates            | 9490       |
|    policy_gradient_loss | -9.96e-05  |
|    value_loss           | 1.39e+04   |
----------------------------------------
Eval num_timesteps=120500, episode_reward=-532.75 +/- 163.59
Episode length: 14.94 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -498     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 118      |
|    time_elapsed    | 281      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=-535.16 +/- 171.31
Episode length: 16.70 +/- 3.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -535         |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 9.511132e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00507     |
|    explained_variance   | 0.713        |
|    learning_rate        | 0.001        |
|    loss                 | 2.72e+03     |
|    n_updates            | 9500         |
|    policy_gradient_loss | -8.38e-05    |
|    value_loss           | 1.14e+04     |
------------------------------------------
Eval num_timesteps=121500, episode_reward=-516.66 +/- 175.00
Episode length: 16.82 +/- 5.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -511     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 119      |
|    time_elapsed    | 283      |
|    total_timesteps | 121856   |
---------------------------------
Eval num_timesteps=122000, episode_reward=-527.71 +/- 193.95
Episode length: 15.82 +/- 5.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -528          |
| time/                   |               |
|    total_timesteps      | 122000        |
| train/                  |               |
|    approx_kl            | 1.2823148e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00455      |
|    explained_variance   | 0.651         |
|    learning_rate        | 0.001         |
|    loss                 | 5.75e+03      |
|    n_updates            | 9510          |
|    policy_gradient_loss | -0.000106     |
|    value_loss           | 1.17e+04      |
-------------------------------------------
Eval num_timesteps=122500, episode_reward=-552.79 +/- 189.71
Episode length: 14.98 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -553     |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.6     |
|    ep_rew_mean     | -547     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 120      |
|    time_elapsed    | 285      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=-509.26 +/- 162.40
Episode length: 16.08 +/- 5.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.1          |
|    mean_reward          | -509          |
| time/                   |               |
|    total_timesteps      | 123000        |
| train/                  |               |
|    approx_kl            | 1.4848774e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00331      |
|    explained_variance   | 0.68          |
|    learning_rate        | 0.001         |
|    loss                 | 5.73e+03      |
|    n_updates            | 9520          |
|    policy_gradient_loss | 2.77e-05      |
|    value_loss           | 1.24e+04      |
-------------------------------------------
Eval num_timesteps=123500, episode_reward=-534.11 +/- 190.71
Episode length: 15.54 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 121      |
|    time_elapsed    | 288      |
|    total_timesteps | 123904   |
---------------------------------
Eval num_timesteps=124000, episode_reward=-501.18 +/- 179.19
Episode length: 16.28 +/- 5.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -501         |
| time/                   |              |
|    total_timesteps      | 124000       |
| train/                  |              |
|    approx_kl            | 9.208452e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00477     |
|    explained_variance   | 0.697        |
|    learning_rate        | 0.001        |
|    loss                 | 4.58e+03     |
|    n_updates            | 9530         |
|    policy_gradient_loss | -8.81e-05    |
|    value_loss           | 1.12e+04     |
------------------------------------------
Eval num_timesteps=124500, episode_reward=-587.47 +/- 178.64
Episode length: 14.90 +/- 4.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -587     |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 122      |
|    time_elapsed    | 290      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-512.72 +/- 172.54
Episode length: 16.72 +/- 5.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.7          |
|    mean_reward          | -513          |
| time/                   |               |
|    total_timesteps      | 125000        |
| train/                  |               |
|    approx_kl            | 9.7439624e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00335      |
|    explained_variance   | 0.723         |
|    learning_rate        | 0.001         |
|    loss                 | 4.91e+03      |
|    n_updates            | 9540          |
|    policy_gradient_loss | -4.53e-05     |
|    value_loss           | 1.14e+04      |
-------------------------------------------
Eval num_timesteps=125500, episode_reward=-514.19 +/- 179.90
Episode length: 16.38 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -552     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 123      |
|    time_elapsed    | 292      |
|    total_timesteps | 125952   |
---------------------------------
Eval num_timesteps=126000, episode_reward=-515.94 +/- 187.12
Episode length: 17.02 +/- 4.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | -516          |
| time/                   |               |
|    total_timesteps      | 126000        |
| train/                  |               |
|    approx_kl            | 7.6775905e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00273      |
|    explained_variance   | 0.7           |
|    learning_rate        | 0.001         |
|    loss                 | 3.98e+03      |
|    n_updates            | 9550          |
|    policy_gradient_loss | -8.21e-05     |
|    value_loss           | 1.18e+04      |
-------------------------------------------
Eval num_timesteps=126500, episode_reward=-495.97 +/- 190.40
Episode length: 16.90 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -496     |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -537     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 124      |
|    time_elapsed    | 295      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=-529.52 +/- 190.48
Episode length: 16.16 +/- 4.78
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -530          |
| time/                   |               |
|    total_timesteps      | 127000        |
| train/                  |               |
|    approx_kl            | 2.7519418e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00355      |
|    explained_variance   | 0.718         |
|    learning_rate        | 0.001         |
|    loss                 | 5.11e+03      |
|    n_updates            | 9560          |
|    policy_gradient_loss | -6.35e-05     |
|    value_loss           | 8.85e+03      |
-------------------------------------------
Eval num_timesteps=127500, episode_reward=-581.88 +/- 161.83
Episode length: 15.76 +/- 4.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -582     |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=-509.18 +/- 204.57
Episode length: 16.46 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 125      |
|    time_elapsed    | 298      |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=-562.44 +/- 148.12
Episode length: 15.74 +/- 4.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -562         |
| time/                   |              |
|    total_timesteps      | 128500       |
| train/                  |              |
|    approx_kl            | 5.745096e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00391     |
|    explained_variance   | 0.73         |
|    learning_rate        | 0.001        |
|    loss                 | 6.45e+03     |
|    n_updates            | 9570         |
|    policy_gradient_loss | -5.53e-05    |
|    value_loss           | 1.1e+04      |
------------------------------------------
Eval num_timesteps=129000, episode_reward=-536.79 +/- 181.39
Episode length: 15.94 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -539     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 126      |
|    time_elapsed    | 300      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=-514.46 +/- 228.40
Episode length: 15.96 +/- 5.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -514         |
| time/                   |              |
|    total_timesteps      | 129500       |
| train/                  |              |
|    approx_kl            | 9.051291e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00385     |
|    explained_variance   | 0.715        |
|    learning_rate        | 0.001        |
|    loss                 | 5.51e+03     |
|    n_updates            | 9580         |
|    policy_gradient_loss | -6.31e-05    |
|    value_loss           | 1.02e+04     |
------------------------------------------
Eval num_timesteps=130000, episode_reward=-493.30 +/- 197.36
Episode length: 17.04 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -493     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 127      |
|    time_elapsed    | 303      |
|    total_timesteps | 130048   |
---------------------------------
Eval num_timesteps=130500, episode_reward=-516.39 +/- 177.50
Episode length: 17.16 +/- 5.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.2         |
|    mean_reward          | -516         |
| time/                   |              |
|    total_timesteps      | 130500       |
| train/                  |              |
|    approx_kl            | 6.717164e-07 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00521     |
|    explained_variance   | 0.696        |
|    learning_rate        | 0.001        |
|    loss                 | 3.83e+03     |
|    n_updates            | 9590         |
|    policy_gradient_loss | 1.01e-05     |
|    value_loss           | 1.09e+04     |
------------------------------------------
Eval num_timesteps=131000, episode_reward=-551.00 +/- 170.96
Episode length: 15.08 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -551     |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -489     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 128      |
|    time_elapsed    | 305      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=-534.82 +/- 169.70
Episode length: 17.16 +/- 4.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.2          |
|    mean_reward          | -535          |
| time/                   |               |
|    total_timesteps      | 131500        |
| train/                  |               |
|    approx_kl            | 1.5611295e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00685      |
|    explained_variance   | 0.608         |
|    learning_rate        | 0.001         |
|    loss                 | 7.14e+03      |
|    n_updates            | 9600          |
|    policy_gradient_loss | -9.86e-05     |
|    value_loss           | 1.25e+04      |
-------------------------------------------
Eval num_timesteps=132000, episode_reward=-528.81 +/- 198.46
Episode length: 15.24 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -511     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 129      |
|    time_elapsed    | 307      |
|    total_timesteps | 132096   |
---------------------------------
Eval num_timesteps=132500, episode_reward=-570.47 +/- 176.36
Episode length: 15.30 +/- 4.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.3          |
|    mean_reward          | -570          |
| time/                   |               |
|    total_timesteps      | 132500        |
| train/                  |               |
|    approx_kl            | 0.00026946532 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00568      |
|    explained_variance   | 0.722         |
|    learning_rate        | 0.001         |
|    loss                 | 4.55e+03      |
|    n_updates            | 9610          |
|    policy_gradient_loss | -0.000259     |
|    value_loss           | 9.86e+03      |
-------------------------------------------
Eval num_timesteps=133000, episode_reward=-515.64 +/- 178.15
Episode length: 15.94 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 130      |
|    time_elapsed    | 310      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=-522.56 +/- 184.78
Episode length: 14.68 +/- 4.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.7          |
|    mean_reward          | -523          |
| time/                   |               |
|    total_timesteps      | 133500        |
| train/                  |               |
|    approx_kl            | 1.1053635e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00631      |
|    explained_variance   | 0.625         |
|    learning_rate        | 0.001         |
|    loss                 | 5.24e+03      |
|    n_updates            | 9620          |
|    policy_gradient_loss | -7.52e-05     |
|    value_loss           | 1.21e+04      |
-------------------------------------------
Eval num_timesteps=134000, episode_reward=-543.07 +/- 177.67
Episode length: 17.02 +/- 5.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 131      |
|    time_elapsed    | 312      |
|    total_timesteps | 134144   |
---------------------------------
Eval num_timesteps=134500, episode_reward=-502.47 +/- 200.64
Episode length: 15.30 +/- 4.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.3          |
|    mean_reward          | -502          |
| time/                   |               |
|    total_timesteps      | 134500        |
| train/                  |               |
|    approx_kl            | 3.6854006e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00733      |
|    explained_variance   | 0.701         |
|    learning_rate        | 0.001         |
|    loss                 | 4.06e+03      |
|    n_updates            | 9630          |
|    policy_gradient_loss | 5.16e-05      |
|    value_loss           | 9.96e+03      |
-------------------------------------------
Eval num_timesteps=135000, episode_reward=-513.17 +/- 180.49
Episode length: 15.12 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 132      |
|    time_elapsed    | 314      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=-504.27 +/- 152.95
Episode length: 16.44 +/- 5.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.4          |
|    mean_reward          | -504          |
| time/                   |               |
|    total_timesteps      | 135500        |
| train/                  |               |
|    approx_kl            | 2.5623012e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00701      |
|    explained_variance   | 0.699         |
|    learning_rate        | 0.001         |
|    loss                 | 3.65e+03      |
|    n_updates            | 9640          |
|    policy_gradient_loss | -0.000108     |
|    value_loss           | 1.18e+04      |
-------------------------------------------
Eval num_timesteps=136000, episode_reward=-538.25 +/- 184.39
Episode length: 16.48 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 133      |
|    time_elapsed    | 317      |
|    total_timesteps | 136192   |
---------------------------------
Eval num_timesteps=136500, episode_reward=-468.16 +/- 204.74
Episode length: 15.82 +/- 5.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -468          |
| time/                   |               |
|    total_timesteps      | 136500        |
| train/                  |               |
|    approx_kl            | 3.1930977e-06 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00631      |
|    explained_variance   | 0.721         |
|    learning_rate        | 0.001         |
|    loss                 | 4.25e+03      |
|    n_updates            | 9650          |
|    policy_gradient_loss | 0.000216      |
|    value_loss           | 1.27e+04      |
-------------------------------------------
Eval num_timesteps=137000, episode_reward=-557.09 +/- 180.18
Episode length: 16.32 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -557     |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -551     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 134      |
|    time_elapsed    | 319      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=-522.98 +/- 178.01
Episode length: 16.00 +/- 5.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -523          |
| time/                   |               |
|    total_timesteps      | 137500        |
| train/                  |               |
|    approx_kl            | 2.9197545e-06 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00976      |
|    explained_variance   | 0.697         |
|    learning_rate        | 0.001         |
|    loss                 | 6.1e+03       |
|    n_updates            | 9660          |
|    policy_gradient_loss | -0.000174     |
|    value_loss           | 1.13e+04      |
-------------------------------------------
Eval num_timesteps=138000, episode_reward=-514.53 +/- 181.43
Episode length: 15.60 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -537     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 135      |
|    time_elapsed    | 321      |
|    total_timesteps | 138240   |
---------------------------------
Eval num_timesteps=138500, episode_reward=-511.08 +/- 182.08
Episode length: 16.00 +/- 4.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -511         |
| time/                   |              |
|    total_timesteps      | 138500       |
| train/                  |              |
|    approx_kl            | 0.0001950764 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0126      |
|    explained_variance   | 0.687        |
|    learning_rate        | 0.001        |
|    loss                 | 4.02e+03     |
|    n_updates            | 9670         |
|    policy_gradient_loss | -0.000245    |
|    value_loss           | 1.11e+04     |
------------------------------------------
Eval num_timesteps=139000, episode_reward=-497.01 +/- 201.76
Episode length: 15.60 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -497     |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 136      |
|    time_elapsed    | 324      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=-530.61 +/- 180.63
Episode length: 15.02 +/- 4.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | -531          |
| time/                   |               |
|    total_timesteps      | 139500        |
| train/                  |               |
|    approx_kl            | 7.8131794e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0113       |
|    explained_variance   | 0.64          |
|    learning_rate        | 0.001         |
|    loss                 | 4.84e+03      |
|    n_updates            | 9680          |
|    policy_gradient_loss | 0.000144      |
|    value_loss           | 1.19e+04      |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=-519.09 +/- 165.40
Episode length: 15.26 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -504     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 137      |
|    time_elapsed    | 326      |
|    total_timesteps | 140288   |
---------------------------------
Eval num_timesteps=140500, episode_reward=-536.10 +/- 200.16
Episode length: 16.20 +/- 5.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -536          |
| time/                   |               |
|    total_timesteps      | 140500        |
| train/                  |               |
|    approx_kl            | 1.1102355e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00873      |
|    explained_variance   | 0.68          |
|    learning_rate        | 0.001         |
|    loss                 | 7.38e+03      |
|    n_updates            | 9690          |
|    policy_gradient_loss | -0.000243     |
|    value_loss           | 1.14e+04      |
-------------------------------------------
Eval num_timesteps=141000, episode_reward=-511.00 +/- 174.70
Episode length: 14.90 +/- 3.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 138      |
|    time_elapsed    | 328      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=-549.23 +/- 173.44
Episode length: 15.20 +/- 3.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | -549          |
| time/                   |               |
|    total_timesteps      | 141500        |
| train/                  |               |
|    approx_kl            | 2.1106098e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00931      |
|    explained_variance   | 0.698         |
|    learning_rate        | 0.001         |
|    loss                 | 3.14e+03      |
|    n_updates            | 9700          |
|    policy_gradient_loss | -7.06e-05     |
|    value_loss           | 1.07e+04      |
-------------------------------------------
Eval num_timesteps=142000, episode_reward=-537.02 +/- 187.55
Episode length: 15.42 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 139      |
|    time_elapsed    | 331      |
|    total_timesteps | 142336   |
---------------------------------
Eval num_timesteps=142500, episode_reward=-503.22 +/- 171.72
Episode length: 15.04 +/- 3.89
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | -503          |
| time/                   |               |
|    total_timesteps      | 142500        |
| train/                  |               |
|    approx_kl            | 2.6798807e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00929      |
|    explained_variance   | 0.73          |
|    learning_rate        | 0.001         |
|    loss                 | 5.62e+03      |
|    n_updates            | 9710          |
|    policy_gradient_loss | -0.000173     |
|    value_loss           | 9.56e+03      |
-------------------------------------------
Eval num_timesteps=143000, episode_reward=-494.27 +/- 185.67
Episode length: 15.64 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -494     |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -499     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 140      |
|    time_elapsed    | 333      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=-540.81 +/- 195.05
Episode length: 16.28 +/- 5.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -541        |
| time/                   |             |
|    total_timesteps      | 143500      |
| train/                  |             |
|    approx_kl            | 8.07317e-05 |
|    clip_fraction        | 0.00117     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0111     |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.001       |
|    loss                 | 5.81e+03    |
|    n_updates            | 9720        |
|    policy_gradient_loss | 4.24e-05    |
|    value_loss           | 1.02e+04    |
-----------------------------------------
Eval num_timesteps=144000, episode_reward=-510.64 +/- 149.97
Episode length: 17.10 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 141      |
|    time_elapsed    | 335      |
|    total_timesteps | 144384   |
---------------------------------
Eval num_timesteps=144500, episode_reward=-502.85 +/- 183.21
Episode length: 16.86 +/- 5.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.9         |
|    mean_reward          | -503         |
| time/                   |              |
|    total_timesteps      | 144500       |
| train/                  |              |
|    approx_kl            | 5.985977e-05 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0107      |
|    explained_variance   | 0.698        |
|    learning_rate        | 0.001        |
|    loss                 | 6.52e+03     |
|    n_updates            | 9730         |
|    policy_gradient_loss | -0.000382    |
|    value_loss           | 1.29e+04     |
------------------------------------------
Eval num_timesteps=145000, episode_reward=-528.18 +/- 173.08
Episode length: 16.02 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 142      |
|    time_elapsed    | 338      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=-488.37 +/- 181.49
Episode length: 16.18 +/- 5.03
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -488          |
| time/                   |               |
|    total_timesteps      | 145500        |
| train/                  |               |
|    approx_kl            | 3.9261067e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0131       |
|    explained_variance   | 0.757         |
|    learning_rate        | 0.001         |
|    loss                 | 3.58e+03      |
|    n_updates            | 9740          |
|    policy_gradient_loss | -0.000193     |
|    value_loss           | 1.02e+04      |
-------------------------------------------
Eval num_timesteps=146000, episode_reward=-532.95 +/- 221.87
Episode length: 16.36 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -558     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 143      |
|    time_elapsed    | 340      |
|    total_timesteps | 146432   |
---------------------------------
Eval num_timesteps=146500, episode_reward=-534.92 +/- 184.71
Episode length: 16.48 +/- 4.61
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -535         |
| time/                   |              |
|    total_timesteps      | 146500       |
| train/                  |              |
|    approx_kl            | 0.0002794534 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0103      |
|    explained_variance   | 0.728        |
|    learning_rate        | 0.001        |
|    loss                 | 4.33e+03     |
|    n_updates            | 9750         |
|    policy_gradient_loss | -0.000752    |
|    value_loss           | 1.14e+04     |
------------------------------------------
Eval num_timesteps=147000, episode_reward=-529.19 +/- 194.85
Episode length: 15.62 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -561     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 144      |
|    time_elapsed    | 343      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=-559.39 +/- 178.33
Episode length: 13.86 +/- 4.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 13.9          |
|    mean_reward          | -559          |
| time/                   |               |
|    total_timesteps      | 147500        |
| train/                  |               |
|    approx_kl            | 3.6057783e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00946      |
|    explained_variance   | 0.716         |
|    learning_rate        | 0.001         |
|    loss                 | 4.05e+03      |
|    n_updates            | 9760          |
|    policy_gradient_loss | -0.000236     |
|    value_loss           | 1.1e+04       |
-------------------------------------------
Eval num_timesteps=148000, episode_reward=-523.00 +/- 179.29
Episode length: 15.88 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -551     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 145      |
|    time_elapsed    | 345      |
|    total_timesteps | 148480   |
---------------------------------
Eval num_timesteps=148500, episode_reward=-512.78 +/- 167.74
Episode length: 16.50 +/- 4.79
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.5          |
|    mean_reward          | -513          |
| time/                   |               |
|    total_timesteps      | 148500        |
| train/                  |               |
|    approx_kl            | 8.8545494e-05 |
|    clip_fraction        | 0.00146       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0132       |
|    explained_variance   | 0.686         |
|    learning_rate        | 0.001         |
|    loss                 | 7.84e+03      |
|    n_updates            | 9770          |
|    policy_gradient_loss | 0.000462      |
|    value_loss           | 1.24e+04      |
-------------------------------------------
Eval num_timesteps=149000, episode_reward=-551.99 +/- 173.23
Episode length: 15.36 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -552     |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=-514.77 +/- 181.64
Episode length: 16.42 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -534     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 146      |
|    time_elapsed    | 348      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-539.59 +/- 172.05
Episode length: 16.34 +/- 4.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -540         |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 6.635883e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.021       |
|    explained_variance   | 0.727        |
|    learning_rate        | 0.001        |
|    loss                 | 4.62e+03     |
|    n_updates            | 9780         |
|    policy_gradient_loss | -0.000177    |
|    value_loss           | 1.16e+04     |
------------------------------------------
Eval num_timesteps=150500, episode_reward=-531.29 +/- 160.60
Episode length: 15.58 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -533     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 147      |
|    time_elapsed    | 350      |
|    total_timesteps | 150528   |
---------------------------------
Eval num_timesteps=151000, episode_reward=-508.62 +/- 172.57
Episode length: 15.88 +/- 4.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -509          |
| time/                   |               |
|    total_timesteps      | 151000        |
| train/                  |               |
|    approx_kl            | 3.6861165e-06 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0186       |
|    explained_variance   | 0.654         |
|    learning_rate        | 0.001         |
|    loss                 | 5.39e+03      |
|    n_updates            | 9790          |
|    policy_gradient_loss | -0.000161     |
|    value_loss           | 1.4e+04       |
-------------------------------------------
Eval num_timesteps=151500, episode_reward=-557.78 +/- 166.95
Episode length: 15.48 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -558     |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -533     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 148      |
|    time_elapsed    | 352      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-457.90 +/- 188.18
Episode length: 17.32 +/- 5.49
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.3          |
|    mean_reward          | -458          |
| time/                   |               |
|    total_timesteps      | 152000        |
| train/                  |               |
|    approx_kl            | 0.00021640968 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.013        |
|    explained_variance   | 0.668         |
|    learning_rate        | 0.001         |
|    loss                 | 3.4e+03       |
|    n_updates            | 9800          |
|    policy_gradient_loss | 0.000244      |
|    value_loss           | 1.15e+04      |
-------------------------------------------
Eval num_timesteps=152500, episode_reward=-518.42 +/- 247.40
Episode length: 16.52 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -506     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 149      |
|    time_elapsed    | 355      |
|    total_timesteps | 152576   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-512.84 +/- 179.53
Episode length: 16.00 +/- 4.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -513         |
| time/                   |              |
|    total_timesteps      | 153000       |
| train/                  |              |
|    approx_kl            | 4.202011e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0124      |
|    explained_variance   | 0.649        |
|    learning_rate        | 0.001        |
|    loss                 | 5.18e+03     |
|    n_updates            | 9810         |
|    policy_gradient_loss | -0.000184    |
|    value_loss           | 1.29e+04     |
------------------------------------------
Eval num_timesteps=153500, episode_reward=-597.87 +/- 176.96
Episode length: 15.94 +/- 4.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -598     |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -495     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 150      |
|    time_elapsed    | 357      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=-543.13 +/- 182.21
Episode length: 15.40 +/- 4.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | -543          |
| time/                   |               |
|    total_timesteps      | 154000        |
| train/                  |               |
|    approx_kl            | 4.8216083e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0151       |
|    explained_variance   | 0.675         |
|    learning_rate        | 0.001         |
|    loss                 | 5.75e+03      |
|    n_updates            | 9820          |
|    policy_gradient_loss | -0.00035      |
|    value_loss           | 1.17e+04      |
-------------------------------------------
Eval num_timesteps=154500, episode_reward=-512.65 +/- 163.93
Episode length: 16.00 +/- 4.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 151      |
|    time_elapsed    | 360      |
|    total_timesteps | 154624   |
---------------------------------
Eval num_timesteps=155000, episode_reward=-511.25 +/- 172.33
Episode length: 15.86 +/- 4.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -511          |
| time/                   |               |
|    total_timesteps      | 155000        |
| train/                  |               |
|    approx_kl            | 1.1950266e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0144       |
|    explained_variance   | 0.769         |
|    learning_rate        | 0.001         |
|    loss                 | 2.92e+03      |
|    n_updates            | 9830          |
|    policy_gradient_loss | -0.000327     |
|    value_loss           | 9.16e+03      |
-------------------------------------------
Eval num_timesteps=155500, episode_reward=-544.69 +/- 225.91
Episode length: 15.14 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -545     |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 152      |
|    time_elapsed    | 362      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=-492.88 +/- 163.69
Episode length: 16.36 +/- 5.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.4          |
|    mean_reward          | -493          |
| time/                   |               |
|    total_timesteps      | 156000        |
| train/                  |               |
|    approx_kl            | 0.00014308153 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.012        |
|    explained_variance   | 0.634         |
|    learning_rate        | 0.001         |
|    loss                 | 5.5e+03       |
|    n_updates            | 9840          |
|    policy_gradient_loss | -0.000452     |
|    value_loss           | 1.17e+04      |
-------------------------------------------
Eval num_timesteps=156500, episode_reward=-540.78 +/- 198.06
Episode length: 15.56 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -517     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 153      |
|    time_elapsed    | 364      |
|    total_timesteps | 156672   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-545.96 +/- 166.22
Episode length: 15.72 +/- 4.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -546         |
| time/                   |              |
|    total_timesteps      | 157000       |
| train/                  |              |
|    approx_kl            | 8.200668e-05 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0113      |
|    explained_variance   | 0.659        |
|    learning_rate        | 0.001        |
|    loss                 | 7.81e+03     |
|    n_updates            | 9850         |
|    policy_gradient_loss | -0.000205    |
|    value_loss           | 1.3e+04      |
------------------------------------------
Eval num_timesteps=157500, episode_reward=-518.03 +/- 175.13
Episode length: 15.00 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -496     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 154      |
|    time_elapsed    | 366      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=-547.67 +/- 191.97
Episode length: 16.42 +/- 5.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -548         |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0005190235 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0132      |
|    explained_variance   | 0.621        |
|    learning_rate        | 0.001        |
|    loss                 | 7.36e+03     |
|    n_updates            | 9860         |
|    policy_gradient_loss | -5.41e-06    |
|    value_loss           | 1.37e+04     |
------------------------------------------
Eval num_timesteps=158500, episode_reward=-526.60 +/- 151.86
Episode length: 15.96 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 155      |
|    time_elapsed    | 369      |
|    total_timesteps | 158720   |
---------------------------------
Eval num_timesteps=159000, episode_reward=-456.58 +/- 255.64
Episode length: 17.04 +/- 5.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17           |
|    mean_reward          | -457         |
| time/                   |              |
|    total_timesteps      | 159000       |
| train/                  |              |
|    approx_kl            | 6.565824e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0134      |
|    explained_variance   | 0.668        |
|    learning_rate        | 0.001        |
|    loss                 | 5.97e+03     |
|    n_updates            | 9870         |
|    policy_gradient_loss | -0.000143    |
|    value_loss           | 1.31e+04     |
------------------------------------------
Eval num_timesteps=159500, episode_reward=-486.51 +/- 175.41
Episode length: 15.72 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -487     |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -538     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 156      |
|    time_elapsed    | 371      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-527.47 +/- 158.75
Episode length: 14.92 +/- 4.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -527          |
| time/                   |               |
|    total_timesteps      | 160000        |
| train/                  |               |
|    approx_kl            | 5.1104056e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0134       |
|    explained_variance   | 0.68          |
|    learning_rate        | 0.001         |
|    loss                 | 4.09e+03      |
|    n_updates            | 9880          |
|    policy_gradient_loss | -0.000359     |
|    value_loss           | 1.15e+04      |
-------------------------------------------
Eval num_timesteps=160500, episode_reward=-522.36 +/- 154.44
Episode length: 16.70 +/- 5.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 157      |
|    time_elapsed    | 374      |
|    total_timesteps | 160768   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-578.35 +/- 190.01
Episode length: 13.70 +/- 3.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 13.7         |
|    mean_reward          | -578         |
| time/                   |              |
|    total_timesteps      | 161000       |
| train/                  |              |
|    approx_kl            | 7.515773e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0122      |
|    explained_variance   | 0.586        |
|    learning_rate        | 0.001        |
|    loss                 | 6.65e+03     |
|    n_updates            | 9890         |
|    policy_gradient_loss | -0.000239    |
|    value_loss           | 1.23e+04     |
------------------------------------------
Eval num_timesteps=161500, episode_reward=-518.84 +/- 155.20
Episode length: 15.20 +/- 4.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -507     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 158      |
|    time_elapsed    | 376      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=-522.24 +/- 182.24
Episode length: 15.78 +/- 5.58
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -522          |
| time/                   |               |
|    total_timesteps      | 162000        |
| train/                  |               |
|    approx_kl            | 4.2153988e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0127       |
|    explained_variance   | 0.659         |
|    learning_rate        | 0.001         |
|    loss                 | 6.56e+03      |
|    n_updates            | 9900          |
|    policy_gradient_loss | -0.000185     |
|    value_loss           | 1.2e+04       |
-------------------------------------------
Eval num_timesteps=162500, episode_reward=-543.73 +/- 172.77
Episode length: 16.20 +/- 4.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -544     |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -488     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 159      |
|    time_elapsed    | 378      |
|    total_timesteps | 162816   |
---------------------------------
Eval num_timesteps=163000, episode_reward=-537.62 +/- 193.27
Episode length: 14.94 +/- 5.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.9         |
|    mean_reward          | -538         |
| time/                   |              |
|    total_timesteps      | 163000       |
| train/                  |              |
|    approx_kl            | 0.0003460028 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0161      |
|    explained_variance   | 0.665        |
|    learning_rate        | 0.001        |
|    loss                 | 4.14e+03     |
|    n_updates            | 9910         |
|    policy_gradient_loss | 0.000374     |
|    value_loss           | 1.14e+04     |
------------------------------------------
Eval num_timesteps=163500, episode_reward=-605.67 +/- 180.08
Episode length: 14.98 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -606     |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -549     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 160      |
|    time_elapsed    | 380      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=-482.93 +/- 196.66
Episode length: 16.90 +/- 5.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.9         |
|    mean_reward          | -483         |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0006311674 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.032       |
|    explained_variance   | 0.754        |
|    learning_rate        | 0.001        |
|    loss                 | 4.15e+03     |
|    n_updates            | 9920         |
|    policy_gradient_loss | -0.000676    |
|    value_loss           | 1.02e+04     |
------------------------------------------
Eval num_timesteps=164500, episode_reward=-518.74 +/- 154.84
Episode length: 17.70 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -574     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 161      |
|    time_elapsed    | 383      |
|    total_timesteps | 164864   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-486.72 +/- 210.15
Episode length: 17.28 +/- 5.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.3         |
|    mean_reward          | -487         |
| time/                   |              |
|    total_timesteps      | 165000       |
| train/                  |              |
|    approx_kl            | 0.0018151118 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0219      |
|    explained_variance   | 0.724        |
|    learning_rate        | 0.001        |
|    loss                 | 4.08e+03     |
|    n_updates            | 9930         |
|    policy_gradient_loss | -0.000875    |
|    value_loss           | 1.19e+04     |
------------------------------------------
Eval num_timesteps=165500, episode_reward=-486.04 +/- 213.14
Episode length: 17.06 +/- 5.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -486     |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -550     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 162      |
|    time_elapsed    | 385      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=-555.16 +/- 188.11
Episode length: 15.68 +/- 4.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.7         |
|    mean_reward          | -555         |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 0.0026027681 |
|    clip_fraction        | 0.00361      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.014       |
|    explained_variance   | 0.716        |
|    learning_rate        | 0.001        |
|    loss                 | 3.84e+03     |
|    n_updates            | 9940         |
|    policy_gradient_loss | -0.00039     |
|    value_loss           | 1.07e+04     |
------------------------------------------
Eval num_timesteps=166500, episode_reward=-519.96 +/- 192.79
Episode length: 15.78 +/- 4.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 163      |
|    time_elapsed    | 388      |
|    total_timesteps | 166912   |
---------------------------------
Eval num_timesteps=167000, episode_reward=-552.00 +/- 183.84
Episode length: 15.76 +/- 4.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -552         |
| time/                   |              |
|    total_timesteps      | 167000       |
| train/                  |              |
|    approx_kl            | 9.977911e-05 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00818     |
|    explained_variance   | 0.762        |
|    learning_rate        | 0.001        |
|    loss                 | 3.89e+03     |
|    n_updates            | 9950         |
|    policy_gradient_loss | -0.000381    |
|    value_loss           | 9.94e+03     |
------------------------------------------
Eval num_timesteps=167500, episode_reward=-535.14 +/- 178.98
Episode length: 14.44 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.4     |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -540     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 164      |
|    time_elapsed    | 390      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-539.89 +/- 194.61
Episode length: 14.84 +/- 4.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | -540          |
| time/                   |               |
|    total_timesteps      | 168000        |
| train/                  |               |
|    approx_kl            | 4.5441557e-06 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00814      |
|    explained_variance   | 0.64          |
|    learning_rate        | 0.001         |
|    loss                 | 6.68e+03      |
|    n_updates            | 9960          |
|    policy_gradient_loss | -0.000129     |
|    value_loss           | 1.27e+04      |
-------------------------------------------
Eval num_timesteps=168500, episode_reward=-507.66 +/- 200.33
Episode length: 16.34 +/- 5.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 165      |
|    time_elapsed    | 392      |
|    total_timesteps | 168960   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-515.67 +/- 155.67
Episode length: 15.18 +/- 4.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | -516          |
| time/                   |               |
|    total_timesteps      | 169000        |
| train/                  |               |
|    approx_kl            | 0.00025235902 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00853      |
|    explained_variance   | 0.716         |
|    learning_rate        | 0.001         |
|    loss                 | 5.44e+03      |
|    n_updates            | 9970          |
|    policy_gradient_loss | 0.000239      |
|    value_loss           | 1.04e+04      |
-------------------------------------------
Eval num_timesteps=169500, episode_reward=-499.26 +/- 199.34
Episode length: 15.98 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -499     |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 166      |
|    time_elapsed    | 395      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-553.15 +/- 206.51
Episode length: 16.00 +/- 4.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -553         |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 5.644397e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.011       |
|    explained_variance   | 0.634        |
|    learning_rate        | 0.001        |
|    loss                 | 6.19e+03     |
|    n_updates            | 9980         |
|    policy_gradient_loss | -0.000291    |
|    value_loss           | 1.26e+04     |
------------------------------------------
Eval num_timesteps=170500, episode_reward=-485.78 +/- 179.95
Episode length: 16.88 +/- 4.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -486     |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=-517.89 +/- 186.34
Episode length: 15.90 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -540     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 167      |
|    time_elapsed    | 398      |
|    total_timesteps | 171008   |
---------------------------------
Eval num_timesteps=171500, episode_reward=-572.27 +/- 185.37
Episode length: 15.34 +/- 4.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -572         |
| time/                   |              |
|    total_timesteps      | 171500       |
| train/                  |              |
|    approx_kl            | 0.0004538928 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.012       |
|    explained_variance   | 0.746        |
|    learning_rate        | 0.001        |
|    loss                 | 5.2e+03      |
|    n_updates            | 9990         |
|    policy_gradient_loss | -0.000156    |
|    value_loss           | 8.69e+03     |
------------------------------------------
Eval num_timesteps=172000, episode_reward=-569.47 +/- 158.64
Episode length: 14.92 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -559     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 168      |
|    time_elapsed    | 400      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=-536.16 +/- 181.81
Episode length: 15.44 +/- 4.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | -536          |
| time/                   |               |
|    total_timesteps      | 172500        |
| train/                  |               |
|    approx_kl            | 1.8078776e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0118       |
|    explained_variance   | 0.751         |
|    learning_rate        | 0.001         |
|    loss                 | 7.47e+03      |
|    n_updates            | 10000         |
|    policy_gradient_loss | -0.000113     |
|    value_loss           | 9.95e+03      |
-------------------------------------------
Eval num_timesteps=173000, episode_reward=-519.28 +/- 183.83
Episode length: 15.56 +/- 5.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -551     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 169      |
|    time_elapsed    | 402      |
|    total_timesteps | 173056   |
---------------------------------
Eval num_timesteps=173500, episode_reward=-444.84 +/- 215.40
Episode length: 15.88 +/- 5.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -445         |
| time/                   |              |
|    total_timesteps      | 173500       |
| train/                  |              |
|    approx_kl            | 0.0002122111 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00927     |
|    explained_variance   | 0.787        |
|    learning_rate        | 0.001        |
|    loss                 | 3.98e+03     |
|    n_updates            | 10010        |
|    policy_gradient_loss | -0.000306    |
|    value_loss           | 8.46e+03     |
------------------------------------------
New best mean reward!
Eval num_timesteps=174000, episode_reward=-529.44 +/- 170.58
Episode length: 15.14 +/- 4.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -537     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 170      |
|    time_elapsed    | 405      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=-521.71 +/- 184.78
Episode length: 14.82 +/- 4.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.8         |
|    mean_reward          | -522         |
| time/                   |              |
|    total_timesteps      | 174500       |
| train/                  |              |
|    approx_kl            | 9.804906e-06 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00872     |
|    explained_variance   | 0.668        |
|    learning_rate        | 0.001        |
|    loss                 | 4.98e+03     |
|    n_updates            | 10020        |
|    policy_gradient_loss | 5.03e-06     |
|    value_loss           | 1.11e+04     |
------------------------------------------
Eval num_timesteps=175000, episode_reward=-538.80 +/- 193.51
Episode length: 16.44 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -539     |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -551     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 171      |
|    time_elapsed    | 407      |
|    total_timesteps | 175104   |
---------------------------------
Eval num_timesteps=175500, episode_reward=-483.67 +/- 191.69
Episode length: 15.52 +/- 4.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -484         |
| time/                   |              |
|    total_timesteps      | 175500       |
| train/                  |              |
|    approx_kl            | 8.938485e-05 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00883     |
|    explained_variance   | 0.707        |
|    learning_rate        | 0.001        |
|    loss                 | 5.31e+03     |
|    n_updates            | 10030        |
|    policy_gradient_loss | -0.000706    |
|    value_loss           | 1.15e+04     |
------------------------------------------
Eval num_timesteps=176000, episode_reward=-496.17 +/- 223.44
Episode length: 16.80 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -496     |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 172      |
|    time_elapsed    | 409      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=-541.07 +/- 144.13
Episode length: 16.04 +/- 4.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -541          |
| time/                   |               |
|    total_timesteps      | 176500        |
| train/                  |               |
|    approx_kl            | 4.2402768e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00997      |
|    explained_variance   | 0.726         |
|    learning_rate        | 0.001         |
|    loss                 | 4.8e+03       |
|    n_updates            | 10040         |
|    policy_gradient_loss | -0.000217     |
|    value_loss           | 1.03e+04      |
-------------------------------------------
Eval num_timesteps=177000, episode_reward=-507.90 +/- 185.21
Episode length: 15.48 +/- 4.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -527     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 173      |
|    time_elapsed    | 412      |
|    total_timesteps | 177152   |
---------------------------------
Eval num_timesteps=177500, episode_reward=-521.12 +/- 163.62
Episode length: 15.66 +/- 4.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.7          |
|    mean_reward          | -521          |
| time/                   |               |
|    total_timesteps      | 177500        |
| train/                  |               |
|    approx_kl            | 4.5372872e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0118       |
|    explained_variance   | 0.686         |
|    learning_rate        | 0.001         |
|    loss                 | 2.68e+03      |
|    n_updates            | 10050         |
|    policy_gradient_loss | -0.000209     |
|    value_loss           | 1.21e+04      |
-------------------------------------------
Eval num_timesteps=178000, episode_reward=-479.90 +/- 175.33
Episode length: 16.26 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -480     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 174      |
|    time_elapsed    | 414      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=-512.23 +/- 166.45
Episode length: 16.38 +/- 4.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -512         |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0007732981 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0163      |
|    explained_variance   | 0.707        |
|    learning_rate        | 0.001        |
|    loss                 | 5.71e+03     |
|    n_updates            | 10060        |
|    policy_gradient_loss | -0.000616    |
|    value_loss           | 1.05e+04     |
------------------------------------------
Eval num_timesteps=179000, episode_reward=-529.97 +/- 173.47
Episode length: 16.26 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -486     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 175      |
|    time_elapsed    | 416      |
|    total_timesteps | 179200   |
---------------------------------
Eval num_timesteps=179500, episode_reward=-565.46 +/- 162.58
Episode length: 15.20 +/- 3.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | -565          |
| time/                   |               |
|    total_timesteps      | 179500        |
| train/                  |               |
|    approx_kl            | 3.4687226e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0174       |
|    explained_variance   | 0.583         |
|    learning_rate        | 0.001         |
|    loss                 | 7.29e+03      |
|    n_updates            | 10070         |
|    policy_gradient_loss | -0.000363     |
|    value_loss           | 1.23e+04      |
-------------------------------------------
Eval num_timesteps=180000, episode_reward=-494.35 +/- 174.19
Episode length: 15.46 +/- 4.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -494     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -504     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 176      |
|    time_elapsed    | 419      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=-519.25 +/- 151.56
Episode length: 16.66 +/- 5.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -519         |
| time/                   |              |
|    total_timesteps      | 180500       |
| train/                  |              |
|    approx_kl            | 0.0018279986 |
|    clip_fraction        | 0.00342      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.018       |
|    explained_variance   | 0.701        |
|    learning_rate        | 0.001        |
|    loss                 | 5.77e+03     |
|    n_updates            | 10080        |
|    policy_gradient_loss | 0.0017       |
|    value_loss           | 1.12e+04     |
------------------------------------------
Eval num_timesteps=181000, episode_reward=-508.00 +/- 228.98
Episode length: 16.94 +/- 5.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -545     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 177      |
|    time_elapsed    | 421      |
|    total_timesteps | 181248   |
---------------------------------
Eval num_timesteps=181500, episode_reward=-549.23 +/- 181.21
Episode length: 15.28 +/- 4.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.3          |
|    mean_reward          | -549          |
| time/                   |               |
|    total_timesteps      | 181500        |
| train/                  |               |
|    approx_kl            | 5.7457713e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0137       |
|    explained_variance   | 0.745         |
|    learning_rate        | 0.001         |
|    loss                 | 5.89e+03      |
|    n_updates            | 10090         |
|    policy_gradient_loss | -0.000313     |
|    value_loss           | 1.03e+04      |
-------------------------------------------
Eval num_timesteps=182000, episode_reward=-579.85 +/- 171.60
Episode length: 14.88 +/- 4.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -580     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 178      |
|    time_elapsed    | 423      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=-503.82 +/- 186.49
Episode length: 17.50 +/- 5.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.5          |
|    mean_reward          | -504          |
| time/                   |               |
|    total_timesteps      | 182500        |
| train/                  |               |
|    approx_kl            | 0.00019522267 |
|    clip_fraction        | 0.00146       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0147       |
|    explained_variance   | 0.615         |
|    learning_rate        | 0.001         |
|    loss                 | 8.03e+03      |
|    n_updates            | 10100         |
|    policy_gradient_loss | -0.000231     |
|    value_loss           | 1.54e+04      |
-------------------------------------------
Eval num_timesteps=183000, episode_reward=-508.09 +/- 174.35
Episode length: 15.78 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -508     |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 179      |
|    time_elapsed    | 426      |
|    total_timesteps | 183296   |
---------------------------------
Eval num_timesteps=183500, episode_reward=-520.27 +/- 208.13
Episode length: 15.20 +/- 4.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.2         |
|    mean_reward          | -520         |
| time/                   |              |
|    total_timesteps      | 183500       |
| train/                  |              |
|    approx_kl            | 9.132782e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0158      |
|    explained_variance   | 0.707        |
|    learning_rate        | 0.001        |
|    loss                 | 5.09e+03     |
|    n_updates            | 10110        |
|    policy_gradient_loss | -0.000338    |
|    value_loss           | 1.09e+04     |
------------------------------------------
Eval num_timesteps=184000, episode_reward=-537.18 +/- 170.45
Episode length: 16.32 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -546     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 180      |
|    time_elapsed    | 428      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=-510.62 +/- 166.50
Episode length: 15.44 +/- 3.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | -511          |
| time/                   |               |
|    total_timesteps      | 184500        |
| train/                  |               |
|    approx_kl            | 1.7966377e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0189       |
|    explained_variance   | 0.73          |
|    learning_rate        | 0.001         |
|    loss                 | 4.81e+03      |
|    n_updates            | 10120         |
|    policy_gradient_loss | -0.000155     |
|    value_loss           | 9.44e+03      |
-------------------------------------------
Eval num_timesteps=185000, episode_reward=-514.43 +/- 236.98
Episode length: 15.98 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 181      |
|    time_elapsed    | 431      |
|    total_timesteps | 185344   |
---------------------------------
Eval num_timesteps=185500, episode_reward=-538.15 +/- 187.89
Episode length: 15.44 +/- 4.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -538         |
| time/                   |              |
|    total_timesteps      | 185500       |
| train/                  |              |
|    approx_kl            | 0.0004953278 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0286      |
|    explained_variance   | 0.686        |
|    learning_rate        | 0.001        |
|    loss                 | 4.83e+03     |
|    n_updates            | 10130        |
|    policy_gradient_loss | -0.000534    |
|    value_loss           | 1.06e+04     |
------------------------------------------
Eval num_timesteps=186000, episode_reward=-552.23 +/- 169.71
Episode length: 15.30 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -552     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 182      |
|    time_elapsed    | 433      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=-507.90 +/- 173.73
Episode length: 17.04 +/- 5.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | -508          |
| time/                   |               |
|    total_timesteps      | 186500        |
| train/                  |               |
|    approx_kl            | 0.00056601677 |
|    clip_fraction        | 0.00293       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0277       |
|    explained_variance   | 0.688         |
|    learning_rate        | 0.001         |
|    loss                 | 3e+03         |
|    n_updates            | 10140         |
|    policy_gradient_loss | -0.000294     |
|    value_loss           | 1.24e+04      |
-------------------------------------------
Eval num_timesteps=187000, episode_reward=-585.53 +/- 164.28
Episode length: 15.04 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -586     |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 183      |
|    time_elapsed    | 435      |
|    total_timesteps | 187392   |
---------------------------------
Eval num_timesteps=187500, episode_reward=-552.61 +/- 142.02
Episode length: 16.40 +/- 4.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -553         |
| time/                   |              |
|    total_timesteps      | 187500       |
| train/                  |              |
|    approx_kl            | 0.0010706098 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.025       |
|    explained_variance   | 0.772        |
|    learning_rate        | 0.001        |
|    loss                 | 4.98e+03     |
|    n_updates            | 10150        |
|    policy_gradient_loss | 0.000886     |
|    value_loss           | 9.01e+03     |
------------------------------------------
Eval num_timesteps=188000, episode_reward=-545.06 +/- 187.26
Episode length: 16.62 +/- 5.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -545     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -553     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 184      |
|    time_elapsed    | 438      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=-523.30 +/- 179.12
Episode length: 15.92 +/- 5.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -523          |
| time/                   |               |
|    total_timesteps      | 188500        |
| train/                  |               |
|    approx_kl            | 0.00028462202 |
|    clip_fraction        | 0.00264       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0482       |
|    explained_variance   | 0.682         |
|    learning_rate        | 0.001         |
|    loss                 | 5.98e+03      |
|    n_updates            | 10160         |
|    policy_gradient_loss | -0.00101      |
|    value_loss           | 1.29e+04      |
-------------------------------------------
Eval num_timesteps=189000, episode_reward=-532.40 +/- 193.76
Episode length: 15.24 +/- 3.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -508     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 185      |
|    time_elapsed    | 440      |
|    total_timesteps | 189440   |
---------------------------------
Eval num_timesteps=189500, episode_reward=-507.14 +/- 177.71
Episode length: 16.14 +/- 5.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -507         |
| time/                   |              |
|    total_timesteps      | 189500       |
| train/                  |              |
|    approx_kl            | 0.0005302249 |
|    clip_fraction        | 0.0043       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0501      |
|    explained_variance   | 0.66         |
|    learning_rate        | 0.001        |
|    loss                 | 3.59e+03     |
|    n_updates            | 10170        |
|    policy_gradient_loss | 2.19e-05     |
|    value_loss           | 1.28e+04     |
------------------------------------------
Eval num_timesteps=190000, episode_reward=-520.11 +/- 190.55
Episode length: 15.42 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -509     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 186      |
|    time_elapsed    | 442      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=-605.23 +/- 170.50
Episode length: 15.86 +/- 4.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -605         |
| time/                   |              |
|    total_timesteps      | 190500       |
| train/                  |              |
|    approx_kl            | 0.0028142822 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0384      |
|    explained_variance   | 0.767        |
|    learning_rate        | 0.001        |
|    loss                 | 2.34e+03     |
|    n_updates            | 10180        |
|    policy_gradient_loss | -0.000232    |
|    value_loss           | 8.62e+03     |
------------------------------------------
Eval num_timesteps=191000, episode_reward=-550.80 +/- 198.11
Episode length: 15.46 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -551     |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 187      |
|    time_elapsed    | 445      |
|    total_timesteps | 191488   |
---------------------------------
Eval num_timesteps=191500, episode_reward=-518.21 +/- 154.35
Episode length: 15.94 +/- 4.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -518          |
| time/                   |               |
|    total_timesteps      | 191500        |
| train/                  |               |
|    approx_kl            | 0.00081592076 |
|    clip_fraction        | 0.00391       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0324       |
|    explained_variance   | 0.656         |
|    learning_rate        | 0.001         |
|    loss                 | 4.18e+03      |
|    n_updates            | 10190         |
|    policy_gradient_loss | -0.00118      |
|    value_loss           | 1.18e+04      |
-------------------------------------------
Eval num_timesteps=192000, episode_reward=-506.48 +/- 220.52
Episode length: 15.66 +/- 5.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -506     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=-473.23 +/- 176.91
Episode length: 17.28 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -473     |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 188      |
|    time_elapsed    | 448      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=-551.95 +/- 159.57
Episode length: 14.66 +/- 4.25
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.7         |
|    mean_reward          | -552         |
| time/                   |              |
|    total_timesteps      | 193000       |
| train/                  |              |
|    approx_kl            | 0.0006944583 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0266      |
|    explained_variance   | 0.614        |
|    learning_rate        | 0.001        |
|    loss                 | 5.75e+03     |
|    n_updates            | 10200        |
|    policy_gradient_loss | -0.000312    |
|    value_loss           | 1.51e+04     |
------------------------------------------
Eval num_timesteps=193500, episode_reward=-505.89 +/- 165.47
Episode length: 15.98 +/- 4.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -506     |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -535     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 189      |
|    time_elapsed    | 450      |
|    total_timesteps | 193536   |
---------------------------------
Eval num_timesteps=194000, episode_reward=-537.76 +/- 193.30
Episode length: 17.16 +/- 5.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.2          |
|    mean_reward          | -538          |
| time/                   |               |
|    total_timesteps      | 194000        |
| train/                  |               |
|    approx_kl            | 0.00036075275 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0188       |
|    explained_variance   | 0.653         |
|    learning_rate        | 0.001         |
|    loss                 | 6.46e+03      |
|    n_updates            | 10210         |
|    policy_gradient_loss | -0.000208     |
|    value_loss           | 1.39e+04      |
-------------------------------------------
Eval num_timesteps=194500, episode_reward=-513.52 +/- 197.39
Episode length: 16.82 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -555     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 190      |
|    time_elapsed    | 452      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=-525.23 +/- 194.26
Episode length: 15.56 +/- 5.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | -525          |
| time/                   |               |
|    total_timesteps      | 195000        |
| train/                  |               |
|    approx_kl            | 0.00011395855 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0146       |
|    explained_variance   | 0.705         |
|    learning_rate        | 0.001         |
|    loss                 | 4.41e+03      |
|    n_updates            | 10220         |
|    policy_gradient_loss | -0.000109     |
|    value_loss           | 1.28e+04      |
-------------------------------------------
Eval num_timesteps=195500, episode_reward=-523.44 +/- 165.71
Episode length: 16.38 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -548     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 191      |
|    time_elapsed    | 455      |
|    total_timesteps | 195584   |
---------------------------------
Eval num_timesteps=196000, episode_reward=-515.85 +/- 171.34
Episode length: 15.70 +/- 4.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.7          |
|    mean_reward          | -516          |
| time/                   |               |
|    total_timesteps      | 196000        |
| train/                  |               |
|    approx_kl            | 2.5245827e-06 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0164       |
|    explained_variance   | 0.605         |
|    learning_rate        | 0.001         |
|    loss                 | 5.35e+03      |
|    n_updates            | 10230         |
|    policy_gradient_loss | -0.000414     |
|    value_loss           | 1.39e+04      |
-------------------------------------------
Eval num_timesteps=196500, episode_reward=-513.88 +/- 181.88
Episode length: 15.14 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | -529     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 192      |
|    time_elapsed    | 457      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=-503.31 +/- 144.98
Episode length: 16.90 +/- 5.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.9          |
|    mean_reward          | -503          |
| time/                   |               |
|    total_timesteps      | 197000        |
| train/                  |               |
|    approx_kl            | 2.3069442e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0179       |
|    explained_variance   | 0.683         |
|    learning_rate        | 0.001         |
|    loss                 | 4.67e+03      |
|    n_updates            | 10240         |
|    policy_gradient_loss | -0.000333     |
|    value_loss           | 1.22e+04      |
-------------------------------------------
Eval num_timesteps=197500, episode_reward=-537.82 +/- 162.84
Episode length: 15.30 +/- 4.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -537     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 193      |
|    time_elapsed    | 459      |
|    total_timesteps | 197632   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-499.08 +/- 173.84
Episode length: 16.36 +/- 4.83
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -499         |
| time/                   |              |
|    total_timesteps      | 198000       |
| train/                  |              |
|    approx_kl            | 0.0012225581 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0172      |
|    explained_variance   | 0.711        |
|    learning_rate        | 0.001        |
|    loss                 | 6.44e+03     |
|    n_updates            | 10250        |
|    policy_gradient_loss | -0.000256    |
|    value_loss           | 9.02e+03     |
------------------------------------------
Eval num_timesteps=198500, episode_reward=-496.23 +/- 163.18
Episode length: 15.62 +/- 4.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -496     |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 194      |
|    time_elapsed    | 462      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=-514.40 +/- 209.83
Episode length: 15.88 +/- 4.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -514          |
| time/                   |               |
|    total_timesteps      | 199000        |
| train/                  |               |
|    approx_kl            | 2.9231887e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0101       |
|    explained_variance   | 0.598         |
|    learning_rate        | 0.001         |
|    loss                 | 5.3e+03       |
|    n_updates            | 10260         |
|    policy_gradient_loss | -0.000135     |
|    value_loss           | 1.4e+04       |
-------------------------------------------
Eval num_timesteps=199500, episode_reward=-549.69 +/- 210.30
Episode length: 16.26 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -550     |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 195      |
|    time_elapsed    | 464      |
|    total_timesteps | 199680   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-473.05 +/- 175.53
Episode length: 17.00 +/- 4.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | -473          |
| time/                   |               |
|    total_timesteps      | 200000        |
| train/                  |               |
|    approx_kl            | 1.9278377e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00944      |
|    explained_variance   | 0.666         |
|    learning_rate        | 0.001         |
|    loss                 | 5.32e+03      |
|    n_updates            | 10270         |
|    policy_gradient_loss | -0.000143     |
|    value_loss           | 1.29e+04      |
-------------------------------------------
Eval num_timesteps=200500, episode_reward=-558.75 +/- 162.40
Episode length: 15.76 +/- 3.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -559     |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -545     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 196      |
|    time_elapsed    | 466      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=-565.12 +/- 175.47
Episode length: 16.00 +/- 4.06
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -565          |
| time/                   |               |
|    total_timesteps      | 201000        |
| train/                  |               |
|    approx_kl            | 0.00017210608 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00898      |
|    explained_variance   | 0.716         |
|    learning_rate        | 0.001         |
|    loss                 | 6.23e+03      |
|    n_updates            | 10280         |
|    policy_gradient_loss | -1.36e-05     |
|    value_loss           | 1.15e+04      |
-------------------------------------------
Eval num_timesteps=201500, episode_reward=-516.47 +/- 182.53
Episode length: 15.70 +/- 4.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -552     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 197      |
|    time_elapsed    | 469      |
|    total_timesteps | 201728   |
---------------------------------
Eval num_timesteps=202000, episode_reward=-553.89 +/- 149.55
Episode length: 16.18 +/- 4.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -554         |
| time/                   |              |
|    total_timesteps      | 202000       |
| train/                  |              |
|    approx_kl            | 4.310836e-05 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00881     |
|    explained_variance   | 0.764        |
|    learning_rate        | 0.001        |
|    loss                 | 6.4e+03      |
|    n_updates            | 10290        |
|    policy_gradient_loss | -0.000159    |
|    value_loss           | 1.01e+04     |
------------------------------------------
Eval num_timesteps=202500, episode_reward=-562.73 +/- 181.78
Episode length: 15.74 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -563     |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -563     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 198      |
|    time_elapsed    | 471      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=-477.12 +/- 190.38
Episode length: 16.34 +/- 5.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.3         |
|    mean_reward          | -477         |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 3.458216e-05 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00873     |
|    explained_variance   | 0.699        |
|    learning_rate        | 0.001        |
|    loss                 | 3.97e+03     |
|    n_updates            | 10300        |
|    policy_gradient_loss | -0.000271    |
|    value_loss           | 1.22e+04     |
------------------------------------------
Eval num_timesteps=203500, episode_reward=-564.79 +/- 165.22
Episode length: 16.50 +/- 4.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -565     |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -540     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 199      |
|    time_elapsed    | 474      |
|    total_timesteps | 203776   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-557.80 +/- 160.11
Episode length: 16.08 +/- 4.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -558         |
| time/                   |              |
|    total_timesteps      | 204000       |
| train/                  |              |
|    approx_kl            | 4.209578e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | 0.699        |
|    learning_rate        | 0.001        |
|    loss                 | 4.14e+03     |
|    n_updates            | 10310        |
|    policy_gradient_loss | -0.000126    |
|    value_loss           | 1.03e+04     |
------------------------------------------
Eval num_timesteps=204500, episode_reward=-583.46 +/- 152.20
Episode length: 15.28 +/- 4.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -583     |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 200      |
|    time_elapsed    | 476      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=-477.18 +/- 184.94
Episode length: 15.48 +/- 4.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -477         |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 7.819297e-05 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00869     |
|    explained_variance   | 0.64         |
|    learning_rate        | 0.001        |
|    loss                 | 7.02e+03     |
|    n_updates            | 10320        |
|    policy_gradient_loss | -0.000216    |
|    value_loss           | 1.32e+04     |
------------------------------------------
Eval num_timesteps=205500, episode_reward=-521.65 +/- 162.28
Episode length: 15.72 +/- 5.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 201      |
|    time_elapsed    | 478      |
|    total_timesteps | 205824   |
---------------------------------
Eval num_timesteps=206000, episode_reward=-543.04 +/- 204.79
Episode length: 15.74 +/- 4.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.7          |
|    mean_reward          | -543          |
| time/                   |               |
|    total_timesteps      | 206000        |
| train/                  |               |
|    approx_kl            | 5.1712734e-05 |
|    clip_fraction        | 0.00127       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00795      |
|    explained_variance   | 0.58          |
|    learning_rate        | 0.001         |
|    loss                 | 5.85e+03      |
|    n_updates            | 10330         |
|    policy_gradient_loss | -0.000318     |
|    value_loss           | 1.49e+04      |
-------------------------------------------
Eval num_timesteps=206500, episode_reward=-515.72 +/- 189.11
Episode length: 16.52 +/- 5.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 202      |
|    time_elapsed    | 481      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=-517.52 +/- 139.63
Episode length: 16.66 +/- 4.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.7          |
|    mean_reward          | -518          |
| time/                   |               |
|    total_timesteps      | 207000        |
| train/                  |               |
|    approx_kl            | 0.00029316393 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00614      |
|    explained_variance   | 0.726         |
|    learning_rate        | 0.001         |
|    loss                 | 3.37e+03      |
|    n_updates            | 10340         |
|    policy_gradient_loss | -0.000235     |
|    value_loss           | 1.02e+04      |
-------------------------------------------
Eval num_timesteps=207500, episode_reward=-548.55 +/- 152.25
Episode length: 15.12 +/- 4.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -549     |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -537     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 203      |
|    time_elapsed    | 483      |
|    total_timesteps | 207872   |
---------------------------------
Eval num_timesteps=208000, episode_reward=-545.44 +/- 162.06
Episode length: 16.00 +/- 5.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -545         |
| time/                   |              |
|    total_timesteps      | 208000       |
| train/                  |              |
|    approx_kl            | 0.0006421047 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00533     |
|    explained_variance   | 0.708        |
|    learning_rate        | 0.001        |
|    loss                 | 4.44e+03     |
|    n_updates            | 10350        |
|    policy_gradient_loss | -0.000217    |
|    value_loss           | 1.19e+04     |
------------------------------------------
Eval num_timesteps=208500, episode_reward=-551.05 +/- 169.08
Episode length: 15.60 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -551     |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 204      |
|    time_elapsed    | 485      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=-535.83 +/- 166.52
Episode length: 16.42 +/- 4.93
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.4          |
|    mean_reward          | -536          |
| time/                   |               |
|    total_timesteps      | 209000        |
| train/                  |               |
|    approx_kl            | 2.7403818e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00608      |
|    explained_variance   | 0.634         |
|    learning_rate        | 0.001         |
|    loss                 | 5.59e+03      |
|    n_updates            | 10360         |
|    policy_gradient_loss | -0.000366     |
|    value_loss           | 1.29e+04      |
-------------------------------------------
Eval num_timesteps=209500, episode_reward=-472.13 +/- 179.60
Episode length: 16.30 +/- 4.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -472     |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 205      |
|    time_elapsed    | 488      |
|    total_timesteps | 209920   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-546.79 +/- 206.60
Episode length: 16.30 +/- 5.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.3          |
|    mean_reward          | -547          |
| time/                   |               |
|    total_timesteps      | 210000        |
| train/                  |               |
|    approx_kl            | 0.00015849195 |
|    clip_fraction        | 0.00234       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00507      |
|    explained_variance   | 0.646         |
|    learning_rate        | 0.001         |
|    loss                 | 4.29e+03      |
|    n_updates            | 10370         |
|    policy_gradient_loss | -0.000864     |
|    value_loss           | 1.36e+04      |
-------------------------------------------
Eval num_timesteps=210500, episode_reward=-583.87 +/- 178.95
Episode length: 15.36 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -584     |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -535     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 206      |
|    time_elapsed    | 490      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=-550.68 +/- 207.84
Episode length: 15.06 +/- 4.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.1         |
|    mean_reward          | -551         |
| time/                   |              |
|    total_timesteps      | 211000       |
| train/                  |              |
|    approx_kl            | 6.426126e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0048      |
|    explained_variance   | 0.653        |
|    learning_rate        | 0.001        |
|    loss                 | 5.37e+03     |
|    n_updates            | 10380        |
|    policy_gradient_loss | -7.04e-05    |
|    value_loss           | 1.24e+04     |
------------------------------------------
Eval num_timesteps=211500, episode_reward=-505.18 +/- 170.78
Episode length: 17.04 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -505     |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -557     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 207      |
|    time_elapsed    | 492      |
|    total_timesteps | 211968   |
---------------------------------
Eval num_timesteps=212000, episode_reward=-487.76 +/- 149.73
Episode length: 15.92 +/- 4.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -488          |
| time/                   |               |
|    total_timesteps      | 212000        |
| train/                  |               |
|    approx_kl            | 5.2154064e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00447      |
|    explained_variance   | 0.712         |
|    learning_rate        | 0.001         |
|    loss                 | 4.55e+03      |
|    n_updates            | 10390         |
|    policy_gradient_loss | -6.77e-05     |
|    value_loss           | 1.04e+04      |
-------------------------------------------
Eval num_timesteps=212500, episode_reward=-499.53 +/- 186.65
Episode length: 17.44 +/- 5.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.4     |
|    mean_reward     | -500     |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 208      |
|    time_elapsed    | 495      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=-561.74 +/- 178.92
Episode length: 16.26 +/- 4.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | -562        |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 1.95147e-06 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00501    |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.001       |
|    loss                 | 4.97e+03    |
|    n_updates            | 10400       |
|    policy_gradient_loss | -4.13e-05   |
|    value_loss           | 1.03e+04    |
-----------------------------------------
Eval num_timesteps=213500, episode_reward=-543.47 +/- 185.46
Episode length: 15.54 +/- 4.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=-573.94 +/- 181.92
Episode length: 15.28 +/- 4.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -546     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 209      |
|    time_elapsed    | 498      |
|    total_timesteps | 214016   |
---------------------------------
Eval num_timesteps=214500, episode_reward=-496.64 +/- 181.49
Episode length: 16.14 +/- 4.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.1          |
|    mean_reward          | -497          |
| time/                   |               |
|    total_timesteps      | 214500        |
| train/                  |               |
|    approx_kl            | 4.8370566e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00551      |
|    explained_variance   | 0.772         |
|    learning_rate        | 0.001         |
|    loss                 | 4e+03         |
|    n_updates            | 10410         |
|    policy_gradient_loss | -4.41e-05     |
|    value_loss           | 9.77e+03      |
-------------------------------------------
Eval num_timesteps=215000, episode_reward=-568.67 +/- 174.73
Episode length: 15.08 +/- 4.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -536     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 210      |
|    time_elapsed    | 500      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=-469.05 +/- 177.32
Episode length: 16.10 +/- 4.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.1         |
|    mean_reward          | -469         |
| time/                   |              |
|    total_timesteps      | 215500       |
| train/                  |              |
|    approx_kl            | 9.015738e-05 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00572     |
|    explained_variance   | 0.729        |
|    learning_rate        | 0.001        |
|    loss                 | 5.48e+03     |
|    n_updates            | 10420        |
|    policy_gradient_loss | -0.000555    |
|    value_loss           | 9.98e+03     |
------------------------------------------
Eval num_timesteps=216000, episode_reward=-549.21 +/- 192.04
Episode length: 15.46 +/- 4.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -549     |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -529     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 211      |
|    time_elapsed    | 503      |
|    total_timesteps | 216064   |
---------------------------------
Eval num_timesteps=216500, episode_reward=-506.09 +/- 178.23
Episode length: 16.32 +/- 5.40
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.3          |
|    mean_reward          | -506          |
| time/                   |               |
|    total_timesteps      | 216500        |
| train/                  |               |
|    approx_kl            | 7.7241566e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00639      |
|    explained_variance   | 0.722         |
|    learning_rate        | 0.001         |
|    loss                 | 2.45e+03      |
|    n_updates            | 10430         |
|    policy_gradient_loss | -6.45e-05     |
|    value_loss           | 1.04e+04      |
-------------------------------------------
Eval num_timesteps=217000, episode_reward=-511.32 +/- 211.22
Episode length: 15.86 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 212      |
|    time_elapsed    | 505      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=-530.56 +/- 173.91
Episode length: 15.96 +/- 4.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -531          |
| time/                   |               |
|    total_timesteps      | 217500        |
| train/                  |               |
|    approx_kl            | 9.6159056e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00692      |
|    explained_variance   | 0.735         |
|    learning_rate        | 0.001         |
|    loss                 | 6.4e+03       |
|    n_updates            | 10440         |
|    policy_gradient_loss | -0.00011      |
|    value_loss           | 1.04e+04      |
-------------------------------------------
Eval num_timesteps=218000, episode_reward=-515.74 +/- 190.07
Episode length: 13.78 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 13.8     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -537     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 213      |
|    time_elapsed    | 507      |
|    total_timesteps | 218112   |
---------------------------------
Eval num_timesteps=218500, episode_reward=-512.84 +/- 208.13
Episode length: 15.56 +/- 4.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | -513          |
| time/                   |               |
|    total_timesteps      | 218500        |
| train/                  |               |
|    approx_kl            | 3.5892997e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00806      |
|    explained_variance   | 0.699         |
|    learning_rate        | 0.001         |
|    loss                 | 4.49e+03      |
|    n_updates            | 10450         |
|    policy_gradient_loss | -5.47e-06     |
|    value_loss           | 1.03e+04      |
-------------------------------------------
Eval num_timesteps=219000, episode_reward=-515.62 +/- 171.71
Episode length: 16.56 +/- 4.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 214      |
|    time_elapsed    | 510      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=-539.24 +/- 202.80
Episode length: 15.10 +/- 4.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.1          |
|    mean_reward          | -539          |
| time/                   |               |
|    total_timesteps      | 219500        |
| train/                  |               |
|    approx_kl            | 0.00062306446 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.01         |
|    explained_variance   | 0.724         |
|    learning_rate        | 0.001         |
|    loss                 | 5.29e+03      |
|    n_updates            | 10460         |
|    policy_gradient_loss | -0.00029      |
|    value_loss           | 1.03e+04      |
-------------------------------------------
Eval num_timesteps=220000, episode_reward=-536.57 +/- 161.91
Episode length: 16.60 +/- 5.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 215      |
|    time_elapsed    | 512      |
|    total_timesteps | 220160   |
---------------------------------
Eval num_timesteps=220500, episode_reward=-516.27 +/- 171.00
Episode length: 15.98 +/- 4.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -516         |
| time/                   |              |
|    total_timesteps      | 220500       |
| train/                  |              |
|    approx_kl            | 3.025052e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0109      |
|    explained_variance   | 0.749        |
|    learning_rate        | 0.001        |
|    loss                 | 3.59e+03     |
|    n_updates            | 10470        |
|    policy_gradient_loss | -0.000191    |
|    value_loss           | 8.99e+03     |
------------------------------------------
Eval num_timesteps=221000, episode_reward=-545.41 +/- 200.91
Episode length: 16.56 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -545     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -535     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 216      |
|    time_elapsed    | 514      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=-510.68 +/- 163.60
Episode length: 15.96 +/- 5.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -511          |
| time/                   |               |
|    total_timesteps      | 221500        |
| train/                  |               |
|    approx_kl            | 0.00019590213 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0098       |
|    explained_variance   | 0.724         |
|    learning_rate        | 0.001         |
|    loss                 | 5.39e+03      |
|    n_updates            | 10480         |
|    policy_gradient_loss | -0.000282     |
|    value_loss           | 1.1e+04       |
-------------------------------------------
Eval num_timesteps=222000, episode_reward=-542.87 +/- 183.06
Episode length: 15.52 +/- 5.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -544     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 217      |
|    time_elapsed    | 517      |
|    total_timesteps | 222208   |
---------------------------------
Eval num_timesteps=222500, episode_reward=-558.12 +/- 197.17
Episode length: 14.96 +/- 4.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | -558          |
| time/                   |               |
|    total_timesteps      | 222500        |
| train/                  |               |
|    approx_kl            | 5.6450954e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00568      |
|    explained_variance   | 0.706         |
|    learning_rate        | 0.001         |
|    loss                 | 7.23e+03      |
|    n_updates            | 10490         |
|    policy_gradient_loss | -0.000421     |
|    value_loss           | 1.28e+04      |
-------------------------------------------
Eval num_timesteps=223000, episode_reward=-574.32 +/- 185.27
Episode length: 15.16 +/- 4.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.7     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 218      |
|    time_elapsed    | 519      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=-571.31 +/- 144.00
Episode length: 16.22 +/- 4.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.2         |
|    mean_reward          | -571         |
| time/                   |              |
|    total_timesteps      | 223500       |
| train/                  |              |
|    approx_kl            | 8.865027e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00568     |
|    explained_variance   | 0.661        |
|    learning_rate        | 0.001        |
|    loss                 | 3.53e+03     |
|    n_updates            | 10500        |
|    policy_gradient_loss | -9e-05       |
|    value_loss           | 1.25e+04     |
------------------------------------------
Eval num_timesteps=224000, episode_reward=-531.40 +/- 157.18
Episode length: 16.74 +/- 4.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -541     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 219      |
|    time_elapsed    | 521      |
|    total_timesteps | 224256   |
---------------------------------
Eval num_timesteps=224500, episode_reward=-536.51 +/- 172.55
Episode length: 15.40 +/- 4.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.4         |
|    mean_reward          | -537         |
| time/                   |              |
|    total_timesteps      | 224500       |
| train/                  |              |
|    approx_kl            | 3.519206e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00591     |
|    explained_variance   | 0.653        |
|    learning_rate        | 0.001        |
|    loss                 | 7.31e+03     |
|    n_updates            | 10510        |
|    policy_gradient_loss | -0.000248    |
|    value_loss           | 1.18e+04     |
------------------------------------------
Eval num_timesteps=225000, episode_reward=-520.04 +/- 179.24
Episode length: 15.52 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -533     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 220      |
|    time_elapsed    | 524      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=-491.95 +/- 192.04
Episode length: 17.08 +/- 5.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.1          |
|    mean_reward          | -492          |
| time/                   |               |
|    total_timesteps      | 225500        |
| train/                  |               |
|    approx_kl            | 3.4028024e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00513      |
|    explained_variance   | 0.738         |
|    learning_rate        | 0.001         |
|    loss                 | 5.34e+03      |
|    n_updates            | 10520         |
|    policy_gradient_loss | -0.000435     |
|    value_loss           | 9.97e+03      |
-------------------------------------------
Eval num_timesteps=226000, episode_reward=-529.64 +/- 151.99
Episode length: 16.82 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 221      |
|    time_elapsed    | 526      |
|    total_timesteps | 226304   |
---------------------------------
Eval num_timesteps=226500, episode_reward=-527.70 +/- 188.57
Episode length: 16.40 +/- 5.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.4         |
|    mean_reward          | -528         |
| time/                   |              |
|    total_timesteps      | 226500       |
| train/                  |              |
|    approx_kl            | 5.457847e-05 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00451     |
|    explained_variance   | 0.739        |
|    learning_rate        | 0.001        |
|    loss                 | 6.05e+03     |
|    n_updates            | 10530        |
|    policy_gradient_loss | -0.000119    |
|    value_loss           | 1.07e+04     |
------------------------------------------
Eval num_timesteps=227000, episode_reward=-523.43 +/- 183.67
Episode length: 15.42 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 222      |
|    time_elapsed    | 528      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=-544.05 +/- 172.61
Episode length: 16.62 +/- 4.71
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | -544          |
| time/                   |               |
|    total_timesteps      | 227500        |
| train/                  |               |
|    approx_kl            | 2.8092356e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00519      |
|    explained_variance   | 0.733         |
|    learning_rate        | 0.001         |
|    loss                 | 4.99e+03      |
|    n_updates            | 10540         |
|    policy_gradient_loss | -5.67e-05     |
|    value_loss           | 1.13e+04      |
-------------------------------------------
Eval num_timesteps=228000, episode_reward=-475.81 +/- 182.57
Episode length: 15.78 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -476     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 223      |
|    time_elapsed    | 531      |
|    total_timesteps | 228352   |
---------------------------------
Eval num_timesteps=228500, episode_reward=-487.61 +/- 184.13
Episode length: 16.78 +/- 4.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.8         |
|    mean_reward          | -488         |
| time/                   |              |
|    total_timesteps      | 228500       |
| train/                  |              |
|    approx_kl            | 8.703669e-05 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00421     |
|    explained_variance   | 0.682        |
|    learning_rate        | 0.001        |
|    loss                 | 6.14e+03     |
|    n_updates            | 10550        |
|    policy_gradient_loss | -0.000306    |
|    value_loss           | 1.32e+04     |
------------------------------------------
Eval num_timesteps=229000, episode_reward=-541.22 +/- 161.25
Episode length: 16.72 +/- 4.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 224      |
|    time_elapsed    | 533      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=-504.17 +/- 205.76
Episode length: 16.66 +/- 5.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.7          |
|    mean_reward          | -504          |
| time/                   |               |
|    total_timesteps      | 229500        |
| train/                  |               |
|    approx_kl            | 3.6030542e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00411      |
|    explained_variance   | 0.76          |
|    learning_rate        | 0.001         |
|    loss                 | 4.84e+03      |
|    n_updates            | 10560         |
|    policy_gradient_loss | -4.18e-05     |
|    value_loss           | 9.7e+03       |
-------------------------------------------
Eval num_timesteps=230000, episode_reward=-508.93 +/- 152.32
Episode length: 15.68 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 225      |
|    time_elapsed    | 536      |
|    total_timesteps | 230400   |
---------------------------------
Eval num_timesteps=230500, episode_reward=-505.00 +/- 170.97
Episode length: 15.74 +/- 5.01
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.7          |
|    mean_reward          | -505          |
| time/                   |               |
|    total_timesteps      | 230500        |
| train/                  |               |
|    approx_kl            | 3.3294782e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00437      |
|    explained_variance   | 0.733         |
|    learning_rate        | 0.001         |
|    loss                 | 4.12e+03      |
|    n_updates            | 10570         |
|    policy_gradient_loss | -4.47e-05     |
|    value_loss           | 9.59e+03      |
-------------------------------------------
Eval num_timesteps=231000, episode_reward=-510.47 +/- 176.32
Episode length: 16.46 +/- 5.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -506     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 226      |
|    time_elapsed    | 538      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=-536.63 +/- 158.88
Episode length: 16.64 +/- 4.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | -537          |
| time/                   |               |
|    total_timesteps      | 231500        |
| train/                  |               |
|    approx_kl            | 1.8862193e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00486      |
|    explained_variance   | 0.693         |
|    learning_rate        | 0.001         |
|    loss                 | 4.11e+03      |
|    n_updates            | 10580         |
|    policy_gradient_loss | -7.08e-05     |
|    value_loss           | 1.04e+04      |
-------------------------------------------
Eval num_timesteps=232000, episode_reward=-547.20 +/- 183.59
Episode length: 15.48 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -547     |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -535     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 227      |
|    time_elapsed    | 540      |
|    total_timesteps | 232448   |
---------------------------------
Eval num_timesteps=232500, episode_reward=-535.87 +/- 197.54
Episode length: 16.58 +/- 5.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -536         |
| time/                   |              |
|    total_timesteps      | 232500       |
| train/                  |              |
|    approx_kl            | 3.312016e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0047      |
|    explained_variance   | 0.747        |
|    learning_rate        | 0.001        |
|    loss                 | 4.79e+03     |
|    n_updates            | 10590        |
|    policy_gradient_loss | -5.25e-05    |
|    value_loss           | 1.01e+04     |
------------------------------------------
Eval num_timesteps=233000, episode_reward=-500.77 +/- 194.63
Episode length: 15.60 +/- 4.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -501     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -519     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 228      |
|    time_elapsed    | 543      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=-507.21 +/- 175.62
Episode length: 15.52 +/- 4.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.5          |
|    mean_reward          | -507          |
| time/                   |               |
|    total_timesteps      | 233500        |
| train/                  |               |
|    approx_kl            | 8.7674125e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00474      |
|    explained_variance   | 0.73          |
|    learning_rate        | 0.001         |
|    loss                 | 4.53e+03      |
|    n_updates            | 10600         |
|    policy_gradient_loss | -8.94e-05     |
|    value_loss           | 9.26e+03      |
-------------------------------------------
Eval num_timesteps=234000, episode_reward=-484.82 +/- 178.28
Episode length: 15.40 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -485     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -543     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 229      |
|    time_elapsed    | 545      |
|    total_timesteps | 234496   |
---------------------------------
Eval num_timesteps=234500, episode_reward=-538.98 +/- 157.99
Episode length: 15.48 +/- 4.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.5          |
|    mean_reward          | -539          |
| time/                   |               |
|    total_timesteps      | 234500        |
| train/                  |               |
|    approx_kl            | 3.5506673e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00459      |
|    explained_variance   | 0.751         |
|    learning_rate        | 0.001         |
|    loss                 | 3.77e+03      |
|    n_updates            | 10610         |
|    policy_gradient_loss | -3.69e-05     |
|    value_loss           | 1.02e+04      |
-------------------------------------------
Eval num_timesteps=235000, episode_reward=-529.70 +/- 211.22
Episode length: 15.12 +/- 4.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=-529.32 +/- 147.83
Episode length: 16.96 +/- 4.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17       |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -522     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 230      |
|    time_elapsed    | 548      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=-540.40 +/- 192.00
Episode length: 15.96 +/- 4.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -540         |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 4.085846e-05 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00578     |
|    explained_variance   | 0.71         |
|    learning_rate        | 0.001        |
|    loss                 | 4.32e+03     |
|    n_updates            | 10620        |
|    policy_gradient_loss | -0.000101    |
|    value_loss           | 9.6e+03      |
------------------------------------------
Eval num_timesteps=236500, episode_reward=-546.75 +/- 198.95
Episode length: 17.08 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -547     |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -516     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 231      |
|    time_elapsed    | 550      |
|    total_timesteps | 236544   |
---------------------------------
Eval num_timesteps=237000, episode_reward=-550.93 +/- 214.01
Episode length: 15.76 +/- 5.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -551          |
| time/                   |               |
|    total_timesteps      | 237000        |
| train/                  |               |
|    approx_kl            | 0.00013232889 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00547      |
|    explained_variance   | 0.649         |
|    learning_rate        | 0.001         |
|    loss                 | 5.3e+03       |
|    n_updates            | 10630         |
|    policy_gradient_loss | -0.000108     |
|    value_loss           | 1.12e+04      |
-------------------------------------------
Eval num_timesteps=237500, episode_reward=-536.08 +/- 231.43
Episode length: 15.98 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 232      |
|    time_elapsed    | 553      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=-557.58 +/- 185.70
Episode length: 15.14 +/- 4.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.1          |
|    mean_reward          | -558          |
| time/                   |               |
|    total_timesteps      | 238000        |
| train/                  |               |
|    approx_kl            | 6.7346264e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00485      |
|    explained_variance   | 0.678         |
|    learning_rate        | 0.001         |
|    loss                 | 4.4e+03       |
|    n_updates            | 10640         |
|    policy_gradient_loss | -0.000101     |
|    value_loss           | 1.19e+04      |
-------------------------------------------
Eval num_timesteps=238500, episode_reward=-529.01 +/- 188.51
Episode length: 16.40 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.4     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -549     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 233      |
|    time_elapsed    | 555      |
|    total_timesteps | 238592   |
---------------------------------
Eval num_timesteps=239000, episode_reward=-514.20 +/- 177.14
Episode length: 16.96 +/- 5.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17            |
|    mean_reward          | -514          |
| time/                   |               |
|    total_timesteps      | 239000        |
| train/                  |               |
|    approx_kl            | 6.6298526e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00451      |
|    explained_variance   | 0.752         |
|    learning_rate        | 0.001         |
|    loss                 | 3.62e+03      |
|    n_updates            | 10650         |
|    policy_gradient_loss | -0.00013      |
|    value_loss           | 8.85e+03      |
-------------------------------------------
Eval num_timesteps=239500, episode_reward=-572.73 +/- 182.32
Episode length: 15.30 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -573     |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 234      |
|    time_elapsed    | 557      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-485.60 +/- 237.26
Episode length: 16.68 +/- 6.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -486         |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 6.094342e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00489     |
|    explained_variance   | 0.643        |
|    learning_rate        | 0.001        |
|    loss                 | 6.25e+03     |
|    n_updates            | 10660        |
|    policy_gradient_loss | -6.48e-05    |
|    value_loss           | 1.32e+04     |
------------------------------------------
Eval num_timesteps=240500, episode_reward=-553.34 +/- 208.15
Episode length: 15.38 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.4     |
|    mean_reward     | -553     |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | -484     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 235      |
|    time_elapsed    | 560      |
|    total_timesteps | 240640   |
---------------------------------
Eval num_timesteps=241000, episode_reward=-526.73 +/- 204.41
Episode length: 15.26 +/- 4.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.3         |
|    mean_reward          | -527         |
| time/                   |              |
|    total_timesteps      | 241000       |
| train/                  |              |
|    approx_kl            | 2.601376e-05 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00496     |
|    explained_variance   | 0.732        |
|    learning_rate        | 0.001        |
|    loss                 | 5.11e+03     |
|    n_updates            | 10670        |
|    policy_gradient_loss | -0.000133    |
|    value_loss           | 8.52e+03     |
------------------------------------------
Eval num_timesteps=241500, episode_reward=-536.58 +/- 174.23
Episode length: 16.76 +/- 5.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -505     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 236      |
|    time_elapsed    | 562      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=-509.58 +/- 191.26
Episode length: 15.96 +/- 4.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -510          |
| time/                   |               |
|    total_timesteps      | 242000        |
| train/                  |               |
|    approx_kl            | 0.00010044768 |
|    clip_fraction        | 0.00225       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00347      |
|    explained_variance   | 0.775         |
|    learning_rate        | 0.001         |
|    loss                 | 4.07e+03      |
|    n_updates            | 10680         |
|    policy_gradient_loss | -0.000403     |
|    value_loss           | 8.43e+03      |
-------------------------------------------
Eval num_timesteps=242500, episode_reward=-542.59 +/- 157.18
Episode length: 15.98 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -523     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 237      |
|    time_elapsed    | 564      |
|    total_timesteps | 242688   |
---------------------------------
Eval num_timesteps=243000, episode_reward=-533.91 +/- 160.34
Episode length: 16.04 +/- 5.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -534          |
| time/                   |               |
|    total_timesteps      | 243000        |
| train/                  |               |
|    approx_kl            | 1.9732397e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00356      |
|    explained_variance   | 0.761         |
|    learning_rate        | 0.001         |
|    loss                 | 5.7e+03       |
|    n_updates            | 10690         |
|    policy_gradient_loss | -4.17e-05     |
|    value_loss           | 9.54e+03      |
-------------------------------------------
Eval num_timesteps=243500, episode_reward=-520.63 +/- 188.30
Episode length: 16.78 +/- 4.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.8     |
|    mean_reward     | -521     |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -543     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 238      |
|    time_elapsed    | 567      |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=-497.67 +/- 176.43
Episode length: 16.24 +/- 4.62
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -498          |
| time/                   |               |
|    total_timesteps      | 244000        |
| train/                  |               |
|    approx_kl            | 4.0652405e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00367      |
|    explained_variance   | 0.697         |
|    learning_rate        | 0.001         |
|    loss                 | 4.29e+03      |
|    n_updates            | 10700         |
|    policy_gradient_loss | -0.000136     |
|    value_loss           | 1.18e+04      |
-------------------------------------------
Eval num_timesteps=244500, episode_reward=-552.66 +/- 167.69
Episode length: 16.54 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -553     |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -469     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 239      |
|    time_elapsed    | 569      |
|    total_timesteps | 244736   |
---------------------------------
Eval num_timesteps=245000, episode_reward=-510.78 +/- 189.80
Episode length: 15.78 +/- 5.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -511          |
| time/                   |               |
|    total_timesteps      | 245000        |
| train/                  |               |
|    approx_kl            | 7.4564014e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00446      |
|    explained_variance   | 0.631         |
|    learning_rate        | 0.001         |
|    loss                 | 5.08e+03      |
|    n_updates            | 10710         |
|    policy_gradient_loss | -7.96e-05     |
|    value_loss           | 1.19e+04      |
-------------------------------------------
Eval num_timesteps=245500, episode_reward=-506.85 +/- 162.37
Episode length: 17.58 +/- 5.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -476     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 240      |
|    time_elapsed    | 572      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=-555.42 +/- 189.74
Episode length: 15.90 +/- 5.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.9         |
|    mean_reward          | -555         |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 6.743806e-05 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0066      |
|    explained_variance   | 0.637        |
|    learning_rate        | 0.001        |
|    loss                 | 4.84e+03     |
|    n_updates            | 10720        |
|    policy_gradient_loss | -0.00046     |
|    value_loss           | 1.25e+04     |
------------------------------------------
Eval num_timesteps=246500, episode_reward=-532.54 +/- 193.00
Episode length: 14.88 +/- 3.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -508     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 241      |
|    time_elapsed    | 574      |
|    total_timesteps | 246784   |
---------------------------------
Eval num_timesteps=247000, episode_reward=-524.30 +/- 171.64
Episode length: 15.90 +/- 4.43
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -524          |
| time/                   |               |
|    total_timesteps      | 247000        |
| train/                  |               |
|    approx_kl            | 1.3069366e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00676      |
|    explained_variance   | 0.694         |
|    learning_rate        | 0.001         |
|    loss                 | 3.54e+03      |
|    n_updates            | 10730         |
|    policy_gradient_loss | -8.16e-05     |
|    value_loss           | 1.15e+04      |
-------------------------------------------
Eval num_timesteps=247500, episode_reward=-537.22 +/- 171.41
Episode length: 15.58 +/- 4.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -538     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 242      |
|    time_elapsed    | 576      |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=-530.27 +/- 199.42
Episode length: 16.00 +/- 5.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -530          |
| time/                   |               |
|    total_timesteps      | 248000        |
| train/                  |               |
|    approx_kl            | 2.8957787e-05 |
|    clip_fraction        | 0.000195      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0104       |
|    explained_variance   | 0.681         |
|    learning_rate        | 0.001         |
|    loss                 | 6.69e+03      |
|    n_updates            | 10740         |
|    policy_gradient_loss | -0.000202     |
|    value_loss           | 1.18e+04      |
-------------------------------------------
Eval num_timesteps=248500, episode_reward=-550.32 +/- 168.92
Episode length: 17.10 +/- 5.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -550     |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 243      |
|    time_elapsed    | 579      |
|    total_timesteps | 248832   |
---------------------------------
Eval num_timesteps=249000, episode_reward=-529.45 +/- 172.68
Episode length: 15.76 +/- 4.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.8          |
|    mean_reward          | -529          |
| time/                   |               |
|    total_timesteps      | 249000        |
| train/                  |               |
|    approx_kl            | 0.00034679478 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0157       |
|    explained_variance   | 0.666         |
|    learning_rate        | 0.001         |
|    loss                 | 4.54e+03      |
|    n_updates            | 10750         |
|    policy_gradient_loss | -0.000437     |
|    value_loss           | 1.09e+04      |
-------------------------------------------
Eval num_timesteps=249500, episode_reward=-537.23 +/- 229.97
Episode length: 15.12 +/- 4.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -525     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 244      |
|    time_elapsed    | 581      |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-562.51 +/- 161.26
Episode length: 15.22 +/- 4.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.2          |
|    mean_reward          | -563          |
| time/                   |               |
|    total_timesteps      | 250000        |
| train/                  |               |
|    approx_kl            | 4.9089955e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0198       |
|    explained_variance   | 0.689         |
|    learning_rate        | 0.001         |
|    loss                 | 5.18e+03      |
|    n_updates            | 10760         |
|    policy_gradient_loss | -0.00044      |
|    value_loss           | 1.08e+04      |
-------------------------------------------
Eval num_timesteps=250500, episode_reward=-508.77 +/- 164.64
Episode length: 15.46 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -544     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 245      |
|    time_elapsed    | 583      |
|    total_timesteps | 250880   |
---------------------------------
Eval num_timesteps=251000, episode_reward=-561.84 +/- 162.86
Episode length: 15.48 +/- 3.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.5         |
|    mean_reward          | -562         |
| time/                   |              |
|    total_timesteps      | 251000       |
| train/                  |              |
|    approx_kl            | 6.960769e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0198      |
|    explained_variance   | 0.75         |
|    learning_rate        | 0.001        |
|    loss                 | 2e+03        |
|    n_updates            | 10770        |
|    policy_gradient_loss | -0.000399    |
|    value_loss           | 9.25e+03     |
------------------------------------------
Eval num_timesteps=251500, episode_reward=-465.66 +/- 150.40
Episode length: 17.48 +/- 5.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.5     |
|    mean_reward     | -466     |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -538     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 246      |
|    time_elapsed    | 586      |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=-523.48 +/- 226.66
Episode length: 15.96 +/- 4.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16            |
|    mean_reward          | -523          |
| time/                   |               |
|    total_timesteps      | 252000        |
| train/                  |               |
|    approx_kl            | 0.00030626787 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.019        |
|    explained_variance   | 0.706         |
|    learning_rate        | 0.001         |
|    loss                 | 4.77e+03      |
|    n_updates            | 10780         |
|    policy_gradient_loss | 0.000205      |
|    value_loss           | 1.23e+04      |
-------------------------------------------
Eval num_timesteps=252500, episode_reward=-551.29 +/- 177.60
Episode length: 15.70 +/- 4.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -551     |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 247      |
|    time_elapsed    | 588      |
|    total_timesteps | 252928   |
---------------------------------
Eval num_timesteps=253000, episode_reward=-487.85 +/- 180.54
Episode length: 14.06 +/- 4.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 14.1         |
|    mean_reward          | -488         |
| time/                   |              |
|    total_timesteps      | 253000       |
| train/                  |              |
|    approx_kl            | 0.0013284541 |
|    clip_fraction        | 0.00479      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0222      |
|    explained_variance   | 0.573        |
|    learning_rate        | 0.001        |
|    loss                 | 8.29e+03     |
|    n_updates            | 10790        |
|    policy_gradient_loss | 0.000886     |
|    value_loss           | 1.55e+04     |
------------------------------------------
Eval num_timesteps=253500, episode_reward=-531.79 +/- 190.50
Episode length: 16.74 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.7     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -509     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 248      |
|    time_elapsed    | 590      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=-508.32 +/- 188.44
Episode length: 16.04 +/- 4.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -508         |
| time/                   |              |
|    total_timesteps      | 254000       |
| train/                  |              |
|    approx_kl            | 7.236801e-05 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0198      |
|    explained_variance   | 0.712        |
|    learning_rate        | 0.001        |
|    loss                 | 3.81e+03     |
|    n_updates            | 10800        |
|    policy_gradient_loss | -0.000568    |
|    value_loss           | 9.91e+03     |
------------------------------------------
Eval num_timesteps=254500, episode_reward=-503.85 +/- 170.38
Episode length: 16.24 +/- 5.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -497     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 249      |
|    time_elapsed    | 593      |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=255000, episode_reward=-481.81 +/- 190.03
Episode length: 15.88 +/- 5.35
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -482          |
| time/                   |               |
|    total_timesteps      | 255000        |
| train/                  |               |
|    approx_kl            | 0.00012818218 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0207       |
|    explained_variance   | 0.713         |
|    learning_rate        | 0.001         |
|    loss                 | 4.88e+03      |
|    n_updates            | 10810         |
|    policy_gradient_loss | -0.000658     |
|    value_loss           | 1.01e+04      |
-------------------------------------------
Eval num_timesteps=255500, episode_reward=-542.98 +/- 161.98
Episode length: 16.26 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-523.71 +/- 178.32
Episode length: 16.04 +/- 4.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -524     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -500     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 250      |
|    time_elapsed    | 596      |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=-558.02 +/- 146.27
Episode length: 15.54 +/- 4.09
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.5          |
|    mean_reward          | -558          |
| time/                   |               |
|    total_timesteps      | 256500        |
| train/                  |               |
|    approx_kl            | 4.7970738e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0236       |
|    explained_variance   | 0.728         |
|    learning_rate        | 0.001         |
|    loss                 | 4.89e+03      |
|    n_updates            | 10820         |
|    policy_gradient_loss | -0.000704     |
|    value_loss           | 8.73e+03      |
-------------------------------------------
Eval num_timesteps=257000, episode_reward=-563.66 +/- 223.27
Episode length: 15.54 +/- 4.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.5     |
|    ep_rew_mean     | -499     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 251      |
|    time_elapsed    | 598      |
|    total_timesteps | 257024   |
---------------------------------
Eval num_timesteps=257500, episode_reward=-558.45 +/- 200.68
Episode length: 15.08 +/- 3.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.1         |
|    mean_reward          | -558         |
| time/                   |              |
|    total_timesteps      | 257500       |
| train/                  |              |
|    approx_kl            | 5.970098e-05 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.021       |
|    explained_variance   | 0.622        |
|    learning_rate        | 0.001        |
|    loss                 | 4.91e+03     |
|    n_updates            | 10830        |
|    policy_gradient_loss | -0.000764    |
|    value_loss           | 1.43e+04     |
------------------------------------------
Eval num_timesteps=258000, episode_reward=-491.68 +/- 208.33
Episode length: 17.16 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -492     |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 252      |
|    time_elapsed    | 600      |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=-476.01 +/- 192.27
Episode length: 16.06 +/- 5.59
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.1          |
|    mean_reward          | -476          |
| time/                   |               |
|    total_timesteps      | 258500        |
| train/                  |               |
|    approx_kl            | 0.00036810548 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0315       |
|    explained_variance   | 0.7           |
|    learning_rate        | 0.001         |
|    loss                 | 4.06e+03      |
|    n_updates            | 10840         |
|    policy_gradient_loss | -0.000887     |
|    value_loss           | 1.12e+04      |
-------------------------------------------
Eval num_timesteps=259000, episode_reward=-498.25 +/- 210.75
Episode length: 16.28 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.3     |
|    mean_reward     | -498     |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -520     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 253      |
|    time_elapsed    | 603      |
|    total_timesteps | 259072   |
---------------------------------
Eval num_timesteps=259500, episode_reward=-516.02 +/- 164.32
Episode length: 15.82 +/- 4.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -516        |
| time/                   |             |
|    total_timesteps      | 259500      |
| train/                  |             |
|    approx_kl            | 0.001964931 |
|    clip_fraction        | 0.00439     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.035      |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.001       |
|    loss                 | 7.11e+03    |
|    n_updates            | 10850       |
|    policy_gradient_loss | 0.00137     |
|    value_loss           | 1.35e+04    |
-----------------------------------------
Eval num_timesteps=260000, episode_reward=-525.20 +/- 161.26
Episode length: 16.22 +/- 4.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | -510     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 254      |
|    time_elapsed    | 605      |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=-556.61 +/- 148.32
Episode length: 15.92 +/- 4.38
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -557          |
| time/                   |               |
|    total_timesteps      | 260500        |
| train/                  |               |
|    approx_kl            | 2.6042282e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0305       |
|    explained_variance   | 0.691         |
|    learning_rate        | 0.001         |
|    loss                 | 5.99e+03      |
|    n_updates            | 10860         |
|    policy_gradient_loss | -0.000843     |
|    value_loss           | 1.23e+04      |
-------------------------------------------
Eval num_timesteps=261000, episode_reward=-542.50 +/- 164.21
Episode length: 14.68 +/- 4.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | -524     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 255      |
|    time_elapsed    | 607      |
|    total_timesteps | 261120   |
---------------------------------
Eval num_timesteps=261500, episode_reward=-535.83 +/- 186.50
Episode length: 16.04 +/- 4.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -536         |
| time/                   |              |
|    total_timesteps      | 261500       |
| train/                  |              |
|    approx_kl            | 0.0009853092 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0243      |
|    explained_variance   | 0.696        |
|    learning_rate        | 0.001        |
|    loss                 | 6.7e+03      |
|    n_updates            | 10870        |
|    policy_gradient_loss | -0.00079     |
|    value_loss           | 1.1e+04      |
------------------------------------------
Eval num_timesteps=262000, episode_reward=-495.11 +/- 198.06
Episode length: 17.22 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -495     |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | -512     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 256      |
|    time_elapsed    | 610      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=-537.68 +/- 159.79
Episode length: 16.46 +/- 4.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.5         |
|    mean_reward          | -538         |
| time/                   |              |
|    total_timesteps      | 262500       |
| train/                  |              |
|    approx_kl            | 8.468842e-05 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0183      |
|    explained_variance   | 0.744        |
|    learning_rate        | 0.001        |
|    loss                 | 3.69e+03     |
|    n_updates            | 10880        |
|    policy_gradient_loss | -0.000885    |
|    value_loss           | 8.86e+03     |
------------------------------------------
Eval num_timesteps=263000, episode_reward=-506.83 +/- 202.66
Episode length: 16.04 +/- 5.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16       |
|    mean_reward     | -507     |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -527     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 257      |
|    time_elapsed    | 612      |
|    total_timesteps | 263168   |
---------------------------------
Eval num_timesteps=263500, episode_reward=-515.81 +/- 169.74
Episode length: 17.18 +/- 4.36
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.2          |
|    mean_reward          | -516          |
| time/                   |               |
|    total_timesteps      | 263500        |
| train/                  |               |
|    approx_kl            | 0.00046310684 |
|    clip_fraction        | 0.00254       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0144       |
|    explained_variance   | 0.697         |
|    learning_rate        | 0.001         |
|    loss                 | 3.69e+03      |
|    n_updates            | 10890         |
|    policy_gradient_loss | -0.000603     |
|    value_loss           | 1.18e+04      |
-------------------------------------------
Eval num_timesteps=264000, episode_reward=-497.99 +/- 195.44
Episode length: 17.22 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.2     |
|    mean_reward     | -498     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -544     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 258      |
|    time_elapsed    | 615      |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=-547.09 +/- 202.88
Episode length: 15.02 +/- 4.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15            |
|    mean_reward          | -547          |
| time/                   |               |
|    total_timesteps      | 264500        |
| train/                  |               |
|    approx_kl            | 0.00037949887 |
|    clip_fraction        | 0.00195       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00862      |
|    explained_variance   | 0.76          |
|    learning_rate        | 0.001         |
|    loss                 | 4.95e+03      |
|    n_updates            | 10900         |
|    policy_gradient_loss | -0.000683     |
|    value_loss           | 8.93e+03      |
-------------------------------------------
Eval num_timesteps=265000, episode_reward=-460.10 +/- 188.18
Episode length: 17.14 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -460     |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -505     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 259      |
|    time_elapsed    | 617      |
|    total_timesteps | 265216   |
---------------------------------
Eval num_timesteps=265500, episode_reward=-590.46 +/- 198.68
Episode length: 14.92 +/- 3.92
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.9          |
|    mean_reward          | -590          |
| time/                   |               |
|    total_timesteps      | 265500        |
| train/                  |               |
|    approx_kl            | 2.8022972e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00812      |
|    explained_variance   | 0.648         |
|    learning_rate        | 0.001         |
|    loss                 | 4.99e+03      |
|    n_updates            | 10910         |
|    policy_gradient_loss | -0.000209     |
|    value_loss           | 1.26e+04      |
-------------------------------------------
Eval num_timesteps=266000, episode_reward=-528.41 +/- 190.80
Episode length: 14.58 +/- 4.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | -528     |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.8     |
|    ep_rew_mean     | -490     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 260      |
|    time_elapsed    | 619      |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=-503.25 +/- 167.09
Episode length: 15.96 +/- 4.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | -503        |
| time/                   |             |
|    total_timesteps      | 266500      |
| train/                  |             |
|    approx_kl            | 2.81143e-07 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00918    |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.001       |
|    loss                 | 3.92e+03    |
|    n_updates            | 10920       |
|    policy_gradient_loss | -0.000178   |
|    value_loss           | 9.19e+03    |
-----------------------------------------
Eval num_timesteps=267000, episode_reward=-527.38 +/- 181.82
Episode length: 15.54 +/- 5.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | -486     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 261      |
|    time_elapsed    | 622      |
|    total_timesteps | 267264   |
---------------------------------
Eval num_timesteps=267500, episode_reward=-522.29 +/- 199.75
Episode length: 15.08 +/- 4.22
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.1          |
|    mean_reward          | -522          |
| time/                   |               |
|    total_timesteps      | 267500        |
| train/                  |               |
|    approx_kl            | 5.7465513e-06 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00955      |
|    explained_variance   | 0.646         |
|    learning_rate        | 0.001         |
|    loss                 | 5.86e+03      |
|    n_updates            | 10930         |
|    policy_gradient_loss | -7.97e-05     |
|    value_loss           | 1.19e+04      |
-------------------------------------------
Eval num_timesteps=268000, episode_reward=-514.51 +/- 187.56
Episode length: 15.66 +/- 4.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -533     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 262      |
|    time_elapsed    | 624      |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=-516.60 +/- 184.23
Episode length: 15.06 +/- 4.72
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.1          |
|    mean_reward          | -517          |
| time/                   |               |
|    total_timesteps      | 268500        |
| train/                  |               |
|    approx_kl            | 0.00016809616 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00826      |
|    explained_variance   | 0.717         |
|    learning_rate        | 0.001         |
|    loss                 | 6.05e+03      |
|    n_updates            | 10940         |
|    policy_gradient_loss | -0.000386     |
|    value_loss           | 1.26e+04      |
-------------------------------------------
Eval num_timesteps=269000, episode_reward=-554.34 +/- 175.04
Episode length: 15.00 +/- 4.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -554     |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -554     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 263      |
|    time_elapsed    | 626      |
|    total_timesteps | 269312   |
---------------------------------
Eval num_timesteps=269500, episode_reward=-528.06 +/- 188.03
Episode length: 15.98 +/- 5.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16           |
|    mean_reward          | -528         |
| time/                   |              |
|    total_timesteps      | 269500       |
| train/                  |              |
|    approx_kl            | 0.0006493676 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.011       |
|    explained_variance   | 0.687        |
|    learning_rate        | 0.001        |
|    loss                 | 4.51e+03     |
|    n_updates            | 10950        |
|    policy_gradient_loss | 0.00304      |
|    value_loss           | 1.21e+04     |
------------------------------------------
Eval num_timesteps=270000, episode_reward=-536.74 +/- 186.65
Episode length: 15.10 +/- 4.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.1     |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -492     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 264      |
|    time_elapsed    | 628      |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=-537.67 +/- 166.91
Episode length: 15.48 +/- 4.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.5          |
|    mean_reward          | -538          |
| time/                   |               |
|    total_timesteps      | 270500        |
| train/                  |               |
|    approx_kl            | 4.7437556e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0163       |
|    explained_variance   | 0.639         |
|    learning_rate        | 0.001         |
|    loss                 | 5.01e+03      |
|    n_updates            | 10960         |
|    policy_gradient_loss | -0.000519     |
|    value_loss           | 1.2e+04       |
-------------------------------------------
Eval num_timesteps=271000, episode_reward=-537.51 +/- 186.52
Episode length: 15.20 +/- 4.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -515     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 265      |
|    time_elapsed    | 631      |
|    total_timesteps | 271360   |
---------------------------------
Eval num_timesteps=271500, episode_reward=-544.47 +/- 179.78
Episode length: 15.60 +/- 4.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | -544          |
| time/                   |               |
|    total_timesteps      | 271500        |
| train/                  |               |
|    approx_kl            | 3.5161735e-05 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0186       |
|    explained_variance   | 0.725         |
|    learning_rate        | 0.001         |
|    loss                 | 4.47e+03      |
|    n_updates            | 10970         |
|    policy_gradient_loss | -0.000285     |
|    value_loss           | 1.08e+04      |
-------------------------------------------
Eval num_timesteps=272000, episode_reward=-513.51 +/- 148.26
Episode length: 16.08 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15       |
|    ep_rew_mean     | -532     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 266      |
|    time_elapsed    | 633      |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=-526.79 +/- 173.43
Episode length: 16.50 +/- 5.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.5          |
|    mean_reward          | -527          |
| time/                   |               |
|    total_timesteps      | 272500        |
| train/                  |               |
|    approx_kl            | 5.8347534e-05 |
|    clip_fraction        | 0.00156       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0174       |
|    explained_variance   | 0.685         |
|    learning_rate        | 0.001         |
|    loss                 | 5.94e+03      |
|    n_updates            | 10980         |
|    policy_gradient_loss | -0.000511     |
|    value_loss           | 1.33e+04      |
-------------------------------------------
Eval num_timesteps=273000, episode_reward=-517.90 +/- 205.55
Episode length: 15.82 +/- 4.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.8     |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -541     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 267      |
|    time_elapsed    | 635      |
|    total_timesteps | 273408   |
---------------------------------
Eval num_timesteps=273500, episode_reward=-563.45 +/- 159.66
Episode length: 15.80 +/- 4.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.8        |
|    mean_reward          | -563        |
| time/                   |             |
|    total_timesteps      | 273500      |
| train/                  |             |
|    approx_kl            | 7.82829e-05 |
|    clip_fraction        | 0.00166     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0135     |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.001       |
|    loss                 | 3.55e+03    |
|    n_updates            | 10990       |
|    policy_gradient_loss | 0.000373    |
|    value_loss           | 9.51e+03    |
-----------------------------------------
Eval num_timesteps=274000, episode_reward=-514.09 +/- 234.48
Episode length: 15.64 +/- 5.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.6     |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -514     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 268      |
|    time_elapsed    | 638      |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=-525.36 +/- 157.24
Episode length: 15.48 +/- 4.53
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.5          |
|    mean_reward          | -525          |
| time/                   |               |
|    total_timesteps      | 274500        |
| train/                  |               |
|    approx_kl            | 0.00035541703 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0127       |
|    explained_variance   | 0.729         |
|    learning_rate        | 0.001         |
|    loss                 | 4.23e+03      |
|    n_updates            | 11000         |
|    policy_gradient_loss | 4.52e-05      |
|    value_loss           | 9.35e+03      |
-------------------------------------------
Eval num_timesteps=275000, episode_reward=-563.69 +/- 178.87
Episode length: 15.68 +/- 4.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -539     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 269      |
|    time_elapsed    | 640      |
|    total_timesteps | 275456   |
---------------------------------
Eval num_timesteps=275500, episode_reward=-492.67 +/- 136.74
Episode length: 16.28 +/- 4.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.3          |
|    mean_reward          | -493          |
| time/                   |               |
|    total_timesteps      | 275500        |
| train/                  |               |
|    approx_kl            | 0.00020919944 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0136       |
|    explained_variance   | 0.755         |
|    learning_rate        | 0.001         |
|    loss                 | 4.5e+03       |
|    n_updates            | 11010         |
|    policy_gradient_loss | 0.000503      |
|    value_loss           | 8.6e+03       |
-------------------------------------------
Eval num_timesteps=276000, episode_reward=-515.63 +/- 161.34
Episode length: 14.74 +/- 4.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.7     |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -559     |
| time/              |          |
|    fps             | 430      |
|    iterations      | 270      |
|    time_elapsed    | 642      |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=-510.85 +/- 191.68
Episode length: 15.80 +/- 5.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -511         |
| time/                   |              |
|    total_timesteps      | 276500       |
| train/                  |              |
|    approx_kl            | 4.123611e-05 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0166      |
|    explained_variance   | 0.73         |
|    learning_rate        | 0.001        |
|    loss                 | 4.63e+03     |
|    n_updates            | 11020        |
|    policy_gradient_loss | -0.000363    |
|    value_loss           | 1.04e+04     |
------------------------------------------
Eval num_timesteps=277000, episode_reward=-480.59 +/- 217.67
Episode length: 15.04 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15       |
|    mean_reward     | -481     |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=-531.76 +/- 171.78
Episode length: 15.30 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 271      |
|    time_elapsed    | 645      |
|    total_timesteps | 277504   |
---------------------------------
Eval num_timesteps=278000, episode_reward=-551.25 +/- 138.21
Episode length: 16.70 +/- 4.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.7          |
|    mean_reward          | -551          |
| time/                   |               |
|    total_timesteps      | 278000        |
| train/                  |               |
|    approx_kl            | 2.5503454e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0157       |
|    explained_variance   | 0.731         |
|    learning_rate        | 0.001         |
|    loss                 | 2.77e+03      |
|    n_updates            | 11030         |
|    policy_gradient_loss | -0.000354     |
|    value_loss           | 1.05e+04      |
-------------------------------------------
Eval num_timesteps=278500, episode_reward=-513.08 +/- 173.14
Episode length: 16.58 +/- 4.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -535     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 272      |
|    time_elapsed    | 648      |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=-513.59 +/- 155.64
Episode length: 16.10 +/- 4.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.1          |
|    mean_reward          | -514          |
| time/                   |               |
|    total_timesteps      | 279000        |
| train/                  |               |
|    approx_kl            | 0.00011987932 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0111       |
|    explained_variance   | 0.793         |
|    learning_rate        | 0.001         |
|    loss                 | 3.74e+03      |
|    n_updates            | 11040         |
|    policy_gradient_loss | -0.000214     |
|    value_loss           | 7.41e+03      |
-------------------------------------------
Eval num_timesteps=279500, episode_reward=-549.28 +/- 161.08
Episode length: 16.10 +/- 4.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -549     |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.2     |
|    ep_rew_mean     | -518     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 273      |
|    time_elapsed    | 650      |
|    total_timesteps | 279552   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-469.65 +/- 191.40
Episode length: 15.62 +/- 5.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | -470          |
| time/                   |               |
|    total_timesteps      | 280000        |
| train/                  |               |
|    approx_kl            | 1.9854633e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0085       |
|    explained_variance   | 0.677         |
|    learning_rate        | 0.001         |
|    loss                 | 5.09e+03      |
|    n_updates            | 11050         |
|    policy_gradient_loss | -0.000137     |
|    value_loss           | 1.28e+04      |
-------------------------------------------
Eval num_timesteps=280500, episode_reward=-526.62 +/- 146.92
Episode length: 16.94 +/- 4.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | -526     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 274      |
|    time_elapsed    | 652      |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=-471.38 +/- 211.46
Episode length: 17.32 +/- 6.75
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 17.3          |
|    mean_reward          | -471          |
| time/                   |               |
|    total_timesteps      | 281000        |
| train/                  |               |
|    approx_kl            | 0.00067957205 |
|    clip_fraction        | 0.00215       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0114       |
|    explained_variance   | 0.587         |
|    learning_rate        | 0.001         |
|    loss                 | 6.44e+03      |
|    n_updates            | 11060         |
|    policy_gradient_loss | 0.00126       |
|    value_loss           | 1.45e+04      |
-------------------------------------------
Eval num_timesteps=281500, episode_reward=-512.77 +/- 213.90
Episode length: 14.58 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.6     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -530     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 275      |
|    time_elapsed    | 655      |
|    total_timesteps | 281600   |
---------------------------------
Eval num_timesteps=282000, episode_reward=-564.20 +/- 153.06
Episode length: 15.56 +/- 4.55
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.6          |
|    mean_reward          | -564          |
| time/                   |               |
|    total_timesteps      | 282000        |
| train/                  |               |
|    approx_kl            | 1.0599615e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00991      |
|    explained_variance   | 0.691         |
|    learning_rate        | 0.001         |
|    loss                 | 4.86e+03      |
|    n_updates            | 11070         |
|    policy_gradient_loss | -0.000102     |
|    value_loss           | 1.06e+04      |
-------------------------------------------
Eval num_timesteps=282500, episode_reward=-553.99 +/- 206.31
Episode length: 16.16 +/- 4.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -554     |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -541     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 276      |
|    time_elapsed    | 657      |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=-572.18 +/- 154.37
Episode length: 15.54 +/- 4.04
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.5          |
|    mean_reward          | -572          |
| time/                   |               |
|    total_timesteps      | 283000        |
| train/                  |               |
|    approx_kl            | 2.1152664e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00989      |
|    explained_variance   | 0.732         |
|    learning_rate        | 0.001         |
|    loss                 | 4.41e+03      |
|    n_updates            | 11080         |
|    policy_gradient_loss | -0.000134     |
|    value_loss           | 9.7e+03       |
-------------------------------------------
Eval num_timesteps=283500, episode_reward=-498.45 +/- 187.35
Episode length: 16.14 +/- 5.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.1     |
|    mean_reward     | -498     |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.3     |
|    ep_rew_mean     | -544     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 277      |
|    time_elapsed    | 660      |
|    total_timesteps | 283648   |
---------------------------------
Eval num_timesteps=284000, episode_reward=-520.77 +/- 203.47
Episode length: 16.50 +/- 6.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.5          |
|    mean_reward          | -521          |
| time/                   |               |
|    total_timesteps      | 284000        |
| train/                  |               |
|    approx_kl            | 3.5900448e-05 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00923      |
|    explained_variance   | 0.726         |
|    learning_rate        | 0.001         |
|    loss                 | 6.59e+03      |
|    n_updates            | 11090         |
|    policy_gradient_loss | -0.000273     |
|    value_loss           | 1.11e+04      |
-------------------------------------------
Eval num_timesteps=284500, episode_reward=-547.22 +/- 182.33
Episode length: 15.90 +/- 4.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -547     |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -531     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 278      |
|    time_elapsed    | 662      |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=-485.24 +/- 188.09
Episode length: 16.64 +/- 4.98
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.6          |
|    mean_reward          | -485          |
| time/                   |               |
|    total_timesteps      | 285000        |
| train/                  |               |
|    approx_kl            | 4.3131877e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00969      |
|    explained_variance   | 0.631         |
|    learning_rate        | 0.001         |
|    loss                 | 5.55e+03      |
|    n_updates            | 11100         |
|    policy_gradient_loss | -0.000108     |
|    value_loss           | 1.57e+04      |
-------------------------------------------
Eval num_timesteps=285500, episode_reward=-525.98 +/- 189.08
Episode length: 16.64 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -546     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 279      |
|    time_elapsed    | 664      |
|    total_timesteps | 285696   |
---------------------------------
Eval num_timesteps=286000, episode_reward=-503.31 +/- 175.49
Episode length: 16.18 +/- 5.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -503          |
| time/                   |               |
|    total_timesteps      | 286000        |
| train/                  |               |
|    approx_kl            | 2.2025779e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00878      |
|    explained_variance   | 0.733         |
|    learning_rate        | 0.001         |
|    loss                 | 4.01e+03      |
|    n_updates            | 11110         |
|    policy_gradient_loss | -0.000138     |
|    value_loss           | 1.01e+04      |
-------------------------------------------
Eval num_timesteps=286500, episode_reward=-502.07 +/- 188.65
Episode length: 15.54 +/- 4.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.5     |
|    mean_reward     | -502     |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.4     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 280      |
|    time_elapsed    | 667      |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=-556.99 +/- 159.41
Episode length: 16.70 +/- 5.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.7         |
|    mean_reward          | -557         |
| time/                   |              |
|    total_timesteps      | 287000       |
| train/                  |              |
|    approx_kl            | 8.450006e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0116      |
|    explained_variance   | 0.591        |
|    learning_rate        | 0.001        |
|    loss                 | 7.86e+03     |
|    n_updates            | 11120        |
|    policy_gradient_loss | -0.000231    |
|    value_loss           | 1.39e+04     |
------------------------------------------
Eval num_timesteps=287500, episode_reward=-529.41 +/- 190.72
Episode length: 17.60 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -529     |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 281      |
|    time_elapsed    | 669      |
|    total_timesteps | 287744   |
---------------------------------
Eval num_timesteps=288000, episode_reward=-551.05 +/- 205.67
Episode length: 15.80 +/- 4.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 15.8         |
|    mean_reward          | -551         |
| time/                   |              |
|    total_timesteps      | 288000       |
| train/                  |              |
|    approx_kl            | 0.0002293106 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00983     |
|    explained_variance   | 0.693        |
|    learning_rate        | 0.001        |
|    loss                 | 4.56e+03     |
|    n_updates            | 11130        |
|    policy_gradient_loss | -0.000309    |
|    value_loss           | 1.18e+04     |
------------------------------------------
Eval num_timesteps=288500, episode_reward=-487.11 +/- 196.99
Episode length: 15.34 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.3     |
|    mean_reward     | -487     |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | -539     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 282      |
|    time_elapsed    | 671      |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=-549.26 +/- 159.84
Episode length: 16.14 +/- 4.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.1          |
|    mean_reward          | -549          |
| time/                   |               |
|    total_timesteps      | 289000        |
| train/                  |               |
|    approx_kl            | 0.00030121312 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0114       |
|    explained_variance   | 0.713         |
|    learning_rate        | 0.001         |
|    loss                 | 6.77e+03      |
|    n_updates            | 11140         |
|    policy_gradient_loss | -0.000304     |
|    value_loss           | 1.19e+04      |
-------------------------------------------
Eval num_timesteps=289500, episode_reward=-475.49 +/- 204.14
Episode length: 17.74 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.7     |
|    mean_reward     | -475     |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.2     |
|    ep_rew_mean     | -528     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 283      |
|    time_elapsed    | 674      |
|    total_timesteps | 289792   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-527.70 +/- 185.32
Episode length: 16.28 +/- 5.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.3          |
|    mean_reward          | -528          |
| time/                   |               |
|    total_timesteps      | 290000        |
| train/                  |               |
|    approx_kl            | 4.2672036e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00956      |
|    explained_variance   | 0.646         |
|    learning_rate        | 0.001         |
|    loss                 | 5.41e+03      |
|    n_updates            | 11150         |
|    policy_gradient_loss | -0.000177     |
|    value_loss           | 1.21e+04      |
-------------------------------------------
Eval num_timesteps=290500, episode_reward=-515.04 +/- 171.28
Episode length: 17.58 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.6     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.1     |
|    ep_rew_mean     | -504     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 284      |
|    time_elapsed    | 676      |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=-498.76 +/- 164.92
Episode length: 15.86 +/- 5.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.9          |
|    mean_reward          | -499          |
| time/                   |               |
|    total_timesteps      | 291000        |
| train/                  |               |
|    approx_kl            | 7.8348676e-05 |
|    clip_fraction        | 0.00117       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00955      |
|    explained_variance   | 0.68          |
|    learning_rate        | 0.001         |
|    loss                 | 4.1e+03       |
|    n_updates            | 11160         |
|    policy_gradient_loss | -0.000674     |
|    value_loss           | 1.08e+04      |
-------------------------------------------
Eval num_timesteps=291500, episode_reward=-478.14 +/- 214.75
Episode length: 17.32 +/- 6.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.3     |
|    mean_reward     | -478     |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | -536     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 285      |
|    time_elapsed    | 679      |
|    total_timesteps | 291840   |
---------------------------------
Eval num_timesteps=292000, episode_reward=-486.34 +/- 182.70
Episode length: 16.24 +/- 6.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.2        |
|    mean_reward          | -486        |
| time/                   |             |
|    total_timesteps      | 292000      |
| train/                  |             |
|    approx_kl            | 1.91445e-07 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00706    |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.001       |
|    loss                 | 4.34e+03    |
|    n_updates            | 11170       |
|    policy_gradient_loss | -8.53e-05   |
|    value_loss           | 1.07e+04    |
-----------------------------------------
Eval num_timesteps=292500, episode_reward=-536.39 +/- 171.20
Episode length: 15.68 +/- 5.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.7     |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -546     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 286      |
|    time_elapsed    | 681      |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=-459.68 +/- 221.46
Episode length: 16.88 +/- 6.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.9          |
|    mean_reward          | -460          |
| time/                   |               |
|    total_timesteps      | 293000        |
| train/                  |               |
|    approx_kl            | 2.1693995e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00781      |
|    explained_variance   | 0.688         |
|    learning_rate        | 0.001         |
|    loss                 | 4.99e+03      |
|    n_updates            | 11180         |
|    policy_gradient_loss | -8.75e-05     |
|    value_loss           | 1.21e+04      |
-------------------------------------------
Eval num_timesteps=293500, episode_reward=-519.12 +/- 164.19
Episode length: 16.24 +/- 4.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.2     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | -560     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 287      |
|    time_elapsed    | 684      |
|    total_timesteps | 293888   |
---------------------------------
Eval num_timesteps=294000, episode_reward=-497.67 +/- 183.43
Episode length: 16.18 +/- 5.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.2          |
|    mean_reward          | -498          |
| time/                   |               |
|    total_timesteps      | 294000        |
| train/                  |               |
|    approx_kl            | 1.9883737e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00829      |
|    explained_variance   | 0.721         |
|    learning_rate        | 0.001         |
|    loss                 | 4.3e+03       |
|    n_updates            | 11190         |
|    policy_gradient_loss | -0.000129     |
|    value_loss           | 1.02e+04      |
-------------------------------------------
Eval num_timesteps=294500, episode_reward=-567.10 +/- 164.28
Episode length: 14.92 +/- 4.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 14.9     |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -547     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 288      |
|    time_elapsed    | 686      |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=-489.27 +/- 190.78
Episode length: 14.76 +/- 3.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 14.8          |
|    mean_reward          | -489          |
| time/                   |               |
|    total_timesteps      | 295000        |
| train/                  |               |
|    approx_kl            | 0.00012730242 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00773      |
|    explained_variance   | 0.744         |
|    learning_rate        | 0.001         |
|    loss                 | 4.36e+03      |
|    n_updates            | 11200         |
|    policy_gradient_loss | -0.00027      |
|    value_loss           | 1.07e+04      |
-------------------------------------------
Eval num_timesteps=295500, episode_reward=-477.84 +/- 202.33
Episode length: 15.94 +/- 4.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.9     |
|    mean_reward     | -478     |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.6     |
|    ep_rew_mean     | -540     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 289      |
|    time_elapsed    | 688      |
|    total_timesteps | 295936   |
---------------------------------
Eval num_timesteps=296000, episode_reward=-500.18 +/- 148.32
Episode length: 17.26 +/- 5.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 17.3         |
|    mean_reward          | -500         |
| time/                   |              |
|    total_timesteps      | 296000       |
| train/                  |              |
|    approx_kl            | 0.0004416504 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00632     |
|    explained_variance   | 0.739        |
|    learning_rate        | 0.001        |
|    loss                 | 4.55e+03     |
|    n_updates            | 11210        |
|    policy_gradient_loss | -0.000426    |
|    value_loss           | 9.3e+03      |
------------------------------------------
Eval num_timesteps=296500, episode_reward=-470.78 +/- 178.23
Episode length: 16.46 +/- 5.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.5     |
|    mean_reward     | -471     |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16       |
|    ep_rew_mean     | -521     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 290      |
|    time_elapsed    | 691      |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=-511.29 +/- 180.73
Episode length: 16.56 +/- 5.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 16.6         |
|    mean_reward          | -511         |
| time/                   |              |
|    total_timesteps      | 297000       |
| train/                  |              |
|    approx_kl            | 0.0007168776 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00409     |
|    explained_variance   | 0.657        |
|    learning_rate        | 0.001        |
|    loss                 | 6.38e+03     |
|    n_updates            | 11220        |
|    policy_gradient_loss | -0.000894    |
|    value_loss           | 1.49e+04     |
------------------------------------------
Eval num_timesteps=297500, episode_reward=-486.42 +/- 158.67
Episode length: 15.16 +/- 4.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 15.2     |
|    mean_reward     | -486     |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.3     |
|    ep_rew_mean     | -487     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 291      |
|    time_elapsed    | 693      |
|    total_timesteps | 297984   |
---------------------------------
Eval num_timesteps=298000, episode_reward=-489.60 +/- 170.96
Episode length: 16.52 +/- 5.18
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 16.5          |
|    mean_reward          | -490          |
| time/                   |               |
|    total_timesteps      | 298000        |
| train/                  |               |
|    approx_kl            | 0.00014500564 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00353      |
|    explained_variance   | 0.732         |
|    learning_rate        | 0.001         |
|    loss                 | 5.02e+03      |
|    n_updates            | 11230         |
|    policy_gradient_loss | -0.000171     |
|    value_loss           | 9.66e+03      |
-------------------------------------------
Eval num_timesteps=298500, episode_reward=-484.98 +/- 191.57
Episode length: 16.88 +/- 5.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.9     |
|    mean_reward     | -485     |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=-536.44 +/- 232.82
Episode length: 16.64 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 16.6     |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.9     |
|    ep_rew_mean     | -503     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 292      |
|    time_elapsed    | 696      |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=-532.36 +/- 207.37
Episode length: 15.42 +/- 5.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 15.4          |
|    mean_reward          | -532          |
| time/                   |               |
|    total_timesteps      | 299500        |
| train/                  |               |
|    approx_kl            | 3.1432137e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00345      |
|    explained_variance   | 0.728         |
|    learning_rate        | 0.001         |
|    loss                 | 3.71e+03      |
|    n_updates            | 11240         |
|    policy_gradient_loss | -4.23e-05     |
|    value_loss           | 9.13e+03      |
-------------------------------------------
Eval num_timesteps=300000, episode_reward=-492.00 +/- 200.55
Episode length: 17.08 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 17.1     |
|    mean_reward     | -492     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -507     |
| time/              |          |
|    fps             | 429      |
|    iterations      | 293      |
|    time_elapsed    | 698      |
|    total_timesteps | 300032   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-6/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 1024, 'batch_size': 64, 'learning_rate': 0.001, 'gamma': 0.9635, 'gae_lambda': 0.879}
Training steps: 300000
Frame skip: 4
