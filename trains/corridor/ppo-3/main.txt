/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Eval num_timesteps=500, episode_reward=400.71 +/- 712.96
Episode length: 35.14 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=314.71 +/- 643.06
Episode length: 35.12 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 491      |
| time/              |          |
|    fps             | 300      |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 1024     |
---------------------------------
Eval num_timesteps=1500, episode_reward=510.67 +/- 708.49
Episode length: 35.38 +/- 6.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 511           |
| time/                   |               |
|    total_timesteps      | 1500          |
| train/                  |               |
|    approx_kl            | 6.1118044e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00303      |
|    explained_variance   | -0.274        |
|    learning_rate        | 0.001         |
|    loss                 | 3.23e+04      |
|    n_updates            | 2870          |
|    policy_gradient_loss | -4.1e-05      |
|    value_loss           | 9.83e+04      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=417.02 +/- 763.49
Episode length: 35.38 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 327      |
| time/              |          |
|    fps             | 285      |
|    iterations      | 2        |
|    time_elapsed    | 7        |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=415.92 +/- 647.79
Episode length: 36.04 +/- 6.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 416           |
| time/                   |               |
|    total_timesteps      | 2500          |
| train/                  |               |
|    approx_kl            | 1.0768417e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00373      |
|    explained_variance   | -0.253        |
|    learning_rate        | 0.001         |
|    loss                 | 1.82e+04      |
|    n_updates            | 2880          |
|    policy_gradient_loss | 4.27e-06      |
|    value_loss           | 7.09e+04      |
-------------------------------------------
Eval num_timesteps=3000, episode_reward=525.85 +/- 773.09
Episode length: 35.40 +/- 7.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 526      |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 374      |
| time/              |          |
|    fps             | 282      |
|    iterations      | 3        |
|    time_elapsed    | 10       |
|    total_timesteps | 3072     |
---------------------------------
Eval num_timesteps=3500, episode_reward=553.49 +/- 703.45
Episode length: 37.40 +/- 5.07
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.4          |
|    mean_reward          | 553           |
| time/                   |               |
|    total_timesteps      | 3500          |
| train/                  |               |
|    approx_kl            | 1.2922101e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000529     |
|    explained_variance   | -0.267        |
|    learning_rate        | 0.001         |
|    loss                 | 2.34e+04      |
|    n_updates            | 2890          |
|    policy_gradient_loss | 1.56e-05      |
|    value_loss           | 8.36e+04      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=558.77 +/- 747.01
Episode length: 36.14 +/- 6.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 559      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 4        |
|    time_elapsed    | 14       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=396.61 +/- 715.60
Episode length: 34.98 +/- 6.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 397          |
| time/                   |              |
|    total_timesteps      | 4500         |
| train/                  |              |
|    approx_kl            | 9.895302e-10 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000167    |
|    explained_variance   | 0.0111       |
|    learning_rate        | 0.001        |
|    loss                 | 1.72e+04     |
|    n_updates            | 2900         |
|    policy_gradient_loss | 6.07e-06     |
|    value_loss           | 7.14e+04     |
------------------------------------------
Eval num_timesteps=5000, episode_reward=435.80 +/- 728.20
Episode length: 35.08 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 436      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 324      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 5        |
|    time_elapsed    | 18       |
|    total_timesteps | 5120     |
---------------------------------
Eval num_timesteps=5500, episode_reward=391.97 +/- 628.66
Episode length: 36.42 +/- 5.50
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 36.4           |
|    mean_reward          | 392            |
| time/                   |                |
|    total_timesteps      | 5500           |
| train/                  |                |
|    approx_kl            | -6.9849193e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -6.51e-05      |
|    explained_variance   | -0.391         |
|    learning_rate        | 0.001          |
|    loss                 | 1.74e+04       |
|    n_updates            | 2910           |
|    policy_gradient_loss | 1.84e-06       |
|    value_loss           | 6.28e+04       |
--------------------------------------------
Eval num_timesteps=6000, episode_reward=483.75 +/- 731.83
Episode length: 36.26 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 484      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 403      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 6        |
|    time_elapsed    | 22       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=529.97 +/- 728.37
Episode length: 36.58 +/- 6.15
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 530           |
| time/                   |               |
|    total_timesteps      | 6500          |
| train/                  |               |
|    approx_kl            | 2.1536835e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000198     |
|    explained_variance   | -0.114        |
|    learning_rate        | 0.001         |
|    loss                 | 2.8e+04       |
|    n_updates            | 2920          |
|    policy_gradient_loss | 3.27e-06      |
|    value_loss           | 8.05e+04      |
-------------------------------------------
Eval num_timesteps=7000, episode_reward=519.74 +/- 749.17
Episode length: 36.00 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 520      |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 336      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 7        |
|    time_elapsed    | 26       |
|    total_timesteps | 7168     |
---------------------------------
Eval num_timesteps=7500, episode_reward=426.79 +/- 768.77
Episode length: 34.34 +/- 6.72
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 34.3      |
|    mean_reward          | 427       |
| time/                   |           |
|    total_timesteps      | 7500      |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.000147 |
|    explained_variance   | -0.0571   |
|    learning_rate        | 0.001     |
|    loss                 | 1.2e+04   |
|    n_updates            | 2930      |
|    policy_gradient_loss | 3.22e-06  |
|    value_loss           | 5.95e+04  |
---------------------------------------
Eval num_timesteps=8000, episode_reward=253.50 +/- 667.02
Episode length: 33.82 +/- 7.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 254      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 353      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 8        |
|    time_elapsed    | 29       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=518.10 +/- 734.28
Episode length: 35.58 +/- 7.59
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.6           |
|    mean_reward          | 518            |
| time/                   |                |
|    total_timesteps      | 8500           |
| train/                  |                |
|    approx_kl            | -6.9849193e-10 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.27e-05      |
|    explained_variance   | -0.128         |
|    learning_rate        | 0.001          |
|    loss                 | 1.74e+04       |
|    n_updates            | 2940           |
|    policy_gradient_loss | 2.41e-06       |
|    value_loss           | 6.75e+04       |
--------------------------------------------
Eval num_timesteps=9000, episode_reward=507.97 +/- 797.18
Episode length: 34.92 +/- 7.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 508      |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 9        |
|    time_elapsed    | 33       |
|    total_timesteps | 9216     |
---------------------------------
Eval num_timesteps=9500, episode_reward=353.56 +/- 710.72
Episode length: 33.84 +/- 7.99
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.8           |
|    mean_reward          | 354            |
| time/                   |                |
|    total_timesteps      | 9500           |
| train/                  |                |
|    approx_kl            | -1.0477379e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -3.18e-05      |
|    explained_variance   | -0.237         |
|    learning_rate        | 0.001          |
|    loss                 | 1.91e+04       |
|    n_updates            | 2950           |
|    policy_gradient_loss | 2.42e-06       |
|    value_loss           | 7.47e+04       |
--------------------------------------------
Eval num_timesteps=10000, episode_reward=515.49 +/- 807.99
Episode length: 36.18 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 515      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 297      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 10       |
|    time_elapsed    | 36       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=447.43 +/- 717.37
Episode length: 35.42 +/- 6.43
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.4           |
|    mean_reward          | 447            |
| time/                   |                |
|    total_timesteps      | 10500          |
| train/                  |                |
|    approx_kl            | -1.2223609e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -4.23e-05      |
|    explained_variance   | -0.0657        |
|    learning_rate        | 0.001          |
|    loss                 | 3.09e+04       |
|    n_updates            | 2960           |
|    policy_gradient_loss | 1.52e-06       |
|    value_loss           | 8.05e+04       |
--------------------------------------------
Eval num_timesteps=11000, episode_reward=251.71 +/- 568.23
Episode length: 34.84 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 252      |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 337      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 11       |
|    time_elapsed    | 40       |
|    total_timesteps | 11264    |
---------------------------------
Eval num_timesteps=11500, episode_reward=483.01 +/- 713.14
Episode length: 35.80 +/- 7.26
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 35.8           |
|    mean_reward          | 483            |
| time/                   |                |
|    total_timesteps      | 11500          |
| train/                  |                |
|    approx_kl            | -1.2223609e-09 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -9.65e-05      |
|    explained_variance   | -0.0266        |
|    learning_rate        | 0.001          |
|    loss                 | 1.29e+04       |
|    n_updates            | 2970           |
|    policy_gradient_loss | 2.1e-06        |
|    value_loss           | 6.38e+04       |
--------------------------------------------
Eval num_timesteps=12000, episode_reward=482.51 +/- 746.45
Episode length: 36.00 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 483      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 400      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 12       |
|    time_elapsed    | 44       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=266.35 +/- 649.31
Episode length: 33.90 +/- 6.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.9          |
|    mean_reward          | 266           |
| time/                   |               |
|    total_timesteps      | 12500         |
| train/                  |               |
|    approx_kl            | 1.6472768e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000452     |
|    explained_variance   | 0.032         |
|    learning_rate        | 0.001         |
|    loss                 | 1.96e+04      |
|    n_updates            | 2980          |
|    policy_gradient_loss | 6.86e-06      |
|    value_loss           | 7.53e+04      |
-------------------------------------------
Eval num_timesteps=13000, episode_reward=529.99 +/- 719.48
Episode length: 36.80 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 530      |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 442      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 13       |
|    time_elapsed    | 47       |
|    total_timesteps | 13312    |
---------------------------------
Eval num_timesteps=13500, episode_reward=424.79 +/- 715.23
Episode length: 35.84 +/- 6.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 425           |
| time/                   |               |
|    total_timesteps      | 13500         |
| train/                  |               |
|    approx_kl            | 3.4749974e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000892     |
|    explained_variance   | -0.323        |
|    learning_rate        | 0.001         |
|    loss                 | 1.95e+04      |
|    n_updates            | 2990          |
|    policy_gradient_loss | 4.78e-06      |
|    value_loss           | 7.89e+04      |
-------------------------------------------
Eval num_timesteps=14000, episode_reward=427.52 +/- 739.07
Episode length: 34.86 +/- 7.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 424      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 14       |
|    time_elapsed    | 51       |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=535.39 +/- 774.06
Episode length: 35.40 +/- 6.25
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 535           |
| time/                   |               |
|    total_timesteps      | 14500         |
| train/                  |               |
|    approx_kl            | 1.6880222e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000195     |
|    explained_variance   | -0.00225      |
|    learning_rate        | 0.001         |
|    loss                 | 1.55e+04      |
|    n_updates            | 3000          |
|    policy_gradient_loss | 4.02e-06      |
|    value_loss           | 6.18e+04      |
-------------------------------------------
Eval num_timesteps=15000, episode_reward=499.69 +/- 676.79
Episode length: 36.14 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 500      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 430      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 15       |
|    time_elapsed    | 55       |
|    total_timesteps | 15360    |
---------------------------------
Eval num_timesteps=15500, episode_reward=483.08 +/- 715.62
Episode length: 36.02 +/- 5.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 483           |
| time/                   |               |
|    total_timesteps      | 15500         |
| train/                  |               |
|    approx_kl            | 3.8999133e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000181     |
|    explained_variance   | -0.161        |
|    learning_rate        | 0.001         |
|    loss                 | 3.11e+04      |
|    n_updates            | 3010          |
|    policy_gradient_loss | 7e-06         |
|    value_loss           | 9.02e+04      |
-------------------------------------------
Eval num_timesteps=16000, episode_reward=344.09 +/- 708.68
Episode length: 34.34 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 344      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 385      |
| time/              |          |
|    fps             | 278      |
|    iterations      | 16       |
|    time_elapsed    | 58       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=290.80 +/- 667.78
Episode length: 34.16 +/- 6.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 291           |
| time/                   |               |
|    total_timesteps      | 16500         |
| train/                  |               |
|    approx_kl            | 4.0745363e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000167     |
|    explained_variance   | -0.0881       |
|    learning_rate        | 0.001         |
|    loss                 | 2.76e+04      |
|    n_updates            | 3020          |
|    policy_gradient_loss | 3.19e-06      |
|    value_loss           | 9.71e+04      |
-------------------------------------------
Eval num_timesteps=17000, episode_reward=496.62 +/- 700.84
Episode length: 36.38 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 17       |
|    time_elapsed    | 62       |
|    total_timesteps | 17408    |
---------------------------------
Eval num_timesteps=17500, episode_reward=471.58 +/- 718.10
Episode length: 35.74 +/- 6.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 472          |
| time/                   |              |
|    total_timesteps      | 17500        |
| train/                  |              |
|    approx_kl            | 7.043127e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000553    |
|    explained_variance   | -0.172       |
|    learning_rate        | 0.001        |
|    loss                 | 2.8e+04      |
|    n_updates            | 3030         |
|    policy_gradient_loss | 2.83e-06     |
|    value_loss           | 8.92e+04     |
------------------------------------------
Eval num_timesteps=18000, episode_reward=355.13 +/- 735.22
Episode length: 33.44 +/- 7.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 355      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 381      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 18       |
|    time_elapsed    | 65       |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=539.66 +/- 731.56
Episode length: 36.22 +/- 6.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.2          |
|    mean_reward          | 540           |
| time/                   |               |
|    total_timesteps      | 18500         |
| train/                  |               |
|    approx_kl            | 1.4551915e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00101      |
|    explained_variance   | -0.191        |
|    learning_rate        | 0.001         |
|    loss                 | 2.69e+04      |
|    n_updates            | 3040          |
|    policy_gradient_loss | 1.13e-05      |
|    value_loss           | 8.52e+04      |
-------------------------------------------
Eval num_timesteps=19000, episode_reward=268.56 +/- 647.00
Episode length: 34.06 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 269      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 19       |
|    time_elapsed    | 69       |
|    total_timesteps | 19456    |
---------------------------------
Eval num_timesteps=19500, episode_reward=565.09 +/- 812.14
Episode length: 36.82 +/- 6.44
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.8          |
|    mean_reward          | 565           |
| time/                   |               |
|    total_timesteps      | 19500         |
| train/                  |               |
|    approx_kl            | -5.820766e-11 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000179     |
|    explained_variance   | -0.0451       |
|    learning_rate        | 0.001         |
|    loss                 | 2.04e+04      |
|    n_updates            | 3050          |
|    policy_gradient_loss | 5.35e-06      |
|    value_loss           | 7.9e+04       |
-------------------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=338.77 +/- 668.68
Episode length: 34.26 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 424      |
| time/              |          |
|    fps             | 279      |
|    iterations      | 20       |
|    time_elapsed    | 73       |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=474.01 +/- 780.93
Episode length: 35.84 +/- 6.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 474          |
| time/                   |              |
|    total_timesteps      | 20500        |
| train/                  |              |
|    approx_kl            | 5.820766e-11 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000205    |
|    explained_variance   | -0.0979      |
|    learning_rate        | 0.001        |
|    loss                 | 2.63e+04     |
|    n_updates            | 3060         |
|    policy_gradient_loss | 3.58e-06     |
|    value_loss           | 7.91e+04     |
------------------------------------------
Eval num_timesteps=21000, episode_reward=132.26 +/- 460.29
Episode length: 33.32 +/- 6.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 132      |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=447.15 +/- 687.33
Episode length: 35.78 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 446      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 21       |
|    time_elapsed    | 77       |
|    total_timesteps | 21504    |
---------------------------------
Eval num_timesteps=22000, episode_reward=593.69 +/- 763.85
Episode length: 37.24 +/- 5.73
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.2          |
|    mean_reward          | 594           |
| time/                   |               |
|    total_timesteps      | 22000         |
| train/                  |               |
|    approx_kl            | 3.3178367e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00022      |
|    explained_variance   | -0.0923       |
|    learning_rate        | 0.001         |
|    loss                 | 2.75e+04      |
|    n_updates            | 3070          |
|    policy_gradient_loss | 5.43e-06      |
|    value_loss           | 8.66e+04      |
-------------------------------------------
New best mean reward!
Eval num_timesteps=22500, episode_reward=523.08 +/- 674.14
Episode length: 36.70 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 523      |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 495      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 22       |
|    time_elapsed    | 81       |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=430.79 +/- 717.61
Episode length: 35.44 +/- 6.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 431           |
| time/                   |               |
|    total_timesteps      | 23000         |
| train/                  |               |
|    approx_kl            | 3.4924597e-10 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000224     |
|    explained_variance   | -0.0881       |
|    learning_rate        | 0.001         |
|    loss                 | 2.58e+04      |
|    n_updates            | 3080          |
|    policy_gradient_loss | 6.78e-06      |
|    value_loss           | 7.83e+04      |
-------------------------------------------
Eval num_timesteps=23500, episode_reward=440.42 +/- 735.74
Episode length: 35.18 +/- 6.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 440      |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 460      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 23       |
|    time_elapsed    | 85       |
|    total_timesteps | 23552    |
---------------------------------
Eval num_timesteps=24000, episode_reward=464.68 +/- 703.41
Episode length: 36.22 +/- 6.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.2         |
|    mean_reward          | 465          |
| time/                   |              |
|    total_timesteps      | 24000        |
| train/                  |              |
|    approx_kl            | 2.910383e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000142    |
|    explained_variance   | -0.0249      |
|    learning_rate        | 0.001        |
|    loss                 | 5e+04        |
|    n_updates            | 3090         |
|    policy_gradient_loss | 4.84e-06     |
|    value_loss           | 1.07e+05     |
------------------------------------------
Eval num_timesteps=24500, episode_reward=347.59 +/- 625.43
Episode length: 35.64 +/- 5.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 348      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 487      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 24       |
|    time_elapsed    | 89       |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=470.52 +/- 697.27
Episode length: 36.50 +/- 5.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.5          |
|    mean_reward          | 471           |
| time/                   |               |
|    total_timesteps      | 25000         |
| train/                  |               |
|    approx_kl            | 2.1338928e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000412     |
|    explained_variance   | -0.211        |
|    learning_rate        | 0.001         |
|    loss                 | 1.94e+04      |
|    n_updates            | 3100          |
|    policy_gradient_loss | 5.91e-06      |
|    value_loss           | 8.02e+04      |
-------------------------------------------
Eval num_timesteps=25500, episode_reward=499.27 +/- 711.19
Episode length: 36.10 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 499      |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 509      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 25       |
|    time_elapsed    | 92       |
|    total_timesteps | 25600    |
---------------------------------
Eval num_timesteps=26000, episode_reward=282.67 +/- 621.74
Episode length: 34.48 +/- 6.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 283           |
| time/                   |               |
|    total_timesteps      | 26000         |
| train/                  |               |
|    approx_kl            | 1.2514647e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000294     |
|    explained_variance   | -0.058        |
|    learning_rate        | 0.001         |
|    loss                 | 3.73e+04      |
|    n_updates            | 3110          |
|    policy_gradient_loss | 5.83e-06      |
|    value_loss           | 9.08e+04      |
-------------------------------------------
Eval num_timesteps=26500, episode_reward=420.91 +/- 729.67
Episode length: 35.16 +/- 7.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 510      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 26       |
|    time_elapsed    | 96       |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=502.05 +/- 756.94
Episode length: 35.96 +/- 7.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 502           |
| time/                   |               |
|    total_timesteps      | 27000         |
| train/                  |               |
|    approx_kl            | 6.8102963e-09 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000605     |
|    explained_variance   | -0.161        |
|    learning_rate        | 0.001         |
|    loss                 | 3.42e+04      |
|    n_updates            | 3120          |
|    policy_gradient_loss | 4.61e-06      |
|    value_loss           | 9.03e+04      |
-------------------------------------------
Eval num_timesteps=27500, episode_reward=327.82 +/- 721.42
Episode length: 33.92 +/- 6.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 328      |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 510      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 27       |
|    time_elapsed    | 100      |
|    total_timesteps | 27648    |
---------------------------------
Eval num_timesteps=28000, episode_reward=486.24 +/- 826.18
Episode length: 34.58 +/- 7.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 486          |
| time/                   |              |
|    total_timesteps      | 28000        |
| train/                  |              |
|    approx_kl            | 8.556526e-09 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000395    |
|    explained_variance   | -0.00324     |
|    learning_rate        | 0.001        |
|    loss                 | 2.48e+04     |
|    n_updates            | 3130         |
|    policy_gradient_loss | 6.18e-06     |
|    value_loss           | 8.5e+04      |
------------------------------------------
Eval num_timesteps=28500, episode_reward=457.15 +/- 700.58
Episode length: 35.72 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 451      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 28       |
|    time_elapsed    | 103      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=341.72 +/- 618.94
Episode length: 35.24 +/- 5.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 342          |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 7.641502e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00782     |
|    explained_variance   | -0.126       |
|    learning_rate        | 0.001        |
|    loss                 | 2.41e+04     |
|    n_updates            | 3140         |
|    policy_gradient_loss | -8.8e-05     |
|    value_loss           | 7.22e+04     |
------------------------------------------
Eval num_timesteps=29500, episode_reward=536.20 +/- 753.78
Episode length: 36.78 +/- 5.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 536      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 364      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 29       |
|    time_elapsed    | 107      |
|    total_timesteps | 29696    |
---------------------------------
Eval num_timesteps=30000, episode_reward=363.44 +/- 674.80
Episode length: 34.88 +/- 6.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 363          |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 6.641494e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00106     |
|    explained_variance   | 0.0343       |
|    learning_rate        | 0.001        |
|    loss                 | 2.98e+04     |
|    n_updates            | 3150         |
|    policy_gradient_loss | 2.16e-05     |
|    value_loss           | 7.28e+04     |
------------------------------------------
Eval num_timesteps=30500, episode_reward=248.88 +/- 559.96
Episode length: 34.58 +/- 5.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 249      |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 425      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 30       |
|    time_elapsed    | 111      |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=474.02 +/- 720.31
Episode length: 36.02 +/- 6.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 474          |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 2.264278e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00158     |
|    explained_variance   | -0.117       |
|    learning_rate        | 0.001        |
|    loss                 | 3.35e+04     |
|    n_updates            | 3160         |
|    policy_gradient_loss | -1.92e-08    |
|    value_loss           | 8.43e+04     |
------------------------------------------
Eval num_timesteps=31500, episode_reward=543.04 +/- 796.56
Episode length: 35.10 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 543      |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 454      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 31       |
|    time_elapsed    | 114      |
|    total_timesteps | 31744    |
---------------------------------
Eval num_timesteps=32000, episode_reward=414.45 +/- 722.82
Episode length: 36.30 +/- 5.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 414          |
| time/                   |              |
|    total_timesteps      | 32000        |
| train/                  |              |
|    approx_kl            | 7.631024e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00249     |
|    explained_variance   | -0.134       |
|    learning_rate        | 0.001        |
|    loss                 | 3.38e+04     |
|    n_updates            | 3170         |
|    policy_gradient_loss | 1.78e-05     |
|    value_loss           | 9.27e+04     |
------------------------------------------
Eval num_timesteps=32500, episode_reward=495.26 +/- 765.15
Episode length: 36.14 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 532      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 32       |
|    time_elapsed    | 118      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=375.27 +/- 623.62
Episode length: 35.50 +/- 5.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.5         |
|    mean_reward          | 375          |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0011272144 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0096      |
|    explained_variance   | -0.189       |
|    learning_rate        | 0.001        |
|    loss                 | 4.05e+04     |
|    n_updates            | 3180         |
|    policy_gradient_loss | 0.00174      |
|    value_loss           | 9.72e+04     |
------------------------------------------
Eval num_timesteps=33500, episode_reward=529.65 +/- 722.31
Episode length: 36.18 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 530      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 426      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 33       |
|    time_elapsed    | 122      |
|    total_timesteps | 33792    |
---------------------------------
Eval num_timesteps=34000, episode_reward=261.08 +/- 644.78
Episode length: 33.98 +/- 6.12
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 261          |
| time/                   |              |
|    total_timesteps      | 34000        |
| train/                  |              |
|    approx_kl            | 7.778057e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0225      |
|    explained_variance   | -0.0433      |
|    learning_rate        | 0.001        |
|    loss                 | 2.21e+04     |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.000626    |
|    value_loss           | 7.76e+04     |
------------------------------------------
Eval num_timesteps=34500, episode_reward=321.76 +/- 727.24
Episode length: 34.78 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 322      |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 487      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 34       |
|    time_elapsed    | 125      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=513.29 +/- 751.33
Episode length: 35.46 +/- 6.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 513           |
| time/                   |               |
|    total_timesteps      | 35000         |
| train/                  |               |
|    approx_kl            | 0.00084463245 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00583      |
|    explained_variance   | -0.0829       |
|    learning_rate        | 0.001         |
|    loss                 | 1.66e+04      |
|    n_updates            | 3200          |
|    policy_gradient_loss | 0.000221      |
|    value_loss           | 7.01e+04      |
-------------------------------------------
Eval num_timesteps=35500, episode_reward=538.31 +/- 824.21
Episode length: 35.22 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 538      |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 491      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 35       |
|    time_elapsed    | 129      |
|    total_timesteps | 35840    |
---------------------------------
Eval num_timesteps=36000, episode_reward=548.38 +/- 769.80
Episode length: 36.58 +/- 6.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.6        |
|    mean_reward          | 548         |
| time/                   |             |
|    total_timesteps      | 36000       |
| train/                  |             |
|    approx_kl            | 4.81552e-07 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.000933   |
|    explained_variance   | -0.0933     |
|    learning_rate        | 0.001       |
|    loss                 | 4.02e+04    |
|    n_updates            | 3210        |
|    policy_gradient_loss | -2.57e-05   |
|    value_loss           | 1.02e+05    |
-----------------------------------------
Eval num_timesteps=36500, episode_reward=526.99 +/- 783.83
Episode length: 35.40 +/- 7.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 527      |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 454      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 36       |
|    time_elapsed    | 133      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=359.14 +/- 704.59
Episode length: 34.62 +/- 7.24
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 359          |
| time/                   |              |
|    total_timesteps      | 37000        |
| train/                  |              |
|    approx_kl            | 4.778849e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00302     |
|    explained_variance   | -0.231       |
|    learning_rate        | 0.001        |
|    loss                 | 3.27e+04     |
|    n_updates            | 3220         |
|    policy_gradient_loss | -1.32e-05    |
|    value_loss           | 7.37e+04     |
------------------------------------------
Eval num_timesteps=37500, episode_reward=293.78 +/- 599.09
Episode length: 34.26 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 37       |
|    time_elapsed    | 136      |
|    total_timesteps | 37888    |
---------------------------------
Eval num_timesteps=38000, episode_reward=510.98 +/- 753.41
Episode length: 35.20 +/- 7.85
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 511           |
| time/                   |               |
|    total_timesteps      | 38000         |
| train/                  |               |
|    approx_kl            | 2.3806933e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000892     |
|    explained_variance   | -0.017        |
|    learning_rate        | 0.001         |
|    loss                 | 2.46e+04      |
|    n_updates            | 3230          |
|    policy_gradient_loss | 3.17e-05      |
|    value_loss           | 8.25e+04      |
-------------------------------------------
Eval num_timesteps=38500, episode_reward=224.29 +/- 629.40
Episode length: 33.92 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 224      |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 278      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 38       |
|    time_elapsed    | 140      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=533.98 +/- 757.70
Episode length: 35.96 +/- 6.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 534          |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0007348981 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.121       |
|    explained_variance   | -0.198       |
|    learning_rate        | 0.001        |
|    loss                 | 2.15e+04     |
|    n_updates            | 3240         |
|    policy_gradient_loss | -0.00397     |
|    value_loss           | 6.67e+04     |
------------------------------------------
Eval num_timesteps=39500, episode_reward=405.53 +/- 749.78
Episode length: 34.30 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 406      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 282      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 39       |
|    time_elapsed    | 143      |
|    total_timesteps | 39936    |
---------------------------------
Eval num_timesteps=40000, episode_reward=389.26 +/- 745.77
Episode length: 34.22 +/- 6.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.2       |
|    mean_reward          | 389        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.02886823 |
|    clip_fraction        | 0.0243     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.103     |
|    explained_variance   | -0.00791   |
|    learning_rate        | 0.001      |
|    loss                 | 2.09e+04   |
|    n_updates            | 3250       |
|    policy_gradient_loss | 0.00174    |
|    value_loss           | 6.49e+04   |
----------------------------------------
Eval num_timesteps=40500, episode_reward=378.74 +/- 710.61
Episode length: 35.26 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 379      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 312      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 40       |
|    time_elapsed    | 147      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=641.60 +/- 775.55
Episode length: 36.46 +/- 6.26
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.5        |
|    mean_reward          | 642         |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.023992173 |
|    clip_fraction        | 0.00566     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00642    |
|    explained_variance   | 0.0686      |
|    learning_rate        | 0.001       |
|    loss                 | 2.86e+04    |
|    n_updates            | 3260        |
|    policy_gradient_loss | 0.00266     |
|    value_loss           | 8.35e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=41500, episode_reward=477.22 +/- 790.77
Episode length: 35.22 +/- 7.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 370      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 41       |
|    time_elapsed    | 151      |
|    total_timesteps | 41984    |
---------------------------------
Eval num_timesteps=42000, episode_reward=587.97 +/- 794.60
Episode length: 35.82 +/- 6.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 588          |
| time/                   |              |
|    total_timesteps      | 42000        |
| train/                  |              |
|    approx_kl            | 1.815788e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0337      |
|    explained_variance   | 0.000208     |
|    learning_rate        | 0.001        |
|    loss                 | 2.22e+04     |
|    n_updates            | 3270         |
|    policy_gradient_loss | -0.000337    |
|    value_loss           | 8.68e+04     |
------------------------------------------
Eval num_timesteps=42500, episode_reward=497.34 +/- 763.78
Episode length: 35.08 +/- 8.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=360.65 +/- 683.08
Episode length: 34.78 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 378      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 42       |
|    time_elapsed    | 156      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=428.74 +/- 773.95
Episode length: 34.74 +/- 7.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 429           |
| time/                   |               |
|    total_timesteps      | 43500         |
| train/                  |               |
|    approx_kl            | 1.2040371e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0324       |
|    explained_variance   | -0.115        |
|    learning_rate        | 0.001         |
|    loss                 | 2.16e+04      |
|    n_updates            | 3280          |
|    policy_gradient_loss | -0.000268     |
|    value_loss           | 7.33e+04      |
-------------------------------------------
Eval num_timesteps=44000, episode_reward=370.56 +/- 678.86
Episode length: 35.40 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 318      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 43       |
|    time_elapsed    | 159      |
|    total_timesteps | 44032    |
---------------------------------
Eval num_timesteps=44500, episode_reward=450.67 +/- 614.35
Episode length: 36.98 +/- 4.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37           |
|    mean_reward          | 451          |
| time/                   |              |
|    total_timesteps      | 44500        |
| train/                  |              |
|    approx_kl            | 0.0017685083 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0232      |
|    explained_variance   | -0.178       |
|    learning_rate        | 0.001        |
|    loss                 | 2e+04        |
|    n_updates            | 3290         |
|    policy_gradient_loss | 1.9e-06      |
|    value_loss           | 6.77e+04     |
------------------------------------------
Eval num_timesteps=45000, episode_reward=359.43 +/- 714.92
Episode length: 34.30 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 310      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 44       |
|    time_elapsed    | 163      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=342.15 +/- 662.82
Episode length: 35.24 +/- 6.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 342           |
| time/                   |               |
|    total_timesteps      | 45500         |
| train/                  |               |
|    approx_kl            | 2.8404174e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00951      |
|    explained_variance   | -0.0488       |
|    learning_rate        | 0.001         |
|    loss                 | 2.54e+04      |
|    n_updates            | 3300          |
|    policy_gradient_loss | -6.51e-05     |
|    value_loss           | 8.98e+04      |
-------------------------------------------
Eval num_timesteps=46000, episode_reward=332.90 +/- 664.91
Episode length: 34.26 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 333      |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 419      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 45       |
|    time_elapsed    | 167      |
|    total_timesteps | 46080    |
---------------------------------
Eval num_timesteps=46500, episode_reward=464.71 +/- 701.42
Episode length: 35.38 +/- 7.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 465          |
| time/                   |              |
|    total_timesteps      | 46500        |
| train/                  |              |
|    approx_kl            | 0.0027260075 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00504     |
|    explained_variance   | 0.00408      |
|    learning_rate        | 0.001        |
|    loss                 | 3.24e+04     |
|    n_updates            | 3310         |
|    policy_gradient_loss | 0.00012      |
|    value_loss           | 8.72e+04     |
------------------------------------------
Eval num_timesteps=47000, episode_reward=265.63 +/- 541.20
Episode length: 35.22 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 46       |
|    time_elapsed    | 170      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=485.17 +/- 729.58
Episode length: 35.50 +/- 6.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.5         |
|    mean_reward          | 485          |
| time/                   |              |
|    total_timesteps      | 47500        |
| train/                  |              |
|    approx_kl            | 3.160676e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00431     |
|    explained_variance   | -0.0254      |
|    learning_rate        | 0.001        |
|    loss                 | 3.27e+04     |
|    n_updates            | 3320         |
|    policy_gradient_loss | -5.53e-05    |
|    value_loss           | 8.5e+04      |
------------------------------------------
Eval num_timesteps=48000, episode_reward=288.20 +/- 633.83
Episode length: 34.50 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 502      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 47       |
|    time_elapsed    | 174      |
|    total_timesteps | 48128    |
---------------------------------
Eval num_timesteps=48500, episode_reward=464.90 +/- 806.33
Episode length: 35.52 +/- 6.83
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 465           |
| time/                   |               |
|    total_timesteps      | 48500         |
| train/                  |               |
|    approx_kl            | 3.2868702e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00692      |
|    explained_variance   | -0.0342       |
|    learning_rate        | 0.001         |
|    loss                 | 3.7e+04       |
|    n_updates            | 3330          |
|    policy_gradient_loss | -7.9e-05      |
|    value_loss           | 9.04e+04      |
-------------------------------------------
Eval num_timesteps=49000, episode_reward=549.42 +/- 759.05
Episode length: 36.14 +/- 6.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 505      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 48       |
|    time_elapsed    | 177      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=544.67 +/- 706.17
Episode length: 36.60 +/- 5.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 545           |
| time/                   |               |
|    total_timesteps      | 49500         |
| train/                  |               |
|    approx_kl            | 1.9031577e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0114       |
|    explained_variance   | 0.0444        |
|    learning_rate        | 0.001         |
|    loss                 | 2.18e+04      |
|    n_updates            | 3340          |
|    policy_gradient_loss | -4.62e-06     |
|    value_loss           | 8.49e+04      |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=337.58 +/- 691.64
Episode length: 34.00 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 338      |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 506      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 49       |
|    time_elapsed    | 181      |
|    total_timesteps | 50176    |
---------------------------------
Eval num_timesteps=50500, episode_reward=365.20 +/- 665.96
Episode length: 35.54 +/- 6.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.5        |
|    mean_reward          | 365         |
| time/                   |             |
|    total_timesteps      | 50500       |
| train/                  |             |
|    approx_kl            | 0.001618722 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00569    |
|    explained_variance   | -0.0754     |
|    learning_rate        | 0.001       |
|    loss                 | 2.81e+04    |
|    n_updates            | 3350        |
|    policy_gradient_loss | -0.000336   |
|    value_loss           | 8.85e+04    |
-----------------------------------------
Eval num_timesteps=51000, episode_reward=333.77 +/- 600.02
Episode length: 34.88 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 334      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 457      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 50       |
|    time_elapsed    | 185      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=375.85 +/- 661.11
Episode length: 35.52 +/- 6.31
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 376           |
| time/                   |               |
|    total_timesteps      | 51500         |
| train/                  |               |
|    approx_kl            | 4.0149316e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0118       |
|    explained_variance   | -0.0334       |
|    learning_rate        | 0.001         |
|    loss                 | 2.54e+04      |
|    n_updates            | 3360          |
|    policy_gradient_loss | -0.000121     |
|    value_loss           | 9.76e+04      |
-------------------------------------------
Eval num_timesteps=52000, episode_reward=629.20 +/- 785.50
Episode length: 37.10 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.1     |
|    mean_reward     | 629      |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 405      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 51       |
|    time_elapsed    | 188      |
|    total_timesteps | 52224    |
---------------------------------
Eval num_timesteps=52500, episode_reward=473.28 +/- 692.44
Episode length: 35.96 +/- 6.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 473          |
| time/                   |              |
|    total_timesteps      | 52500        |
| train/                  |              |
|    approx_kl            | 0.0018972782 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0193      |
|    explained_variance   | -0.175       |
|    learning_rate        | 0.001        |
|    loss                 | 3.28e+04     |
|    n_updates            | 3370         |
|    policy_gradient_loss | -0.000376    |
|    value_loss           | 1.02e+05     |
------------------------------------------
Eval num_timesteps=53000, episode_reward=350.90 +/- 641.13
Episode length: 36.22 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 351      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 413      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 52       |
|    time_elapsed    | 192      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=403.22 +/- 747.67
Episode length: 35.06 +/- 6.61
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 403           |
| time/                   |               |
|    total_timesteps      | 53500         |
| train/                  |               |
|    approx_kl            | 3.4753524e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00832      |
|    explained_variance   | -0.178        |
|    learning_rate        | 0.001         |
|    loss                 | 2.35e+04      |
|    n_updates            | 3380          |
|    policy_gradient_loss | 0.000109      |
|    value_loss           | 8.72e+04      |
-------------------------------------------
Eval num_timesteps=54000, episode_reward=415.37 +/- 718.23
Episode length: 34.78 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 445      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 53       |
|    time_elapsed    | 196      |
|    total_timesteps | 54272    |
---------------------------------
Eval num_timesteps=54500, episode_reward=346.04 +/- 727.97
Episode length: 34.22 +/- 7.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 346           |
| time/                   |               |
|    total_timesteps      | 54500         |
| train/                  |               |
|    approx_kl            | 0.00019180408 |
|    clip_fraction        | 0.00146       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0261       |
|    explained_variance   | -0.252        |
|    learning_rate        | 0.001         |
|    loss                 | 4.78e+04      |
|    n_updates            | 3390          |
|    policy_gradient_loss | 0.00137       |
|    value_loss           | 8.51e+04      |
-------------------------------------------
Eval num_timesteps=55000, episode_reward=486.52 +/- 745.22
Episode length: 35.72 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 450      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 54       |
|    time_elapsed    | 199      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=350.43 +/- 614.32
Episode length: 35.12 +/- 6.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 350          |
| time/                   |              |
|    total_timesteps      | 55500        |
| train/                  |              |
|    approx_kl            | 0.0039908877 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00856     |
|    explained_variance   | -0.0375      |
|    learning_rate        | 0.001        |
|    loss                 | 4.27e+04     |
|    n_updates            | 3400         |
|    policy_gradient_loss | 0.00919      |
|    value_loss           | 8.32e+04     |
------------------------------------------
Eval num_timesteps=56000, episode_reward=462.59 +/- 743.67
Episode length: 34.94 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 545      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 55       |
|    time_elapsed    | 203      |
|    total_timesteps | 56320    |
---------------------------------
Eval num_timesteps=56500, episode_reward=392.87 +/- 672.23
Episode length: 35.22 +/- 6.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 393          |
| time/                   |              |
|    total_timesteps      | 56500        |
| train/                  |              |
|    approx_kl            | 1.909444e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00161     |
|    explained_variance   | -0.15        |
|    learning_rate        | 0.001        |
|    loss                 | 2.95e+04     |
|    n_updates            | 3410         |
|    policy_gradient_loss | -6.29e-06    |
|    value_loss           | 8.77e+04     |
------------------------------------------
Eval num_timesteps=57000, episode_reward=427.88 +/- 731.97
Episode length: 34.30 +/- 7.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 493      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 56       |
|    time_elapsed    | 207      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=328.59 +/- 724.01
Episode length: 33.38 +/- 7.11
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.4         |
|    mean_reward          | 329          |
| time/                   |              |
|    total_timesteps      | 57500        |
| train/                  |              |
|    approx_kl            | 5.676411e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00203     |
|    explained_variance   | -0.22        |
|    learning_rate        | 0.001        |
|    loss                 | 2.76e+04     |
|    n_updates            | 3420         |
|    policy_gradient_loss | -0.000164    |
|    value_loss           | 7.52e+04     |
------------------------------------------
Eval num_timesteps=58000, episode_reward=602.59 +/- 795.51
Episode length: 36.20 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 603      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 553      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 57       |
|    time_elapsed    | 210      |
|    total_timesteps | 58368    |
---------------------------------
Eval num_timesteps=58500, episode_reward=549.76 +/- 763.46
Episode length: 35.82 +/- 6.73
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 550          |
| time/                   |              |
|    total_timesteps      | 58500        |
| train/                  |              |
|    approx_kl            | 3.388035e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00126     |
|    explained_variance   | 0.0154       |
|    learning_rate        | 0.001        |
|    loss                 | 2.5e+04      |
|    n_updates            | 3430         |
|    policy_gradient_loss | -0.000118    |
|    value_loss           | 9.26e+04     |
------------------------------------------
Eval num_timesteps=59000, episode_reward=345.61 +/- 687.96
Episode length: 34.34 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 513      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 58       |
|    time_elapsed    | 214      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=406.78 +/- 658.42
Episode length: 35.78 +/- 6.34
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 407          |
| time/                   |              |
|    total_timesteps      | 59500        |
| train/                  |              |
|    approx_kl            | 0.0006150794 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00304     |
|    explained_variance   | -0.0975      |
|    learning_rate        | 0.001        |
|    loss                 | 3.79e+04     |
|    n_updates            | 3440         |
|    policy_gradient_loss | -0.000276    |
|    value_loss           | 8.45e+04     |
------------------------------------------
Eval num_timesteps=60000, episode_reward=471.91 +/- 735.49
Episode length: 35.72 +/- 7.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 453      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 59       |
|    time_elapsed    | 218      |
|    total_timesteps | 60416    |
---------------------------------
Eval num_timesteps=60500, episode_reward=466.32 +/- 762.44
Episode length: 35.12 +/- 7.08
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 466          |
| time/                   |              |
|    total_timesteps      | 60500        |
| train/                  |              |
|    approx_kl            | 6.656628e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000761    |
|    explained_variance   | -0.0354      |
|    learning_rate        | 0.001        |
|    loss                 | 4e+04        |
|    n_updates            | 3450         |
|    policy_gradient_loss | -8.13e-05    |
|    value_loss           | 9.42e+04     |
------------------------------------------
Eval num_timesteps=61000, episode_reward=274.60 +/- 655.33
Episode length: 33.82 +/- 7.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 275      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 400      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 60       |
|    time_elapsed    | 221      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=505.77 +/- 814.18
Episode length: 35.00 +/- 7.46
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 506          |
| time/                   |              |
|    total_timesteps      | 61500        |
| train/                  |              |
|    approx_kl            | 7.607741e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00058     |
|    explained_variance   | 0.0114       |
|    learning_rate        | 0.001        |
|    loss                 | 2.34e+04     |
|    n_updates            | 3460         |
|    policy_gradient_loss | -2.82e-05    |
|    value_loss           | 7.35e+04     |
------------------------------------------
Eval num_timesteps=62000, episode_reward=480.66 +/- 737.26
Episode length: 35.74 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 481      |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 369      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 61       |
|    time_elapsed    | 225      |
|    total_timesteps | 62464    |
---------------------------------
Eval num_timesteps=62500, episode_reward=439.39 +/- 757.53
Episode length: 34.72 +/- 6.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 439          |
| time/                   |              |
|    total_timesteps      | 62500        |
| train/                  |              |
|    approx_kl            | 9.211642e-05 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.002       |
|    explained_variance   | 0.027        |
|    learning_rate        | 0.001        |
|    loss                 | 1.87e+04     |
|    n_updates            | 3470         |
|    policy_gradient_loss | -0.000314    |
|    value_loss           | 6.57e+04     |
------------------------------------------
Eval num_timesteps=63000, episode_reward=522.37 +/- 773.65
Episode length: 35.64 +/- 7.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 522      |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 62       |
|    time_elapsed    | 229      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=215.43 +/- 573.50
Episode length: 33.78 +/- 6.54
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 33.8           |
|    mean_reward          | 215            |
| time/                   |                |
|    total_timesteps      | 63500          |
| train/                  |                |
|    approx_kl            | 1.49371335e-05 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.00226       |
|    explained_variance   | -0.0654        |
|    learning_rate        | 0.001          |
|    loss                 | 3.19e+04       |
|    n_updates            | 3480           |
|    policy_gradient_loss | -0.000142      |
|    value_loss           | 7.07e+04       |
--------------------------------------------
Eval num_timesteps=64000, episode_reward=425.63 +/- 665.14
Episode length: 36.16 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=390.29 +/- 635.97
Episode length: 35.54 +/- 6.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 390      |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 380      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 63       |
|    time_elapsed    | 234      |
|    total_timesteps | 64512    |
---------------------------------
Eval num_timesteps=65000, episode_reward=394.99 +/- 732.44
Episode length: 34.40 +/- 6.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 395          |
| time/                   |              |
|    total_timesteps      | 65000        |
| train/                  |              |
|    approx_kl            | 0.0014875239 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00195     |
|    explained_variance   | -0.0488      |
|    learning_rate        | 0.001        |
|    loss                 | 3.61e+04     |
|    n_updates            | 3490         |
|    policy_gradient_loss | -0.000155    |
|    value_loss           | 8.48e+04     |
------------------------------------------
Eval num_timesteps=65500, episode_reward=183.24 +/- 550.89
Episode length: 33.26 +/- 6.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 183      |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.5     |
|    ep_rew_mean     | 448      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 64       |
|    time_elapsed    | 237      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=425.13 +/- 731.47
Episode length: 35.38 +/- 6.13
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 425           |
| time/                   |               |
|    total_timesteps      | 66000         |
| train/                  |               |
|    approx_kl            | 1.9912841e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00167      |
|    explained_variance   | 0.00427       |
|    learning_rate        | 0.001         |
|    loss                 | 3.18e+04      |
|    n_updates            | 3500          |
|    policy_gradient_loss | -6.14e-06     |
|    value_loss           | 8.69e+04      |
-------------------------------------------
Eval num_timesteps=66500, episode_reward=265.60 +/- 688.84
Episode length: 33.58 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 473      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 65       |
|    time_elapsed    | 241      |
|    total_timesteps | 66560    |
---------------------------------
Eval num_timesteps=67000, episode_reward=242.85 +/- 557.63
Episode length: 34.56 +/- 6.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 243           |
| time/                   |               |
|    total_timesteps      | 67000         |
| train/                  |               |
|    approx_kl            | 2.0605512e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000776     |
|    explained_variance   | -0.0743       |
|    learning_rate        | 0.001         |
|    loss                 | 2.61e+04      |
|    n_updates            | 3510          |
|    policy_gradient_loss | -3.56e-05     |
|    value_loss           | 7.58e+04      |
-------------------------------------------
Eval num_timesteps=67500, episode_reward=526.36 +/- 752.46
Episode length: 36.20 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 526      |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 458      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 66       |
|    time_elapsed    | 244      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=303.97 +/- 679.63
Episode length: 34.24 +/- 6.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 304           |
| time/                   |               |
|    total_timesteps      | 68000         |
| train/                  |               |
|    approx_kl            | 2.6501948e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00176      |
|    explained_variance   | -0.078        |
|    learning_rate        | 0.001         |
|    loss                 | 3.95e+04      |
|    n_updates            | 3520          |
|    policy_gradient_loss | -5.57e-05     |
|    value_loss           | 7.72e+04      |
-------------------------------------------
Eval num_timesteps=68500, episode_reward=427.90 +/- 757.24
Episode length: 34.92 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 67       |
|    time_elapsed    | 248      |
|    total_timesteps | 68608    |
---------------------------------
Eval num_timesteps=69000, episode_reward=385.64 +/- 681.76
Episode length: 34.72 +/- 7.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 386          |
| time/                   |              |
|    total_timesteps      | 69000        |
| train/                  |              |
|    approx_kl            | 6.466871e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00124     |
|    explained_variance   | -0.153       |
|    learning_rate        | 0.001        |
|    loss                 | 3.04e+04     |
|    n_updates            | 3530         |
|    policy_gradient_loss | -3.73e-05    |
|    value_loss           | 7.66e+04     |
------------------------------------------
Eval num_timesteps=69500, episode_reward=657.72 +/- 822.36
Episode length: 36.20 +/- 7.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 68       |
|    time_elapsed    | 252      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=327.86 +/- 631.63
Episode length: 35.68 +/- 6.05
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 328           |
| time/                   |               |
|    total_timesteps      | 70000         |
| train/                  |               |
|    approx_kl            | 1.8050196e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000504     |
|    explained_variance   | -0.00297      |
|    learning_rate        | 0.001         |
|    loss                 | 3.18e+04      |
|    n_updates            | 3540          |
|    policy_gradient_loss | -4.46e-05     |
|    value_loss           | 9.17e+04      |
-------------------------------------------
Eval num_timesteps=70500, episode_reward=439.26 +/- 689.90
Episode length: 35.46 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 451      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 69       |
|    time_elapsed    | 255      |
|    total_timesteps | 70656    |
---------------------------------
Eval num_timesteps=71000, episode_reward=501.08 +/- 733.90
Episode length: 35.88 +/- 5.86
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 501           |
| time/                   |               |
|    total_timesteps      | 71000         |
| train/                  |               |
|    approx_kl            | 6.1351457e-06 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00217      |
|    explained_variance   | -0.159        |
|    learning_rate        | 0.001         |
|    loss                 | 2.97e+04      |
|    n_updates            | 3550          |
|    policy_gradient_loss | -0.000131     |
|    value_loss           | 8.14e+04      |
-------------------------------------------
Eval num_timesteps=71500, episode_reward=667.90 +/- 789.88
Episode length: 37.04 +/- 6.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 365      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 70       |
|    time_elapsed    | 259      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=485.56 +/- 754.27
Episode length: 35.72 +/- 6.52
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.7         |
|    mean_reward          | 486          |
| time/                   |              |
|    total_timesteps      | 72000        |
| train/                  |              |
|    approx_kl            | 6.984669e-05 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0024      |
|    explained_variance   | -0.123       |
|    learning_rate        | 0.001        |
|    loss                 | 1.16e+04     |
|    n_updates            | 3560         |
|    policy_gradient_loss | -0.000326    |
|    value_loss           | 4.32e+04     |
------------------------------------------
Eval num_timesteps=72500, episode_reward=432.92 +/- 740.30
Episode length: 34.78 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 410      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 71       |
|    time_elapsed    | 263      |
|    total_timesteps | 72704    |
---------------------------------
Eval num_timesteps=73000, episode_reward=316.34 +/- 641.08
Episode length: 34.18 +/- 6.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.2         |
|    mean_reward          | 316          |
| time/                   |              |
|    total_timesteps      | 73000        |
| train/                  |              |
|    approx_kl            | 7.486029e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00136     |
|    explained_variance   | -0.0191      |
|    learning_rate        | 0.001        |
|    loss                 | 2.98e+04     |
|    n_updates            | 3570         |
|    policy_gradient_loss | -0.00023     |
|    value_loss           | 8.24e+04     |
------------------------------------------
Eval num_timesteps=73500, episode_reward=494.04 +/- 751.04
Episode length: 35.38 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 494      |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 72       |
|    time_elapsed    | 266      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=372.66 +/- 659.52
Episode length: 35.56 +/- 5.97
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 373           |
| time/                   |               |
|    total_timesteps      | 74000         |
| train/                  |               |
|    approx_kl            | 4.6638306e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00359      |
|    explained_variance   | 0.0458        |
|    learning_rate        | 0.001         |
|    loss                 | 3e+04         |
|    n_updates            | 3580          |
|    policy_gradient_loss | -0.000154     |
|    value_loss           | 8.86e+04      |
-------------------------------------------
Eval num_timesteps=74500, episode_reward=261.78 +/- 710.80
Episode length: 33.00 +/- 6.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33       |
|    mean_reward     | 262      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 437      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 73       |
|    time_elapsed    | 270      |
|    total_timesteps | 74752    |
---------------------------------
Eval num_timesteps=75000, episode_reward=424.80 +/- 648.29
Episode length: 36.14 +/- 5.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 425           |
| time/                   |               |
|    total_timesteps      | 75000         |
| train/                  |               |
|    approx_kl            | 0.00072019437 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00134      |
|    explained_variance   | -0.0449       |
|    learning_rate        | 0.001         |
|    loss                 | 4.09e+04      |
|    n_updates            | 3590          |
|    policy_gradient_loss | 0.000626      |
|    value_loss           | 9.97e+04      |
-------------------------------------------
Eval num_timesteps=75500, episode_reward=434.60 +/- 751.77
Episode length: 35.32 +/- 7.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 435      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 482      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 74       |
|    time_elapsed    | 273      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=302.79 +/- 610.52
Episode length: 34.18 +/- 6.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.2         |
|    mean_reward          | 303          |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 6.239279e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0016      |
|    explained_variance   | -0.126       |
|    learning_rate        | 0.001        |
|    loss                 | 2.51e+04     |
|    n_updates            | 3600         |
|    policy_gradient_loss | -2.24e-05    |
|    value_loss           | 8.1e+04      |
------------------------------------------
Eval num_timesteps=76500, episode_reward=472.77 +/- 737.09
Episode length: 35.54 +/- 6.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 472      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 75       |
|    time_elapsed    | 277      |
|    total_timesteps | 76800    |
---------------------------------
Eval num_timesteps=77000, episode_reward=457.25 +/- 764.81
Episode length: 35.52 +/- 6.29
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 457           |
| time/                   |               |
|    total_timesteps      | 77000         |
| train/                  |               |
|    approx_kl            | 0.00041700283 |
|    clip_fraction        | 0.000977      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00188      |
|    explained_variance   | -0.0699       |
|    learning_rate        | 0.001         |
|    loss                 | 4.7e+04       |
|    n_updates            | 3610          |
|    policy_gradient_loss | 7.94e-05      |
|    value_loss           | 9.35e+04      |
-------------------------------------------
Eval num_timesteps=77500, episode_reward=362.00 +/- 722.07
Episode length: 35.26 +/- 5.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 362      |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 429      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 76       |
|    time_elapsed    | 281      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=443.63 +/- 818.55
Episode length: 34.50 +/- 7.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.5          |
|    mean_reward          | 444           |
| time/                   |               |
|    total_timesteps      | 78000         |
| train/                  |               |
|    approx_kl            | 5.7916623e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000487     |
|    explained_variance   | -0.252        |
|    learning_rate        | 0.001         |
|    loss                 | 2.63e+04      |
|    n_updates            | 3620          |
|    policy_gradient_loss | -2.69e-05     |
|    value_loss           | 6.41e+04      |
-------------------------------------------
Eval num_timesteps=78500, episode_reward=330.71 +/- 700.47
Episode length: 35.00 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 331      |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 448      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 77       |
|    time_elapsed    | 284      |
|    total_timesteps | 78848    |
---------------------------------
Eval num_timesteps=79000, episode_reward=419.63 +/- 656.22
Episode length: 35.52 +/- 5.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 420           |
| time/                   |               |
|    total_timesteps      | 79000         |
| train/                  |               |
|    approx_kl            | 2.9918738e-08 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000317     |
|    explained_variance   | 0.0809        |
|    learning_rate        | 0.001         |
|    loss                 | 3.41e+04      |
|    n_updates            | 3630          |
|    policy_gradient_loss | -3.55e-06     |
|    value_loss           | 9.84e+04      |
-------------------------------------------
Eval num_timesteps=79500, episode_reward=330.69 +/- 620.21
Episode length: 35.28 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 331      |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 540      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 78       |
|    time_elapsed    | 288      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=505.79 +/- 666.71
Episode length: 37.36 +/- 5.28
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.4          |
|    mean_reward          | 506           |
| time/                   |               |
|    total_timesteps      | 80000         |
| train/                  |               |
|    approx_kl            | 2.1496089e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00235      |
|    explained_variance   | -0.0543       |
|    learning_rate        | 0.001         |
|    loss                 | 3.33e+04      |
|    n_updates            | 3640          |
|    policy_gradient_loss | -6.95e-05     |
|    value_loss           | 8.7e+04       |
-------------------------------------------
Eval num_timesteps=80500, episode_reward=418.49 +/- 761.71
Episode length: 35.32 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 418      |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 501      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 79       |
|    time_elapsed    | 292      |
|    total_timesteps | 80896    |
---------------------------------
Eval num_timesteps=81000, episode_reward=471.71 +/- 756.31
Episode length: 35.90 +/- 6.24
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 472           |
| time/                   |               |
|    total_timesteps      | 81000         |
| train/                  |               |
|    approx_kl            | 2.4656765e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0011       |
|    explained_variance   | -0.0998       |
|    learning_rate        | 0.001         |
|    loss                 | 3.2e+04       |
|    n_updates            | 3650          |
|    policy_gradient_loss | -5.45e-05     |
|    value_loss           | 8.86e+04      |
-------------------------------------------
Eval num_timesteps=81500, episode_reward=418.63 +/- 754.73
Episode length: 34.98 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 536      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 80       |
|    time_elapsed    | 295      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=464.17 +/- 708.01
Episode length: 36.00 +/- 6.03
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36           |
|    mean_reward          | 464          |
| time/                   |              |
|    total_timesteps      | 82000        |
| train/                  |              |
|    approx_kl            | 1.168868e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00353     |
|    explained_variance   | 0.0871       |
|    learning_rate        | 0.001        |
|    loss                 | 3.63e+04     |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.000138    |
|    value_loss           | 1e+05        |
------------------------------------------
Eval num_timesteps=82500, episode_reward=421.16 +/- 745.86
Episode length: 35.10 +/- 6.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 398      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 81       |
|    time_elapsed    | 299      |
|    total_timesteps | 82944    |
---------------------------------
Eval num_timesteps=83000, episode_reward=407.44 +/- 704.80
Episode length: 35.24 +/- 5.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 407          |
| time/                   |              |
|    total_timesteps      | 83000        |
| train/                  |              |
|    approx_kl            | 0.0010749425 |
|    clip_fraction        | 0.00127      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00722     |
|    explained_variance   | -0.114       |
|    learning_rate        | 0.001        |
|    loss                 | 3.24e+04     |
|    n_updates            | 3670         |
|    policy_gradient_loss | -0.000467    |
|    value_loss           | 7e+04        |
------------------------------------------
Eval num_timesteps=83500, episode_reward=261.49 +/- 584.33
Episode length: 34.48 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 314      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 82       |
|    time_elapsed    | 303      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=504.70 +/- 763.89
Episode length: 35.36 +/- 6.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 505          |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 5.251495e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00446     |
|    explained_variance   | -0.0778      |
|    learning_rate        | 0.001        |
|    loss                 | 2.92e+04     |
|    n_updates            | 3680         |
|    policy_gradient_loss | -8.68e-05    |
|    value_loss           | 8.1e+04      |
------------------------------------------
Eval num_timesteps=84500, episode_reward=549.03 +/- 783.13
Episode length: 35.70 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 350      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 83       |
|    time_elapsed    | 306      |
|    total_timesteps | 84992    |
---------------------------------
Eval num_timesteps=85000, episode_reward=431.12 +/- 679.44
Episode length: 35.70 +/- 6.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 431           |
| time/                   |               |
|    total_timesteps      | 85000         |
| train/                  |               |
|    approx_kl            | 2.8350216e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00777      |
|    explained_variance   | -0.122        |
|    learning_rate        | 0.001         |
|    loss                 | 5.03e+04      |
|    n_updates            | 3690          |
|    policy_gradient_loss | -0.000436     |
|    value_loss           | 8.19e+04      |
-------------------------------------------
Eval num_timesteps=85500, episode_reward=533.41 +/- 734.20
Episode length: 36.56 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=390.12 +/- 714.08
Episode length: 34.38 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 390      |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 373      |
| time/              |          |
|    fps             | 275      |
|    iterations      | 84       |
|    time_elapsed    | 311      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=332.27 +/- 633.13
Episode length: 35.62 +/- 6.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 332         |
| time/                   |             |
|    total_timesteps      | 86500       |
| train/                  |             |
|    approx_kl            | 0.000691177 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0162     |
|    explained_variance   | -0.055      |
|    learning_rate        | 0.001       |
|    loss                 | 3.57e+04    |
|    n_updates            | 3700        |
|    policy_gradient_loss | 0.000722    |
|    value_loss           | 9.32e+04    |
-----------------------------------------
Eval num_timesteps=87000, episode_reward=352.18 +/- 681.32
Episode length: 34.98 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 352      |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 485      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 85       |
|    time_elapsed    | 315      |
|    total_timesteps | 87040    |
---------------------------------
Eval num_timesteps=87500, episode_reward=467.08 +/- 719.35
Episode length: 35.30 +/- 6.63
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 467           |
| time/                   |               |
|    total_timesteps      | 87500         |
| train/                  |               |
|    approx_kl            | 9.7849406e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00798      |
|    explained_variance   | -0.0535       |
|    learning_rate        | 0.001         |
|    loss                 | 1.93e+04      |
|    n_updates            | 3710          |
|    policy_gradient_loss | -0.000739     |
|    value_loss           | 8.39e+04      |
-------------------------------------------
Eval num_timesteps=88000, episode_reward=317.23 +/- 687.18
Episode length: 34.08 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 463      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 86       |
|    time_elapsed    | 318      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=156.37 +/- 537.05
Episode length: 31.78 +/- 7.66
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 31.8          |
|    mean_reward          | 156           |
| time/                   |               |
|    total_timesteps      | 88500         |
| train/                  |               |
|    approx_kl            | 1.6499369e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0078       |
|    explained_variance   | -0.228        |
|    learning_rate        | 0.001         |
|    loss                 | 2.16e+04      |
|    n_updates            | 3720          |
|    policy_gradient_loss | -0.000763     |
|    value_loss           | 7.27e+04      |
-------------------------------------------
Eval num_timesteps=89000, episode_reward=414.25 +/- 717.97
Episode length: 34.84 +/- 6.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 567      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 87       |
|    time_elapsed    | 322      |
|    total_timesteps | 89088    |
---------------------------------
Eval num_timesteps=89500, episode_reward=462.36 +/- 698.02
Episode length: 36.56 +/- 6.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 462           |
| time/                   |               |
|    total_timesteps      | 89500         |
| train/                  |               |
|    approx_kl            | 5.2375253e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000886     |
|    explained_variance   | 0.0082        |
|    learning_rate        | 0.001         |
|    loss                 | 3.02e+04      |
|    n_updates            | 3730          |
|    policy_gradient_loss | -4.68e-05     |
|    value_loss           | 9.69e+04      |
-------------------------------------------
Eval num_timesteps=90000, episode_reward=369.13 +/- 726.96
Episode length: 34.70 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 369      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 477      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 88       |
|    time_elapsed    | 326      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=427.80 +/- 724.69
Episode length: 35.46 +/- 6.67
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 428           |
| time/                   |               |
|    total_timesteps      | 90500         |
| train/                  |               |
|    approx_kl            | 1.1518714e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0041       |
|    explained_variance   | -0.122        |
|    learning_rate        | 0.001         |
|    loss                 | 3.33e+04      |
|    n_updates            | 3740          |
|    policy_gradient_loss | -0.000135     |
|    value_loss           | 9.69e+04      |
-------------------------------------------
Eval num_timesteps=91000, episode_reward=380.69 +/- 759.02
Episode length: 33.70 +/- 7.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.7     |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 481      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 89       |
|    time_elapsed    | 329      |
|    total_timesteps | 91136    |
---------------------------------
Eval num_timesteps=91500, episode_reward=501.08 +/- 742.90
Episode length: 35.78 +/- 6.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 501          |
| time/                   |              |
|    total_timesteps      | 91500        |
| train/                  |              |
|    approx_kl            | 5.104812e-08 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000833    |
|    explained_variance   | -0.0158      |
|    learning_rate        | 0.001        |
|    loss                 | 3.17e+04     |
|    n_updates            | 3750         |
|    policy_gradient_loss | -3.46e-05    |
|    value_loss           | 9.94e+04     |
------------------------------------------
Eval num_timesteps=92000, episode_reward=458.39 +/- 752.02
Episode length: 35.24 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 458      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 413      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 90       |
|    time_elapsed    | 333      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=460.42 +/- 732.53
Episode length: 35.58 +/- 6.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.6          |
|    mean_reward          | 460           |
| time/                   |               |
|    total_timesteps      | 92500         |
| train/                  |               |
|    approx_kl            | 5.6946883e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00306      |
|    explained_variance   | -0.175        |
|    learning_rate        | 0.001         |
|    loss                 | 3.25e+04      |
|    n_updates            | 3760          |
|    policy_gradient_loss | -1.35e-06     |
|    value_loss           | 7.6e+04       |
-------------------------------------------
Eval num_timesteps=93000, episode_reward=443.36 +/- 678.86
Episode length: 35.84 +/- 5.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 91       |
|    time_elapsed    | 337      |
|    total_timesteps | 93184    |
---------------------------------
Eval num_timesteps=93500, episode_reward=494.17 +/- 724.63
Episode length: 36.62 +/- 6.02
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 494           |
| time/                   |               |
|    total_timesteps      | 93500         |
| train/                  |               |
|    approx_kl            | 5.2677933e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00033      |
|    explained_variance   | -0.121        |
|    learning_rate        | 0.001         |
|    loss                 | 4.51e+04      |
|    n_updates            | 3770          |
|    policy_gradient_loss | -6.56e-05     |
|    value_loss           | 8.35e+04      |
-------------------------------------------
Eval num_timesteps=94000, episode_reward=273.55 +/- 684.48
Episode length: 33.76 +/- 6.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 456      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 92       |
|    time_elapsed    | 340      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=714.94 +/- 742.58
Episode length: 38.50 +/- 4.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 38.5         |
|    mean_reward          | 715          |
| time/                   |              |
|    total_timesteps      | 94500        |
| train/                  |              |
|    approx_kl            | 8.023344e-07 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00153     |
|    explained_variance   | -0.0134      |
|    learning_rate        | 0.001        |
|    loss                 | 3.84e+04     |
|    n_updates            | 3780         |
|    policy_gradient_loss | -7.9e-05     |
|    value_loss           | 1.02e+05     |
------------------------------------------
New best mean reward!
Eval num_timesteps=95000, episode_reward=398.80 +/- 649.80
Episode length: 35.18 +/- 6.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 399      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 419      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 93       |
|    time_elapsed    | 344      |
|    total_timesteps | 95232    |
---------------------------------
Eval num_timesteps=95500, episode_reward=330.63 +/- 698.24
Episode length: 34.36 +/- 6.47
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 331           |
| time/                   |               |
|    total_timesteps      | 95500         |
| train/                  |               |
|    approx_kl            | 0.00026619463 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0149       |
|    explained_variance   | -0.0524       |
|    learning_rate        | 0.001         |
|    loss                 | 2.87e+04      |
|    n_updates            | 3790          |
|    policy_gradient_loss | -0.00109      |
|    value_loss           | 8.88e+04      |
-------------------------------------------
Eval num_timesteps=96000, episode_reward=311.81 +/- 682.59
Episode length: 34.10 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 94       |
|    time_elapsed    | 347      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=542.65 +/- 796.02
Episode length: 35.52 +/- 6.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.5         |
|    mean_reward          | 543          |
| time/                   |              |
|    total_timesteps      | 96500        |
| train/                  |              |
|    approx_kl            | 0.0012383076 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00475     |
|    explained_variance   | -0.205       |
|    learning_rate        | 0.001        |
|    loss                 | 2.75e+04     |
|    n_updates            | 3800         |
|    policy_gradient_loss | -0.000536    |
|    value_loss           | 7.98e+04     |
------------------------------------------
Eval num_timesteps=97000, episode_reward=443.26 +/- 717.17
Episode length: 34.60 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 95       |
|    time_elapsed    | 351      |
|    total_timesteps | 97280    |
---------------------------------
Eval num_timesteps=97500, episode_reward=356.78 +/- 687.08
Episode length: 34.74 +/- 7.11
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 357           |
| time/                   |               |
|    total_timesteps      | 97500         |
| train/                  |               |
|    approx_kl            | 2.1369895e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00775      |
|    explained_variance   | -0.121        |
|    learning_rate        | 0.001         |
|    loss                 | 3.73e+04      |
|    n_updates            | 3810          |
|    policy_gradient_loss | -0.000379     |
|    value_loss           | 8.56e+04      |
-------------------------------------------
Eval num_timesteps=98000, episode_reward=634.56 +/- 798.97
Episode length: 36.62 +/- 6.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 635      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 351      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 96       |
|    time_elapsed    | 355      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=523.03 +/- 751.32
Episode length: 36.08 +/- 6.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.1          |
|    mean_reward          | 523           |
| time/                   |               |
|    total_timesteps      | 98500         |
| train/                  |               |
|    approx_kl            | 0.00043203618 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00238      |
|    explained_variance   | -0.142        |
|    learning_rate        | 0.001         |
|    loss                 | 2.07e+04      |
|    n_updates            | 3820          |
|    policy_gradient_loss | -0.000683     |
|    value_loss           | 7.64e+04      |
-------------------------------------------
Eval num_timesteps=99000, episode_reward=463.97 +/- 771.65
Episode length: 35.12 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 464      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 360      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 97       |
|    time_elapsed    | 358      |
|    total_timesteps | 99328    |
---------------------------------
Eval num_timesteps=99500, episode_reward=438.90 +/- 715.04
Episode length: 35.10 +/- 6.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 439          |
| time/                   |              |
|    total_timesteps      | 99500        |
| train/                  |              |
|    approx_kl            | 0.0001638059 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0164      |
|    explained_variance   | -0.0952      |
|    learning_rate        | 0.001        |
|    loss                 | 3.14e+04     |
|    n_updates            | 3830         |
|    policy_gradient_loss | -0.00146     |
|    value_loss           | 7.33e+04     |
------------------------------------------
Eval num_timesteps=100000, episode_reward=605.71 +/- 816.86
Episode length: 36.84 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 606      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 485      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 98       |
|    time_elapsed    | 362      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=384.56 +/- 690.08
Episode length: 35.26 +/- 6.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.3        |
|    mean_reward          | 385         |
| time/                   |             |
|    total_timesteps      | 100500      |
| train/                  |             |
|    approx_kl            | 0.006391256 |
|    clip_fraction        | 0.0043      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0166     |
|    explained_variance   | -0.0331     |
|    learning_rate        | 0.001       |
|    loss                 | 2.82e+04    |
|    n_updates            | 3840        |
|    policy_gradient_loss | 0.0016      |
|    value_loss           | 1.07e+05    |
-----------------------------------------
Eval num_timesteps=101000, episode_reward=376.30 +/- 653.56
Episode length: 35.46 +/- 5.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 510      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 99       |
|    time_elapsed    | 366      |
|    total_timesteps | 101376   |
---------------------------------
Eval num_timesteps=101500, episode_reward=338.12 +/- 605.74
Episode length: 34.70 +/- 6.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 338           |
| time/                   |               |
|    total_timesteps      | 101500        |
| train/                  |               |
|    approx_kl            | 5.7798985e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0053       |
|    explained_variance   | -0.12         |
|    learning_rate        | 0.001         |
|    loss                 | 2.34e+04      |
|    n_updates            | 3850          |
|    policy_gradient_loss | -0.00153      |
|    value_loss           | 8.98e+04      |
-------------------------------------------
Eval num_timesteps=102000, episode_reward=615.61 +/- 749.91
Episode length: 36.50 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 616      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 519      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 100      |
|    time_elapsed    | 369      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=344.67 +/- 651.96
Episode length: 34.72 +/- 6.60
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 345           |
| time/                   |               |
|    total_timesteps      | 102500        |
| train/                  |               |
|    approx_kl            | 2.7942006e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.000805     |
|    explained_variance   | 0.125         |
|    learning_rate        | 0.001         |
|    loss                 | 3.93e+04      |
|    n_updates            | 3860          |
|    policy_gradient_loss | -0.000165     |
|    value_loss           | 8.79e+04      |
-------------------------------------------
Eval num_timesteps=103000, episode_reward=423.85 +/- 767.65
Episode length: 34.16 +/- 7.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 529      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 101      |
|    time_elapsed    | 373      |
|    total_timesteps | 103424   |
---------------------------------
Eval num_timesteps=103500, episode_reward=399.74 +/- 643.05
Episode length: 35.98 +/- 5.14
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 400           |
| time/                   |               |
|    total_timesteps      | 103500        |
| train/                  |               |
|    approx_kl            | 1.9774016e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00098      |
|    explained_variance   | -0.123        |
|    learning_rate        | 0.001         |
|    loss                 | 4.72e+04      |
|    n_updates            | 3870          |
|    policy_gradient_loss | -0.000189     |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=104000, episode_reward=485.77 +/- 722.64
Episode length: 35.78 +/- 5.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 380      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 102      |
|    time_elapsed    | 376      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=339.07 +/- 679.18
Episode length: 34.70 +/- 6.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 339          |
| time/                   |              |
|    total_timesteps      | 104500       |
| train/                  |              |
|    approx_kl            | 0.0003399012 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0125      |
|    explained_variance   | -0.153       |
|    learning_rate        | 0.001        |
|    loss                 | 2.49e+04     |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.000846    |
|    value_loss           | 6.67e+04     |
------------------------------------------
Eval num_timesteps=105000, episode_reward=317.41 +/- 629.27
Episode length: 34.36 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 412      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 103      |
|    time_elapsed    | 380      |
|    total_timesteps | 105472   |
---------------------------------
Eval num_timesteps=105500, episode_reward=556.23 +/- 781.14
Episode length: 35.82 +/- 7.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 556          |
| time/                   |              |
|    total_timesteps      | 105500       |
| train/                  |              |
|    approx_kl            | 0.0018403739 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.015       |
|    explained_variance   | -0.0326      |
|    learning_rate        | 0.001        |
|    loss                 | 2.75e+04     |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.00131     |
|    value_loss           | 9.01e+04     |
------------------------------------------
Eval num_timesteps=106000, episode_reward=432.59 +/- 793.98
Episode length: 34.88 +/- 6.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 423      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 104      |
|    time_elapsed    | 384      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=497.43 +/- 739.40
Episode length: 36.88 +/- 5.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.9         |
|    mean_reward          | 497          |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0033720254 |
|    clip_fraction        | 0.00488      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.027       |
|    explained_variance   | -0.073       |
|    learning_rate        | 0.001        |
|    loss                 | 2.59e+04     |
|    n_updates            | 3900         |
|    policy_gradient_loss | -0.000921    |
|    value_loss           | 8.54e+04     |
------------------------------------------
Eval num_timesteps=107000, episode_reward=436.96 +/- 684.80
Episode length: 36.10 +/- 5.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=315.77 +/- 621.31
Episode length: 35.00 +/- 6.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 105      |
|    time_elapsed    | 389      |
|    total_timesteps | 107520   |
---------------------------------
Eval num_timesteps=108000, episode_reward=440.09 +/- 674.70
Episode length: 35.26 +/- 6.39
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 440          |
| time/                   |              |
|    total_timesteps      | 108000       |
| train/                  |              |
|    approx_kl            | 0.0002714969 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0241      |
|    explained_variance   | -0.119       |
|    learning_rate        | 0.001        |
|    loss                 | 3.82e+04     |
|    n_updates            | 3910         |
|    policy_gradient_loss | -0.00088     |
|    value_loss           | 9.24e+04     |
------------------------------------------
Eval num_timesteps=108500, episode_reward=493.53 +/- 749.19
Episode length: 35.40 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 494      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 387      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 106      |
|    time_elapsed    | 392      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=381.84 +/- 718.23
Episode length: 34.62 +/- 6.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 382           |
| time/                   |               |
|    total_timesteps      | 109000        |
| train/                  |               |
|    approx_kl            | 3.7348596e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0053       |
|    explained_variance   | 0.0213        |
|    learning_rate        | 0.001         |
|    loss                 | 2.88e+04      |
|    n_updates            | 3920          |
|    policy_gradient_loss | -0.00103      |
|    value_loss           | 8.08e+04      |
-------------------------------------------
Eval num_timesteps=109500, episode_reward=270.75 +/- 671.92
Episode length: 33.60 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 328      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 107      |
|    time_elapsed    | 396      |
|    total_timesteps | 109568   |
---------------------------------
Eval num_timesteps=110000, episode_reward=339.63 +/- 629.89
Episode length: 35.24 +/- 6.05
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 340          |
| time/                   |              |
|    total_timesteps      | 110000       |
| train/                  |              |
|    approx_kl            | 0.0018191233 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00827     |
|    explained_variance   | -0.171       |
|    learning_rate        | 0.001        |
|    loss                 | 2.95e+04     |
|    n_updates            | 3930         |
|    policy_gradient_loss | 0.000895     |
|    value_loss           | 8.82e+04     |
------------------------------------------
Eval num_timesteps=110500, episode_reward=222.77 +/- 586.93
Episode length: 32.82 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 32.8     |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 108      |
|    time_elapsed    | 399      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=559.15 +/- 732.27
Episode length: 37.04 +/- 5.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37            |
|    mean_reward          | 559           |
| time/                   |               |
|    total_timesteps      | 111000        |
| train/                  |               |
|    approx_kl            | 0.00014038157 |
|    clip_fraction        | 0.00283       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00809      |
|    explained_variance   | -0.0551       |
|    learning_rate        | 0.001         |
|    loss                 | 2.74e+04      |
|    n_updates            | 3940          |
|    policy_gradient_loss | -0.000973     |
|    value_loss           | 8.16e+04      |
-------------------------------------------
Eval num_timesteps=111500, episode_reward=496.71 +/- 781.95
Episode length: 36.08 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 468      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 109      |
|    time_elapsed    | 403      |
|    total_timesteps | 111616   |
---------------------------------
Eval num_timesteps=112000, episode_reward=380.74 +/- 733.57
Episode length: 34.42 +/- 6.94
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.4          |
|    mean_reward          | 381           |
| time/                   |               |
|    total_timesteps      | 112000        |
| train/                  |               |
|    approx_kl            | 0.00013461593 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00369      |
|    explained_variance   | -0.00797      |
|    learning_rate        | 0.001         |
|    loss                 | 3.06e+04      |
|    n_updates            | 3950          |
|    policy_gradient_loss | -0.000554     |
|    value_loss           | 9.51e+04      |
-------------------------------------------
Eval num_timesteps=112500, episode_reward=376.31 +/- 717.54
Episode length: 34.42 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 500      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 110      |
|    time_elapsed    | 407      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=481.41 +/- 729.33
Episode length: 35.84 +/- 6.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 481          |
| time/                   |              |
|    total_timesteps      | 113000       |
| train/                  |              |
|    approx_kl            | 8.766772e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00615     |
|    explained_variance   | 0.0226       |
|    learning_rate        | 0.001        |
|    loss                 | 2.76e+04     |
|    n_updates            | 3960         |
|    policy_gradient_loss | -0.00017     |
|    value_loss           | 8.15e+04     |
------------------------------------------
Eval num_timesteps=113500, episode_reward=375.61 +/- 616.14
Episode length: 35.50 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 507      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 111      |
|    time_elapsed    | 410      |
|    total_timesteps | 113664   |
---------------------------------
Eval num_timesteps=114000, episode_reward=539.54 +/- 794.89
Episode length: 35.96 +/- 6.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 540           |
| time/                   |               |
|    total_timesteps      | 114000        |
| train/                  |               |
|    approx_kl            | 1.1586933e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00667      |
|    explained_variance   | 0.0292        |
|    learning_rate        | 0.001         |
|    loss                 | 3.86e+04      |
|    n_updates            | 3970          |
|    policy_gradient_loss | -0.000274     |
|    value_loss           | 9.62e+04      |
-------------------------------------------
Eval num_timesteps=114500, episode_reward=565.41 +/- 790.07
Episode length: 35.64 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 565      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 499      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 112      |
|    time_elapsed    | 414      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=303.09 +/- 681.46
Episode length: 34.12 +/- 7.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.1         |
|    mean_reward          | 303          |
| time/                   |              |
|    total_timesteps      | 115000       |
| train/                  |              |
|    approx_kl            | 0.0056698467 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0163      |
|    explained_variance   | 0.0236       |
|    learning_rate        | 0.001        |
|    loss                 | 3.59e+04     |
|    n_updates            | 3980         |
|    policy_gradient_loss | 5.9e-05      |
|    value_loss           | 9.61e+04     |
------------------------------------------
Eval num_timesteps=115500, episode_reward=434.60 +/- 674.64
Episode length: 35.20 +/- 7.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 435      |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 434      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 113      |
|    time_elapsed    | 417      |
|    total_timesteps | 115712   |
---------------------------------
Eval num_timesteps=116000, episode_reward=486.98 +/- 706.58
Episode length: 36.08 +/- 5.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.1        |
|    mean_reward          | 487         |
| time/                   |             |
|    total_timesteps      | 116000      |
| train/                  |             |
|    approx_kl            | 0.001493046 |
|    clip_fraction        | 0.00254     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0301     |
|    explained_variance   | -0.0592     |
|    learning_rate        | 0.001       |
|    loss                 | 3.6e+04     |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.000958   |
|    value_loss           | 9.38e+04    |
-----------------------------------------
Eval num_timesteps=116500, episode_reward=532.86 +/- 788.30
Episode length: 36.44 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 412      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 114      |
|    time_elapsed    | 421      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=462.46 +/- 713.08
Episode length: 35.84 +/- 6.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 462          |
| time/                   |              |
|    total_timesteps      | 117000       |
| train/                  |              |
|    approx_kl            | 0.0010043246 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0196      |
|    explained_variance   | 0.0756       |
|    learning_rate        | 0.001        |
|    loss                 | 2.91e+04     |
|    n_updates            | 4000         |
|    policy_gradient_loss | -0.000882    |
|    value_loss           | 8.34e+04     |
------------------------------------------
Eval num_timesteps=117500, episode_reward=275.31 +/- 582.61
Episode length: 34.24 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 275      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 403      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 115      |
|    time_elapsed    | 425      |
|    total_timesteps | 117760   |
---------------------------------
Eval num_timesteps=118000, episode_reward=249.34 +/- 626.01
Episode length: 34.26 +/- 6.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.3        |
|    mean_reward          | 249         |
| time/                   |             |
|    total_timesteps      | 118000      |
| train/                  |             |
|    approx_kl            | 0.001705151 |
|    clip_fraction        | 0.00254     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0253     |
|    explained_variance   | -0.158      |
|    learning_rate        | 0.001       |
|    loss                 | 2.54e+04    |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.000886   |
|    value_loss           | 9.53e+04    |
-----------------------------------------
Eval num_timesteps=118500, episode_reward=353.23 +/- 672.52
Episode length: 35.44 +/- 6.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 397      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 116      |
|    time_elapsed    | 428      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=474.59 +/- 767.66
Episode length: 35.64 +/- 6.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 475          |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 0.0019076758 |
|    clip_fraction        | 0.00381      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0172      |
|    explained_variance   | -0.151       |
|    learning_rate        | 0.001        |
|    loss                 | 2.7e+04      |
|    n_updates            | 4020         |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 8.23e+04     |
------------------------------------------
Eval num_timesteps=119500, episode_reward=479.23 +/- 768.59
Episode length: 35.44 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 479      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 436      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 117      |
|    time_elapsed    | 432      |
|    total_timesteps | 119808   |
---------------------------------
Eval num_timesteps=120000, episode_reward=470.00 +/- 728.39
Episode length: 35.58 +/- 6.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 470          |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 7.410493e-05 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00458     |
|    explained_variance   | 0.184        |
|    learning_rate        | 0.001        |
|    loss                 | 3.68e+04     |
|    n_updates            | 4030         |
|    policy_gradient_loss | -0.000643    |
|    value_loss           | 9.17e+04     |
------------------------------------------
Eval num_timesteps=120500, episode_reward=476.11 +/- 726.50
Episode length: 36.24 +/- 5.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 476      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 346      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 118      |
|    time_elapsed    | 436      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=487.21 +/- 732.48
Episode length: 35.24 +/- 6.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 487          |
| time/                   |              |
|    total_timesteps      | 121000       |
| train/                  |              |
|    approx_kl            | 0.0013085057 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0357      |
|    explained_variance   | -0.0246      |
|    learning_rate        | 0.001        |
|    loss                 | 3.6e+04      |
|    n_updates            | 4040         |
|    policy_gradient_loss | 0.002        |
|    value_loss           | 8.16e+04     |
------------------------------------------
Eval num_timesteps=121500, episode_reward=485.00 +/- 725.19
Episode length: 36.12 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 485      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 119      |
|    time_elapsed    | 439      |
|    total_timesteps | 121856   |
---------------------------------
Eval num_timesteps=122000, episode_reward=350.91 +/- 661.51
Episode length: 35.62 +/- 6.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 351          |
| time/                   |              |
|    total_timesteps      | 122000       |
| train/                  |              |
|    approx_kl            | 0.0026647556 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0563      |
|    explained_variance   | -0.0834      |
|    learning_rate        | 0.001        |
|    loss                 | 1.68e+04     |
|    n_updates            | 4050         |
|    policy_gradient_loss | 0.00287      |
|    value_loss           | 5.88e+04     |
------------------------------------------
Eval num_timesteps=122500, episode_reward=403.47 +/- 754.83
Episode length: 34.20 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 403      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 231      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 120      |
|    time_elapsed    | 443      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=504.46 +/- 754.90
Episode length: 36.06 +/- 6.62
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 504          |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0019323342 |
|    clip_fraction        | 0.00645      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0951      |
|    explained_variance   | -0.146       |
|    learning_rate        | 0.001        |
|    loss                 | 2.46e+04     |
|    n_updates            | 4060         |
|    policy_gradient_loss | 0.00459      |
|    value_loss           | 6.54e+04     |
------------------------------------------
Eval num_timesteps=123500, episode_reward=352.07 +/- 702.34
Episode length: 34.34 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 352      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 250      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 121      |
|    time_elapsed    | 446      |
|    total_timesteps | 123904   |
---------------------------------
Eval num_timesteps=124000, episode_reward=600.92 +/- 789.51
Episode length: 37.00 +/- 6.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37           |
|    mean_reward          | 601          |
| time/                   |              |
|    total_timesteps      | 124000       |
| train/                  |              |
|    approx_kl            | 0.0035902741 |
|    clip_fraction        | 0.00693      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0342      |
|    explained_variance   | 0.0247       |
|    learning_rate        | 0.001        |
|    loss                 | 2.18e+04     |
|    n_updates            | 4070         |
|    policy_gradient_loss | -0.000894    |
|    value_loss           | 6.84e+04     |
------------------------------------------
Eval num_timesteps=124500, episode_reward=431.65 +/- 715.00
Episode length: 35.40 +/- 6.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 216      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 122      |
|    time_elapsed    | 450      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=387.57 +/- 730.40
Episode length: 34.40 +/- 6.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.4        |
|    mean_reward          | 388         |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.002002633 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0286     |
|    explained_variance   | -0.0497     |
|    learning_rate        | 0.001       |
|    loss                 | 2.42e+04    |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.00111    |
|    value_loss           | 5.87e+04    |
-----------------------------------------
Eval num_timesteps=125500, episode_reward=177.82 +/- 542.84
Episode length: 33.10 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.1     |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 346      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 123      |
|    time_elapsed    | 454      |
|    total_timesteps | 125952   |
---------------------------------
Eval num_timesteps=126000, episode_reward=376.68 +/- 709.12
Episode length: 34.86 +/- 6.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.9        |
|    mean_reward          | 377         |
| time/                   |             |
|    total_timesteps      | 126000      |
| train/                  |             |
|    approx_kl            | 0.015577991 |
|    clip_fraction        | 0.00596     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.039      |
|    explained_variance   | -0.0801     |
|    learning_rate        | 0.001       |
|    loss                 | 2.08e+04    |
|    n_updates            | 4090        |
|    policy_gradient_loss | 0.00223     |
|    value_loss           | 6.21e+04    |
-----------------------------------------
Eval num_timesteps=126500, episode_reward=326.93 +/- 643.29
Episode length: 35.74 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 327      |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 347      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 124      |
|    time_elapsed    | 457      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=490.22 +/- 713.02
Episode length: 35.60 +/- 7.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 490         |
| time/                   |             |
|    total_timesteps      | 127000      |
| train/                  |             |
|    approx_kl            | 0.015639596 |
|    clip_fraction        | 0.0105      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.067      |
|    explained_variance   | -0.21       |
|    learning_rate        | 0.001       |
|    loss                 | 2.02e+04    |
|    n_updates            | 4100        |
|    policy_gradient_loss | 0.000945    |
|    value_loss           | 6.76e+04    |
-----------------------------------------
Eval num_timesteps=127500, episode_reward=341.82 +/- 724.68
Episode length: 33.58 +/- 6.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=280.23 +/- 601.05
Episode length: 34.48 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 280      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 358      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 125      |
|    time_elapsed    | 462      |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=434.13 +/- 676.28
Episode length: 35.96 +/- 5.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36          |
|    mean_reward          | 434         |
| time/                   |             |
|    total_timesteps      | 128500      |
| train/                  |             |
|    approx_kl            | 0.011381004 |
|    clip_fraction        | 0.0135      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0322     |
|    explained_variance   | 0.136       |
|    learning_rate        | 0.001       |
|    loss                 | 2.46e+04    |
|    n_updates            | 4110        |
|    policy_gradient_loss | 8.97e-05    |
|    value_loss           | 7.92e+04    |
-----------------------------------------
Eval num_timesteps=129000, episode_reward=365.80 +/- 740.96
Episode length: 33.82 +/- 7.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 366      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 384      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 126      |
|    time_elapsed    | 466      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=372.28 +/- 700.94
Episode length: 33.74 +/- 8.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.7        |
|    mean_reward          | 372         |
| time/                   |             |
|    total_timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.013835581 |
|    clip_fraction        | 0.00459     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0132     |
|    explained_variance   | -0.223      |
|    learning_rate        | 0.001       |
|    loss                 | 2.67e+04    |
|    n_updates            | 4120        |
|    policy_gradient_loss | 0.00148     |
|    value_loss           | 1.07e+05    |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=528.47 +/- 766.74
Episode length: 35.46 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 528      |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 359      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 127      |
|    time_elapsed    | 469      |
|    total_timesteps | 130048   |
---------------------------------
Eval num_timesteps=130500, episode_reward=475.70 +/- 797.21
Episode length: 35.38 +/- 6.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 476          |
| time/                   |              |
|    total_timesteps      | 130500       |
| train/                  |              |
|    approx_kl            | 0.0009715566 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00656     |
|    explained_variance   | -0.461       |
|    learning_rate        | 0.001        |
|    loss                 | 2.64e+04     |
|    n_updates            | 4130         |
|    policy_gradient_loss | -0.000708    |
|    value_loss           | 6.99e+04     |
------------------------------------------
Eval num_timesteps=131000, episode_reward=496.43 +/- 746.67
Episode length: 35.62 +/- 5.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 496      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 366      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 128      |
|    time_elapsed    | 473      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=301.26 +/- 568.15
Episode length: 35.94 +/- 5.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 301           |
| time/                   |               |
|    total_timesteps      | 131500        |
| train/                  |               |
|    approx_kl            | 3.0799012e-05 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.013        |
|    explained_variance   | -0.0843       |
|    learning_rate        | 0.001         |
|    loss                 | 2.88e+04      |
|    n_updates            | 4140          |
|    policy_gradient_loss | -0.00047      |
|    value_loss           | 6.61e+04      |
-------------------------------------------
Eval num_timesteps=132000, episode_reward=428.26 +/- 744.87
Episode length: 35.22 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 357      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 129      |
|    time_elapsed    | 477      |
|    total_timesteps | 132096   |
---------------------------------
Eval num_timesteps=132500, episode_reward=312.38 +/- 666.40
Episode length: 33.56 +/- 7.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.6        |
|    mean_reward          | 312         |
| time/                   |             |
|    total_timesteps      | 132500      |
| train/                  |             |
|    approx_kl            | 0.003191736 |
|    clip_fraction        | 0.00186     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0125     |
|    explained_variance   | -0.00628    |
|    learning_rate        | 0.001       |
|    loss                 | 1.92e+04    |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.000885   |
|    value_loss           | 6.15e+04    |
-----------------------------------------
Eval num_timesteps=133000, episode_reward=603.35 +/- 814.04
Episode length: 36.18 +/- 6.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 603      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 130      |
|    time_elapsed    | 480      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=440.33 +/- 738.42
Episode length: 35.36 +/- 6.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 440           |
| time/                   |               |
|    total_timesteps      | 133500        |
| train/                  |               |
|    approx_kl            | 0.00036387268 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00813      |
|    explained_variance   | -0.0669       |
|    learning_rate        | 0.001         |
|    loss                 | 2.29e+04      |
|    n_updates            | 4160          |
|    policy_gradient_loss | 0.000697      |
|    value_loss           | 7.73e+04      |
-------------------------------------------
Eval num_timesteps=134000, episode_reward=525.31 +/- 793.44
Episode length: 35.80 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 525      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 422      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 131      |
|    time_elapsed    | 484      |
|    total_timesteps | 134144   |
---------------------------------
Eval num_timesteps=134500, episode_reward=411.64 +/- 749.46
Episode length: 34.18 +/- 7.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.2          |
|    mean_reward          | 412           |
| time/                   |               |
|    total_timesteps      | 134500        |
| train/                  |               |
|    approx_kl            | 4.1168416e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0136       |
|    explained_variance   | -0.0473       |
|    learning_rate        | 0.001         |
|    loss                 | 2.21e+04      |
|    n_updates            | 4170          |
|    policy_gradient_loss | -0.000771     |
|    value_loss           | 8.24e+04      |
-------------------------------------------
Eval num_timesteps=135000, episode_reward=357.83 +/- 765.82
Episode length: 33.38 +/- 7.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 358      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 444      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 132      |
|    time_elapsed    | 487      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=442.70 +/- 733.12
Episode length: 35.54 +/- 5.69
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 443           |
| time/                   |               |
|    total_timesteps      | 135500        |
| train/                  |               |
|    approx_kl            | 1.3500452e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00739      |
|    explained_variance   | -0.14         |
|    learning_rate        | 0.001         |
|    loss                 | 3.5e+04       |
|    n_updates            | 4180          |
|    policy_gradient_loss | -0.000471     |
|    value_loss           | 9.34e+04      |
-------------------------------------------
Eval num_timesteps=136000, episode_reward=535.08 +/- 791.06
Episode length: 35.02 +/- 6.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 434      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 133      |
|    time_elapsed    | 491      |
|    total_timesteps | 136192   |
---------------------------------
Eval num_timesteps=136500, episode_reward=433.05 +/- 738.48
Episode length: 34.38 +/- 7.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 433          |
| time/                   |              |
|    total_timesteps      | 136500       |
| train/                  |              |
|    approx_kl            | 0.0057470845 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00979     |
|    explained_variance   | -0.0749      |
|    learning_rate        | 0.001        |
|    loss                 | 3.71e+04     |
|    n_updates            | 4190         |
|    policy_gradient_loss | 0.000747     |
|    value_loss           | 9.95e+04     |
------------------------------------------
Eval num_timesteps=137000, episode_reward=404.75 +/- 740.11
Episode length: 34.90 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 291      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 134      |
|    time_elapsed    | 495      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=576.55 +/- 711.20
Episode length: 37.70 +/- 5.51
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.7          |
|    mean_reward          | 577           |
| time/                   |               |
|    total_timesteps      | 137500        |
| train/                  |               |
|    approx_kl            | 8.5549196e-05 |
|    clip_fraction        | 0.000391      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0362       |
|    explained_variance   | -0.281        |
|    learning_rate        | 0.001         |
|    loss                 | 2.31e+04      |
|    n_updates            | 4200          |
|    policy_gradient_loss | -0.000916     |
|    value_loss           | 4.89e+04      |
-------------------------------------------
Eval num_timesteps=138000, episode_reward=477.56 +/- 761.91
Episode length: 35.82 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 318      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 135      |
|    time_elapsed    | 498      |
|    total_timesteps | 138240   |
---------------------------------
Eval num_timesteps=138500, episode_reward=542.36 +/- 737.38
Episode length: 36.48 +/- 5.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.5         |
|    mean_reward          | 542          |
| time/                   |              |
|    total_timesteps      | 138500       |
| train/                  |              |
|    approx_kl            | 0.0029453756 |
|    clip_fraction        | 0.00645      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.118       |
|    explained_variance   | -0.0719      |
|    learning_rate        | 0.001        |
|    loss                 | 1.77e+04     |
|    n_updates            | 4210         |
|    policy_gradient_loss | -0.00108     |
|    value_loss           | 6.74e+04     |
------------------------------------------
Eval num_timesteps=139000, episode_reward=452.06 +/- 692.10
Episode length: 35.72 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 226      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 136      |
|    time_elapsed    | 502      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=315.42 +/- 713.44
Episode length: 33.96 +/- 6.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34          |
|    mean_reward          | 315         |
| time/                   |             |
|    total_timesteps      | 139500      |
| train/                  |             |
|    approx_kl            | 0.012512856 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.204      |
|    explained_variance   | -0.113      |
|    learning_rate        | 0.001       |
|    loss                 | 2.25e+04    |
|    n_updates            | 4220        |
|    policy_gradient_loss | -0.00478    |
|    value_loss           | 6.13e+04    |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=259.00 +/- 650.32
Episode length: 33.40 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.4     |
|    mean_reward     | 259      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 256      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 137      |
|    time_elapsed    | 506      |
|    total_timesteps | 140288   |
---------------------------------
Eval num_timesteps=140500, episode_reward=658.18 +/- 826.84
Episode length: 35.86 +/- 7.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.9       |
|    mean_reward          | 658        |
| time/                   |            |
|    total_timesteps      | 140500     |
| train/                  |            |
|    approx_kl            | 0.02973941 |
|    clip_fraction        | 0.035      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.102     |
|    explained_variance   | -0.0881    |
|    learning_rate        | 0.001      |
|    loss                 | 1.98e+04   |
|    n_updates            | 4230       |
|    policy_gradient_loss | 0.000495   |
|    value_loss           | 7.66e+04   |
----------------------------------------
Eval num_timesteps=141000, episode_reward=350.06 +/- 705.46
Episode length: 34.26 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 350      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 236      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 138      |
|    time_elapsed    | 510      |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=448.40 +/- 791.92
Episode length: 34.38 +/- 7.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.4        |
|    mean_reward          | 448         |
| time/                   |             |
|    total_timesteps      | 141500      |
| train/                  |             |
|    approx_kl            | 0.007723914 |
|    clip_fraction        | 0.0145      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0926     |
|    explained_variance   | -0.0736     |
|    learning_rate        | 0.001       |
|    loss                 | 3.34e+04    |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.000789   |
|    value_loss           | 7.21e+04    |
-----------------------------------------
Eval num_timesteps=142000, episode_reward=438.81 +/- 712.13
Episode length: 35.66 +/- 6.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 312      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 139      |
|    time_elapsed    | 513      |
|    total_timesteps | 142336   |
---------------------------------
Eval num_timesteps=142500, episode_reward=293.25 +/- 536.14
Episode length: 35.52 +/- 5.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.5        |
|    mean_reward          | 293         |
| time/                   |             |
|    total_timesteps      | 142500      |
| train/                  |             |
|    approx_kl            | 0.018818371 |
|    clip_fraction        | 0.00918     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00884    |
|    explained_variance   | -0.0653     |
|    learning_rate        | 0.001       |
|    loss                 | 2.77e+04    |
|    n_updates            | 4250        |
|    policy_gradient_loss | 0.00125     |
|    value_loss           | 9.93e+04    |
-----------------------------------------
Eval num_timesteps=143000, episode_reward=356.73 +/- 707.88
Episode length: 35.12 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 357      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 140      |
|    time_elapsed    | 517      |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=481.41 +/- 711.44
Episode length: 36.76 +/- 5.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.8         |
|    mean_reward          | 481          |
| time/                   |              |
|    total_timesteps      | 143500       |
| train/                  |              |
|    approx_kl            | 2.009736e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00439     |
|    explained_variance   | 0.0378       |
|    learning_rate        | 0.001        |
|    loss                 | 2.43e+04     |
|    n_updates            | 4260         |
|    policy_gradient_loss | -0.00013     |
|    value_loss           | 7.62e+04     |
------------------------------------------
Eval num_timesteps=144000, episode_reward=543.43 +/- 791.22
Episode length: 35.82 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 543      |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 455      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 141      |
|    time_elapsed    | 521      |
|    total_timesteps | 144384   |
---------------------------------
Eval num_timesteps=144500, episode_reward=416.80 +/- 774.08
Episode length: 34.84 +/- 7.16
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 417           |
| time/                   |               |
|    total_timesteps      | 144500        |
| train/                  |               |
|    approx_kl            | 2.9303716e-05 |
|    clip_fraction        | 0.000293      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00529      |
|    explained_variance   | 0.00937       |
|    learning_rate        | 0.001         |
|    loss                 | 3.24e+04      |
|    n_updates            | 4270          |
|    policy_gradient_loss | -0.000341     |
|    value_loss           | 9.25e+04      |
-------------------------------------------
Eval num_timesteps=145000, episode_reward=428.36 +/- 796.42
Episode length: 34.20 +/- 8.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 501      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 142      |
|    time_elapsed    | 524      |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=267.95 +/- 590.54
Episode length: 33.88 +/- 6.54
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.9          |
|    mean_reward          | 268           |
| time/                   |               |
|    total_timesteps      | 145500        |
| train/                  |               |
|    approx_kl            | 5.7874655e-05 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0134       |
|    explained_variance   | -0.0733       |
|    learning_rate        | 0.001         |
|    loss                 | 1.75e+04      |
|    n_updates            | 4280          |
|    policy_gradient_loss | 0.000737      |
|    value_loss           | 7.04e+04      |
-------------------------------------------
Eval num_timesteps=146000, episode_reward=291.80 +/- 598.52
Episode length: 34.16 +/- 7.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 421      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 143      |
|    time_elapsed    | 528      |
|    total_timesteps | 146432   |
---------------------------------
Eval num_timesteps=146500, episode_reward=575.94 +/- 754.04
Episode length: 36.42 +/- 6.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.4        |
|    mean_reward          | 576         |
| time/                   |             |
|    total_timesteps      | 146500      |
| train/                  |             |
|    approx_kl            | 0.005913812 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00268    |
|    explained_variance   | 0.0248      |
|    learning_rate        | 0.001       |
|    loss                 | 2.19e+04    |
|    n_updates            | 4290        |
|    policy_gradient_loss | -0.000728   |
|    value_loss           | 8.08e+04    |
-----------------------------------------
Eval num_timesteps=147000, episode_reward=483.00 +/- 721.67
Episode length: 35.92 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 483      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 416      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 144      |
|    time_elapsed    | 531      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=364.12 +/- 680.40
Episode length: 34.84 +/- 6.84
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.8          |
|    mean_reward          | 364           |
| time/                   |               |
|    total_timesteps      | 147500        |
| train/                  |               |
|    approx_kl            | 6.3727493e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00836      |
|    explained_variance   | -0.174        |
|    learning_rate        | 0.001         |
|    loss                 | 1.89e+04      |
|    n_updates            | 4300          |
|    policy_gradient_loss | -0.000158     |
|    value_loss           | 7.6e+04       |
-------------------------------------------
Eval num_timesteps=148000, episode_reward=370.90 +/- 735.72
Episode length: 34.92 +/- 6.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 376      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 145      |
|    time_elapsed    | 535      |
|    total_timesteps | 148480   |
---------------------------------
Eval num_timesteps=148500, episode_reward=464.27 +/- 733.84
Episode length: 35.46 +/- 5.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.5         |
|    mean_reward          | 464          |
| time/                   |              |
|    total_timesteps      | 148500       |
| train/                  |              |
|    approx_kl            | 0.0027229304 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0205      |
|    explained_variance   | 0.0291       |
|    learning_rate        | 0.001        |
|    loss                 | 2.25e+04     |
|    n_updates            | 4310         |
|    policy_gradient_loss | 0.000438     |
|    value_loss           | 7.79e+04     |
------------------------------------------
Eval num_timesteps=149000, episode_reward=457.09 +/- 716.76
Episode length: 35.84 +/- 5.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=477.47 +/- 716.12
Episode length: 36.00 +/- 6.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 289      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 146      |
|    time_elapsed    | 540      |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=423.47 +/- 690.28
Episode length: 35.26 +/- 7.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 423          |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0005802331 |
|    clip_fraction        | 0.00459      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0538      |
|    explained_variance   | -0.157       |
|    learning_rate        | 0.001        |
|    loss                 | 2.16e+04     |
|    n_updates            | 4320         |
|    policy_gradient_loss | -0.00194     |
|    value_loss           | 6.68e+04     |
------------------------------------------
Eval num_timesteps=150500, episode_reward=567.94 +/- 755.32
Episode length: 35.52 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 568      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 323      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 147      |
|    time_elapsed    | 544      |
|    total_timesteps | 150528   |
---------------------------------
Eval num_timesteps=151000, episode_reward=391.82 +/- 731.95
Episode length: 34.66 +/- 7.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.7        |
|    mean_reward          | 392         |
| time/                   |             |
|    total_timesteps      | 151000      |
| train/                  |             |
|    approx_kl            | 0.016889792 |
|    clip_fraction        | 0.0157      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0616     |
|    explained_variance   | -0.0112     |
|    learning_rate        | 0.001       |
|    loss                 | 2.93e+04    |
|    n_updates            | 4330        |
|    policy_gradient_loss | 0.0063      |
|    value_loss           | 7.4e+04     |
-----------------------------------------
Eval num_timesteps=151500, episode_reward=510.69 +/- 661.85
Episode length: 37.86 +/- 4.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.9     |
|    mean_reward     | 511      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 281      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 148      |
|    time_elapsed    | 547      |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=505.76 +/- 793.56
Episode length: 35.08 +/- 7.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.1        |
|    mean_reward          | 506         |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.008009802 |
|    clip_fraction        | 0.00771     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0326     |
|    explained_variance   | -0.196      |
|    learning_rate        | 0.001       |
|    loss                 | 2.24e+04    |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.000268   |
|    value_loss           | 6.63e+04    |
-----------------------------------------
Eval num_timesteps=152500, episode_reward=508.54 +/- 773.44
Episode length: 35.54 +/- 6.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 509      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 308      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 149      |
|    time_elapsed    | 551      |
|    total_timesteps | 152576   |
---------------------------------
Eval num_timesteps=153000, episode_reward=528.93 +/- 728.04
Episode length: 36.52 +/- 6.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.5         |
|    mean_reward          | 529          |
| time/                   |              |
|    total_timesteps      | 153000       |
| train/                  |              |
|    approx_kl            | 0.0058659697 |
|    clip_fraction        | 0.00537      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0365      |
|    explained_variance   | -0.204       |
|    learning_rate        | 0.001        |
|    loss                 | 1.72e+04     |
|    n_updates            | 4350         |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 7.47e+04     |
------------------------------------------
Eval num_timesteps=153500, episode_reward=319.42 +/- 662.22
Episode length: 35.10 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 290      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 150      |
|    time_elapsed    | 555      |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=592.27 +/- 750.22
Episode length: 36.10 +/- 6.65
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.1        |
|    mean_reward          | 592         |
| time/                   |             |
|    total_timesteps      | 154000      |
| train/                  |             |
|    approx_kl            | 0.007933262 |
|    clip_fraction        | 0.00625     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00954    |
|    explained_variance   | -0.16       |
|    learning_rate        | 0.001       |
|    loss                 | 3.51e+04    |
|    n_updates            | 4360        |
|    policy_gradient_loss | -0.000832   |
|    value_loss           | 8.2e+04     |
-----------------------------------------
Eval num_timesteps=154500, episode_reward=448.92 +/- 693.12
Episode length: 35.62 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 449      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 151      |
|    time_elapsed    | 558      |
|    total_timesteps | 154624   |
---------------------------------
Eval num_timesteps=155000, episode_reward=479.59 +/- 705.07
Episode length: 36.96 +/- 5.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37          |
|    mean_reward          | 480         |
| time/                   |             |
|    total_timesteps      | 155000      |
| train/                  |             |
|    approx_kl            | 0.009330419 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00926    |
|    explained_variance   | -0.0992     |
|    learning_rate        | 0.001       |
|    loss                 | 2.88e+04    |
|    n_updates            | 4370        |
|    policy_gradient_loss | 0.0065      |
|    value_loss           | 8.23e+04    |
-----------------------------------------
Eval num_timesteps=155500, episode_reward=419.99 +/- 755.93
Episode length: 34.76 +/- 6.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 451      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 152      |
|    time_elapsed    | 562      |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=356.94 +/- 625.47
Episode length: 34.82 +/- 6.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.8        |
|    mean_reward          | 357         |
| time/                   |             |
|    total_timesteps      | 156000      |
| train/                  |             |
|    approx_kl            | 0.003449843 |
|    clip_fraction        | 0.00264     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00298    |
|    explained_variance   | 0.085       |
|    learning_rate        | 0.001       |
|    loss                 | 4.11e+04    |
|    n_updates            | 4380        |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 9.04e+04    |
-----------------------------------------
Eval num_timesteps=156500, episode_reward=344.80 +/- 682.58
Episode length: 34.22 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 345      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 153      |
|    time_elapsed    | 566      |
|    total_timesteps | 156672   |
---------------------------------
Eval num_timesteps=157000, episode_reward=339.26 +/- 669.82
Episode length: 35.36 +/- 6.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 339          |
| time/                   |              |
|    total_timesteps      | 157000       |
| train/                  |              |
|    approx_kl            | 2.897007e-05 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00335     |
|    explained_variance   | -0.242       |
|    learning_rate        | 0.001        |
|    loss                 | 3.3e+04      |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.000479    |
|    value_loss           | 8.36e+04     |
------------------------------------------
Eval num_timesteps=157500, episode_reward=300.92 +/- 695.92
Episode length: 34.24 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 301      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 425      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 154      |
|    time_elapsed    | 569      |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=333.93 +/- 642.30
Episode length: 34.88 +/- 5.48
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 334           |
| time/                   |               |
|    total_timesteps      | 158000        |
| train/                  |               |
|    approx_kl            | 6.9097325e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00922      |
|    explained_variance   | -0.027        |
|    learning_rate        | 0.001         |
|    loss                 | 3.82e+04      |
|    n_updates            | 4400          |
|    policy_gradient_loss | -0.000204     |
|    value_loss           | 8.86e+04      |
-------------------------------------------
Eval num_timesteps=158500, episode_reward=397.47 +/- 743.06
Episode length: 34.66 +/- 7.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 411      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 155      |
|    time_elapsed    | 573      |
|    total_timesteps | 158720   |
---------------------------------
Eval num_timesteps=159000, episode_reward=332.86 +/- 647.86
Episode length: 34.88 +/- 6.20
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.9          |
|    mean_reward          | 333           |
| time/                   |               |
|    total_timesteps      | 159000        |
| train/                  |               |
|    approx_kl            | 1.3490091e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0119       |
|    explained_variance   | -0.0421       |
|    learning_rate        | 0.001         |
|    loss                 | 2.2e+04       |
|    n_updates            | 4410          |
|    policy_gradient_loss | -0.000703     |
|    value_loss           | 8.92e+04      |
-------------------------------------------
Eval num_timesteps=159500, episode_reward=321.03 +/- 622.85
Episode length: 34.14 +/- 6.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 450      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 156      |
|    time_elapsed    | 576      |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=405.22 +/- 733.18
Episode length: 34.34 +/- 7.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 405          |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0005676794 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00872     |
|    explained_variance   | -0.144       |
|    learning_rate        | 0.001        |
|    loss                 | 2.5e+04      |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.00127     |
|    value_loss           | 9.4e+04      |
------------------------------------------
Eval num_timesteps=160500, episode_reward=434.65 +/- 744.50
Episode length: 33.54 +/- 8.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 435      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 469      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 157      |
|    time_elapsed    | 580      |
|    total_timesteps | 160768   |
---------------------------------
Eval num_timesteps=161000, episode_reward=353.95 +/- 651.01
Episode length: 35.78 +/- 5.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.8         |
|    mean_reward          | 354          |
| time/                   |              |
|    total_timesteps      | 161000       |
| train/                  |              |
|    approx_kl            | 0.0019081285 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0104      |
|    explained_variance   | -0.203       |
|    learning_rate        | 0.001        |
|    loss                 | 2.61e+04     |
|    n_updates            | 4430         |
|    policy_gradient_loss | -0.00106     |
|    value_loss           | 9.51e+04     |
------------------------------------------
Eval num_timesteps=161500, episode_reward=438.03 +/- 703.74
Episode length: 36.40 +/- 6.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 487      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 158      |
|    time_elapsed    | 583      |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=506.71 +/- 774.97
Episode length: 35.14 +/- 6.68
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 507           |
| time/                   |               |
|    total_timesteps      | 162000        |
| train/                  |               |
|    approx_kl            | 5.7637866e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00851      |
|    explained_variance   | -0.106        |
|    learning_rate        | 0.001         |
|    loss                 | 3.23e+04      |
|    n_updates            | 4440          |
|    policy_gradient_loss | -0.000651     |
|    value_loss           | 8.66e+04      |
-------------------------------------------
Eval num_timesteps=162500, episode_reward=501.63 +/- 743.44
Episode length: 35.44 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 539      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 159      |
|    time_elapsed    | 587      |
|    total_timesteps | 162816   |
---------------------------------
Eval num_timesteps=163000, episode_reward=431.02 +/- 713.62
Episode length: 35.76 +/- 5.87
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 431           |
| time/                   |               |
|    total_timesteps      | 163000        |
| train/                  |               |
|    approx_kl            | 3.9135863e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00349      |
|    explained_variance   | -0.137        |
|    learning_rate        | 0.001         |
|    loss                 | 3.4e+04       |
|    n_updates            | 4450          |
|    policy_gradient_loss | -0.000549     |
|    value_loss           | 1.03e+05      |
-------------------------------------------
Eval num_timesteps=163500, episode_reward=505.29 +/- 746.76
Episode length: 36.22 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 505      |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 475      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 160      |
|    time_elapsed    | 591      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=497.26 +/- 819.80
Episode length: 34.30 +/- 7.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 497          |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0001708626 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0169      |
|    explained_variance   | -0.00857     |
|    learning_rate        | 0.001        |
|    loss                 | 3.05e+04     |
|    n_updates            | 4460         |
|    policy_gradient_loss | -0.00214     |
|    value_loss           | 8.17e+04     |
------------------------------------------
Eval num_timesteps=164500, episode_reward=485.49 +/- 731.58
Episode length: 35.88 +/- 6.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 485      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 451      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 161      |
|    time_elapsed    | 594      |
|    total_timesteps | 164864   |
---------------------------------
Eval num_timesteps=165000, episode_reward=317.79 +/- 637.89
Episode length: 34.38 +/- 6.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.4        |
|    mean_reward          | 318         |
| time/                   |             |
|    total_timesteps      | 165000      |
| train/                  |             |
|    approx_kl            | 0.002705737 |
|    clip_fraction        | 0.00547     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0207     |
|    explained_variance   | -0.154      |
|    learning_rate        | 0.001       |
|    loss                 | 2.36e+04    |
|    n_updates            | 4470        |
|    policy_gradient_loss | -0.00249    |
|    value_loss           | 8.6e+04     |
-----------------------------------------
Eval num_timesteps=165500, episode_reward=466.61 +/- 739.52
Episode length: 34.98 +/- 6.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 467      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 542      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 162      |
|    time_elapsed    | 598      |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=551.88 +/- 710.31
Episode length: 36.78 +/- 5.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.8        |
|    mean_reward          | 552         |
| time/                   |             |
|    total_timesteps      | 166000      |
| train/                  |             |
|    approx_kl            | 0.013395815 |
|    clip_fraction        | 0.00576     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00881    |
|    explained_variance   | 0.125       |
|    learning_rate        | 0.001       |
|    loss                 | 3.3e+04     |
|    n_updates            | 4480        |
|    policy_gradient_loss | 0.00342     |
|    value_loss           | 9.42e+04    |
-----------------------------------------
Eval num_timesteps=166500, episode_reward=512.73 +/- 733.71
Episode length: 35.70 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 513      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 567      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 163      |
|    time_elapsed    | 601      |
|    total_timesteps | 166912   |
---------------------------------
Eval num_timesteps=167000, episode_reward=360.84 +/- 661.75
Episode length: 35.38 +/- 6.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 361          |
| time/                   |              |
|    total_timesteps      | 167000       |
| train/                  |              |
|    approx_kl            | 0.0008146646 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00503     |
|    explained_variance   | -0.0666      |
|    learning_rate        | 0.001        |
|    loss                 | 4.25e+04     |
|    n_updates            | 4490         |
|    policy_gradient_loss | 9.42e-05     |
|    value_loss           | 9.56e+04     |
------------------------------------------
Eval num_timesteps=167500, episode_reward=472.43 +/- 761.54
Episode length: 35.28 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 574      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 164      |
|    time_elapsed    | 605      |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=499.38 +/- 828.54
Episode length: 34.72 +/- 7.88
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.7          |
|    mean_reward          | 499           |
| time/                   |               |
|    total_timesteps      | 168000        |
| train/                  |               |
|    approx_kl            | 0.00040384202 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00439      |
|    explained_variance   | 0.0698        |
|    learning_rate        | 0.001         |
|    loss                 | 5.17e+04      |
|    n_updates            | 4500          |
|    policy_gradient_loss | -0.000239     |
|    value_loss           | 1.21e+05      |
-------------------------------------------
Eval num_timesteps=168500, episode_reward=353.36 +/- 638.97
Episode length: 35.22 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 534      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 165      |
|    time_elapsed    | 609      |
|    total_timesteps | 168960   |
---------------------------------
Eval num_timesteps=169000, episode_reward=384.81 +/- 700.26
Episode length: 34.86 +/- 6.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.9        |
|    mean_reward          | 385         |
| time/                   |             |
|    total_timesteps      | 169000      |
| train/                  |             |
|    approx_kl            | 0.004261721 |
|    clip_fraction        | 0.00186     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00762    |
|    explained_variance   | -0.0518     |
|    learning_rate        | 0.001       |
|    loss                 | 3.22e+04    |
|    n_updates            | 4510        |
|    policy_gradient_loss | 0.00173     |
|    value_loss           | 9.58e+04    |
-----------------------------------------
Eval num_timesteps=169500, episode_reward=290.03 +/- 665.19
Episode length: 34.10 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 290      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 502      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 166      |
|    time_elapsed    | 612      |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=477.24 +/- 811.17
Episode length: 35.46 +/- 7.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.5        |
|    mean_reward          | 477         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.002245565 |
|    clip_fraction        | 0.00156     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0105     |
|    explained_variance   | -0.183      |
|    learning_rate        | 0.001       |
|    loss                 | 3.44e+04    |
|    n_updates            | 4520        |
|    policy_gradient_loss | -0.000809   |
|    value_loss           | 1e+05       |
-----------------------------------------
Eval num_timesteps=170500, episode_reward=398.28 +/- 717.17
Episode length: 34.96 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 398      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=424.10 +/- 717.85
Episode length: 34.96 +/- 7.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 506      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 167      |
|    time_elapsed    | 617      |
|    total_timesteps | 171008   |
---------------------------------
Eval num_timesteps=171500, episode_reward=512.63 +/- 660.45
Episode length: 36.10 +/- 5.95
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 513          |
| time/                   |              |
|    total_timesteps      | 171500       |
| train/                  |              |
|    approx_kl            | 0.0003397037 |
|    clip_fraction        | 0.00254      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00902     |
|    explained_variance   | -0.194       |
|    learning_rate        | 0.001        |
|    loss                 | 4.28e+04     |
|    n_updates            | 4530         |
|    policy_gradient_loss | -0.0021      |
|    value_loss           | 1e+05        |
------------------------------------------
Eval num_timesteps=172000, episode_reward=389.41 +/- 719.30
Episode length: 35.46 +/- 6.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 389      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 515      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 168      |
|    time_elapsed    | 621      |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=421.08 +/- 737.94
Episode length: 35.32 +/- 7.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 421          |
| time/                   |              |
|    total_timesteps      | 172500       |
| train/                  |              |
|    approx_kl            | 0.0014527476 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00593     |
|    explained_variance   | -0.157       |
|    learning_rate        | 0.001        |
|    loss                 | 2.87e+04     |
|    n_updates            | 4540         |
|    policy_gradient_loss | 8.13e-05     |
|    value_loss           | 9.16e+04     |
------------------------------------------
Eval num_timesteps=173000, episode_reward=449.47 +/- 664.98
Episode length: 36.04 +/- 6.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 449      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 411      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 169      |
|    time_elapsed    | 624      |
|    total_timesteps | 173056   |
---------------------------------
Eval num_timesteps=173500, episode_reward=505.56 +/- 758.04
Episode length: 35.32 +/- 6.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 506          |
| time/                   |              |
|    total_timesteps      | 173500       |
| train/                  |              |
|    approx_kl            | 0.0001399513 |
|    clip_fraction        | 0.00186      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0222      |
|    explained_variance   | 0.015        |
|    learning_rate        | 0.001        |
|    loss                 | 1.97e+04     |
|    n_updates            | 4550         |
|    policy_gradient_loss | -0.00151     |
|    value_loss           | 5.74e+04     |
------------------------------------------
Eval num_timesteps=174000, episode_reward=559.35 +/- 747.45
Episode length: 36.90 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 559      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 460      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 170      |
|    time_elapsed    | 628      |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=436.77 +/- 710.19
Episode length: 34.72 +/- 6.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 437          |
| time/                   |              |
|    total_timesteps      | 174500       |
| train/                  |              |
|    approx_kl            | 0.0035076682 |
|    clip_fraction        | 0.00488      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0249      |
|    explained_variance   | -0.281       |
|    learning_rate        | 0.001        |
|    loss                 | 3.66e+04     |
|    n_updates            | 4560         |
|    policy_gradient_loss | 0.000819     |
|    value_loss           | 8.44e+04     |
------------------------------------------
Eval num_timesteps=175000, episode_reward=409.05 +/- 676.62
Episode length: 35.94 +/- 5.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 437      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 171      |
|    time_elapsed    | 632      |
|    total_timesteps | 175104   |
---------------------------------
Eval num_timesteps=175500, episode_reward=365.10 +/- 687.23
Episode length: 34.40 +/- 6.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 365          |
| time/                   |              |
|    total_timesteps      | 175500       |
| train/                  |              |
|    approx_kl            | 0.0065307906 |
|    clip_fraction        | 0.00439      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0213      |
|    explained_variance   | 0.0332       |
|    learning_rate        | 0.001        |
|    loss                 | 4.46e+04     |
|    n_updates            | 4570         |
|    policy_gradient_loss | -0.0015      |
|    value_loss           | 1.15e+05     |
------------------------------------------
Eval num_timesteps=176000, episode_reward=374.89 +/- 618.13
Episode length: 35.74 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 375      |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 448      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 172      |
|    time_elapsed    | 635      |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=416.40 +/- 747.18
Episode length: 35.34 +/- 6.15
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 416          |
| time/                   |              |
|    total_timesteps      | 176500       |
| train/                  |              |
|    approx_kl            | 0.0012843366 |
|    clip_fraction        | 0.00342      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0115      |
|    explained_variance   | 0.0492       |
|    learning_rate        | 0.001        |
|    loss                 | 4e+04        |
|    n_updates            | 4580         |
|    policy_gradient_loss | -0.000231    |
|    value_loss           | 9.4e+04      |
------------------------------------------
Eval num_timesteps=177000, episode_reward=448.38 +/- 746.14
Episode length: 34.62 +/- 6.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 448      |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 361      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 173      |
|    time_elapsed    | 639      |
|    total_timesteps | 177152   |
---------------------------------
Eval num_timesteps=177500, episode_reward=441.42 +/- 769.07
Episode length: 34.64 +/- 6.65
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 441           |
| time/                   |               |
|    total_timesteps      | 177500        |
| train/                  |               |
|    approx_kl            | 0.00010446814 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0334       |
|    explained_variance   | -0.0436       |
|    learning_rate        | 0.001         |
|    loss                 | 2.41e+04      |
|    n_updates            | 4590          |
|    policy_gradient_loss | -0.00207      |
|    value_loss           | 8.67e+04      |
-------------------------------------------
Eval num_timesteps=178000, episode_reward=580.79 +/- 764.32
Episode length: 36.72 +/- 6.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 581      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 277      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 174      |
|    time_elapsed    | 643      |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=197.19 +/- 591.34
Episode length: 33.92 +/- 7.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 33.9         |
|    mean_reward          | 197          |
| time/                   |              |
|    total_timesteps      | 178500       |
| train/                  |              |
|    approx_kl            | 0.0025490143 |
|    clip_fraction        | 0.00566      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0408      |
|    explained_variance   | -0.108       |
|    learning_rate        | 0.001        |
|    loss                 | 1.87e+04     |
|    n_updates            | 4600         |
|    policy_gradient_loss | -0.000739    |
|    value_loss           | 6.86e+04     |
------------------------------------------
Eval num_timesteps=179000, episode_reward=415.10 +/- 684.68
Episode length: 35.68 +/- 5.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 341      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 175      |
|    time_elapsed    | 646      |
|    total_timesteps | 179200   |
---------------------------------
Eval num_timesteps=179500, episode_reward=476.66 +/- 761.21
Episode length: 36.18 +/- 6.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.2        |
|    mean_reward          | 477         |
| time/                   |             |
|    total_timesteps      | 179500      |
| train/                  |             |
|    approx_kl            | 0.035629958 |
|    clip_fraction        | 0.0232      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0402     |
|    explained_variance   | -0.106      |
|    learning_rate        | 0.001       |
|    loss                 | 2.69e+04    |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.00509    |
|    value_loss           | 8.4e+04     |
-----------------------------------------
Eval num_timesteps=180000, episode_reward=427.49 +/- 725.40
Episode length: 35.64 +/- 6.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 357      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 176      |
|    time_elapsed    | 650      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=405.94 +/- 674.18
Episode length: 35.38 +/- 6.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.4         |
|    mean_reward          | 406          |
| time/                   |              |
|    total_timesteps      | 180500       |
| train/                  |              |
|    approx_kl            | 0.0064571537 |
|    clip_fraction        | 0.00771      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0394      |
|    explained_variance   | 0.0781       |
|    learning_rate        | 0.001        |
|    loss                 | 2.44e+04     |
|    n_updates            | 4620         |
|    policy_gradient_loss | 0.000804     |
|    value_loss           | 8.15e+04     |
------------------------------------------
Eval num_timesteps=181000, episode_reward=628.01 +/- 807.46
Episode length: 35.96 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 628      |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 393      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 177      |
|    time_elapsed    | 654      |
|    total_timesteps | 181248   |
---------------------------------
Eval num_timesteps=181500, episode_reward=469.79 +/- 744.94
Episode length: 35.46 +/- 6.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.5         |
|    mean_reward          | 470          |
| time/                   |              |
|    total_timesteps      | 181500       |
| train/                  |              |
|    approx_kl            | 0.0053809993 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0239      |
|    explained_variance   | -0.0976      |
|    learning_rate        | 0.001        |
|    loss                 | 2.42e+04     |
|    n_updates            | 4630         |
|    policy_gradient_loss | -0.000831    |
|    value_loss           | 7.52e+04     |
------------------------------------------
Eval num_timesteps=182000, episode_reward=352.55 +/- 691.01
Episode length: 35.34 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 353      |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 431      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 178      |
|    time_elapsed    | 657      |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=610.06 +/- 737.27
Episode length: 37.16 +/- 5.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.2         |
|    mean_reward          | 610          |
| time/                   |              |
|    total_timesteps      | 182500       |
| train/                  |              |
|    approx_kl            | 0.0013936955 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00892     |
|    explained_variance   | 0.0962       |
|    learning_rate        | 0.001        |
|    loss                 | 2.33e+04     |
|    n_updates            | 4640         |
|    policy_gradient_loss | -0.000728    |
|    value_loss           | 9.3e+04      |
------------------------------------------
Eval num_timesteps=183000, episode_reward=375.82 +/- 656.72
Episode length: 35.00 +/- 6.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 446      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 179      |
|    time_elapsed    | 661      |
|    total_timesteps | 183296   |
---------------------------------
Eval num_timesteps=183500, episode_reward=430.80 +/- 712.60
Episode length: 35.24 +/- 6.45
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 431           |
| time/                   |               |
|    total_timesteps      | 183500        |
| train/                  |               |
|    approx_kl            | 4.7716952e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0077       |
|    explained_variance   | -0.206        |
|    learning_rate        | 0.001         |
|    loss                 | 2.5e+04       |
|    n_updates            | 4650          |
|    policy_gradient_loss | -0.00104      |
|    value_loss           | 8.36e+04      |
-------------------------------------------
Eval num_timesteps=184000, episode_reward=291.48 +/- 615.91
Episode length: 34.42 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 291      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 374      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 180      |
|    time_elapsed    | 664      |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=469.84 +/- 764.80
Episode length: 35.06 +/- 6.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.1        |
|    mean_reward          | 470         |
| time/                   |             |
|    total_timesteps      | 184500      |
| train/                  |             |
|    approx_kl            | 0.010788391 |
|    clip_fraction        | 0.00293     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.022      |
|    explained_variance   | -0.169      |
|    learning_rate        | 0.001       |
|    loss                 | 3.67e+04    |
|    n_updates            | 4660        |
|    policy_gradient_loss | 0.00207     |
|    value_loss           | 9.35e+04    |
-----------------------------------------
Eval num_timesteps=185000, episode_reward=454.53 +/- 675.23
Episode length: 36.16 +/- 6.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 455      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 307      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 181      |
|    time_elapsed    | 668      |
|    total_timesteps | 185344   |
---------------------------------
Eval num_timesteps=185500, episode_reward=448.33 +/- 778.32
Episode length: 35.12 +/- 6.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 448          |
| time/                   |              |
|    total_timesteps      | 185500       |
| train/                  |              |
|    approx_kl            | 0.0036537002 |
|    clip_fraction        | 0.00303      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0151      |
|    explained_variance   | 0.055        |
|    learning_rate        | 0.001        |
|    loss                 | 2.96e+04     |
|    n_updates            | 4670         |
|    policy_gradient_loss | 0.000696     |
|    value_loss           | 9.17e+04     |
------------------------------------------
Eval num_timesteps=186000, episode_reward=495.03 +/- 749.57
Episode length: 35.78 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 311      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 182      |
|    time_elapsed    | 672      |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=273.21 +/- 610.10
Episode length: 34.72 +/- 5.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.7        |
|    mean_reward          | 273         |
| time/                   |             |
|    total_timesteps      | 186500      |
| train/                  |             |
|    approx_kl            | 0.002860832 |
|    clip_fraction        | 0.00342     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00862    |
|    explained_variance   | -0.102      |
|    learning_rate        | 0.001       |
|    loss                 | 3.1e+04     |
|    n_updates            | 4680        |
|    policy_gradient_loss | -0.000208   |
|    value_loss           | 8.92e+04    |
-----------------------------------------
Eval num_timesteps=187000, episode_reward=428.13 +/- 693.32
Episode length: 36.12 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 375      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 183      |
|    time_elapsed    | 675      |
|    total_timesteps | 187392   |
---------------------------------
Eval num_timesteps=187500, episode_reward=271.64 +/- 668.36
Episode length: 32.84 +/- 6.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 32.8          |
|    mean_reward          | 272           |
| time/                   |               |
|    total_timesteps      | 187500        |
| train/                  |               |
|    approx_kl            | 1.8509745e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00699      |
|    explained_variance   | -0.034        |
|    learning_rate        | 0.001         |
|    loss                 | 3.59e+04      |
|    n_updates            | 4690          |
|    policy_gradient_loss | -0.000549     |
|    value_loss           | 8.32e+04      |
-------------------------------------------
Eval num_timesteps=188000, episode_reward=385.97 +/- 701.98
Episode length: 35.06 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 386      |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 405      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 184      |
|    time_elapsed    | 679      |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=290.20 +/- 612.40
Episode length: 35.12 +/- 6.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.1        |
|    mean_reward          | 290         |
| time/                   |             |
|    total_timesteps      | 188500      |
| train/                  |             |
|    approx_kl            | 0.006959965 |
|    clip_fraction        | 0.00732     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0208     |
|    explained_variance   | -0.0479     |
|    learning_rate        | 0.001       |
|    loss                 | 3.99e+04    |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.000216   |
|    value_loss           | 9.17e+04    |
-----------------------------------------
Eval num_timesteps=189000, episode_reward=346.09 +/- 683.67
Episode length: 34.64 +/- 6.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 343      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 185      |
|    time_elapsed    | 682      |
|    total_timesteps | 189440   |
---------------------------------
Eval num_timesteps=189500, episode_reward=145.80 +/- 471.99
Episode length: 33.82 +/- 6.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.8        |
|    mean_reward          | 146         |
| time/                   |             |
|    total_timesteps      | 189500      |
| train/                  |             |
|    approx_kl            | 0.007036634 |
|    clip_fraction        | 0.00205     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00791    |
|    explained_variance   | -0.0993     |
|    learning_rate        | 0.001       |
|    loss                 | 2.7e+04     |
|    n_updates            | 4710        |
|    policy_gradient_loss | -0.000536   |
|    value_loss           | 7.48e+04    |
-----------------------------------------
Eval num_timesteps=190000, episode_reward=317.47 +/- 730.96
Episode length: 33.32 +/- 8.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 423      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 186      |
|    time_elapsed    | 686      |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=305.01 +/- 661.53
Episode length: 34.14 +/- 7.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.1        |
|    mean_reward          | 305         |
| time/                   |             |
|    total_timesteps      | 190500      |
| train/                  |             |
|    approx_kl            | 0.010167948 |
|    clip_fraction        | 0.00107     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0116     |
|    explained_variance   | -0.0192     |
|    learning_rate        | 0.001       |
|    loss                 | 2.92e+04    |
|    n_updates            | 4720        |
|    policy_gradient_loss | 0.0087      |
|    value_loss           | 7.7e+04     |
-----------------------------------------
Eval num_timesteps=191000, episode_reward=359.47 +/- 727.65
Episode length: 34.88 +/- 6.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 411      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 187      |
|    time_elapsed    | 689      |
|    total_timesteps | 191488   |
---------------------------------
Eval num_timesteps=191500, episode_reward=524.90 +/- 781.03
Episode length: 36.14 +/- 6.85
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 525          |
| time/                   |              |
|    total_timesteps      | 191500       |
| train/                  |              |
|    approx_kl            | 0.0025865764 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00303     |
|    explained_variance   | 0.036        |
|    learning_rate        | 0.001        |
|    loss                 | 3.7e+04      |
|    n_updates            | 4730         |
|    policy_gradient_loss | -0.0012      |
|    value_loss           | 1.01e+05     |
------------------------------------------
Eval num_timesteps=192000, episode_reward=487.50 +/- 784.47
Episode length: 35.42 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=475.64 +/- 719.05
Episode length: 35.72 +/- 5.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 476      |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 473      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 188      |
|    time_elapsed    | 694      |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=356.94 +/- 678.22
Episode length: 34.96 +/- 6.82
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35            |
|    mean_reward          | 357           |
| time/                   |               |
|    total_timesteps      | 193000        |
| train/                  |               |
|    approx_kl            | 2.8191542e-05 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00502      |
|    explained_variance   | -0.146        |
|    learning_rate        | 0.001         |
|    loss                 | 3.4e+04       |
|    n_updates            | 4740          |
|    policy_gradient_loss | -0.000206     |
|    value_loss           | 8.76e+04      |
-------------------------------------------
Eval num_timesteps=193500, episode_reward=624.94 +/- 770.94
Episode length: 37.28 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.3     |
|    mean_reward     | 625      |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 403      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 189      |
|    time_elapsed    | 698      |
|    total_timesteps | 193536   |
---------------------------------
Eval num_timesteps=194000, episode_reward=479.47 +/- 773.47
Episode length: 35.22 +/- 7.12
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 479           |
| time/                   |               |
|    total_timesteps      | 194000        |
| train/                  |               |
|    approx_kl            | 0.00011512096 |
|    clip_fraction        | 0.000781      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0224       |
|    explained_variance   | 0.0414        |
|    learning_rate        | 0.001         |
|    loss                 | 2.77e+04      |
|    n_updates            | 4750          |
|    policy_gradient_loss | -0.000994     |
|    value_loss           | 7.78e+04      |
-------------------------------------------
Eval num_timesteps=194500, episode_reward=557.94 +/- 789.42
Episode length: 35.96 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 558      |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 490      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 190      |
|    time_elapsed    | 702      |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=423.61 +/- 757.04
Episode length: 33.90 +/- 6.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.9        |
|    mean_reward          | 424         |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.021248454 |
|    clip_fraction        | 0.00693     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0259     |
|    explained_variance   | -0.0332     |
|    learning_rate        | 0.001       |
|    loss                 | 3.72e+04    |
|    n_updates            | 4760        |
|    policy_gradient_loss | 0.00254     |
|    value_loss           | 8.58e+04    |
-----------------------------------------
Eval num_timesteps=195500, episode_reward=274.29 +/- 629.16
Episode length: 34.22 +/- 6.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 352      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 191      |
|    time_elapsed    | 705      |
|    total_timesteps | 195584   |
---------------------------------
Eval num_timesteps=196000, episode_reward=593.08 +/- 785.21
Episode length: 35.58 +/- 7.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.6        |
|    mean_reward          | 593         |
| time/                   |             |
|    total_timesteps      | 196000      |
| train/                  |             |
|    approx_kl            | 0.018574435 |
|    clip_fraction        | 0.0142      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0306     |
|    explained_variance   | -0.171      |
|    learning_rate        | 0.001       |
|    loss                 | 2.12e+04    |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.00221    |
|    value_loss           | 8.97e+04    |
-----------------------------------------
Eval num_timesteps=196500, episode_reward=488.99 +/- 732.48
Episode length: 36.28 +/- 5.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 489      |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 192      |
|    time_elapsed    | 709      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=295.94 +/- 679.28
Episode length: 34.14 +/- 7.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.1        |
|    mean_reward          | 296         |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.008820691 |
|    clip_fraction        | 0.00273     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0203     |
|    explained_variance   | -0.031      |
|    learning_rate        | 0.001       |
|    loss                 | 2.18e+04    |
|    n_updates            | 4780        |
|    policy_gradient_loss | 0.00019     |
|    value_loss           | 7.42e+04    |
-----------------------------------------
Eval num_timesteps=197500, episode_reward=279.37 +/- 542.43
Episode length: 35.10 +/- 5.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 279      |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 401      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 193      |
|    time_elapsed    | 712      |
|    total_timesteps | 197632   |
---------------------------------
Eval num_timesteps=198000, episode_reward=495.72 +/- 746.22
Episode length: 35.28 +/- 7.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.3        |
|    mean_reward          | 496         |
| time/                   |             |
|    total_timesteps      | 198000      |
| train/                  |             |
|    approx_kl            | 0.008282028 |
|    clip_fraction        | 0.00186     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0122     |
|    explained_variance   | 0.0029      |
|    learning_rate        | 0.001       |
|    loss                 | 3.15e+04    |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.000571   |
|    value_loss           | 7.9e+04     |
-----------------------------------------
Eval num_timesteps=198500, episode_reward=485.82 +/- 751.12
Episode length: 35.82 +/- 6.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 461      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 194      |
|    time_elapsed    | 716      |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=386.48 +/- 747.52
Episode length: 33.86 +/- 7.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.9        |
|    mean_reward          | 386         |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.015852543 |
|    clip_fraction        | 0.00254     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00465    |
|    explained_variance   | -0.0598     |
|    learning_rate        | 0.001       |
|    loss                 | 2.96e+04    |
|    n_updates            | 4800        |
|    policy_gradient_loss | 0.00121     |
|    value_loss           | 9.32e+04    |
-----------------------------------------
Eval num_timesteps=199500, episode_reward=405.09 +/- 650.88
Episode length: 36.32 +/- 5.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 464      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 195      |
|    time_elapsed    | 720      |
|    total_timesteps | 199680   |
---------------------------------
Eval num_timesteps=200000, episode_reward=434.52 +/- 655.97
Episode length: 36.18 +/- 5.70
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.2          |
|    mean_reward          | 435           |
| time/                   |               |
|    total_timesteps      | 200000        |
| train/                  |               |
|    approx_kl            | 0.00015884032 |
|    clip_fraction        | 0.000488      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00771      |
|    explained_variance   | -0.19         |
|    learning_rate        | 0.001         |
|    loss                 | 2.07e+04      |
|    n_updates            | 4810          |
|    policy_gradient_loss | -0.000561     |
|    value_loss           | 7.85e+04      |
-------------------------------------------
Eval num_timesteps=200500, episode_reward=407.27 +/- 718.31
Episode length: 35.06 +/- 7.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 419      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 196      |
|    time_elapsed    | 723      |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=425.63 +/- 666.37
Episode length: 35.36 +/- 6.26
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 426           |
| time/                   |               |
|    total_timesteps      | 201000        |
| train/                  |               |
|    approx_kl            | 0.00043222075 |
|    clip_fraction        | 0.00166       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.022        |
|    explained_variance   | -0.178        |
|    learning_rate        | 0.001         |
|    loss                 | 4.06e+04      |
|    n_updates            | 4820          |
|    policy_gradient_loss | -0.00106      |
|    value_loss           | 7.96e+04      |
-------------------------------------------
Eval num_timesteps=201500, episode_reward=413.13 +/- 739.77
Episode length: 34.88 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 413      |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 338      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 197      |
|    time_elapsed    | 727      |
|    total_timesteps | 201728   |
---------------------------------
Eval num_timesteps=202000, episode_reward=321.70 +/- 586.33
Episode length: 35.28 +/- 6.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 322          |
| time/                   |              |
|    total_timesteps      | 202000       |
| train/                  |              |
|    approx_kl            | 0.0033160578 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0401      |
|    explained_variance   | 0.0204       |
|    learning_rate        | 0.001        |
|    loss                 | 3.32e+04     |
|    n_updates            | 4830         |
|    policy_gradient_loss | 0.000355     |
|    value_loss           | 6.78e+04     |
------------------------------------------
Eval num_timesteps=202500, episode_reward=584.44 +/- 735.33
Episode length: 36.84 +/- 4.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 584      |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 270      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 198      |
|    time_elapsed    | 730      |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=652.25 +/- 769.06
Episode length: 36.02 +/- 7.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36          |
|    mean_reward          | 652         |
| time/                   |             |
|    total_timesteps      | 203000      |
| train/                  |             |
|    approx_kl            | 0.010519853 |
|    clip_fraction        | 0.00342     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | -0.132      |
|    learning_rate        | 0.001       |
|    loss                 | 2.75e+04    |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.00719    |
|    value_loss           | 6.13e+04    |
-----------------------------------------
Eval num_timesteps=203500, episode_reward=393.72 +/- 652.61
Episode length: 35.26 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 225      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 199      |
|    time_elapsed    | 734      |
|    total_timesteps | 203776   |
---------------------------------
Eval num_timesteps=204000, episode_reward=446.29 +/- 727.53
Episode length: 34.88 +/- 6.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.9         |
|    mean_reward          | 446          |
| time/                   |              |
|    total_timesteps      | 204000       |
| train/                  |              |
|    approx_kl            | 0.0077339225 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.248       |
|    explained_variance   | -0.0956      |
|    learning_rate        | 0.001        |
|    loss                 | 1.98e+04     |
|    n_updates            | 4850         |
|    policy_gradient_loss | -0.00536     |
|    value_loss           | 5.43e+04     |
------------------------------------------
Eval num_timesteps=204500, episode_reward=442.85 +/- 711.02
Episode length: 36.00 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 229      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 200      |
|    time_elapsed    | 738      |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=384.41 +/- 720.38
Episode length: 34.74 +/- 6.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.7       |
|    mean_reward          | 384        |
| time/                   |            |
|    total_timesteps      | 205000     |
| train/                  |            |
|    approx_kl            | 0.07267494 |
|    clip_fraction        | 0.0192     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.21      |
|    explained_variance   | -0.106     |
|    learning_rate        | 0.001      |
|    loss                 | 2.01e+04   |
|    n_updates            | 4860       |
|    policy_gradient_loss | -0.00135   |
|    value_loss           | 7.82e+04   |
----------------------------------------
Eval num_timesteps=205500, episode_reward=424.74 +/- 746.86
Episode length: 35.08 +/- 5.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.9     |
|    ep_rew_mean     | 170      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 201      |
|    time_elapsed    | 741      |
|    total_timesteps | 205824   |
---------------------------------
Eval num_timesteps=206000, episode_reward=542.75 +/- 762.71
Episode length: 36.36 +/- 6.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.4        |
|    mean_reward          | 543         |
| time/                   |             |
|    total_timesteps      | 206000      |
| train/                  |             |
|    approx_kl            | 0.024177354 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | -0.0414     |
|    learning_rate        | 0.001       |
|    loss                 | 3.87e+04    |
|    n_updates            | 4870        |
|    policy_gradient_loss | 0.0138      |
|    value_loss           | 8.34e+04    |
-----------------------------------------
Eval num_timesteps=206500, episode_reward=529.53 +/- 728.53
Episode length: 36.76 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 530      |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 174      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 202      |
|    time_elapsed    | 745      |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=442.09 +/- 710.17
Episode length: 35.12 +/- 6.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 35.1       |
|    mean_reward          | 442        |
| time/                   |            |
|    total_timesteps      | 207000     |
| train/                  |            |
|    approx_kl            | 0.01176149 |
|    clip_fraction        | 0.0108     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.192     |
|    explained_variance   | 0.0405     |
|    learning_rate        | 0.001      |
|    loss                 | 2.73e+04   |
|    n_updates            | 4880       |
|    policy_gradient_loss | -0.00796   |
|    value_loss           | 4.7e+04    |
----------------------------------------
Eval num_timesteps=207500, episode_reward=337.03 +/- 684.31
Episode length: 34.96 +/- 6.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 104      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 203      |
|    time_elapsed    | 749      |
|    total_timesteps | 207872   |
---------------------------------
Eval num_timesteps=208000, episode_reward=413.93 +/- 767.35
Episode length: 34.66 +/- 7.04
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.7       |
|    mean_reward          | 414        |
| time/                   |            |
|    total_timesteps      | 208000     |
| train/                  |            |
|    approx_kl            | 0.03341736 |
|    clip_fraction        | 0.0197     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.19      |
|    explained_variance   | 0.055      |
|    learning_rate        | 0.001      |
|    loss                 | 2.53e+04   |
|    n_updates            | 4890       |
|    policy_gradient_loss | -0.00101   |
|    value_loss           | 4.27e+04   |
----------------------------------------
Eval num_timesteps=208500, episode_reward=300.28 +/- 673.45
Episode length: 34.02 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 300      |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 120      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 204      |
|    time_elapsed    | 752      |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=512.83 +/- 731.61
Episode length: 36.46 +/- 5.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 36.5      |
|    mean_reward          | 513       |
| time/                   |           |
|    total_timesteps      | 209000    |
| train/                  |           |
|    approx_kl            | 0.0394914 |
|    clip_fraction        | 0.0143    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0893   |
|    explained_variance   | 0.0932    |
|    learning_rate        | 0.001     |
|    loss                 | 2.37e+04  |
|    n_updates            | 4900      |
|    policy_gradient_loss | -9.9e-06  |
|    value_loss           | 6.73e+04  |
---------------------------------------
Eval num_timesteps=209500, episode_reward=437.12 +/- 701.79
Episode length: 35.22 +/- 6.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.2     |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 213      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 205      |
|    time_elapsed    | 756      |
|    total_timesteps | 209920   |
---------------------------------
Eval num_timesteps=210000, episode_reward=240.62 +/- 570.72
Episode length: 33.98 +/- 6.48
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34         |
|    mean_reward          | 241        |
| time/                   |            |
|    total_timesteps      | 210000     |
| train/                  |            |
|    approx_kl            | 0.09069432 |
|    clip_fraction        | 0.021      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0459    |
|    explained_variance   | -0.168     |
|    learning_rate        | 0.001      |
|    loss                 | 3.15e+04   |
|    n_updates            | 4910       |
|    policy_gradient_loss | -0.00719   |
|    value_loss           | 7.79e+04   |
----------------------------------------
Eval num_timesteps=210500, episode_reward=358.62 +/- 683.15
Episode length: 34.08 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 359      |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 290      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 206      |
|    time_elapsed    | 759      |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=316.06 +/- 604.09
Episode length: 35.20 +/- 5.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.2        |
|    mean_reward          | 316         |
| time/                   |             |
|    total_timesteps      | 211000      |
| train/                  |             |
|    approx_kl            | 0.008170369 |
|    clip_fraction        | 0.00166     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00907    |
|    explained_variance   | 0.00649     |
|    learning_rate        | 0.001       |
|    loss                 | 2.85e+04    |
|    n_updates            | 4920        |
|    policy_gradient_loss | 0.00343     |
|    value_loss           | 8.26e+04    |
-----------------------------------------
Eval num_timesteps=211500, episode_reward=565.97 +/- 750.72
Episode length: 36.78 +/- 5.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 566      |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 276      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 207      |
|    time_elapsed    | 763      |
|    total_timesteps | 211968   |
---------------------------------
Eval num_timesteps=212000, episode_reward=404.07 +/- 670.60
Episode length: 36.18 +/- 6.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.2         |
|    mean_reward          | 404          |
| time/                   |              |
|    total_timesteps      | 212000       |
| train/                  |              |
|    approx_kl            | 0.0039457283 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0458      |
|    explained_variance   | -0.134       |
|    learning_rate        | 0.001        |
|    loss                 | 2.62e+04     |
|    n_updates            | 4930         |
|    policy_gradient_loss | -0.000134    |
|    value_loss           | 7.33e+04     |
------------------------------------------
Eval num_timesteps=212500, episode_reward=453.90 +/- 723.19
Episode length: 35.12 +/- 7.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 245      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 208      |
|    time_elapsed    | 767      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=418.55 +/- 678.50
Episode length: 36.08 +/- 5.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 36.1       |
|    mean_reward          | 419        |
| time/                   |            |
|    total_timesteps      | 213000     |
| train/                  |            |
|    approx_kl            | 0.02961637 |
|    clip_fraction        | 0.00703    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0245    |
|    explained_variance   | -0.119     |
|    learning_rate        | 0.001      |
|    loss                 | 2.53e+04   |
|    n_updates            | 4940       |
|    policy_gradient_loss | 0.000571   |
|    value_loss           | 8.23e+04   |
----------------------------------------
Eval num_timesteps=213500, episode_reward=337.59 +/- 661.84
Episode length: 35.02 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 338      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=436.44 +/- 654.32
Episode length: 36.26 +/- 6.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 436      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 294      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 209      |
|    time_elapsed    | 772      |
|    total_timesteps | 214016   |
---------------------------------
Eval num_timesteps=214500, episode_reward=504.48 +/- 698.04
Episode length: 36.34 +/- 5.81
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 504          |
| time/                   |              |
|    total_timesteps      | 214500       |
| train/                  |              |
|    approx_kl            | 0.0076048756 |
|    clip_fraction        | 0.00479      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00957     |
|    explained_variance   | -0.0498      |
|    learning_rate        | 0.001        |
|    loss                 | 2.91e+04     |
|    n_updates            | 4950         |
|    policy_gradient_loss | -0.000363    |
|    value_loss           | 7.83e+04     |
------------------------------------------
Eval num_timesteps=215000, episode_reward=229.98 +/- 620.81
Episode length: 34.26 +/- 6.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 335      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 210      |
|    time_elapsed    | 775      |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=207.44 +/- 490.69
Episode length: 34.62 +/- 5.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 207          |
| time/                   |              |
|    total_timesteps      | 215500       |
| train/                  |              |
|    approx_kl            | 0.0011263394 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00573     |
|    explained_variance   | -0.0816      |
|    learning_rate        | 0.001        |
|    loss                 | 2.5e+04      |
|    n_updates            | 4960         |
|    policy_gradient_loss | 0.000777     |
|    value_loss           | 7.66e+04     |
------------------------------------------
Eval num_timesteps=216000, episode_reward=538.08 +/- 739.30
Episode length: 36.62 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 538      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 351      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 211      |
|    time_elapsed    | 779      |
|    total_timesteps | 216064   |
---------------------------------
Eval num_timesteps=216500, episode_reward=638.68 +/- 783.37
Episode length: 36.54 +/- 6.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.5         |
|    mean_reward          | 639          |
| time/                   |              |
|    total_timesteps      | 216500       |
| train/                  |              |
|    approx_kl            | 1.453032e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00129     |
|    explained_variance   | 0.0939       |
|    learning_rate        | 0.001        |
|    loss                 | 2.84e+04     |
|    n_updates            | 4970         |
|    policy_gradient_loss | -0.000638    |
|    value_loss           | 8.08e+04     |
------------------------------------------
Eval num_timesteps=217000, episode_reward=404.74 +/- 701.66
Episode length: 35.08 +/- 6.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 387      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 212      |
|    time_elapsed    | 783      |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=541.24 +/- 765.19
Episode length: 36.26 +/- 5.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.3        |
|    mean_reward          | 541         |
| time/                   |             |
|    total_timesteps      | 217500      |
| train/                  |             |
|    approx_kl            | 0.013003555 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0022     |
|    explained_variance   | -0.0151     |
|    learning_rate        | 0.001       |
|    loss                 | 3.8e+04     |
|    n_updates            | 4980        |
|    policy_gradient_loss | -0.000136   |
|    value_loss           | 1.01e+05    |
-----------------------------------------
Eval num_timesteps=218000, episode_reward=402.23 +/- 792.89
Episode length: 34.24 +/- 7.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 402      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 515      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 213      |
|    time_elapsed    | 786      |
|    total_timesteps | 218112   |
---------------------------------
Eval num_timesteps=218500, episode_reward=445.53 +/- 794.45
Episode length: 34.38 +/- 7.26
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.4         |
|    mean_reward          | 446          |
| time/                   |              |
|    total_timesteps      | 218500       |
| train/                  |              |
|    approx_kl            | 4.469184e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000114    |
|    explained_variance   | 0.0377       |
|    learning_rate        | 0.001        |
|    loss                 | 5e+04        |
|    n_updates            | 4990         |
|    policy_gradient_loss | -0.000414    |
|    value_loss           | 1.04e+05     |
------------------------------------------
Eval num_timesteps=219000, episode_reward=497.16 +/- 721.45
Episode length: 36.48 +/- 6.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 460      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 214      |
|    time_elapsed    | 790      |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=435.11 +/- 706.10
Episode length: 36.10 +/- 5.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.1         |
|    mean_reward          | 435          |
| time/                   |              |
|    total_timesteps      | 219500       |
| train/                  |              |
|    approx_kl            | 3.871275e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.000669    |
|    explained_variance   | -0.0839      |
|    learning_rate        | 0.001        |
|    loss                 | 5.63e+04     |
|    n_updates            | 5000         |
|    policy_gradient_loss | -0.000117    |
|    value_loss           | 1.1e+05      |
------------------------------------------
Eval num_timesteps=220000, episode_reward=288.13 +/- 660.51
Episode length: 34.10 +/- 6.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 555      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 215      |
|    time_elapsed    | 793      |
|    total_timesteps | 220160   |
---------------------------------
Eval num_timesteps=220500, episode_reward=560.12 +/- 792.57
Episode length: 35.88 +/- 6.30
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 560           |
| time/                   |               |
|    total_timesteps      | 220500        |
| train/                  |               |
|    approx_kl            | 2.4709152e-07 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00025      |
|    explained_variance   | 0.00509       |
|    learning_rate        | 0.001         |
|    loss                 | 3.86e+04      |
|    n_updates            | 5010          |
|    policy_gradient_loss | -2.8e-05      |
|    value_loss           | 9.91e+04      |
-------------------------------------------
Eval num_timesteps=221000, episode_reward=457.25 +/- 710.94
Episode length: 36.60 +/- 6.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 540      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 216      |
|    time_elapsed    | 797      |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=243.08 +/- 665.76
Episode length: 33.02 +/- 7.23
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33            |
|    mean_reward          | 243           |
| time/                   |               |
|    total_timesteps      | 221500        |
| train/                  |               |
|    approx_kl            | 2.7916976e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00252      |
|    explained_variance   | -0.0929       |
|    learning_rate        | 0.001         |
|    loss                 | 3.33e+04      |
|    n_updates            | 5020          |
|    policy_gradient_loss | -0.000608     |
|    value_loss           | 1.09e+05      |
-------------------------------------------
Eval num_timesteps=222000, episode_reward=474.26 +/- 688.50
Episode length: 36.54 +/- 5.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 474      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 426      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 217      |
|    time_elapsed    | 801      |
|    total_timesteps | 222208   |
---------------------------------
Eval num_timesteps=222500, episode_reward=465.80 +/- 698.70
Episode length: 35.26 +/- 6.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.3         |
|    mean_reward          | 466          |
| time/                   |              |
|    total_timesteps      | 222500       |
| train/                  |              |
|    approx_kl            | 7.741607e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0205      |
|    explained_variance   | -0.0872      |
|    learning_rate        | 0.001        |
|    loss                 | 2.44e+04     |
|    n_updates            | 5030         |
|    policy_gradient_loss | -0.00141     |
|    value_loss           | 5.64e+04     |
------------------------------------------
Eval num_timesteps=223000, episode_reward=261.04 +/- 622.11
Episode length: 34.34 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 340      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 218      |
|    time_elapsed    | 804      |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=298.42 +/- 659.05
Episode length: 34.70 +/- 6.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.7         |
|    mean_reward          | 298          |
| time/                   |              |
|    total_timesteps      | 223500       |
| train/                  |              |
|    approx_kl            | 0.0049229437 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.014       |
|    explained_variance   | 0.0562       |
|    learning_rate        | 0.001        |
|    loss                 | 3.18e+04     |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.00255     |
|    value_loss           | 8.13e+04     |
------------------------------------------
Eval num_timesteps=224000, episode_reward=466.18 +/- 729.13
Episode length: 35.02 +/- 7.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 219      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 219      |
|    time_elapsed    | 808      |
|    total_timesteps | 224256   |
---------------------------------
Eval num_timesteps=224500, episode_reward=331.02 +/- 692.63
Episode length: 33.54 +/- 7.27
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.5          |
|    mean_reward          | 331           |
| time/                   |               |
|    total_timesteps      | 224500        |
| train/                  |               |
|    approx_kl            | 0.00041362713 |
|    clip_fraction        | 0.00137       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0413       |
|    explained_variance   | -0.208        |
|    learning_rate        | 0.001         |
|    loss                 | 2.47e+04      |
|    n_updates            | 5050          |
|    policy_gradient_loss | -0.00141      |
|    value_loss           | 5.24e+04      |
-------------------------------------------
Eval num_timesteps=225000, episode_reward=352.50 +/- 727.56
Episode length: 34.16 +/- 6.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.2     |
|    mean_reward     | 352      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 243      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 220      |
|    time_elapsed    | 811      |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=412.96 +/- 713.06
Episode length: 35.10 +/- 6.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.1        |
|    mean_reward          | 413         |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 0.016125618 |
|    clip_fraction        | 0.00986     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0732     |
|    explained_variance   | 0.0455      |
|    learning_rate        | 0.001       |
|    loss                 | 2.73e+04    |
|    n_updates            | 5060        |
|    policy_gradient_loss | -0.0021     |
|    value_loss           | 6.96e+04    |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=501.34 +/- 738.37
Episode length: 36.56 +/- 5.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.6     |
|    mean_reward     | 501      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 276      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 221      |
|    time_elapsed    | 815      |
|    total_timesteps | 226304   |
---------------------------------
Eval num_timesteps=226500, episode_reward=541.63 +/- 784.52
Episode length: 36.42 +/- 5.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.4        |
|    mean_reward          | 542         |
| time/                   |             |
|    total_timesteps      | 226500      |
| train/                  |             |
|    approx_kl            | 0.021960769 |
|    clip_fraction        | 0.00605     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0128     |
|    explained_variance   | 0.112       |
|    learning_rate        | 0.001       |
|    loss                 | 3.75e+04    |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.00325    |
|    value_loss           | 9.68e+04    |
-----------------------------------------
Eval num_timesteps=227000, episode_reward=309.99 +/- 610.30
Episode length: 35.02 +/- 6.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 332      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 222      |
|    time_elapsed    | 819      |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=299.93 +/- 647.80
Episode length: 35.12 +/- 5.96
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 300           |
| time/                   |               |
|    total_timesteps      | 227500        |
| train/                  |               |
|    approx_kl            | 0.00061973184 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00512      |
|    explained_variance   | -0.218        |
|    learning_rate        | 0.001         |
|    loss                 | 3.36e+04      |
|    n_updates            | 5080          |
|    policy_gradient_loss | -0.000506     |
|    value_loss           | 8.05e+04      |
-------------------------------------------
Eval num_timesteps=228000, episode_reward=355.55 +/- 692.65
Episode length: 34.94 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 356      |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 425      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 223      |
|    time_elapsed    | 822      |
|    total_timesteps | 228352   |
---------------------------------
Eval num_timesteps=228500, episode_reward=614.57 +/- 740.81
Episode length: 37.16 +/- 5.52
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.2          |
|    mean_reward          | 615           |
| time/                   |               |
|    total_timesteps      | 228500        |
| train/                  |               |
|    approx_kl            | 5.3812983e-06 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00161      |
|    explained_variance   | -0.124        |
|    learning_rate        | 0.001         |
|    loss                 | 5.22e+04      |
|    n_updates            | 5090          |
|    policy_gradient_loss | -0.000274     |
|    value_loss           | 9.65e+04      |
-------------------------------------------
Eval num_timesteps=229000, episode_reward=595.31 +/- 740.61
Episode length: 37.36 +/- 5.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.4     |
|    mean_reward     | 595      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 445      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 224      |
|    time_elapsed    | 826      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=533.22 +/- 753.63
Episode length: 35.24 +/- 6.90
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.2          |
|    mean_reward          | 533           |
| time/                   |               |
|    total_timesteps      | 229500        |
| train/                  |               |
|    approx_kl            | 1.0542048e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00301      |
|    explained_variance   | -0.0173       |
|    learning_rate        | 0.001         |
|    loss                 | 3.02e+04      |
|    n_updates            | 5100          |
|    policy_gradient_loss | -0.00032      |
|    value_loss           | 8.2e+04       |
-------------------------------------------
Eval num_timesteps=230000, episode_reward=562.17 +/- 718.59
Episode length: 36.92 +/- 6.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 562      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 458      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 225      |
|    time_elapsed    | 830      |
|    total_timesteps | 230400   |
---------------------------------
Eval num_timesteps=230500, episode_reward=436.07 +/- 695.89
Episode length: 35.84 +/- 6.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.8          |
|    mean_reward          | 436           |
| time/                   |               |
|    total_timesteps      | 230500        |
| train/                  |               |
|    approx_kl            | 2.1411746e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00421      |
|    explained_variance   | -0.218        |
|    learning_rate        | 0.001         |
|    loss                 | 2.17e+04      |
|    n_updates            | 5110          |
|    policy_gradient_loss | -0.000876     |
|    value_loss           | 8.33e+04      |
-------------------------------------------
Eval num_timesteps=231000, episode_reward=230.26 +/- 552.18
Episode length: 34.56 +/- 6.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.6     |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 226      |
|    time_elapsed    | 833      |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=610.76 +/- 780.12
Episode length: 37.42 +/- 5.41
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 37.4         |
|    mean_reward          | 611          |
| time/                   |              |
|    total_timesteps      | 231500       |
| train/                  |              |
|    approx_kl            | 9.925978e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00196     |
|    explained_variance   | -0.112       |
|    learning_rate        | 0.001        |
|    loss                 | 2e+04        |
|    n_updates            | 5120         |
|    policy_gradient_loss | -0.000328    |
|    value_loss           | 8.19e+04     |
------------------------------------------
Eval num_timesteps=232000, episode_reward=282.17 +/- 642.04
Episode length: 34.10 +/- 6.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.1     |
|    mean_reward     | 282      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 472      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 227      |
|    time_elapsed    | 837      |
|    total_timesteps | 232448   |
---------------------------------
Eval num_timesteps=232500, episode_reward=270.25 +/- 588.56
Episode length: 34.26 +/- 6.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.3        |
|    mean_reward          | 270         |
| time/                   |             |
|    total_timesteps      | 232500      |
| train/                  |             |
|    approx_kl            | 0.004451195 |
|    clip_fraction        | 0.00283     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00122    |
|    explained_variance   | -0.00106    |
|    learning_rate        | 0.001       |
|    loss                 | 4.42e+04    |
|    n_updates            | 5130        |
|    policy_gradient_loss | 0.000931    |
|    value_loss           | 9.09e+04    |
-----------------------------------------
Eval num_timesteps=233000, episode_reward=318.31 +/- 727.23
Episode length: 33.34 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 425      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 228      |
|    time_elapsed    | 841      |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=612.90 +/- 760.58
Episode length: 37.18 +/- 5.42
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 37.2          |
|    mean_reward          | 613           |
| time/                   |               |
|    total_timesteps      | 233500        |
| train/                  |               |
|    approx_kl            | 1.4048186e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00615      |
|    explained_variance   | 0.0469        |
|    learning_rate        | 0.001         |
|    loss                 | 4.46e+04      |
|    n_updates            | 5140          |
|    policy_gradient_loss | -0.000329     |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=234000, episode_reward=487.33 +/- 777.64
Episode length: 34.42 +/- 8.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 386      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 229      |
|    time_elapsed    | 844      |
|    total_timesteps | 234496   |
---------------------------------
Eval num_timesteps=234500, episode_reward=324.68 +/- 617.78
Episode length: 34.84 +/- 6.34
--------------------------------------------
| eval/                   |                |
|    mean_ep_length       | 34.8           |
|    mean_reward          | 325            |
| time/                   |                |
|    total_timesteps      | 234500         |
| train/                  |                |
|    approx_kl            | 0.000117555435 |
|    clip_fraction        | 0              |
|    clip_range           | 0.2            |
|    entropy_loss         | -0.04          |
|    explained_variance   | -0.0116        |
|    learning_rate        | 0.001          |
|    loss                 | 2.11e+04       |
|    n_updates            | 5150           |
|    policy_gradient_loss | -0.00258       |
|    value_loss           | 5.75e+04       |
--------------------------------------------
Eval num_timesteps=235000, episode_reward=548.88 +/- 719.49
Episode length: 37.00 +/- 6.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=373.22 +/- 737.28
Episode length: 34.38 +/- 6.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.4     |
|    mean_reward     | 373      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 343      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 230      |
|    time_elapsed    | 849      |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=355.49 +/- 661.42
Episode length: 34.04 +/- 6.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 355          |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 0.0038576298 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0294      |
|    explained_variance   | 0.0118       |
|    learning_rate        | 0.001        |
|    loss                 | 3.48e+04     |
|    n_updates            | 5160         |
|    policy_gradient_loss | -0.00414     |
|    value_loss           | 9.5e+04      |
------------------------------------------
Eval num_timesteps=236500, episode_reward=501.19 +/- 763.24
Episode length: 36.06 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 501      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 349      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 231      |
|    time_elapsed    | 853      |
|    total_timesteps | 236544   |
---------------------------------
Eval num_timesteps=237000, episode_reward=386.29 +/- 693.50
Episode length: 35.70 +/- 5.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.7        |
|    mean_reward          | 386         |
| time/                   |             |
|    total_timesteps      | 237000      |
| train/                  |             |
|    approx_kl            | 0.007273894 |
|    clip_fraction        | 0.00381     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0121     |
|    explained_variance   | -0.066      |
|    learning_rate        | 0.001       |
|    loss                 | 3.98e+04    |
|    n_updates            | 5170        |
|    policy_gradient_loss | -0.00222    |
|    value_loss           | 8.42e+04    |
-----------------------------------------
Eval num_timesteps=237500, episode_reward=236.67 +/- 568.83
Episode length: 34.48 +/- 6.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 237      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 412      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 232      |
|    time_elapsed    | 856      |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=408.72 +/- 694.75
Episode length: 35.48 +/- 6.08
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.5          |
|    mean_reward          | 409           |
| time/                   |               |
|    total_timesteps      | 238000        |
| train/                  |               |
|    approx_kl            | 0.00034170545 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0121       |
|    explained_variance   | 0.094         |
|    learning_rate        | 0.001         |
|    loss                 | 3.06e+04      |
|    n_updates            | 5180          |
|    policy_gradient_loss | -0.00102      |
|    value_loss           | 8.53e+04      |
-------------------------------------------
Eval num_timesteps=238500, episode_reward=373.57 +/- 640.39
Episode length: 36.14 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 374      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 509      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 233      |
|    time_elapsed    | 860      |
|    total_timesteps | 238592   |
---------------------------------
Eval num_timesteps=239000, episode_reward=447.12 +/- 780.45
Episode length: 34.58 +/- 7.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.6         |
|    mean_reward          | 447          |
| time/                   |              |
|    total_timesteps      | 239000       |
| train/                  |              |
|    approx_kl            | 0.0031768866 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0244      |
|    explained_variance   | 0.0916       |
|    learning_rate        | 0.001        |
|    loss                 | 3.16e+04     |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.00153     |
|    value_loss           | 9.62e+04     |
------------------------------------------
Eval num_timesteps=239500, episode_reward=350.06 +/- 618.36
Episode length: 34.60 +/- 6.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 350      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 489      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 234      |
|    time_elapsed    | 864      |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=442.78 +/- 742.58
Episode length: 35.02 +/- 7.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35          |
|    mean_reward          | 443         |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.014220625 |
|    clip_fraction        | 0.00283     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00685    |
|    explained_variance   | -0.19       |
|    learning_rate        | 0.001       |
|    loss                 | 2.3e+04     |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.000917   |
|    value_loss           | 7.17e+04    |
-----------------------------------------
Eval num_timesteps=240500, episode_reward=359.88 +/- 644.19
Episode length: 35.00 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 479      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 235      |
|    time_elapsed    | 867      |
|    total_timesteps | 240640   |
---------------------------------
Eval num_timesteps=241000, episode_reward=426.63 +/- 758.00
Episode length: 34.54 +/- 7.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.5        |
|    mean_reward          | 427         |
| time/                   |             |
|    total_timesteps      | 241000      |
| train/                  |             |
|    approx_kl            | 0.012138812 |
|    clip_fraction        | 0.00186     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00305    |
|    explained_variance   | 0.0281      |
|    learning_rate        | 0.001       |
|    loss                 | 3.59e+04    |
|    n_updates            | 5210        |
|    policy_gradient_loss | 0.00352     |
|    value_loss           | 8.73e+04    |
-----------------------------------------
Eval num_timesteps=241500, episode_reward=474.84 +/- 729.08
Episode length: 34.86 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 473      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 236      |
|    time_elapsed    | 871      |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=484.37 +/- 703.89
Episode length: 35.72 +/- 6.37
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 484           |
| time/                   |               |
|    total_timesteps      | 242000        |
| train/                  |               |
|    approx_kl            | 0.00089536567 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00517      |
|    explained_variance   | 0.0699        |
|    learning_rate        | 0.001         |
|    loss                 | 3.13e+04      |
|    n_updates            | 5220          |
|    policy_gradient_loss | 0.000431      |
|    value_loss           | 9.07e+04      |
-------------------------------------------
Eval num_timesteps=242500, episode_reward=445.37 +/- 677.78
Episode length: 36.76 +/- 5.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 481      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 237      |
|    time_elapsed    | 875      |
|    total_timesteps | 242688   |
---------------------------------
Eval num_timesteps=243000, episode_reward=209.04 +/- 539.50
Episode length: 33.82 +/- 6.50
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.8          |
|    mean_reward          | 209           |
| time/                   |               |
|    total_timesteps      | 243000        |
| train/                  |               |
|    approx_kl            | 2.2001972e-05 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00268      |
|    explained_variance   | -0.036        |
|    learning_rate        | 0.001         |
|    loss                 | 4.06e+04      |
|    n_updates            | 5230          |
|    policy_gradient_loss | -0.000599     |
|    value_loss           | 9.75e+04      |
-------------------------------------------
Eval num_timesteps=243500, episode_reward=384.08 +/- 672.25
Episode length: 34.84 +/- 6.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 384      |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36       |
|    ep_rew_mean     | 478      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 238      |
|    time_elapsed    | 878      |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=477.08 +/- 814.76
Episode length: 34.62 +/- 7.19
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.6          |
|    mean_reward          | 477           |
| time/                   |               |
|    total_timesteps      | 244000        |
| train/                  |               |
|    approx_kl            | 1.9371684e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00105      |
|    explained_variance   | -0.245        |
|    learning_rate        | 0.001         |
|    loss                 | 3.45e+04      |
|    n_updates            | 5240          |
|    policy_gradient_loss | -0.000611     |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=244500, episode_reward=407.71 +/- 696.50
Episode length: 34.98 +/- 6.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.7     |
|    ep_rew_mean     | 487      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 239      |
|    time_elapsed    | 882      |
|    total_timesteps | 244736   |
---------------------------------
Eval num_timesteps=245000, episode_reward=408.49 +/- 730.90
Episode length: 35.54 +/- 6.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.5        |
|    mean_reward          | 408         |
| time/                   |             |
|    total_timesteps      | 245000      |
| train/                  |             |
|    approx_kl            | 7.67858e-06 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00259    |
|    explained_variance   | -0.196      |
|    learning_rate        | 0.001       |
|    loss                 | 3.68e+04    |
|    n_updates            | 5250        |
|    policy_gradient_loss | -0.000125   |
|    value_loss           | 8.11e+04    |
-----------------------------------------
Eval num_timesteps=245500, episode_reward=327.83 +/- 684.33
Episode length: 33.96 +/- 6.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34       |
|    mean_reward     | 328      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.6     |
|    ep_rew_mean     | 520      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 240      |
|    time_elapsed    | 886      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=502.22 +/- 728.57
Episode length: 36.30 +/- 5.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.3         |
|    mean_reward          | 502          |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0051842667 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00454     |
|    explained_variance   | -0.0363      |
|    learning_rate        | 0.001        |
|    loss                 | 3.26e+04     |
|    n_updates            | 5260         |
|    policy_gradient_loss | 0.000642     |
|    value_loss           | 9.07e+04     |
------------------------------------------
Eval num_timesteps=246500, episode_reward=332.16 +/- 630.44
Episode length: 34.66 +/- 6.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 332      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.2     |
|    ep_rew_mean     | 506      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 241      |
|    time_elapsed    | 889      |
|    total_timesteps | 246784   |
---------------------------------
Eval num_timesteps=247000, episode_reward=314.92 +/- 694.81
Episode length: 34.34 +/- 6.45
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.3         |
|    mean_reward          | 315          |
| time/                   |              |
|    total_timesteps      | 247000       |
| train/                  |              |
|    approx_kl            | 2.738781e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0085      |
|    explained_variance   | 0.0703       |
|    learning_rate        | 0.001        |
|    loss                 | 3.68e+04     |
|    n_updates            | 5270         |
|    policy_gradient_loss | -0.000401    |
|    value_loss           | 7.67e+04     |
------------------------------------------
Eval num_timesteps=247500, episode_reward=551.77 +/- 727.16
Episode length: 36.98 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37       |
|    mean_reward     | 552      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 461      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 242      |
|    time_elapsed    | 893      |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=591.20 +/- 825.68
Episode length: 35.92 +/- 6.96
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.9         |
|    mean_reward          | 591          |
| time/                   |              |
|    total_timesteps      | 248000       |
| train/                  |              |
|    approx_kl            | 0.0013458857 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.014       |
|    explained_variance   | -0.0207      |
|    learning_rate        | 0.001        |
|    loss                 | 2.67e+04     |
|    n_updates            | 5280         |
|    policy_gradient_loss | 0.000382     |
|    value_loss           | 7.84e+04     |
------------------------------------------
Eval num_timesteps=248500, episode_reward=440.89 +/- 719.69
Episode length: 35.96 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 437      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 243      |
|    time_elapsed    | 897      |
|    total_timesteps | 248832   |
---------------------------------
Eval num_timesteps=249000, episode_reward=371.00 +/- 672.38
Episode length: 35.32 +/- 6.46
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.3          |
|    mean_reward          | 371           |
| time/                   |               |
|    total_timesteps      | 249000        |
| train/                  |               |
|    approx_kl            | 0.00074317533 |
|    clip_fraction        | 0.00176       |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0153       |
|    explained_variance   | 0.0371        |
|    learning_rate        | 0.001         |
|    loss                 | 4.99e+04      |
|    n_updates            | 5290          |
|    policy_gradient_loss | -0.000726     |
|    value_loss           | 1.12e+05      |
-------------------------------------------
Eval num_timesteps=249500, episode_reward=206.38 +/- 542.86
Episode length: 33.60 +/- 5.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 206      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 407      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 244      |
|    time_elapsed    | 900      |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=827.98 +/- 786.37
Episode length: 38.34 +/- 6.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38.3        |
|    mean_reward          | 828         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.015591742 |
|    clip_fraction        | 0.00654     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0198     |
|    explained_variance   | -0.0532     |
|    learning_rate        | 0.001       |
|    loss                 | 3.42e+04    |
|    n_updates            | 5300        |
|    policy_gradient_loss | 0.00218     |
|    value_loss           | 9.23e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=250500, episode_reward=339.34 +/- 671.32
Episode length: 34.70 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 462      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 245      |
|    time_elapsed    | 904      |
|    total_timesteps | 250880   |
---------------------------------
Eval num_timesteps=251000, episode_reward=484.15 +/- 722.01
Episode length: 36.90 +/- 5.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 36.9       |
|    mean_reward          | 484        |
| time/                   |            |
|    total_timesteps      | 251000     |
| train/                  |            |
|    approx_kl            | 0.01060481 |
|    clip_fraction        | 0.00186    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0194    |
|    explained_variance   | -0.209     |
|    learning_rate        | 0.001      |
|    loss                 | 2.94e+04   |
|    n_updates            | 5310       |
|    policy_gradient_loss | -0.00255   |
|    value_loss           | 8.54e+04   |
----------------------------------------
Eval num_timesteps=251500, episode_reward=383.78 +/- 681.26
Episode length: 34.84 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 384      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 463      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 246      |
|    time_elapsed    | 908      |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=432.62 +/- 736.77
Episode length: 35.44 +/- 6.33
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.4          |
|    mean_reward          | 433           |
| time/                   |               |
|    total_timesteps      | 252000        |
| train/                  |               |
|    approx_kl            | 4.2368833e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0153       |
|    explained_variance   | 0.0697        |
|    learning_rate        | 0.001         |
|    loss                 | 3.68e+04      |
|    n_updates            | 5320          |
|    policy_gradient_loss | -0.00145      |
|    value_loss           | 8.85e+04      |
-------------------------------------------
Eval num_timesteps=252500, episode_reward=438.41 +/- 699.45
Episode length: 36.32 +/- 5.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.8     |
|    ep_rew_mean     | 414      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 247      |
|    time_elapsed    | 911      |
|    total_timesteps | 252928   |
---------------------------------
Eval num_timesteps=253000, episode_reward=465.44 +/- 724.56
Episode length: 35.70 +/- 6.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.7          |
|    mean_reward          | 465           |
| time/                   |               |
|    total_timesteps      | 253000        |
| train/                  |               |
|    approx_kl            | 6.9734466e-05 |
|    clip_fraction        | 0.000684      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0169       |
|    explained_variance   | -0.0496       |
|    learning_rate        | 0.001         |
|    loss                 | 3.33e+04      |
|    n_updates            | 5330          |
|    policy_gradient_loss | -0.00128      |
|    value_loss           | 7.8e+04       |
-------------------------------------------
Eval num_timesteps=253500, episode_reward=400.60 +/- 694.83
Episode length: 35.14 +/- 7.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.1     |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 385      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 248      |
|    time_elapsed    | 915      |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=444.02 +/- 799.62
Episode length: 34.54 +/- 7.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34.5         |
|    mean_reward          | 444          |
| time/                   |              |
|    total_timesteps      | 254000       |
| train/                  |              |
|    approx_kl            | 0.0005614684 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0114      |
|    explained_variance   | 0.00177      |
|    learning_rate        | 0.001        |
|    loss                 | 2.41e+04     |
|    n_updates            | 5340         |
|    policy_gradient_loss | -0.000927    |
|    value_loss           | 7.86e+04     |
------------------------------------------
Eval num_timesteps=254500, episode_reward=427.45 +/- 695.92
Episode length: 36.18 +/- 5.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 367      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 249      |
|    time_elapsed    | 919      |
|    total_timesteps | 254976   |
---------------------------------
Eval num_timesteps=255000, episode_reward=438.22 +/- 766.05
Episode length: 35.26 +/- 6.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.3        |
|    mean_reward          | 438         |
| time/                   |             |
|    total_timesteps      | 255000      |
| train/                  |             |
|    approx_kl            | 0.007149009 |
|    clip_fraction        | 0.00254     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0365     |
|    explained_variance   | 0.0172      |
|    learning_rate        | 0.001       |
|    loss                 | 5.69e+04    |
|    n_updates            | 5350        |
|    policy_gradient_loss | 0.000652    |
|    value_loss           | 9.7e+04     |
-----------------------------------------
Eval num_timesteps=255500, episode_reward=368.20 +/- 750.43
Episode length: 33.34 +/- 7.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.3     |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=432.86 +/- 684.51
Episode length: 35.74 +/- 5.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 328      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 250      |
|    time_elapsed    | 924      |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=329.21 +/- 624.40
Episode length: 34.90 +/- 6.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.9        |
|    mean_reward          | 329         |
| time/                   |             |
|    total_timesteps      | 256500      |
| train/                  |             |
|    approx_kl            | 0.021509927 |
|    clip_fraction        | 0.00664     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0318     |
|    explained_variance   | -0.179      |
|    learning_rate        | 0.001       |
|    loss                 | 3.16e+04    |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.0015     |
|    value_loss           | 8.24e+04    |
-----------------------------------------
Eval num_timesteps=257000, episode_reward=525.33 +/- 795.31
Episode length: 35.26 +/- 7.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.3     |
|    mean_reward     | 525      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 298      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 251      |
|    time_elapsed    | 927      |
|    total_timesteps | 257024   |
---------------------------------
Eval num_timesteps=257500, episode_reward=376.82 +/- 707.52
Episode length: 34.66 +/- 6.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.7        |
|    mean_reward          | 377         |
| time/                   |             |
|    total_timesteps      | 257500      |
| train/                  |             |
|    approx_kl            | 0.001413238 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.035      |
|    explained_variance   | 0.0801      |
|    learning_rate        | 0.001       |
|    loss                 | 3.38e+04    |
|    n_updates            | 5370        |
|    policy_gradient_loss | -0.00123    |
|    value_loss           | 7.03e+04    |
-----------------------------------------
Eval num_timesteps=258000, episode_reward=463.18 +/- 714.59
Episode length: 35.76 +/- 6.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.8     |
|    ep_rew_mean     | 289      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 252      |
|    time_elapsed    | 931      |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=519.69 +/- 712.37
Episode length: 36.68 +/- 5.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.7        |
|    mean_reward          | 520         |
| time/                   |             |
|    total_timesteps      | 258500      |
| train/                  |             |
|    approx_kl            | 0.001130705 |
|    clip_fraction        | 0.00264     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0292     |
|    explained_variance   | 0.0433      |
|    learning_rate        | 0.001       |
|    loss                 | 2.62e+04    |
|    n_updates            | 5380        |
|    policy_gradient_loss | -0.00181    |
|    value_loss           | 9.01e+04    |
-----------------------------------------
Eval num_timesteps=259000, episode_reward=608.78 +/- 730.82
Episode length: 37.74 +/- 5.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 37.7     |
|    mean_reward     | 609      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 310      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 253      |
|    time_elapsed    | 935      |
|    total_timesteps | 259072   |
---------------------------------
Eval num_timesteps=259500, episode_reward=335.03 +/- 616.46
Episode length: 35.10 +/- 5.64
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.1          |
|    mean_reward          | 335           |
| time/                   |               |
|    total_timesteps      | 259500        |
| train/                  |               |
|    approx_kl            | 0.00013318338 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0497       |
|    explained_variance   | -0.0127       |
|    learning_rate        | 0.001         |
|    loss                 | 2.7e+04       |
|    n_updates            | 5390          |
|    policy_gradient_loss | -0.00203      |
|    value_loss           | 6.61e+04      |
-------------------------------------------
Eval num_timesteps=260000, episode_reward=486.74 +/- 725.00
Episode length: 36.22 +/- 6.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.2     |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 316      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 254      |
|    time_elapsed    | 938      |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=581.14 +/- 807.02
Episode length: 35.82 +/- 6.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.8        |
|    mean_reward          | 581         |
| time/                   |             |
|    total_timesteps      | 260500      |
| train/                  |             |
|    approx_kl            | 0.025617052 |
|    clip_fraction        | 0.0101      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0473     |
|    explained_variance   | -0.131      |
|    learning_rate        | 0.001       |
|    loss                 | 2.99e+04    |
|    n_updates            | 5400        |
|    policy_gradient_loss | -0.00422    |
|    value_loss           | 8.41e+04    |
-----------------------------------------
Eval num_timesteps=261000, episode_reward=288.90 +/- 622.76
Episode length: 33.84 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.8     |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 339      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 255      |
|    time_elapsed    | 942      |
|    total_timesteps | 261120   |
---------------------------------
Eval num_timesteps=261500, episode_reward=524.49 +/- 789.97
Episode length: 34.72 +/- 7.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.7        |
|    mean_reward          | 524         |
| time/                   |             |
|    total_timesteps      | 261500      |
| train/                  |             |
|    approx_kl            | 0.009087389 |
|    clip_fraction        | 0.00615     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0261     |
|    explained_variance   | 0.147       |
|    learning_rate        | 0.001       |
|    loss                 | 3.02e+04    |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.00354    |
|    value_loss           | 7.48e+04    |
-----------------------------------------
Eval num_timesteps=262000, episode_reward=270.10 +/- 582.84
Episode length: 34.76 +/- 6.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 270      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 286      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 256      |
|    time_elapsed    | 946      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=348.47 +/- 718.65
Episode length: 33.92 +/- 6.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.9        |
|    mean_reward          | 348         |
| time/                   |             |
|    total_timesteps      | 262500      |
| train/                  |             |
|    approx_kl            | 0.007089071 |
|    clip_fraction        | 0.00479     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0445     |
|    explained_variance   | 0.0296      |
|    learning_rate        | 0.001       |
|    loss                 | 3.73e+04    |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.000861   |
|    value_loss           | 9.92e+04    |
-----------------------------------------
Eval num_timesteps=263000, episode_reward=355.48 +/- 650.58
Episode length: 35.02 +/- 6.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 355      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.1     |
|    ep_rew_mean     | 297      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 257      |
|    time_elapsed    | 949      |
|    total_timesteps | 263168   |
---------------------------------
Eval num_timesteps=263500, episode_reward=613.70 +/- 779.61
Episode length: 36.88 +/- 6.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.9        |
|    mean_reward          | 614         |
| time/                   |             |
|    total_timesteps      | 263500      |
| train/                  |             |
|    approx_kl            | 0.005375825 |
|    clip_fraction        | 0.00518     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0582     |
|    explained_variance   | 0.177       |
|    learning_rate        | 0.001       |
|    loss                 | 5.35e+04    |
|    n_updates            | 5430        |
|    policy_gradient_loss | -0.00128    |
|    value_loss           | 8.55e+04    |
-----------------------------------------
Eval num_timesteps=264000, episode_reward=547.21 +/- 745.77
Episode length: 35.68 +/- 7.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.7     |
|    mean_reward     | 547      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.2     |
|    ep_rew_mean     | 250      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 258      |
|    time_elapsed    | 953      |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=406.55 +/- 715.41
Episode length: 35.40 +/- 6.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.4        |
|    mean_reward          | 407         |
| time/                   |             |
|    total_timesteps      | 264500      |
| train/                  |             |
|    approx_kl            | 0.013612772 |
|    clip_fraction        | 0.00645     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0522     |
|    explained_variance   | -0.0532     |
|    learning_rate        | 0.001       |
|    loss                 | 2.76e+04    |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.00223    |
|    value_loss           | 7.47e+04    |
-----------------------------------------
Eval num_timesteps=265000, episode_reward=392.12 +/- 651.05
Episode length: 35.46 +/- 5.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 392      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 296      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 259      |
|    time_elapsed    | 957      |
|    total_timesteps | 265216   |
---------------------------------
Eval num_timesteps=265500, episode_reward=354.19 +/- 686.82
Episode length: 35.12 +/- 6.18
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 354          |
| time/                   |              |
|    total_timesteps      | 265500       |
| train/                  |              |
|    approx_kl            | 0.0024699583 |
|    clip_fraction        | 0.00283      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0184      |
|    explained_variance   | 0.0584       |
|    learning_rate        | 0.001        |
|    loss                 | 2.62e+04     |
|    n_updates            | 5450         |
|    policy_gradient_loss | -0.00276     |
|    value_loss           | 6.69e+04     |
------------------------------------------
Eval num_timesteps=266000, episode_reward=526.17 +/- 711.55
Episode length: 36.68 +/- 5.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.7     |
|    mean_reward     | 526      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 287      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 260      |
|    time_elapsed    | 960      |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=633.29 +/- 753.03
Episode length: 36.86 +/- 6.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.9        |
|    mean_reward          | 633         |
| time/                   |             |
|    total_timesteps      | 266500      |
| train/                  |             |
|    approx_kl            | 0.011701597 |
|    clip_fraction        | 0.00293     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00874    |
|    explained_variance   | -0.0937     |
|    learning_rate        | 0.001       |
|    loss                 | 1.61e+04    |
|    n_updates            | 5460        |
|    policy_gradient_loss | -0.00106    |
|    value_loss           | 8.65e+04    |
-----------------------------------------
Eval num_timesteps=267000, episode_reward=408.05 +/- 739.50
Episode length: 34.84 +/- 7.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 362      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 261      |
|    time_elapsed    | 964      |
|    total_timesteps | 267264   |
---------------------------------
Eval num_timesteps=267500, episode_reward=216.19 +/- 590.34
Episode length: 33.06 +/- 6.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 33.1        |
|    mean_reward          | 216         |
| time/                   |             |
|    total_timesteps      | 267500      |
| train/                  |             |
|    approx_kl            | 0.000381111 |
|    clip_fraction        | 0.00254     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0119     |
|    explained_variance   | -0.0707     |
|    learning_rate        | 0.001       |
|    loss                 | 2.38e+04    |
|    n_updates            | 5470        |
|    policy_gradient_loss | -0.000326   |
|    value_loss           | 7.28e+04    |
-----------------------------------------
Eval num_timesteps=268000, episode_reward=439.13 +/- 751.53
Episode length: 34.34 +/- 7.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.3     |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 312      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 262      |
|    time_elapsed    | 967      |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=234.15 +/- 546.47
Episode length: 34.30 +/- 6.95
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 34.3          |
|    mean_reward          | 234           |
| time/                   |               |
|    total_timesteps      | 268500        |
| train/                  |               |
|    approx_kl            | 0.00016979757 |
|    clip_fraction        | 0.000879      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.059        |
|    explained_variance   | -0.113        |
|    learning_rate        | 0.001         |
|    loss                 | 2.62e+04      |
|    n_updates            | 5480          |
|    policy_gradient_loss | -0.00426      |
|    value_loss           | 4.93e+04      |
-------------------------------------------
Eval num_timesteps=269000, episode_reward=506.65 +/- 767.31
Episode length: 35.46 +/- 7.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.5     |
|    mean_reward     | 507      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 289      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 263      |
|    time_elapsed    | 971      |
|    total_timesteps | 269312   |
---------------------------------
Eval num_timesteps=269500, episode_reward=449.69 +/- 710.73
Episode length: 35.74 +/- 7.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.7        |
|    mean_reward          | 450         |
| time/                   |             |
|    total_timesteps      | 269500      |
| train/                  |             |
|    approx_kl            | 0.014502954 |
|    clip_fraction        | 0.015       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0659     |
|    explained_variance   | 0.113       |
|    learning_rate        | 0.001       |
|    loss                 | 2.99e+04    |
|    n_updates            | 5490        |
|    policy_gradient_loss | -0.00139    |
|    value_loss           | 8.52e+04    |
-----------------------------------------
Eval num_timesteps=270000, episode_reward=471.35 +/- 682.32
Episode length: 35.78 +/- 5.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 471      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.4     |
|    ep_rew_mean     | 282      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 264      |
|    time_elapsed    | 975      |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=364.31 +/- 669.78
Episode length: 34.86 +/- 6.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.9        |
|    mean_reward          | 364         |
| time/                   |             |
|    total_timesteps      | 270500      |
| train/                  |             |
|    approx_kl            | 0.022957154 |
|    clip_fraction        | 0.00879     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.058      |
|    explained_variance   | 0.0903      |
|    learning_rate        | 0.001       |
|    loss                 | 3.43e+04    |
|    n_updates            | 5500        |
|    policy_gradient_loss | 0.00129     |
|    value_loss           | 9.28e+04    |
-----------------------------------------
Eval num_timesteps=271000, episode_reward=680.91 +/- 822.36
Episode length: 36.44 +/- 6.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.4     |
|    mean_reward     | 681      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 382      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 265      |
|    time_elapsed    | 978      |
|    total_timesteps | 271360   |
---------------------------------
Eval num_timesteps=271500, episode_reward=580.48 +/- 783.88
Episode length: 36.48 +/- 6.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.5        |
|    mean_reward          | 580         |
| time/                   |             |
|    total_timesteps      | 271500      |
| train/                  |             |
|    approx_kl            | 0.018765744 |
|    clip_fraction        | 0.0085      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0368     |
|    explained_variance   | 0.0237      |
|    learning_rate        | 0.001       |
|    loss                 | 3.83e+04    |
|    n_updates            | 5510        |
|    policy_gradient_loss | -0.00127    |
|    value_loss           | 8.45e+04    |
-----------------------------------------
Eval num_timesteps=272000, episode_reward=634.73 +/- 771.43
Episode length: 36.92 +/- 6.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 635      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35       |
|    ep_rew_mean     | 394      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 266      |
|    time_elapsed    | 982      |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=504.16 +/- 785.51
Episode length: 34.92 +/- 7.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.9        |
|    mean_reward          | 504         |
| time/                   |             |
|    total_timesteps      | 272500      |
| train/                  |             |
|    approx_kl            | 0.014179935 |
|    clip_fraction        | 0.00752     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0437     |
|    explained_variance   | -0.0473     |
|    learning_rate        | 0.001       |
|    loss                 | 3.84e+04    |
|    n_updates            | 5520        |
|    policy_gradient_loss | 0.00951     |
|    value_loss           | 8.92e+04    |
-----------------------------------------
Eval num_timesteps=273000, episode_reward=598.24 +/- 793.37
Episode length: 36.46 +/- 6.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 598      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 374      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 267      |
|    time_elapsed    | 986      |
|    total_timesteps | 273408   |
---------------------------------
Eval num_timesteps=273500, episode_reward=318.31 +/- 684.51
Episode length: 34.28 +/- 6.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 34.3       |
|    mean_reward          | 318        |
| time/                   |            |
|    total_timesteps      | 273500     |
| train/                  |            |
|    approx_kl            | 0.00886416 |
|    clip_fraction        | 0.00459    |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0291    |
|    explained_variance   | 0.123      |
|    learning_rate        | 0.001      |
|    loss                 | 3.48e+04   |
|    n_updates            | 5530       |
|    policy_gradient_loss | -0.000874  |
|    value_loss           | 8.62e+04   |
----------------------------------------
Eval num_timesteps=274000, episode_reward=477.49 +/- 718.76
Episode length: 36.12 +/- 6.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.2     |
|    ep_rew_mean     | 398      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 268      |
|    time_elapsed    | 989      |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=341.10 +/- 610.95
Episode length: 35.64 +/- 6.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 341          |
| time/                   |              |
|    total_timesteps      | 274500       |
| train/                  |              |
|    approx_kl            | 0.0031995943 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0128      |
|    explained_variance   | 0.0313       |
|    learning_rate        | 0.001        |
|    loss                 | 3.6e+04      |
|    n_updates            | 5540         |
|    policy_gradient_loss | 0.00016      |
|    value_loss           | 9.53e+04     |
------------------------------------------
Eval num_timesteps=275000, episode_reward=623.88 +/- 826.94
Episode length: 35.78 +/- 6.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 362      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 269      |
|    time_elapsed    | 993      |
|    total_timesteps | 275456   |
---------------------------------
Eval num_timesteps=275500, episode_reward=369.68 +/- 679.38
Episode length: 35.02 +/- 6.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 370          |
| time/                   |              |
|    total_timesteps      | 275500       |
| train/                  |              |
|    approx_kl            | 0.0005704465 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0088      |
|    explained_variance   | 0.0334       |
|    learning_rate        | 0.001        |
|    loss                 | 5.11e+04     |
|    n_updates            | 5550         |
|    policy_gradient_loss | -0.000345    |
|    value_loss           | 9.74e+04     |
------------------------------------------
Eval num_timesteps=276000, episode_reward=549.03 +/- 690.62
Episode length: 36.88 +/- 5.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.9     |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 448      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 270      |
|    time_elapsed    | 997      |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=480.76 +/- 652.85
Episode length: 36.42 +/- 5.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.4        |
|    mean_reward          | 481         |
| time/                   |             |
|    total_timesteps      | 276500      |
| train/                  |             |
|    approx_kl            | 0.004396858 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0046     |
|    explained_variance   | -0.0245     |
|    learning_rate        | 0.001       |
|    loss                 | 3.74e+04    |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.00109    |
|    value_loss           | 1.01e+05    |
-----------------------------------------
Eval num_timesteps=277000, episode_reward=493.94 +/- 699.76
Episode length: 35.86 +/- 6.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.9     |
|    mean_reward     | 494      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=283.73 +/- 605.83
Episode length: 33.50 +/- 7.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.5     |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 389      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 271      |
|    time_elapsed    | 1002     |
|    total_timesteps | 277504   |
---------------------------------
Eval num_timesteps=278000, episode_reward=383.51 +/- 644.88
Episode length: 35.00 +/- 6.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35           |
|    mean_reward          | 384          |
| time/                   |              |
|    total_timesteps      | 278000       |
| train/                  |              |
|    approx_kl            | 3.114331e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0164      |
|    explained_variance   | 0.00463      |
|    learning_rate        | 0.001        |
|    loss                 | 3.38e+04     |
|    n_updates            | 5570         |
|    policy_gradient_loss | -0.00107     |
|    value_loss           | 8.02e+04     |
------------------------------------------
Eval num_timesteps=278500, episode_reward=513.57 +/- 759.80
Episode length: 36.10 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 514      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 444      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 272      |
|    time_elapsed    | 1005     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=480.13 +/- 754.01
Episode length: 35.50 +/- 6.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.5        |
|    mean_reward          | 480         |
| time/                   |             |
|    total_timesteps      | 279000      |
| train/                  |             |
|    approx_kl            | 0.012552664 |
|    clip_fraction        | 0.00234     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0144     |
|    explained_variance   | -0.206      |
|    learning_rate        | 0.001       |
|    loss                 | 3.37e+04    |
|    n_updates            | 5580        |
|    policy_gradient_loss | 5.38e-05    |
|    value_loss           | 7.08e+04    |
-----------------------------------------
Eval num_timesteps=279500, episode_reward=465.22 +/- 742.91
Episode length: 35.96 +/- 6.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 465      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 355      |
| time/              |          |
|    fps             | 276      |
|    iterations      | 273      |
|    time_elapsed    | 1009     |
|    total_timesteps | 279552   |
---------------------------------
Eval num_timesteps=280000, episode_reward=330.63 +/- 749.59
Episode length: 33.08 +/- 6.77
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 33.1          |
|    mean_reward          | 331           |
| time/                   |               |
|    total_timesteps      | 280000        |
| train/                  |               |
|    approx_kl            | 0.00010177697 |
|    clip_fraction        | 0.000586      |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0164       |
|    explained_variance   | 0.135         |
|    learning_rate        | 0.001         |
|    loss                 | 3.34e+04      |
|    n_updates            | 5590          |
|    policy_gradient_loss | -0.00122      |
|    value_loss           | 8.55e+04      |
-------------------------------------------
Eval num_timesteps=280500, episode_reward=579.17 +/- 774.89
Episode length: 35.82 +/- 6.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.8     |
|    mean_reward     | 579      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 325      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 274      |
|    time_elapsed    | 1012     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=234.06 +/- 792.77
Episode length: 30.78 +/- 8.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 30.8         |
|    mean_reward          | 234          |
| time/                   |              |
|    total_timesteps      | 281000       |
| train/                  |              |
|    approx_kl            | 0.0071589765 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0188      |
|    explained_variance   | -0.0333      |
|    learning_rate        | 0.001        |
|    loss                 | 2.61e+04     |
|    n_updates            | 5600         |
|    policy_gradient_loss | 0.000607     |
|    value_loss           | 9.34e+04     |
------------------------------------------
Eval num_timesteps=281500, episode_reward=456.83 +/- 772.23
Episode length: 35.62 +/- 6.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.8     |
|    ep_rew_mean     | 362      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 275      |
|    time_elapsed    | 1016     |
|    total_timesteps | 281600   |
---------------------------------
Eval num_timesteps=282000, episode_reward=541.74 +/- 748.11
Episode length: 36.16 +/- 6.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.2         |
|    mean_reward          | 542          |
| time/                   |              |
|    total_timesteps      | 282000       |
| train/                  |              |
|    approx_kl            | 0.0035313393 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0132      |
|    explained_variance   | -0.0282      |
|    learning_rate        | 0.001        |
|    loss                 | 3.16e+04     |
|    n_updates            | 5610         |
|    policy_gradient_loss | -0.00114     |
|    value_loss           | 8.42e+04     |
------------------------------------------
Eval num_timesteps=282500, episode_reward=340.56 +/- 681.88
Episode length: 35.04 +/- 6.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 341      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 427      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 276      |
|    time_elapsed    | 1019     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=461.67 +/- 717.75
Episode length: 35.84 +/- 6.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.8        |
|    mean_reward          | 462         |
| time/                   |             |
|    total_timesteps      | 283000      |
| train/                  |             |
|    approx_kl            | 0.007446298 |
|    clip_fraction        | 0.00283     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.00343    |
|    explained_variance   | 0.0894      |
|    learning_rate        | 0.001       |
|    loss                 | 4.82e+04    |
|    n_updates            | 5620        |
|    policy_gradient_loss | -0.00165    |
|    value_loss           | 1.22e+05    |
-----------------------------------------
Eval num_timesteps=283500, episode_reward=440.61 +/- 698.03
Episode length: 36.00 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36       |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 378      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 277      |
|    time_elapsed    | 1023     |
|    total_timesteps | 283648   |
---------------------------------
Eval num_timesteps=284000, episode_reward=555.61 +/- 789.20
Episode length: 35.18 +/- 7.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 556          |
| time/                   |              |
|    total_timesteps      | 284000       |
| train/                  |              |
|    approx_kl            | 7.980445e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00549     |
|    explained_variance   | -0.0515      |
|    learning_rate        | 0.001        |
|    loss                 | 3.79e+04     |
|    n_updates            | 5630         |
|    policy_gradient_loss | -0.000338    |
|    value_loss           | 1.05e+05     |
------------------------------------------
Eval num_timesteps=284500, episode_reward=395.96 +/- 656.72
Episode length: 36.14 +/- 5.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.1     |
|    mean_reward     | 396      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.5     |
|    ep_rew_mean     | 405      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 278      |
|    time_elapsed    | 1027     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=485.73 +/- 715.39
Episode length: 36.68 +/- 5.91
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.7          |
|    mean_reward          | 486           |
| time/                   |               |
|    total_timesteps      | 285000        |
| train/                  |               |
|    approx_kl            | 1.2257427e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00906      |
|    explained_variance   | 0.061         |
|    learning_rate        | 0.001         |
|    loss                 | 3.33e+04      |
|    n_updates            | 5640          |
|    policy_gradient_loss | -0.00036      |
|    value_loss           | 1.01e+05      |
-------------------------------------------
Eval num_timesteps=285500, episode_reward=371.81 +/- 684.32
Episode length: 34.68 +/- 6.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.1     |
|    ep_rew_mean     | 421      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 279      |
|    time_elapsed    | 1030     |
|    total_timesteps | 285696   |
---------------------------------
Eval num_timesteps=286000, episode_reward=458.59 +/- 676.82
Episode length: 36.64 +/- 5.32
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36.6          |
|    mean_reward          | 459           |
| time/                   |               |
|    total_timesteps      | 286000        |
| train/                  |               |
|    approx_kl            | 1.4703139e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0112       |
|    explained_variance   | 0.108         |
|    learning_rate        | 0.001         |
|    loss                 | 3.73e+04      |
|    n_updates            | 5650          |
|    policy_gradient_loss | -0.000568     |
|    value_loss           | 8.2e+04       |
-------------------------------------------
Eval num_timesteps=286500, episode_reward=488.84 +/- 762.03
Episode length: 35.64 +/- 6.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 489      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.9     |
|    ep_rew_mean     | 305      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 280      |
|    time_elapsed    | 1034     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=561.92 +/- 709.80
Episode length: 36.84 +/- 5.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 36.8         |
|    mean_reward          | 562          |
| time/                   |              |
|    total_timesteps      | 287000       |
| train/                  |              |
|    approx_kl            | 0.0006709721 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0433      |
|    explained_variance   | -0.171       |
|    learning_rate        | 0.001        |
|    loss                 | 3.15e+04     |
|    n_updates            | 5660         |
|    policy_gradient_loss | -0.00184     |
|    value_loss           | 8e+04        |
------------------------------------------
Eval num_timesteps=287500, episode_reward=455.04 +/- 803.30
Episode length: 33.94 +/- 7.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.9     |
|    mean_reward     | 455      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.4     |
|    ep_rew_mean     | 330      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 281      |
|    time_elapsed    | 1038     |
|    total_timesteps | 287744   |
---------------------------------
Eval num_timesteps=288000, episode_reward=333.74 +/- 716.97
Episode length: 34.34 +/- 6.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 34.3        |
|    mean_reward          | 334         |
| time/                   |             |
|    total_timesteps      | 288000      |
| train/                  |             |
|    approx_kl            | 0.018761633 |
|    clip_fraction        | 0.0083      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0287     |
|    explained_variance   | 0.0314      |
|    learning_rate        | 0.001       |
|    loss                 | 2.77e+04    |
|    n_updates            | 5670        |
|    policy_gradient_loss | -0.00235    |
|    value_loss           | 7.25e+04    |
-----------------------------------------
Eval num_timesteps=288500, episode_reward=555.81 +/- 766.88
Episode length: 35.62 +/- 7.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.6     |
|    mean_reward     | 556      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.9     |
|    ep_rew_mean     | 388      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 282      |
|    time_elapsed    | 1041     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=485.99 +/- 705.20
Episode length: 36.76 +/- 6.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.8        |
|    mean_reward          | 486         |
| time/                   |             |
|    total_timesteps      | 289000      |
| train/                  |             |
|    approx_kl            | 0.027370509 |
|    clip_fraction        | 0.00449     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0134     |
|    explained_variance   | 0.0386      |
|    learning_rate        | 0.001       |
|    loss                 | 3.01e+04    |
|    n_updates            | 5680        |
|    policy_gradient_loss | 0.00139     |
|    value_loss           | 8.72e+04    |
-----------------------------------------
Eval num_timesteps=289500, episode_reward=468.67 +/- 685.15
Episode length: 36.46 +/- 4.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.5     |
|    mean_reward     | 469      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.3     |
|    ep_rew_mean     | 458      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 283      |
|    time_elapsed    | 1045     |
|    total_timesteps | 289792   |
---------------------------------
Eval num_timesteps=290000, episode_reward=339.09 +/- 668.99
Episode length: 34.00 +/- 6.09
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 34           |
|    mean_reward          | 339          |
| time/                   |              |
|    total_timesteps      | 290000       |
| train/                  |              |
|    approx_kl            | 0.0014186791 |
|    clip_fraction        | 0.00264      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.015       |
|    explained_variance   | 0.138        |
|    learning_rate        | 0.001        |
|    loss                 | 3.84e+04     |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.00132     |
|    value_loss           | 9.02e+04     |
------------------------------------------
Eval num_timesteps=290500, episode_reward=440.09 +/- 738.47
Episode length: 34.48 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.5     |
|    mean_reward     | 440      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 473      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 284      |
|    time_elapsed    | 1048     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=519.51 +/- 751.79
Episode length: 35.62 +/- 6.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 520          |
| time/                   |              |
|    total_timesteps      | 291000       |
| train/                  |              |
|    approx_kl            | 0.0021490615 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00898     |
|    explained_variance   | -0.227       |
|    learning_rate        | 0.001        |
|    loss                 | 4.15e+04     |
|    n_updates            | 5700         |
|    policy_gradient_loss | -0.00102     |
|    value_loss           | 9.8e+04      |
------------------------------------------
Eval num_timesteps=291500, episode_reward=452.52 +/- 720.47
Episode length: 35.38 +/- 7.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35.4     |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 441      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 285      |
|    time_elapsed    | 1052     |
|    total_timesteps | 291840   |
---------------------------------
Eval num_timesteps=292000, episode_reward=524.03 +/- 759.54
Episode length: 35.62 +/- 7.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.6         |
|    mean_reward          | 524          |
| time/                   |              |
|    total_timesteps      | 292000       |
| train/                  |              |
|    approx_kl            | 0.0006532497 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0112      |
|    explained_variance   | 0.0242       |
|    learning_rate        | 0.001        |
|    loss                 | 3.39e+04     |
|    n_updates            | 5710         |
|    policy_gradient_loss | -0.000172    |
|    value_loss           | 7e+04        |
------------------------------------------
Eval num_timesteps=292500, episode_reward=394.24 +/- 717.05
Episode length: 34.98 +/- 6.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.7     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 286      |
|    time_elapsed    | 1056     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=471.13 +/- 757.83
Episode length: 35.84 +/- 6.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.8        |
|    mean_reward          | 471         |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.023623846 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0194     |
|    explained_variance   | 0.0466      |
|    learning_rate        | 0.001       |
|    loss                 | 3.57e+04    |
|    n_updates            | 5720        |
|    policy_gradient_loss | 0.00571     |
|    value_loss           | 9.27e+04    |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=574.38 +/- 687.95
Episode length: 36.80 +/- 6.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.8     |
|    mean_reward     | 574      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.3     |
|    ep_rew_mean     | 326      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 287      |
|    time_elapsed    | 1059     |
|    total_timesteps | 293888   |
---------------------------------
Eval num_timesteps=294000, episode_reward=434.59 +/- 693.60
Episode length: 35.14 +/- 6.27
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.1         |
|    mean_reward          | 435          |
| time/                   |              |
|    total_timesteps      | 294000       |
| train/                  |              |
|    approx_kl            | 0.0105006965 |
|    clip_fraction        | 0.00586      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00813     |
|    explained_variance   | -0.0416      |
|    learning_rate        | 0.001        |
|    loss                 | 3.69e+04     |
|    n_updates            | 5730         |
|    policy_gradient_loss | -0.00128     |
|    value_loss           | 9.72e+04     |
------------------------------------------
Eval num_timesteps=294500, episode_reward=576.21 +/- 770.57
Episode length: 36.34 +/- 5.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 36.3     |
|    mean_reward     | 576      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34       |
|    ep_rew_mean     | 311      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 288      |
|    time_elapsed    | 1063     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=435.42 +/- 720.25
Episode length: 36.04 +/- 6.34
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 36            |
|    mean_reward          | 435           |
| time/                   |               |
|    total_timesteps      | 295000        |
| train/                  |               |
|    approx_kl            | 3.0410825e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.0255       |
|    explained_variance   | 0.0277        |
|    learning_rate        | 0.001         |
|    loss                 | 3e+04         |
|    n_updates            | 5740          |
|    policy_gradient_loss | -0.00079      |
|    value_loss           | 7.9e+04       |
-------------------------------------------
Eval num_timesteps=295500, episode_reward=377.52 +/- 689.31
Episode length: 34.80 +/- 6.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.8     |
|    mean_reward     | 378      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 379      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 289      |
|    time_elapsed    | 1067     |
|    total_timesteps | 295936   |
---------------------------------
Eval num_timesteps=296000, episode_reward=525.61 +/- 769.20
Episode length: 35.18 +/- 6.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 35.2         |
|    mean_reward          | 526          |
| time/                   |              |
|    total_timesteps      | 296000       |
| train/                  |              |
|    approx_kl            | 0.0035444067 |
|    clip_fraction        | 0.00273      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.00929     |
|    explained_variance   | 0.106        |
|    learning_rate        | 0.001        |
|    loss                 | 3.55e+04     |
|    n_updates            | 5750         |
|    policy_gradient_loss | -0.000558    |
|    value_loss           | 9.5e+04      |
------------------------------------------
Eval num_timesteps=296500, episode_reward=397.68 +/- 748.93
Episode length: 34.74 +/- 6.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.7     |
|    mean_reward     | 398      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 34.6     |
|    ep_rew_mean     | 363      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 290      |
|    time_elapsed    | 1070     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=222.48 +/- 668.08
Episode length: 32.92 +/- 7.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 32.9         |
|    mean_reward          | 222          |
| time/                   |              |
|    total_timesteps      | 297000       |
| train/                  |              |
|    approx_kl            | 0.0002916454 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.0214      |
|    explained_variance   | -0.0703      |
|    learning_rate        | 0.001        |
|    loss                 | 3.6e+04      |
|    n_updates            | 5760         |
|    policy_gradient_loss | -0.000721    |
|    value_loss           | 9.33e+04     |
------------------------------------------
Eval num_timesteps=297500, episode_reward=422.07 +/- 713.44
Episode length: 34.62 +/- 7.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.6     |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.5     |
|    ep_rew_mean     | 481      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 291      |
|    time_elapsed    | 1074     |
|    total_timesteps | 297984   |
---------------------------------
Eval num_timesteps=298000, episode_reward=471.52 +/- 751.04
Episode length: 35.54 +/- 6.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.5        |
|    mean_reward          | 472         |
| time/                   |             |
|    total_timesteps      | 298000      |
| train/                  |             |
|    approx_kl            | 0.011263707 |
|    clip_fraction        | 0.00469     |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0148     |
|    explained_variance   | 0.0424      |
|    learning_rate        | 0.001       |
|    loss                 | 3.74e+04    |
|    n_updates            | 5770        |
|    policy_gradient_loss | 0.00139     |
|    value_loss           | 1e+05       |
-----------------------------------------
Eval num_timesteps=298500, episode_reward=448.72 +/- 740.19
Episode length: 35.00 +/- 6.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 35       |
|    mean_reward     | 449      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=252.48 +/- 578.47
Episode length: 33.56 +/- 7.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 33.6     |
|    mean_reward     | 252      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.7     |
|    ep_rew_mean     | 487      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 292      |
|    time_elapsed    | 1079     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=475.41 +/- 727.42
Episode length: 35.90 +/- 6.21
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 35.9          |
|    mean_reward          | 475           |
| time/                   |               |
|    total_timesteps      | 299500        |
| train/                  |               |
|    approx_kl            | 1.2634555e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -0.00816      |
|    explained_variance   | -0.0463       |
|    learning_rate        | 0.001         |
|    loss                 | 4.2e+04       |
|    n_updates            | 5780          |
|    policy_gradient_loss | -0.00103      |
|    value_loss           | 1.03e+05      |
-------------------------------------------
Eval num_timesteps=300000, episode_reward=444.08 +/- 775.56
Episode length: 34.86 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 34.9     |
|    mean_reward     | 444      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 35.3     |
|    ep_rew_mean     | 458      |
| time/              |          |
|    fps             | 277      |
|    iterations      | 293      |
|    time_elapsed    | 1082     |
|    total_timesteps | 300032   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/corridor/ppo-3/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 1024, 'batch_size': 64, 'learning_rate': 0.001, 'gamma': 0.9635, 'gae_lambda': 0.879}
Training steps: 300000
Frame skip: 4
