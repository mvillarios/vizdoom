/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Training steps: 300000
Frame skip: 4
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 0.25     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 4        |
|    fps              | 3553     |
|    time_elapsed     | 0        |
|    total_timesteps  | 329      |
----------------------------------
Eval num_timesteps=500, episode_reward=-1.00 +/- 0.00
Episode length: 72.30 +/- 10.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.3     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 500      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.2     |
|    ep_rew_mean      | 0.25     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 8        |
|    fps              | 297      |
|    time_elapsed     | 2        |
|    total_timesteps  | 682      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.3     |
|    ep_rew_mean      | 0.333    |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 415      |
|    time_elapsed     | 2        |
|    total_timesteps  | 988      |
----------------------------------
Eval num_timesteps=1000, episode_reward=-1.00 +/- 0.00
Episode length: 67.92 +/- 11.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 67.9     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 1000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.6     |
|    ep_rew_mean      | 0.312    |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 312      |
|    time_elapsed     | 4        |
|    total_timesteps  | 1370     |
----------------------------------
Eval num_timesteps=1500, episode_reward=-1.00 +/- 0.00
Episode length: 72.62 +/- 9.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.6     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 1500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.7     |
|    ep_rew_mean      | 0.35     |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 263      |
|    time_elapsed     | 6        |
|    total_timesteps  | 1714     |
----------------------------------
Eval num_timesteps=2000, episode_reward=-1.00 +/- 0.00
Episode length: 70.94 +/- 9.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.9     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 2000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 0.208    |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 233      |
|    time_elapsed     | 8        |
|    total_timesteps  | 2000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.8     |
|    ep_rew_mean      | 0.179    |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 270      |
|    time_elapsed     | 8        |
|    total_timesteps  | 2346     |
----------------------------------
Eval num_timesteps=2500, episode_reward=-1.00 +/- 0.00
Episode length: 70.08 +/- 10.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.1     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 2500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 0.188    |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 250      |
|    time_elapsed     | 10       |
|    total_timesteps  | 2685     |
----------------------------------
Eval num_timesteps=3000, episode_reward=-1.00 +/- 0.00
Episode length: 69.90 +/- 9.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.9     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 3000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.4     |
|    ep_rew_mean      | 0.139    |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 235      |
|    time_elapsed     | 12       |
|    total_timesteps  | 3003     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 0.15     |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 259      |
|    time_elapsed     | 12       |
|    total_timesteps  | 3334     |
----------------------------------
Eval num_timesteps=3500, episode_reward=-1.00 +/- 0.00
Episode length: 70.74 +/- 9.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.7     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 3500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.7     |
|    ep_rew_mean      | 0.227    |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 246      |
|    time_elapsed     | 14       |
|    total_timesteps  | 3682     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.8     |
|    ep_rew_mean      | 0.208    |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 48       |
|    fps              | 264      |
|    time_elapsed     | 15       |
|    total_timesteps  | 3975     |
----------------------------------
Eval num_timesteps=4000, episode_reward=-1.00 +/- 0.00
Episode length: 69.88 +/- 11.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.9     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 4000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | 0.212    |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 252      |
|    time_elapsed     | 17       |
|    total_timesteps  | 4311     |
----------------------------------
Eval num_timesteps=4500, episode_reward=-1.00 +/- 0.00
Episode length: 71.82 +/- 8.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.8     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 4500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.4     |
|    ep_rew_mean      | 0.232    |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 243      |
|    time_elapsed     | 19       |
|    total_timesteps  | 4672     |
----------------------------------
Eval num_timesteps=5000, episode_reward=-1.00 +/- 0.00
Episode length: 71.22 +/- 11.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.2     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 0.283    |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 60       |
|    fps              | 237      |
|    time_elapsed     | 21       |
|    total_timesteps  | 5039     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | 0.312    |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 64       |
|    fps              | 252      |
|    time_elapsed     | 21       |
|    total_timesteps  | 5386     |
----------------------------------
Eval num_timesteps=5500, episode_reward=-1.00 +/- 0.00
Episode length: 69.14 +/- 10.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.1     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 5500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.7     |
|    ep_rew_mean      | 0.294    |
|    exploration_rate | 0.993    |
| time/               |          |
|    episodes         | 68       |
|    fps              | 243      |
|    time_elapsed     | 23       |
|    total_timesteps  | 5692     |
----------------------------------
Eval num_timesteps=6000, episode_reward=-1.00 +/- 0.00
Episode length: 71.28 +/- 9.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.3     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 6000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.6     |
|    ep_rew_mean      | 0.278    |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 72       |
|    fps              | 236      |
|    time_elapsed     | 25       |
|    total_timesteps  | 6016     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.4     |
|    ep_rew_mean      | 0.25     |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 76       |
|    fps              | 248      |
|    time_elapsed     | 25       |
|    total_timesteps  | 6341     |
----------------------------------
Eval num_timesteps=6500, episode_reward=-1.00 +/- 0.00
Episode length: 70.98 +/- 9.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71       |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.991    |
| time/               |          |
|    total_timesteps  | 6500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.8     |
|    ep_rew_mean      | 0.275    |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 80       |
|    fps              | 243      |
|    time_elapsed     | 27       |
|    total_timesteps  | 6700     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 0.238    |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 84       |
|    fps              | 252      |
|    time_elapsed     | 27       |
|    total_timesteps  | 6970     |
----------------------------------
Eval num_timesteps=7000, episode_reward=-1.00 +/- 0.00
Episode length: 69.54 +/- 11.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.5     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.99     |
| time/               |          |
|    total_timesteps  | 7000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.8     |
|    ep_rew_mean      | 0.25     |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 88       |
|    fps              | 245      |
|    time_elapsed     | 29       |
|    total_timesteps  | 7289     |
----------------------------------
Eval num_timesteps=7500, episode_reward=-1.00 +/- 0.00
Episode length: 71.32 +/- 11.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.3     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.989    |
| time/               |          |
|    total_timesteps  | 7500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 0.283    |
|    exploration_rate | 0.989    |
| time/               |          |
|    episodes         | 92       |
|    fps              | 240      |
|    time_elapsed     | 31       |
|    total_timesteps  | 7638     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.8     |
|    ep_rew_mean      | 0.25     |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 96       |
|    fps              | 249      |
|    time_elapsed     | 31       |
|    total_timesteps  | 7952     |
----------------------------------
Eval num_timesteps=8000, episode_reward=-1.00 +/- 0.00
Episode length: 72.84 +/- 9.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.8     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 8000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 0.3      |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 100      |
|    fps              | 244      |
|    time_elapsed     | 33       |
|    total_timesteps  | 8305     |
----------------------------------
Eval num_timesteps=8500, episode_reward=-1.00 +/- 0.00
Episode length: 69.90 +/- 9.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.9     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.987    |
| time/               |          |
|    total_timesteps  | 8500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.1     |
|    ep_rew_mean      | 0.28     |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 104      |
|    fps              | 240      |
|    time_elapsed     | 35       |
|    total_timesteps  | 8637     |
----------------------------------
Eval num_timesteps=9000, episode_reward=-1.00 +/- 0.00
Episode length: 72.02 +/- 10.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72       |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.986    |
| time/               |          |
|    total_timesteps  | 9000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.5     |
|    ep_rew_mean      | 0.3      |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 108      |
|    fps              | 237      |
|    time_elapsed     | 38       |
|    total_timesteps  | 9036     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 0.29     |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 112      |
|    fps              | 244      |
|    time_elapsed     | 38       |
|    total_timesteps  | 9323     |
----------------------------------
Eval num_timesteps=9500, episode_reward=-1.00 +/- 0.00
Episode length: 70.30 +/- 10.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.3     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.984    |
| time/               |          |
|    total_timesteps  | 9500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.7     |
|    ep_rew_mean      | 0.3      |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 116      |
|    fps              | 240      |
|    time_elapsed     | 40       |
|    total_timesteps  | 9643     |
----------------------------------
Eval num_timesteps=10000, episode_reward=-1.00 +/- 0.00
Episode length: 70.78 +/- 8.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.8     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 0.3      |
|    exploration_rate | 0.983    |
| time/               |          |
|    episodes         | 120      |
|    fps              | 236      |
|    time_elapsed     | 42       |
|    total_timesteps  | 10013    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.048    |
|    n_updates        | 3        |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 0.3      |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 124      |
|    fps              | 242      |
|    time_elapsed     | 42       |
|    total_timesteps  | 10300    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 4.44e-05 |
|    n_updates        | 74       |
----------------------------------
Eval num_timesteps=10500, episode_reward=-1.00 +/- 0.00
Episode length: 71.14 +/- 10.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.1     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.982    |
| time/               |          |
|    total_timesteps  | 10500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 124      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.7     |
|    ep_rew_mean      | 0.3      |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 128      |
|    fps              | 237      |
|    time_elapsed     | 44       |
|    total_timesteps  | 10618    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0158   |
|    n_updates        | 154      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 0.31     |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 132      |
|    fps              | 243      |
|    time_elapsed     | 44       |
|    total_timesteps  | 10923    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0161   |
|    n_updates        | 230      |
----------------------------------
Eval num_timesteps=11000, episode_reward=0.54 +/- 0.64
Episode length: 77.42 +/- 14.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.4     |
|    mean_reward      | 0.54     |
| rollout/            |          |
|    exploration_rate | 0.98     |
| time/               |          |
|    total_timesteps  | 11000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 249      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 0.33     |
|    exploration_rate | 0.98     |
| time/               |          |
|    episodes         | 136      |
|    fps              | 237      |
|    time_elapsed     | 47       |
|    total_timesteps  | 11245    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 311      |
----------------------------------
Eval num_timesteps=11500, episode_reward=-1.00 +/- 0.00
Episode length: 71.18 +/- 10.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.2     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.979    |
| time/               |          |
|    total_timesteps  | 11500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0456   |
|    n_updates        | 374      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 0.31     |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 140      |
|    fps              | 232      |
|    time_elapsed     | 49       |
|    total_timesteps  | 11516    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.015    |
|    n_updates        | 378      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 0.28     |
|    exploration_rate | 0.978    |
| time/               |          |
|    episodes         | 144      |
|    fps              | 238      |
|    time_elapsed     | 49       |
|    total_timesteps  | 11868    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 3.65e-05 |
|    n_updates        | 466      |
----------------------------------
Eval num_timesteps=12000, episode_reward=0.72 +/- 0.80
Episode length: 75.86 +/- 13.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.9     |
|    mean_reward      | 0.72     |
| rollout/            |          |
|    exploration_rate | 0.978    |
| time/               |          |
|    total_timesteps  | 12000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0163   |
|    n_updates        | 499      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 0.27     |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 148      |
|    fps              | 233      |
|    time_elapsed     | 51       |
|    total_timesteps  | 12157    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 1.8e-05  |
|    n_updates        | 539      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 0.28     |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 152      |
|    fps              | 238      |
|    time_elapsed     | 52       |
|    total_timesteps  | 12471    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0155   |
|    n_updates        | 617      |
----------------------------------
Eval num_timesteps=12500, episode_reward=-1.00 +/- 0.00
Episode length: 70.12 +/- 12.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.1     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.976    |
| time/               |          |
|    total_timesteps  | 12500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0153   |
|    n_updates        | 624      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 0.27     |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 156      |
|    fps              | 235      |
|    time_elapsed     | 54       |
|    total_timesteps  | 12811    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 702      |
----------------------------------
Eval num_timesteps=13000, episode_reward=0.08 +/- 0.27
Episode length: 71.52 +/- 12.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.5     |
|    mean_reward      | 0.08     |
| rollout/            |          |
|    exploration_rate | 0.975    |
| time/               |          |
|    total_timesteps  | 13000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 749      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 0.21     |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 160      |
|    fps              | 231      |
|    time_elapsed     | 56       |
|    total_timesteps  | 13108    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0315   |
|    n_updates        | 776      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 0.2      |
|    exploration_rate | 0.974    |
| time/               |          |
|    episodes         | 164      |
|    fps              | 236      |
|    time_elapsed     | 56       |
|    total_timesteps  | 13458    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0144   |
|    n_updates        | 864      |
----------------------------------
Eval num_timesteps=13500, episode_reward=0.68 +/- 0.90
Episode length: 77.52 +/- 15.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.5     |
|    mean_reward      | 0.68     |
| rollout/            |          |
|    exploration_rate | 0.973    |
| time/               |          |
|    total_timesteps  | 13500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0299   |
|    n_updates        | 874      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.3     |
|    ep_rew_mean      | 0.18     |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 168      |
|    fps              | 231      |
|    time_elapsed     | 59       |
|    total_timesteps  | 13724    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0324   |
|    n_updates        | 930      |
----------------------------------
Eval num_timesteps=14000, episode_reward=0.74 +/- 0.89
Episode length: 80.94 +/- 18.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.9     |
|    mean_reward      | 0.74     |
| rollout/            |          |
|    exploration_rate | 0.972    |
| time/               |          |
|    total_timesteps  | 14000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 999      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 0.2      |
|    exploration_rate | 0.972    |
| time/               |          |
|    episodes         | 172      |
|    fps              | 228      |
|    time_elapsed     | 61       |
|    total_timesteps  | 14087    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000337 |
|    n_updates        | 1021     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 0.24     |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 176      |
|    fps              | 233      |
|    time_elapsed     | 61       |
|    total_timesteps  | 14459    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 1114     |
----------------------------------
Eval num_timesteps=14500, episode_reward=-1.00 +/- 0.00
Episode length: 68.58 +/- 9.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.6     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.97     |
| time/               |          |
|    total_timesteps  | 14500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0314   |
|    n_updates        | 1124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 0.24     |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 180      |
|    fps              | 231      |
|    time_elapsed     | 64       |
|    total_timesteps  | 14819    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 1204     |
----------------------------------
Eval num_timesteps=15000, episode_reward=-0.36 +/- 0.69
Episode length: 78.66 +/- 16.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.7     |
|    mean_reward      | -0.36    |
| rollout/            |          |
|    exploration_rate | 0.969    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000155 |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.2     |
|    ep_rew_mean      | 0.22     |
|    exploration_rate | 0.969    |
| time/               |          |
|    episodes         | 184      |
|    fps              | 227      |
|    time_elapsed     | 66       |
|    total_timesteps  | 15093    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 1273     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.1     |
|    ep_rew_mean      | 0.18     |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 188      |
|    fps              | 231      |
|    time_elapsed     | 66       |
|    total_timesteps  | 15402    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0275   |
|    n_updates        | 1350     |
----------------------------------
Eval num_timesteps=15500, episode_reward=-1.00 +/- 0.00
Episode length: 73.40 +/- 10.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.4     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.967    |
| time/               |          |
|    total_timesteps  | 15500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000203 |
|    n_updates        | 1374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 0.14     |
|    exploration_rate | 0.967    |
| time/               |          |
|    episodes         | 192      |
|    fps              | 227      |
|    time_elapsed     | 68       |
|    total_timesteps  | 15709    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 1427     |
----------------------------------
Eval num_timesteps=16000, episode_reward=-0.02 +/- 0.65
Episode length: 79.48 +/- 13.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.5     |
|    mean_reward      | -0.02    |
| rollout/            |          |
|    exploration_rate | 0.966    |
| time/               |          |
|    total_timesteps  | 16000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 1499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 0.15     |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 196      |
|    fps              | 224      |
|    time_elapsed     | 71       |
|    total_timesteps  | 16017    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000311 |
|    n_updates        | 1504     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 0.11     |
|    exploration_rate | 0.965    |
| time/               |          |
|    episodes         | 200      |
|    fps              | 227      |
|    time_elapsed     | 71       |
|    total_timesteps  | 16313    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0139   |
|    n_updates        | 1578     |
----------------------------------
Eval num_timesteps=16500, episode_reward=-0.94 +/- 0.24
Episode length: 70.56 +/- 10.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.6     |
|    mean_reward      | -0.94    |
| rollout/            |          |
|    exploration_rate | 0.964    |
| time/               |          |
|    total_timesteps  | 16500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000312 |
|    n_updates        | 1624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.7     |
|    ep_rew_mean      | 0.12     |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 204      |
|    fps              | 225      |
|    time_elapsed     | 73       |
|    total_timesteps  | 16609    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 1652     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79       |
|    ep_rew_mean      | 0.08     |
|    exploration_rate | 0.963    |
| time/               |          |
|    episodes         | 208      |
|    fps              | 228      |
|    time_elapsed     | 73       |
|    total_timesteps  | 16931    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 1732     |
----------------------------------
Eval num_timesteps=17000, episode_reward=-1.00 +/- 0.00
Episode length: 69.68 +/- 8.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.7     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.962    |
| time/               |          |
|    total_timesteps  | 17000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000261 |
|    n_updates        | 1749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.2     |
|    ep_rew_mean      | 0.06     |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 212      |
|    fps              | 226      |
|    time_elapsed     | 76       |
|    total_timesteps  | 17247    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0326   |
|    n_updates        | 1811     |
----------------------------------
Eval num_timesteps=17500, episode_reward=-1.00 +/- 0.00
Episode length: 72.14 +/- 11.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.1     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.961    |
| time/               |          |
|    total_timesteps  | 17500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00108  |
|    n_updates        | 1874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.4     |
|    ep_rew_mean      | 0.06     |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 216      |
|    fps              | 224      |
|    time_elapsed     | 78       |
|    total_timesteps  | 17586    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0004   |
|    n_updates        | 1896     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 0.05     |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 220      |
|    fps              | 228      |
|    time_elapsed     | 78       |
|    total_timesteps  | 17923    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000253 |
|    n_updates        | 1980     |
----------------------------------
Eval num_timesteps=18000, episode_reward=-0.82 +/- 0.65
Episode length: 73.74 +/- 12.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.7     |
|    mean_reward      | -0.82    |
| rollout/            |          |
|    exploration_rate | 0.959    |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0307   |
|    n_updates        | 1999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 0.06     |
|    exploration_rate | 0.958    |
| time/               |          |
|    episodes         | 224      |
|    fps              | 225      |
|    time_elapsed     | 80       |
|    total_timesteps  | 18213    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000333 |
|    n_updates        | 2053     |
----------------------------------
Eval num_timesteps=18500, episode_reward=-1.00 +/- 0.00
Episode length: 71.46 +/- 10.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.5     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.957    |
| time/               |          |
|    total_timesteps  | 18500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0162   |
|    n_updates        | 2124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 0.06     |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 228      |
|    fps              | 223      |
|    time_elapsed     | 83       |
|    total_timesteps  | 18525    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0151   |
|    n_updates        | 2131     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.1     |
|    ep_rew_mean      | 0.05     |
|    exploration_rate | 0.956    |
| time/               |          |
|    episodes         | 232      |
|    fps              | 226      |
|    time_elapsed     | 83       |
|    total_timesteps  | 18835    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0321   |
|    n_updates        | 2208     |
----------------------------------
Eval num_timesteps=19000, episode_reward=-0.56 +/- 0.54
Episode length: 73.58 +/- 9.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.6     |
|    mean_reward      | -0.56    |
| rollout/            |          |
|    exploration_rate | 0.956    |
| time/               |          |
|    total_timesteps  | 19000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000453 |
|    n_updates        | 2249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 79.8     |
|    ep_rew_mean      | 0.07     |
|    exploration_rate | 0.955    |
| time/               |          |
|    episodes         | 236      |
|    fps              | 224      |
|    time_elapsed     | 85       |
|    total_timesteps  | 19227    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0169   |
|    n_updates        | 2306     |
----------------------------------
Eval num_timesteps=19500, episode_reward=-0.80 +/- 0.45
Episode length: 68.82 +/- 8.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68.8     |
|    mean_reward      | -0.8     |
| rollout/            |          |
|    exploration_rate | 0.954    |
| time/               |          |
|    total_timesteps  | 19500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000407 |
|    n_updates        | 2374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 0.08     |
|    exploration_rate | 0.954    |
| time/               |          |
|    episodes         | 240      |
|    fps              | 222      |
|    time_elapsed     | 87       |
|    total_timesteps  | 19528    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00907  |
|    n_updates        | 2381     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.1     |
|    ep_rew_mean      | 0.08     |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 244      |
|    fps              | 225      |
|    time_elapsed     | 87       |
|    total_timesteps  | 19875    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000181 |
|    n_updates        | 2468     |
----------------------------------
Eval num_timesteps=20000, episode_reward=0.32 +/- 0.55
Episode length: 73.48 +/- 11.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.5     |
|    mean_reward      | 0.32     |
| rollout/            |          |
|    exploration_rate | 0.952    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0434   |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.6     |
|    ep_rew_mean      | 0.1      |
|    exploration_rate | 0.951    |
| time/               |          |
|    episodes         | 248      |
|    fps              | 223      |
|    time_elapsed     | 90       |
|    total_timesteps  | 20214    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0286   |
|    n_updates        | 2553     |
----------------------------------
Eval num_timesteps=20500, episode_reward=-1.00 +/- 0.00
Episode length: 70.58 +/- 11.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.6     |
|    mean_reward      | -1       |
| rollout/            |          |
|    exploration_rate | 0.95     |
| time/               |          |
|    total_timesteps  | 20500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000129 |
|    n_updates        | 2624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.1     |
|    ep_rew_mean      | 0.08     |
|    exploration_rate | 0.95     |
| time/               |          |
|    episodes         | 252      |
|    fps              | 222      |
|    time_elapsed     | 92       |
|    total_timesteps  | 20584    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00172  |
|    n_updates        | 2645     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.4     |
|    ep_rew_mean      | 0.05     |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 256      |
|    fps              | 225      |
|    time_elapsed     | 92       |
|    total_timesteps  | 20855    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0145   |
|    n_updates        | 2713     |
----------------------------------
Eval num_timesteps=21000, episode_reward=1.30 +/- 1.02
Episode length: 99.86 +/- 16.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.9     |
|    mean_reward      | 1.3      |
| rollout/            |          |
|    exploration_rate | 0.948    |
| time/               |          |
|    total_timesteps  | 21000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 2749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 0.12     |
|    exploration_rate | 0.948    |
| time/               |          |
|    episodes         | 260      |
|    fps              | 221      |
|    time_elapsed     | 95       |
|    total_timesteps  | 21234    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000778 |
|    n_updates        | 2808     |
----------------------------------
Eval num_timesteps=21500, episode_reward=-0.70 +/- 0.73
Episode length: 73.44 +/- 13.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.4     |
|    mean_reward      | -0.7     |
| rollout/            |          |
|    exploration_rate | 0.947    |
| time/               |          |
|    total_timesteps  | 21500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0208   |
|    n_updates        | 2874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 0.15     |
|    exploration_rate | 0.946    |
| time/               |          |
|    episodes         | 264      |
|    fps              | 220      |
|    time_elapsed     | 98       |
|    total_timesteps  | 21620    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0315   |
|    n_updates        | 2904     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 0.15     |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 268      |
|    fps              | 223      |
|    time_elapsed     | 98       |
|    total_timesteps  | 21947    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 2986     |
----------------------------------
Eval num_timesteps=22000, episode_reward=-0.44 +/- 0.70
Episode length: 78.44 +/- 14.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.4     |
|    mean_reward      | -0.44    |
| rollout/            |          |
|    exploration_rate | 0.945    |
| time/               |          |
|    total_timesteps  | 22000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000511 |
|    n_updates        | 2999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 0.17     |
|    exploration_rate | 0.944    |
| time/               |          |
|    episodes         | 272      |
|    fps              | 221      |
|    time_elapsed     | 100      |
|    total_timesteps  | 22311    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0189   |
|    n_updates        | 3077     |
----------------------------------
Eval num_timesteps=22500, episode_reward=-0.54 +/- 0.70
Episode length: 73.14 +/- 12.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.1     |
|    mean_reward      | -0.54    |
| rollout/            |          |
|    exploration_rate | 0.943    |
| time/               |          |
|    total_timesteps  | 22500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00071  |
|    n_updates        | 3124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 0.15     |
|    exploration_rate | 0.942    |
| time/               |          |
|    episodes         | 276      |
|    fps              | 219      |
|    time_elapsed     | 103      |
|    total_timesteps  | 22652    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 3162     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 0.11     |
|    exploration_rate | 0.941    |
| time/               |          |
|    episodes         | 280      |
|    fps              | 222      |
|    time_elapsed     | 103      |
|    total_timesteps  | 22953    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0143   |
|    n_updates        | 3238     |
----------------------------------
Eval num_timesteps=23000, episode_reward=1.60 +/- 1.11
Episode length: 89.44 +/- 18.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.4     |
|    mean_reward      | 1.6      |
| rollout/            |          |
|    exploration_rate | 0.941    |
| time/               |          |
|    total_timesteps  | 23000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0202   |
|    n_updates        | 3249     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.8     |
|    ep_rew_mean      | 0.16     |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 284      |
|    fps              | 219      |
|    time_elapsed     | 105      |
|    total_timesteps  | 23268    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0293   |
|    n_updates        | 3316     |
----------------------------------
Eval num_timesteps=23500, episode_reward=-0.44 +/- 0.70
Episode length: 75.10 +/- 13.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.1     |
|    mean_reward      | -0.44    |
| rollout/            |          |
|    exploration_rate | 0.939    |
| time/               |          |
|    total_timesteps  | 23500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00107  |
|    n_updates        | 3374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 0.2      |
|    exploration_rate | 0.939    |
| time/               |          |
|    episodes         | 288      |
|    fps              | 218      |
|    time_elapsed     | 108      |
|    total_timesteps  | 23604    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0599   |
|    n_updates        | 3400     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.3     |
|    ep_rew_mean      | 0.23     |
|    exploration_rate | 0.937    |
| time/               |          |
|    episodes         | 292      |
|    fps              | 220      |
|    time_elapsed     | 108      |
|    total_timesteps  | 23939    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00471  |
|    n_updates        | 3484     |
----------------------------------
Eval num_timesteps=24000, episode_reward=-0.88 +/- 0.32
Episode length: 73.16 +/- 12.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.2     |
|    mean_reward      | -0.88    |
| rollout/            |          |
|    exploration_rate | 0.937    |
| time/               |          |
|    total_timesteps  | 24000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0222   |
|    n_updates        | 3499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 0.24     |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 296      |
|    fps              | 218      |
|    time_elapsed     | 110      |
|    total_timesteps  | 24214    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000839 |
|    n_updates        | 3553     |
----------------------------------
Eval num_timesteps=24500, episode_reward=-0.72 +/- 0.49
Episode length: 75.82 +/- 13.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.8     |
|    mean_reward      | -0.72    |
| rollout/            |          |
|    exploration_rate | 0.935    |
| time/               |          |
|    total_timesteps  | 24500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0158   |
|    n_updates        | 3624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.8     |
|    ep_rew_mean      | 0.27     |
|    exploration_rate | 0.935    |
| time/               |          |
|    episodes         | 300      |
|    fps              | 217      |
|    time_elapsed     | 113      |
|    total_timesteps  | 24594    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00459  |
|    n_updates        | 3648     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.6     |
|    ep_rew_mean      | 0.3      |
|    exploration_rate | 0.933    |
| time/               |          |
|    episodes         | 304      |
|    fps              | 220      |
|    time_elapsed     | 113      |
|    total_timesteps  | 24965    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00105  |
|    n_updates        | 3741     |
----------------------------------
Eval num_timesteps=25000, episode_reward=0.40 +/- 0.53
Episode length: 77.58 +/- 14.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.6     |
|    mean_reward      | 0.4      |
| rollout/            |          |
|    exploration_rate | 0.933    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00217  |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 0.33     |
|    exploration_rate | 0.932    |
| time/               |          |
|    episodes         | 308      |
|    fps              | 218      |
|    time_elapsed     | 115      |
|    total_timesteps  | 25318    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0309   |
|    n_updates        | 3829     |
----------------------------------
Eval num_timesteps=25500, episode_reward=-0.78 +/- 0.41
Episode length: 73.06 +/- 9.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.1     |
|    mean_reward      | -0.78    |
| rollout/            |          |
|    exploration_rate | 0.931    |
| time/               |          |
|    total_timesteps  | 25500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0254   |
|    n_updates        | 3874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.2     |
|    ep_rew_mean      | 0.35     |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 312      |
|    fps              | 217      |
|    time_elapsed     | 118      |
|    total_timesteps  | 25671    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 3917     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.8     |
|    ep_rew_mean      | 0.33     |
|    exploration_rate | 0.929    |
| time/               |          |
|    episodes         | 316      |
|    fps              | 219      |
|    time_elapsed     | 118      |
|    total_timesteps  | 25968    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00126  |
|    n_updates        | 3991     |
----------------------------------
Eval num_timesteps=26000, episode_reward=0.08 +/- 0.27
Episode length: 71.84 +/- 9.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.8     |
|    mean_reward      | 0.08     |
| rollout/            |          |
|    exploration_rate | 0.929    |
| time/               |          |
|    total_timesteps  | 26000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 3999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 0.35     |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 320      |
|    fps              | 218      |
|    time_elapsed     | 120      |
|    total_timesteps  | 26327    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0161   |
|    n_updates        | 4081     |
----------------------------------
Eval num_timesteps=26500, episode_reward=0.28 +/- 0.60
Episode length: 71.42 +/- 11.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.4     |
|    mean_reward      | 0.28     |
| rollout/            |          |
|    exploration_rate | 0.927    |
| time/               |          |
|    total_timesteps  | 26500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.017    |
|    n_updates        | 4124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.5     |
|    ep_rew_mean      | 0.37     |
|    exploration_rate | 0.926    |
| time/               |          |
|    episodes         | 324      |
|    fps              | 217      |
|    time_elapsed     | 122      |
|    total_timesteps  | 26665    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00125  |
|    n_updates        | 4166     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.5     |
|    ep_rew_mean      | 0.35     |
|    exploration_rate | 0.925    |
| time/               |          |
|    episodes         | 328      |
|    fps              | 219      |
|    time_elapsed     | 122      |
|    total_timesteps  | 26975    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000724 |
|    n_updates        | 4243     |
----------------------------------
Eval num_timesteps=27000, episode_reward=-0.10 +/- 0.64
Episode length: 73.48 +/- 13.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.5     |
|    mean_reward      | -0.1     |
| rollout/            |          |
|    exploration_rate | 0.925    |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 4249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.9     |
|    ep_rew_mean      | 0.36     |
|    exploration_rate | 0.924    |
| time/               |          |
|    episodes         | 332      |
|    fps              | 218      |
|    time_elapsed     | 125      |
|    total_timesteps  | 27321    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.015    |
|    n_updates        | 4330     |
----------------------------------
Eval num_timesteps=27500, episode_reward=-0.86 +/- 0.35
Episode length: 71.82 +/- 10.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.8     |
|    mean_reward      | -0.86    |
| rollout/            |          |
|    exploration_rate | 0.923    |
| time/               |          |
|    total_timesteps  | 27500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0199   |
|    n_updates        | 4374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 0.35     |
|    exploration_rate | 0.922    |
| time/               |          |
|    episodes         | 336      |
|    fps              | 216      |
|    time_elapsed     | 127      |
|    total_timesteps  | 27659    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0391   |
|    n_updates        | 4414     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 0.36     |
|    exploration_rate | 0.921    |
| time/               |          |
|    episodes         | 340      |
|    fps              | 219      |
|    time_elapsed     | 127      |
|    total_timesteps  | 27998    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00277  |
|    n_updates        | 4499     |
----------------------------------
Eval num_timesteps=28000, episode_reward=-0.72 +/- 0.49
Episode length: 75.68 +/- 13.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.7     |
|    mean_reward      | -0.72    |
| rollout/            |          |
|    exploration_rate | 0.921    |
| time/               |          |
|    total_timesteps  | 28000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.8     |
|    ep_rew_mean      | 0.37     |
|    exploration_rate | 0.919    |
| time/               |          |
|    episodes         | 344      |
|    fps              | 217      |
|    time_elapsed     | 130      |
|    total_timesteps  | 28350    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0265   |
|    n_updates        | 4587     |
----------------------------------
Eval num_timesteps=28500, episode_reward=0.14 +/- 0.60
Episode length: 78.16 +/- 11.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.2     |
|    mean_reward      | 0.14     |
| rollout/            |          |
|    exploration_rate | 0.919    |
| time/               |          |
|    total_timesteps  | 28500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0296   |
|    n_updates        | 4624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85       |
|    ep_rew_mean      | 0.38     |
|    exploration_rate | 0.918    |
| time/               |          |
|    episodes         | 348      |
|    fps              | 216      |
|    time_elapsed     | 132      |
|    total_timesteps  | 28710    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000307 |
|    n_updates        | 4677     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84       |
|    ep_rew_mean      | 0.38     |
|    exploration_rate | 0.916    |
| time/               |          |
|    episodes         | 352      |
|    fps              | 218      |
|    time_elapsed     | 132      |
|    total_timesteps  | 28985    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00186  |
|    n_updates        | 4746     |
----------------------------------
Eval num_timesteps=29000, episode_reward=-0.24 +/- 0.86
Episode length: 78.34 +/- 14.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.3     |
|    mean_reward      | -0.24    |
| rollout/            |          |
|    exploration_rate | 0.916    |
| time/               |          |
|    total_timesteps  | 29000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0155   |
|    n_updates        | 4749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.3     |
|    ep_rew_mean      | 0.39     |
|    exploration_rate | 0.915    |
| time/               |          |
|    episodes         | 356      |
|    fps              | 216      |
|    time_elapsed     | 135      |
|    total_timesteps  | 29287    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 4821     |
----------------------------------
Eval num_timesteps=29500, episode_reward=-0.20 +/- 0.75
Episode length: 76.02 +/- 12.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76       |
|    mean_reward      | -0.2     |
| rollout/            |          |
|    exploration_rate | 0.914    |
| time/               |          |
|    total_timesteps  | 29500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0229   |
|    n_updates        | 4874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.9     |
|    ep_rew_mean      | 0.36     |
|    exploration_rate | 0.914    |
| time/               |          |
|    episodes         | 360      |
|    fps              | 215      |
|    time_elapsed     | 137      |
|    total_timesteps  | 29622    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0174   |
|    n_updates        | 4905     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.1     |
|    ep_rew_mean      | 0.31     |
|    exploration_rate | 0.912    |
| time/               |          |
|    episodes         | 364      |
|    fps              | 217      |
|    time_elapsed     | 137      |
|    total_timesteps  | 29931    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0153   |
|    n_updates        | 4982     |
----------------------------------
Eval num_timesteps=30000, episode_reward=0.56 +/- 1.56
Episode length: 82.12 +/- 24.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.1     |
|    mean_reward      | 0.56     |
| rollout/            |          |
|    exploration_rate | 0.912    |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000931 |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 0.33     |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 368      |
|    fps              | 215      |
|    time_elapsed     | 140      |
|    total_timesteps  | 30282    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0128   |
|    n_updates        | 5070     |
----------------------------------
Eval num_timesteps=30500, episode_reward=-0.38 +/- 0.80
Episode length: 76.84 +/- 14.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.8     |
|    mean_reward      | -0.38    |
| rollout/            |          |
|    exploration_rate | 0.91     |
| time/               |          |
|    total_timesteps  | 30500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0175   |
|    n_updates        | 5124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.5     |
|    ep_rew_mean      | 0.3      |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 372      |
|    fps              | 214      |
|    time_elapsed     | 142      |
|    total_timesteps  | 30563    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00192  |
|    n_updates        | 5140     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.2     |
|    ep_rew_mean      | 0.34     |
|    exploration_rate | 0.908    |
| time/               |          |
|    episodes         | 376      |
|    fps              | 216      |
|    time_elapsed     | 142      |
|    total_timesteps  | 30971    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00171  |
|    n_updates        | 5242     |
----------------------------------
Eval num_timesteps=31000, episode_reward=-0.94 +/- 0.24
Episode length: 67.98 +/- 8.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 68       |
|    mean_reward      | -0.94    |
| rollout/            |          |
|    exploration_rate | 0.908    |
| time/               |          |
|    total_timesteps  | 31000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0113   |
|    n_updates        | 5249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | 0.33     |
|    exploration_rate | 0.906    |
| time/               |          |
|    episodes         | 380      |
|    fps              | 215      |
|    time_elapsed     | 144      |
|    total_timesteps  | 31246    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 5311     |
----------------------------------
Eval num_timesteps=31500, episode_reward=-0.52 +/- 0.73
Episode length: 75.20 +/- 13.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.2     |
|    mean_reward      | -0.52    |
| rollout/            |          |
|    exploration_rate | 0.905    |
| time/               |          |
|    total_timesteps  | 31500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00227  |
|    n_updates        | 5374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 0.36     |
|    exploration_rate | 0.905    |
| time/               |          |
|    episodes         | 384      |
|    fps              | 214      |
|    time_elapsed     | 147      |
|    total_timesteps  | 31599    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0155   |
|    n_updates        | 5399     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.8     |
|    ep_rew_mean      | 0.34     |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 388      |
|    fps              | 216      |
|    time_elapsed     | 147      |
|    total_timesteps  | 31880    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 5469     |
----------------------------------
Eval num_timesteps=32000, episode_reward=1.66 +/- 2.22
Episode length: 95.24 +/- 31.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.2     |
|    mean_reward      | 1.66     |
| rollout/            |          |
|    exploration_rate | 0.903    |
| time/               |          |
|    total_timesteps  | 32000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0129   |
|    n_updates        | 5499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.4     |
|    ep_rew_mean      | 0.34     |
|    exploration_rate | 0.902    |
| time/               |          |
|    episodes         | 392      |
|    fps              | 214      |
|    time_elapsed     | 150      |
|    total_timesteps  | 32277    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00478  |
|    n_updates        | 5569     |
----------------------------------
Eval num_timesteps=32500, episode_reward=-0.28 +/- 0.85
Episode length: 74.88 +/- 13.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.9     |
|    mean_reward      | -0.28    |
| rollout/            |          |
|    exploration_rate | 0.901    |
| time/               |          |
|    total_timesteps  | 32500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000251 |
|    n_updates        | 5624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.4     |
|    ep_rew_mean      | 0.32     |
|    exploration_rate | 0.901    |
| time/               |          |
|    episodes         | 396      |
|    fps              | 213      |
|    time_elapsed     | 152      |
|    total_timesteps  | 32552    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000732 |
|    n_updates        | 5637     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.4     |
|    ep_rew_mean      | 0.31     |
|    exploration_rate | 0.899    |
| time/               |          |
|    episodes         | 400      |
|    fps              | 215      |
|    time_elapsed     | 152      |
|    total_timesteps  | 32932    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000629 |
|    n_updates        | 5732     |
----------------------------------
Eval num_timesteps=33000, episode_reward=0.00 +/- 1.10
Episode length: 80.58 +/- 17.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.6     |
|    mean_reward      | 0        |
| rollout/            |          |
|    exploration_rate | 0.898    |
| time/               |          |
|    total_timesteps  | 33000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00563  |
|    n_updates        | 5749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 0.27     |
|    exploration_rate | 0.897    |
| time/               |          |
|    episodes         | 404      |
|    fps              | 213      |
|    time_elapsed     | 155      |
|    total_timesteps  | 33260    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00212  |
|    n_updates        | 5814     |
----------------------------------
Eval num_timesteps=33500, episode_reward=-0.10 +/- 0.78
Episode length: 77.30 +/- 13.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.3     |
|    mean_reward      | -0.1     |
| rollout/            |          |
|    exploration_rate | 0.896    |
| time/               |          |
|    total_timesteps  | 33500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 5874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.8     |
|    ep_rew_mean      | 0.26     |
|    exploration_rate | 0.896    |
| time/               |          |
|    episodes         | 408      |
|    fps              | 212      |
|    time_elapsed     | 157      |
|    total_timesteps  | 33596    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00491  |
|    n_updates        | 5898     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.6     |
|    ep_rew_mean      | 0.26     |
|    exploration_rate | 0.894    |
| time/               |          |
|    episodes         | 412      |
|    fps              | 214      |
|    time_elapsed     | 158      |
|    total_timesteps  | 33932    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0179   |
|    n_updates        | 5982     |
----------------------------------
Eval num_timesteps=34000, episode_reward=0.36 +/- 0.59
Episode length: 73.66 +/- 11.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.7     |
|    mean_reward      | 0.36     |
| rollout/            |          |
|    exploration_rate | 0.894    |
| time/               |          |
|    total_timesteps  | 34000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00958  |
|    n_updates        | 5999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.7     |
|    ep_rew_mean      | 0.32     |
|    exploration_rate | 0.892    |
| time/               |          |
|    episodes         | 416      |
|    fps              | 214      |
|    time_elapsed     | 160      |
|    total_timesteps  | 34336    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0173   |
|    n_updates        | 6083     |
----------------------------------
Eval num_timesteps=34500, episode_reward=0.80 +/- 1.73
Episode length: 89.20 +/- 24.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.2     |
|    mean_reward      | 0.8      |
| rollout/            |          |
|    exploration_rate | 0.891    |
| time/               |          |
|    total_timesteps  | 34500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00199  |
|    n_updates        | 6124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.6     |
|    ep_rew_mean      | 0.29     |
|    exploration_rate | 0.891    |
| time/               |          |
|    episodes         | 420      |
|    fps              | 212      |
|    time_elapsed     | 163      |
|    total_timesteps  | 34591    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00827  |
|    n_updates        | 6147     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83       |
|    ep_rew_mean      | 0.34     |
|    exploration_rate | 0.889    |
| time/               |          |
|    episodes         | 424      |
|    fps              | 214      |
|    time_elapsed     | 163      |
|    total_timesteps  | 34966    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00323  |
|    n_updates        | 6241     |
----------------------------------
Eval num_timesteps=35000, episode_reward=-0.24 +/- 0.84
Episode length: 76.20 +/- 14.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 76.2     |
|    mean_reward      | -0.24    |
| rollout/            |          |
|    exploration_rate | 0.889    |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00654  |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.1     |
|    ep_rew_mean      | 0.39     |
|    exploration_rate | 0.888    |
| time/               |          |
|    episodes         | 428      |
|    fps              | 212      |
|    time_elapsed     | 165      |
|    total_timesteps  | 35281    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00239  |
|    n_updates        | 6320     |
----------------------------------
Eval num_timesteps=35500, episode_reward=-0.34 +/- 0.86
Episode length: 77.84 +/- 12.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.8     |
|    mean_reward      | -0.34    |
| rollout/            |          |
|    exploration_rate | 0.887    |
| time/               |          |
|    total_timesteps  | 35500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00109  |
|    n_updates        | 6374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | 0.39     |
|    exploration_rate | 0.886    |
| time/               |          |
|    episodes         | 432      |
|    fps              | 211      |
|    time_elapsed     | 168      |
|    total_timesteps  | 35609    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 6402     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.6     |
|    ep_rew_mean      | 0.37     |
|    exploration_rate | 0.885    |
| time/               |          |
|    episodes         | 436      |
|    fps              | 213      |
|    time_elapsed     | 168      |
|    total_timesteps  | 35921    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.015    |
|    n_updates        | 6480     |
----------------------------------
Eval num_timesteps=36000, episode_reward=0.72 +/- 1.58
Episode length: 81.78 +/- 22.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.8     |
|    mean_reward      | 0.72     |
| rollout/            |          |
|    exploration_rate | 0.884    |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 6499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 0.35     |
|    exploration_rate | 0.883    |
| time/               |          |
|    episodes         | 440      |
|    fps              | 211      |
|    time_elapsed     | 170      |
|    total_timesteps  | 36202    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.012    |
|    n_updates        | 6550     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 0.35     |
|    exploration_rate | 0.882    |
| time/               |          |
|    episodes         | 444      |
|    fps              | 213      |
|    time_elapsed     | 171      |
|    total_timesteps  | 36487    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.028    |
|    n_updates        | 6621     |
----------------------------------
Eval num_timesteps=36500, episode_reward=0.14 +/- 1.28
Episode length: 78.48 +/- 18.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.5     |
|    mean_reward      | 0.14     |
| rollout/            |          |
|    exploration_rate | 0.882    |
| time/               |          |
|    total_timesteps  | 36500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0238   |
|    n_updates        | 6624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.5     |
|    ep_rew_mean      | 0.36     |
|    exploration_rate | 0.88     |
| time/               |          |
|    episodes         | 448      |
|    fps              | 212      |
|    time_elapsed     | 173      |
|    total_timesteps  | 36862    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0134   |
|    n_updates        | 6715     |
----------------------------------
Eval num_timesteps=37000, episode_reward=-0.20 +/- 1.04
Episode length: 75.10 +/- 15.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.1     |
|    mean_reward      | -0.2     |
| rollout/            |          |
|    exploration_rate | 0.879    |
| time/               |          |
|    total_timesteps  | 37000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0017   |
|    n_updates        | 6749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 0.39     |
|    exploration_rate | 0.879    |
| time/               |          |
|    episodes         | 452      |
|    fps              | 211      |
|    time_elapsed     | 175      |
|    total_timesteps  | 37180    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0025   |
|    n_updates        | 6794     |
----------------------------------
Eval num_timesteps=37500, episode_reward=-0.26 +/- 0.80
Episode length: 79.52 +/- 13.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.5     |
|    mean_reward      | -0.26    |
| rollout/            |          |
|    exploration_rate | 0.877    |
| time/               |          |
|    total_timesteps  | 37500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0163   |
|    n_updates        | 6874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.2     |
|    ep_rew_mean      | 0.41     |
|    exploration_rate | 0.877    |
| time/               |          |
|    episodes         | 456      |
|    fps              | 210      |
|    time_elapsed     | 178      |
|    total_timesteps  | 37508    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0139   |
|    n_updates        | 6876     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 0.41     |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 460      |
|    fps              | 211      |
|    time_elapsed     | 178      |
|    total_timesteps  | 37815    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00428  |
|    n_updates        | 6953     |
----------------------------------
Eval num_timesteps=38000, episode_reward=-0.66 +/- 0.51
Episode length: 75.54 +/- 14.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.5     |
|    mean_reward      | -0.66    |
| rollout/            |          |
|    exploration_rate | 0.875    |
| time/               |          |
|    total_timesteps  | 38000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0233   |
|    n_updates        | 6999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.4     |
|    ep_rew_mean      | 0.43     |
|    exploration_rate | 0.874    |
| time/               |          |
|    episodes         | 464      |
|    fps              | 211      |
|    time_elapsed     | 180      |
|    total_timesteps  | 38170    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0137   |
|    n_updates        | 7042     |
----------------------------------
Eval num_timesteps=38500, episode_reward=0.22 +/- 1.04
Episode length: 80.92 +/- 17.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.9     |
|    mean_reward      | 0.22     |
| rollout/            |          |
|    exploration_rate | 0.872    |
| time/               |          |
|    total_timesteps  | 38500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00122  |
|    n_updates        | 7124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.3     |
|    ep_rew_mean      | 0.45     |
|    exploration_rate | 0.872    |
| time/               |          |
|    episodes         | 468      |
|    fps              | 210      |
|    time_elapsed     | 183      |
|    total_timesteps  | 38508    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000945 |
|    n_updates        | 7126     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.9     |
|    ep_rew_mean      | 0.45     |
|    exploration_rate | 0.87     |
| time/               |          |
|    episodes         | 472      |
|    fps              | 211      |
|    time_elapsed     | 183      |
|    total_timesteps  | 38854    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00231  |
|    n_updates        | 7213     |
----------------------------------
Eval num_timesteps=39000, episode_reward=-0.92 +/- 0.27
Episode length: 70.24 +/- 10.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.2     |
|    mean_reward      | -0.92    |
| rollout/            |          |
|    exploration_rate | 0.87     |
| time/               |          |
|    total_timesteps  | 39000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 7249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 0.4      |
|    exploration_rate | 0.869    |
| time/               |          |
|    episodes         | 476      |
|    fps              | 210      |
|    time_elapsed     | 185      |
|    total_timesteps  | 39160    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0043   |
|    n_updates        | 7289     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 0.43     |
|    exploration_rate | 0.867    |
| time/               |          |
|    episodes         | 480      |
|    fps              | 212      |
|    time_elapsed     | 185      |
|    total_timesteps  | 39441    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00513  |
|    n_updates        | 7360     |
----------------------------------
Eval num_timesteps=39500, episode_reward=-0.60 +/- 0.60
Episode length: 71.60 +/- 13.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.6     |
|    mean_reward      | -0.6     |
| rollout/            |          |
|    exploration_rate | 0.867    |
| time/               |          |
|    total_timesteps  | 39500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 7374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 0.39     |
|    exploration_rate | 0.866    |
| time/               |          |
|    episodes         | 484      |
|    fps              | 211      |
|    time_elapsed     | 188      |
|    total_timesteps  | 39762    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00248  |
|    n_updates        | 7440     |
----------------------------------
Eval num_timesteps=40000, episode_reward=-0.28 +/- 1.10
Episode length: 75.92 +/- 15.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.9     |
|    mean_reward      | -0.28    |
| rollout/            |          |
|    exploration_rate | 0.865    |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00816  |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 0.4      |
|    exploration_rate | 0.864    |
| time/               |          |
|    episodes         | 488      |
|    fps              | 210      |
|    time_elapsed     | 190      |
|    total_timesteps  | 40070    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 7517     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 0.4      |
|    exploration_rate | 0.862    |
| time/               |          |
|    episodes         | 492      |
|    fps              | 211      |
|    time_elapsed     | 190      |
|    total_timesteps  | 40419    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0327   |
|    n_updates        | 7604     |
----------------------------------
Eval num_timesteps=40500, episode_reward=-0.06 +/- 0.97
Episode length: 80.32 +/- 19.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.3     |
|    mean_reward      | -0.06    |
| rollout/            |          |
|    exploration_rate | 0.862    |
| time/               |          |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0196   |
|    n_updates        | 7624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82       |
|    ep_rew_mean      | 0.42     |
|    exploration_rate | 0.861    |
| time/               |          |
|    episodes         | 496      |
|    fps              | 210      |
|    time_elapsed     | 193      |
|    total_timesteps  | 40757    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 7689     |
----------------------------------
Eval num_timesteps=41000, episode_reward=-0.14 +/- 1.06
Episode length: 80.98 +/- 16.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81       |
|    mean_reward      | -0.14    |
| rollout/            |          |
|    exploration_rate | 0.859    |
| time/               |          |
|    total_timesteps  | 41000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0137   |
|    n_updates        | 7749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.4     |
|    ep_rew_mean      | 0.38     |
|    exploration_rate | 0.859    |
| time/               |          |
|    episodes         | 500      |
|    fps              | 209      |
|    time_elapsed     | 195      |
|    total_timesteps  | 41074    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0135   |
|    n_updates        | 7768     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.7     |
|    ep_rew_mean      | 0.44     |
|    exploration_rate | 0.857    |
| time/               |          |
|    episodes         | 504      |
|    fps              | 211      |
|    time_elapsed     | 195      |
|    total_timesteps  | 41427    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00687  |
|    n_updates        | 7856     |
----------------------------------
Eval num_timesteps=41500, episode_reward=-0.88 +/- 0.38
Episode length: 74.48 +/- 11.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 74.5     |
|    mean_reward      | -0.88    |
| rollout/            |          |
|    exploration_rate | 0.857    |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0158   |
|    n_updates        | 7874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 0.45     |
|    exploration_rate | 0.855    |
| time/               |          |
|    episodes         | 508      |
|    fps              | 210      |
|    time_elapsed     | 198      |
|    total_timesteps  | 41756    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0018   |
|    n_updates        | 7938     |
----------------------------------
Eval num_timesteps=42000, episode_reward=2.68 +/- 2.07
Episode length: 104.52 +/- 34.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.68     |
| rollout/            |          |
|    exploration_rate | 0.854    |
| time/               |          |
|    total_timesteps  | 42000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0155   |
|    n_updates        | 7999     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.6     |
|    ep_rew_mean      | 0.47     |
|    exploration_rate | 0.854    |
| time/               |          |
|    episodes         | 512      |
|    fps              | 208      |
|    time_elapsed     | 201      |
|    total_timesteps  | 42093    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00993  |
|    n_updates        | 8023     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.5     |
|    ep_rew_mean      | 0.42     |
|    exploration_rate | 0.852    |
| time/               |          |
|    episodes         | 516      |
|    fps              | 210      |
|    time_elapsed     | 201      |
|    total_timesteps  | 42386    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00113  |
|    n_updates        | 8096     |
----------------------------------
Eval num_timesteps=42500, episode_reward=-0.92 +/- 0.34
Episode length: 70.84 +/- 11.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 70.8     |
|    mean_reward      | -0.92    |
| rollout/            |          |
|    exploration_rate | 0.852    |
| time/               |          |
|    total_timesteps  | 42500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000817 |
|    n_updates        | 8124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 0.42     |
|    exploration_rate | 0.851    |
| time/               |          |
|    episodes         | 520      |
|    fps              | 209      |
|    time_elapsed     | 203      |
|    total_timesteps  | 42659    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00133  |
|    n_updates        | 8164     |
----------------------------------
Eval num_timesteps=43000, episode_reward=2.10 +/- 1.72
Episode length: 98.36 +/- 27.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.4     |
|    mean_reward      | 2.1      |
| rollout/            |          |
|    exploration_rate | 0.849    |
| time/               |          |
|    total_timesteps  | 43000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00159  |
|    n_updates        | 8249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 80.7     |
|    ep_rew_mean      | 0.43     |
|    exploration_rate | 0.849    |
| time/               |          |
|    episodes         | 524      |
|    fps              | 208      |
|    time_elapsed     | 206      |
|    total_timesteps  | 43032    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000529 |
|    n_updates        | 8257     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.1     |
|    ep_rew_mean      | 0.42     |
|    exploration_rate | 0.847    |
| time/               |          |
|    episodes         | 528      |
|    fps              | 209      |
|    time_elapsed     | 207      |
|    total_timesteps  | 43391    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00348  |
|    n_updates        | 8347     |
----------------------------------
Eval num_timesteps=43500, episode_reward=0.12 +/- 1.61
Episode length: 82.28 +/- 28.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.3     |
|    mean_reward      | 0.12     |
| rollout/            |          |
|    exploration_rate | 0.846    |
| time/               |          |
|    total_timesteps  | 43500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00204  |
|    n_updates        | 8374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.3     |
|    ep_rew_mean      | 0.44     |
|    exploration_rate | 0.845    |
| time/               |          |
|    episodes         | 532      |
|    fps              | 208      |
|    time_elapsed     | 209      |
|    total_timesteps  | 43742    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00141  |
|    n_updates        | 8435     |
----------------------------------
Eval num_timesteps=44000, episode_reward=3.70 +/- 2.76
Episode length: 126.48 +/- 47.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 3.7      |
| rollout/            |          |
|    exploration_rate | 0.844    |
| time/               |          |
|    total_timesteps  | 44000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0127   |
|    n_updates        | 8499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 81.9     |
|    ep_rew_mean      | 0.48     |
|    exploration_rate | 0.843    |
| time/               |          |
|    episodes         | 536      |
|    fps              | 206      |
|    time_elapsed     | 213      |
|    total_timesteps  | 44108    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00958  |
|    n_updates        | 8526     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.7     |
|    ep_rew_mean      | 0.54     |
|    exploration_rate | 0.841    |
| time/               |          |
|    episodes         | 540      |
|    fps              | 208      |
|    time_elapsed     | 213      |
|    total_timesteps  | 44469    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00156  |
|    n_updates        | 8617     |
----------------------------------
Eval num_timesteps=44500, episode_reward=1.82 +/- 1.34
Episode length: 96.60 +/- 21.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.6     |
|    mean_reward      | 1.82     |
| rollout/            |          |
|    exploration_rate | 0.841    |
| time/               |          |
|    total_timesteps  | 44500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00236  |
|    n_updates        | 8624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.1     |
|    ep_rew_mean      | 0.54     |
|    exploration_rate | 0.839    |
| time/               |          |
|    episodes         | 544      |
|    fps              | 206      |
|    time_elapsed     | 216      |
|    total_timesteps  | 44801    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 8700     |
----------------------------------
Eval num_timesteps=45000, episode_reward=0.06 +/- 0.90
Episode length: 73.70 +/- 12.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 73.7     |
|    mean_reward      | 0.06     |
| rollout/            |          |
|    exploration_rate | 0.838    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0193   |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 82.7     |
|    ep_rew_mean      | 0.5      |
|    exploration_rate | 0.838    |
| time/               |          |
|    episodes         | 548      |
|    fps              | 206      |
|    time_elapsed     | 218      |
|    total_timesteps  | 45128    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0393   |
|    n_updates        | 8781     |
----------------------------------
Eval num_timesteps=45500, episode_reward=-0.70 +/- 0.57
Episode length: 72.22 +/- 12.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 72.2     |
|    mean_reward      | -0.7     |
| rollout/            |          |
|    exploration_rate | 0.836    |
| time/               |          |
|    total_timesteps  | 45500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00352  |
|    n_updates        | 8874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.3     |
|    ep_rew_mean      | 0.52     |
|    exploration_rate | 0.836    |
| time/               |          |
|    episodes         | 552      |
|    fps              | 205      |
|    time_elapsed     | 221      |
|    total_timesteps  | 45508    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0272   |
|    n_updates        | 8876     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.1     |
|    ep_rew_mean      | 0.51     |
|    exploration_rate | 0.834    |
| time/               |          |
|    episodes         | 556      |
|    fps              | 206      |
|    time_elapsed     | 221      |
|    total_timesteps  | 45820    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00342  |
|    n_updates        | 8954     |
----------------------------------
Eval num_timesteps=46000, episode_reward=1.10 +/- 1.25
Episode length: 85.68 +/- 21.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.7     |
|    mean_reward      | 1.1      |
| rollout/            |          |
|    exploration_rate | 0.833    |
| time/               |          |
|    total_timesteps  | 46000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000625 |
|    n_updates        | 8999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.4     |
|    ep_rew_mean      | 0.53     |
|    exploration_rate | 0.832    |
| time/               |          |
|    episodes         | 560      |
|    fps              | 206      |
|    time_elapsed     | 224      |
|    total_timesteps  | 46156    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00682  |
|    n_updates        | 9038     |
----------------------------------
Eval num_timesteps=46500, episode_reward=0.62 +/- 0.94
Episode length: 77.68 +/- 14.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.7     |
|    mean_reward      | 0.62     |
| rollout/            |          |
|    exploration_rate | 0.83     |
| time/               |          |
|    total_timesteps  | 46500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00308  |
|    n_updates        | 9124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.6     |
|    ep_rew_mean      | 0.55     |
|    exploration_rate | 0.83     |
| time/               |          |
|    episodes         | 564      |
|    fps              | 205      |
|    time_elapsed     | 226      |
|    total_timesteps  | 46526    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 9131     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.8     |
|    ep_rew_mean      | 0.56     |
|    exploration_rate | 0.828    |
| time/               |          |
|    episodes         | 568      |
|    fps              | 206      |
|    time_elapsed     | 226      |
|    total_timesteps  | 46888    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00289  |
|    n_updates        | 9221     |
----------------------------------
Eval num_timesteps=47000, episode_reward=1.58 +/- 1.61
Episode length: 89.88 +/- 26.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.9     |
|    mean_reward      | 1.58     |
| rollout/            |          |
|    exploration_rate | 0.827    |
| time/               |          |
|    total_timesteps  | 47000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00172  |
|    n_updates        | 9249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 83.8     |
|    ep_rew_mean      | 0.57     |
|    exploration_rate | 0.826    |
| time/               |          |
|    episodes         | 572      |
|    fps              | 205      |
|    time_elapsed     | 229      |
|    total_timesteps  | 47239    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0131   |
|    n_updates        | 9309     |
----------------------------------
Eval num_timesteps=47500, episode_reward=0.88 +/- 1.03
Episode length: 78.50 +/- 19.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78.5     |
|    mean_reward      | 0.88     |
| rollout/            |          |
|    exploration_rate | 0.825    |
| time/               |          |
|    total_timesteps  | 47500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00196  |
|    n_updates        | 9374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 84.7     |
|    ep_rew_mean      | 0.58     |
|    exploration_rate | 0.824    |
| time/               |          |
|    episodes         | 576      |
|    fps              | 205      |
|    time_elapsed     | 231      |
|    total_timesteps  | 47626    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0221   |
|    n_updates        | 9406     |
----------------------------------
Eval num_timesteps=48000, episode_reward=1.90 +/- 1.47
Episode length: 94.06 +/- 23.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.1     |
|    mean_reward      | 1.9      |
| rollout/            |          |
|    exploration_rate | 0.822    |
| time/               |          |
|    total_timesteps  | 48000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0186   |
|    n_updates        | 9499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.8     |
|    ep_rew_mean      | 0.61     |
|    exploration_rate | 0.822    |
| time/               |          |
|    episodes         | 580      |
|    fps              | 204      |
|    time_elapsed     | 234      |
|    total_timesteps  | 48017    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00847  |
|    n_updates        | 9504     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.2     |
|    ep_rew_mean      | 0.66     |
|    exploration_rate | 0.82     |
| time/               |          |
|    episodes         | 584      |
|    fps              | 205      |
|    time_elapsed     | 235      |
|    total_timesteps  | 48385    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0277   |
|    n_updates        | 9596     |
----------------------------------
Eval num_timesteps=48500, episode_reward=2.82 +/- 1.35
Episode length: 107.98 +/- 26.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 2.82     |
| rollout/            |          |
|    exploration_rate | 0.819    |
| time/               |          |
|    total_timesteps  | 48500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0167   |
|    n_updates        | 9624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 0.66     |
|    exploration_rate | 0.818    |
| time/               |          |
|    episodes         | 588      |
|    fps              | 204      |
|    time_elapsed     | 238      |
|    total_timesteps  | 48745    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00242  |
|    n_updates        | 9686     |
----------------------------------
Eval num_timesteps=49000, episode_reward=1.98 +/- 1.95
Episode length: 89.96 +/- 33.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90       |
|    mean_reward      | 1.98     |
| rollout/            |          |
|    exploration_rate | 0.816    |
| time/               |          |
|    total_timesteps  | 49000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 9749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.3     |
|    ep_rew_mean      | 0.61     |
|    exploration_rate | 0.816    |
| time/               |          |
|    episodes         | 592      |
|    fps              | 203      |
|    time_elapsed     | 241      |
|    total_timesteps  | 49050    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0199   |
|    n_updates        | 9762     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.5     |
|    ep_rew_mean      | 0.64     |
|    exploration_rate | 0.814    |
| time/               |          |
|    episodes         | 596      |
|    fps              | 204      |
|    time_elapsed     | 241      |
|    total_timesteps  | 49402    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0143   |
|    n_updates        | 9850     |
----------------------------------
Eval num_timesteps=49500, episode_reward=1.46 +/- 1.28
Episode length: 87.52 +/- 21.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.5     |
|    mean_reward      | 1.46     |
| rollout/            |          |
|    exploration_rate | 0.813    |
| time/               |          |
|    total_timesteps  | 49500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00693  |
|    n_updates        | 9874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 0.66     |
|    exploration_rate | 0.812    |
| time/               |          |
|    episodes         | 600      |
|    fps              | 203      |
|    time_elapsed     | 244      |
|    total_timesteps  | 49759    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000901 |
|    n_updates        | 9939     |
----------------------------------
Eval num_timesteps=50000, episode_reward=1.34 +/- 1.45
Episode length: 85.88 +/- 25.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.9     |
|    mean_reward      | 1.34     |
| rollout/            |          |
|    exploration_rate | 0.811    |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0236   |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.7     |
|    ep_rew_mean      | 0.64     |
|    exploration_rate | 0.81     |
| time/               |          |
|    episodes         | 604      |
|    fps              | 202      |
|    time_elapsed     | 246      |
|    total_timesteps  | 50094    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0118   |
|    n_updates        | 10023    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.7     |
|    ep_rew_mean      | 0.61     |
|    exploration_rate | 0.808    |
| time/               |          |
|    episodes         | 608      |
|    fps              | 204      |
|    time_elapsed     | 247      |
|    total_timesteps  | 50430    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00735  |
|    n_updates        | 10107    |
----------------------------------
Eval num_timesteps=50500, episode_reward=1.80 +/- 1.55
Episode length: 92.56 +/- 21.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.6     |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 0.808    |
| time/               |          |
|    total_timesteps  | 50500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00188  |
|    n_updates        | 10124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.1     |
|    ep_rew_mean      | 0.63     |
|    exploration_rate | 0.806    |
| time/               |          |
|    episodes         | 612      |
|    fps              | 203      |
|    time_elapsed     | 249      |
|    total_timesteps  | 50804    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0245   |
|    n_updates        | 10200    |
----------------------------------
Eval num_timesteps=51000, episode_reward=-0.90 +/- 0.36
Episode length: 71.70 +/- 10.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.7     |
|    mean_reward      | -0.9     |
| rollout/            |          |
|    exploration_rate | 0.805    |
| time/               |          |
|    total_timesteps  | 51000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00732  |
|    n_updates        | 10249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.6     |
|    ep_rew_mean      | 0.66     |
|    exploration_rate | 0.804    |
| time/               |          |
|    episodes         | 616      |
|    fps              | 202      |
|    time_elapsed     | 252      |
|    total_timesteps  | 51145    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00258  |
|    n_updates        | 10286    |
----------------------------------
Eval num_timesteps=51500, episode_reward=2.82 +/- 2.71
Episode length: 105.18 +/- 42.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 2.82     |
| rollout/            |          |
|    exploration_rate | 0.802    |
| time/               |          |
|    total_timesteps  | 51500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000778 |
|    n_updates        | 10374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.5     |
|    ep_rew_mean      | 0.68     |
|    exploration_rate | 0.802    |
| time/               |          |
|    episodes         | 620      |
|    fps              | 201      |
|    time_elapsed     | 255      |
|    total_timesteps  | 51509    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0245   |
|    n_updates        | 10377    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.7     |
|    ep_rew_mean      | 0.67     |
|    exploration_rate | 0.8      |
| time/               |          |
|    episodes         | 624      |
|    fps              | 203      |
|    time_elapsed     | 255      |
|    total_timesteps  | 51906    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0111   |
|    n_updates        | 10476    |
----------------------------------
Eval num_timesteps=52000, episode_reward=1.52 +/- 1.25
Episode length: 82.92 +/- 19.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.9     |
|    mean_reward      | 1.52     |
| rollout/            |          |
|    exploration_rate | 0.799    |
| time/               |          |
|    total_timesteps  | 52000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00849  |
|    n_updates        | 10499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.3     |
|    ep_rew_mean      | 0.67     |
|    exploration_rate | 0.798    |
| time/               |          |
|    episodes         | 628      |
|    fps              | 202      |
|    time_elapsed     | 258      |
|    total_timesteps  | 52224    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00655  |
|    n_updates        | 10555    |
----------------------------------
Eval num_timesteps=52500, episode_reward=1.32 +/- 1.22
Episode length: 79.18 +/- 19.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.2     |
|    mean_reward      | 1.32     |
| rollout/            |          |
|    exploration_rate | 0.796    |
| time/               |          |
|    total_timesteps  | 52500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00826  |
|    n_updates        | 10624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.6     |
|    ep_rew_mean      | 0.7      |
|    exploration_rate | 0.796    |
| time/               |          |
|    episodes         | 632      |
|    fps              | 201      |
|    time_elapsed     | 260      |
|    total_timesteps  | 52601    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00183  |
|    n_updates        | 10650    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88       |
|    ep_rew_mean      | 0.71     |
|    exploration_rate | 0.794    |
| time/               |          |
|    episodes         | 636      |
|    fps              | 202      |
|    time_elapsed     | 260      |
|    total_timesteps  | 52908    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0179   |
|    n_updates        | 10726    |
----------------------------------
Eval num_timesteps=53000, episode_reward=2.32 +/- 2.01
Episode length: 95.12 +/- 31.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.1     |
|    mean_reward      | 2.32     |
| rollout/            |          |
|    exploration_rate | 0.793    |
| time/               |          |
|    total_timesteps  | 53000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00679  |
|    n_updates        | 10749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.9     |
|    ep_rew_mean      | 0.68     |
|    exploration_rate | 0.792    |
| time/               |          |
|    episodes         | 640      |
|    fps              | 201      |
|    time_elapsed     | 263      |
|    total_timesteps  | 53255    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0221   |
|    n_updates        | 10813    |
----------------------------------
Eval num_timesteps=53500, episode_reward=2.34 +/- 1.94
Episode length: 93.42 +/- 32.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.4     |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.79     |
| time/               |          |
|    total_timesteps  | 53500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0139   |
|    n_updates        | 10874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.7     |
|    ep_rew_mean      | 0.68     |
|    exploration_rate | 0.79     |
| time/               |          |
|    episodes         | 644      |
|    fps              | 200      |
|    time_elapsed     | 266      |
|    total_timesteps  | 53571    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 10892    |
----------------------------------
Eval num_timesteps=54000, episode_reward=3.92 +/- 2.73
Episode length: 120.02 +/- 47.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 3.92     |
| rollout/            |          |
|    exploration_rate | 0.787    |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00365  |
|    n_updates        | 10999    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.5     |
|    ep_rew_mean      | 0.84     |
|    exploration_rate | 0.787    |
| time/               |          |
|    episodes         | 648      |
|    fps              | 199      |
|    time_elapsed     | 270      |
|    total_timesteps  | 54075    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00892  |
|    n_updates        | 11018    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.5     |
|    ep_rew_mean      | 0.81     |
|    exploration_rate | 0.785    |
| time/               |          |
|    episodes         | 652      |
|    fps              | 200      |
|    time_elapsed     | 270      |
|    total_timesteps  | 54354    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0256   |
|    n_updates        | 11088    |
----------------------------------
Eval num_timesteps=54500, episode_reward=2.56 +/- 1.87
Episode length: 97.56 +/- 28.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.6     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.785    |
| time/               |          |
|    total_timesteps  | 54500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0289   |
|    n_updates        | 11124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.4     |
|    ep_rew_mean      | 0.85     |
|    exploration_rate | 0.784    |
| time/               |          |
|    episodes         | 656      |
|    fps              | 199      |
|    time_elapsed     | 273      |
|    total_timesteps  | 54661    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0268   |
|    n_updates        | 11165    |
----------------------------------
Eval num_timesteps=55000, episode_reward=1.80 +/- 1.77
Episode length: 86.28 +/- 22.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.3     |
|    mean_reward      | 1.8      |
| rollout/            |          |
|    exploration_rate | 0.782    |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00535  |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.7     |
|    ep_rew_mean      | 0.85     |
|    exploration_rate | 0.781    |
| time/               |          |
|    episodes         | 660      |
|    fps              | 199      |
|    time_elapsed     | 276      |
|    total_timesteps  | 55023    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00529  |
|    n_updates        | 11255    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.6     |
|    ep_rew_mean      | 0.85     |
|    exploration_rate | 0.779    |
| time/               |          |
|    episodes         | 664      |
|    fps              | 200      |
|    time_elapsed     | 276      |
|    total_timesteps  | 55385    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000815 |
|    n_updates        | 11346    |
----------------------------------
Eval num_timesteps=55500, episode_reward=3.50 +/- 1.99
Episode length: 108.28 +/- 30.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.5      |
| rollout/            |          |
|    exploration_rate | 0.779    |
| time/               |          |
|    total_timesteps  | 55500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 11374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.7     |
|    ep_rew_mean      | 0.87     |
|    exploration_rate | 0.777    |
| time/               |          |
|    episodes         | 668      |
|    fps              | 199      |
|    time_elapsed     | 279      |
|    total_timesteps  | 55761    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0379   |
|    n_updates        | 11440    |
----------------------------------
Eval num_timesteps=56000, episode_reward=2.22 +/- 2.32
Episode length: 94.44 +/- 34.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.4     |
|    mean_reward      | 2.22     |
| rollout/            |          |
|    exploration_rate | 0.776    |
| time/               |          |
|    total_timesteps  | 56000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00463  |
|    n_updates        | 11499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.9     |
|    ep_rew_mean      | 0.93     |
|    exploration_rate | 0.775    |
| time/               |          |
|    episodes         | 672      |
|    fps              | 198      |
|    time_elapsed     | 282      |
|    total_timesteps  | 56133    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00666  |
|    n_updates        | 11533    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.4     |
|    ep_rew_mean      | 0.97     |
|    exploration_rate | 0.773    |
| time/               |          |
|    episodes         | 676      |
|    fps              | 199      |
|    time_elapsed     | 282      |
|    total_timesteps  | 56468    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0285   |
|    n_updates        | 11616    |
----------------------------------
Eval num_timesteps=56500, episode_reward=0.66 +/- 0.74
Episode length: 75.34 +/- 16.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 75.3     |
|    mean_reward      | 0.66     |
| rollout/            |          |
|    exploration_rate | 0.773    |
| time/               |          |
|    total_timesteps  | 56500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0142   |
|    n_updates        | 11624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.5     |
|    ep_rew_mean      | 1        |
|    exploration_rate | 0.77     |
| time/               |          |
|    episodes         | 680      |
|    fps              | 199      |
|    time_elapsed     | 285      |
|    total_timesteps  | 56865    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0016   |
|    n_updates        | 11716    |
----------------------------------
Eval num_timesteps=57000, episode_reward=-0.96 +/- 0.20
Episode length: 69.16 +/- 8.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 69.2     |
|    mean_reward      | -0.96    |
| rollout/            |          |
|    exploration_rate | 0.77     |
| time/               |          |
|    total_timesteps  | 57000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 11749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.7     |
|    ep_rew_mean      | 1.01     |
|    exploration_rate | 0.768    |
| time/               |          |
|    episodes         | 684      |
|    fps              | 199      |
|    time_elapsed     | 287      |
|    total_timesteps  | 57253    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00218  |
|    n_updates        | 11813    |
----------------------------------
Eval num_timesteps=57500, episode_reward=1.38 +/- 1.61
Episode length: 84.84 +/- 24.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.8     |
|    mean_reward      | 1.38     |
| rollout/            |          |
|    exploration_rate | 0.766    |
| time/               |          |
|    total_timesteps  | 57500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00427  |
|    n_updates        | 11874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.8     |
|    ep_rew_mean      | 1.05     |
|    exploration_rate | 0.766    |
| time/               |          |
|    episodes         | 688      |
|    fps              | 198      |
|    time_elapsed     | 290      |
|    total_timesteps  | 57627    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00255  |
|    n_updates        | 11906    |
----------------------------------
Eval num_timesteps=58000, episode_reward=4.02 +/- 2.19
Episode length: 115.88 +/- 32.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 4.02     |
| rollout/            |          |
|    exploration_rate | 0.763    |
| time/               |          |
|    total_timesteps  | 58000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00301  |
|    n_updates        | 11999    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.7     |
|    ep_rew_mean      | 1.13     |
|    exploration_rate | 0.763    |
| time/               |          |
|    episodes         | 692      |
|    fps              | 197      |
|    time_elapsed     | 293      |
|    total_timesteps  | 58015    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0177   |
|    n_updates        | 12003    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.5     |
|    ep_rew_mean      | 1.21     |
|    exploration_rate | 0.761    |
| time/               |          |
|    episodes         | 696      |
|    fps              | 198      |
|    time_elapsed     | 294      |
|    total_timesteps  | 58450    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00278  |
|    n_updates        | 12112    |
----------------------------------
Eval num_timesteps=58500, episode_reward=1.22 +/- 1.43
Episode length: 81.50 +/- 20.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.5     |
|    mean_reward      | 1.22     |
| rollout/            |          |
|    exploration_rate | 0.76     |
| time/               |          |
|    total_timesteps  | 58500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.01     |
|    n_updates        | 12124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.6     |
|    ep_rew_mean      | 1.26     |
|    exploration_rate | 0.758    |
| time/               |          |
|    episodes         | 700      |
|    fps              | 198      |
|    time_elapsed     | 296      |
|    total_timesteps  | 58822    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0161   |
|    n_updates        | 12205    |
----------------------------------
Eval num_timesteps=59000, episode_reward=8.78 +/- 5.80
Episode length: 188.06 +/- 94.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 188      |
|    mean_reward      | 8.78     |
| rollout/            |          |
|    exploration_rate | 0.757    |
| time/               |          |
|    total_timesteps  | 59000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0169   |
|    n_updates        | 12249    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.5     |
|    ep_rew_mean      | 1.25     |
|    exploration_rate | 0.756    |
| time/               |          |
|    episodes         | 704      |
|    fps              | 195      |
|    time_elapsed     | 302      |
|    total_timesteps  | 59140    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00152  |
|    n_updates        | 12284    |
----------------------------------
Eval num_timesteps=59500, episode_reward=1.78 +/- 1.74
Episode length: 87.88 +/- 21.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.9     |
|    mean_reward      | 1.78     |
| rollout/            |          |
|    exploration_rate | 0.754    |
| time/               |          |
|    total_timesteps  | 59500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00946  |
|    n_updates        | 12374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.8     |
|    ep_rew_mean      | 1.29     |
|    exploration_rate | 0.754    |
| time/               |          |
|    episodes         | 708      |
|    fps              | 195      |
|    time_elapsed     | 304      |
|    total_timesteps  | 59506    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00337  |
|    n_updates        | 12376    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91       |
|    ep_rew_mean      | 1.36     |
|    exploration_rate | 0.752    |
| time/               |          |
|    episodes         | 712      |
|    fps              | 196      |
|    time_elapsed     | 305      |
|    total_timesteps  | 59899    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0117   |
|    n_updates        | 12474    |
----------------------------------
Eval num_timesteps=60000, episode_reward=0.46 +/- 0.67
Episode length: 78.00 +/- 12.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78       |
|    mean_reward      | 0.46     |
| rollout/            |          |
|    exploration_rate | 0.751    |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 12499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 1.41     |
|    exploration_rate | 0.749    |
| time/               |          |
|    episodes         | 716      |
|    fps              | 196      |
|    time_elapsed     | 307      |
|    total_timesteps  | 60297    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0274   |
|    n_updates        | 12574    |
----------------------------------
Eval num_timesteps=60500, episode_reward=1.20 +/- 1.28
Episode length: 82.88 +/- 20.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.9     |
|    mean_reward      | 1.2      |
| rollout/            |          |
|    exploration_rate | 0.748    |
| time/               |          |
|    total_timesteps  | 60500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0115   |
|    n_updates        | 12624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.1     |
|    ep_rew_mean      | 1.47     |
|    exploration_rate | 0.747    |
| time/               |          |
|    episodes         | 720      |
|    fps              | 195      |
|    time_elapsed     | 310      |
|    total_timesteps  | 60717    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00918  |
|    n_updates        | 12679    |
----------------------------------
Eval num_timesteps=61000, episode_reward=0.94 +/- 1.08
Episode length: 80.14 +/- 15.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.1     |
|    mean_reward      | 0.94     |
| rollout/            |          |
|    exploration_rate | 0.745    |
| time/               |          |
|    total_timesteps  | 61000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00676  |
|    n_updates        | 12749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.5     |
|    ep_rew_mean      | 1.46     |
|    exploration_rate | 0.744    |
| time/               |          |
|    episodes         | 724      |
|    fps              | 195      |
|    time_elapsed     | 312      |
|    total_timesteps  | 61060    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00939  |
|    n_updates        | 12764    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.2     |
|    ep_rew_mean      | 1.5      |
|    exploration_rate | 0.742    |
| time/               |          |
|    episodes         | 728      |
|    fps              | 196      |
|    time_elapsed     | 312      |
|    total_timesteps  | 61447    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00302  |
|    n_updates        | 12861    |
----------------------------------
Eval num_timesteps=61500, episode_reward=5.98 +/- 4.42
Episode length: 149.90 +/- 72.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 5.98     |
| rollout/            |          |
|    exploration_rate | 0.742    |
| time/               |          |
|    total_timesteps  | 61500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0096   |
|    n_updates        | 12874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.4     |
|    ep_rew_mean      | 1.48     |
|    exploration_rate | 0.74     |
| time/               |          |
|    episodes         | 732      |
|    fps              | 194      |
|    time_elapsed     | 317      |
|    total_timesteps  | 61840    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00369  |
|    n_updates        | 12959    |
----------------------------------
Eval num_timesteps=62000, episode_reward=2.54 +/- 1.96
Episode length: 96.02 +/- 29.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96       |
|    mean_reward      | 2.54     |
| rollout/            |          |
|    exploration_rate | 0.739    |
| time/               |          |
|    total_timesteps  | 62000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0107   |
|    n_updates        | 12999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.5     |
|    ep_rew_mean      | 1.45     |
|    exploration_rate | 0.738    |
| time/               |          |
|    episodes         | 736      |
|    fps              | 194      |
|    time_elapsed     | 320      |
|    total_timesteps  | 62158    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00405  |
|    n_updates        | 13039    |
----------------------------------
Eval num_timesteps=62500, episode_reward=3.60 +/- 1.79
Episode length: 109.94 +/- 32.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.6      |
| rollout/            |          |
|    exploration_rate | 0.735    |
| time/               |          |
|    total_timesteps  | 62500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00465  |
|    n_updates        | 13124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.6     |
|    ep_rew_mean      | 1.45     |
|    exploration_rate | 0.735    |
| time/               |          |
|    episodes         | 740      |
|    fps              | 193      |
|    time_elapsed     | 323      |
|    total_timesteps  | 62518    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00558  |
|    n_updates        | 13129    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.6     |
|    ep_rew_mean      | 1.47     |
|    exploration_rate | 0.733    |
| time/               |          |
|    episodes         | 744      |
|    fps              | 194      |
|    time_elapsed     | 323      |
|    total_timesteps  | 62835    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 13208    |
----------------------------------
Eval num_timesteps=63000, episode_reward=0.88 +/- 1.19
Episode length: 80.06 +/- 16.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 80.1     |
|    mean_reward      | 0.88     |
| rollout/            |          |
|    exploration_rate | 0.732    |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00243  |
|    n_updates        | 13249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.9     |
|    ep_rew_mean      | 1.37     |
|    exploration_rate | 0.731    |
| time/               |          |
|    episodes         | 748      |
|    fps              | 193      |
|    time_elapsed     | 326      |
|    total_timesteps  | 63165    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 13291    |
----------------------------------
Eval num_timesteps=63500, episode_reward=3.22 +/- 2.51
Episode length: 107.40 +/- 40.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.22     |
| rollout/            |          |
|    exploration_rate | 0.729    |
| time/               |          |
|    total_timesteps  | 63500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000933 |
|    n_updates        | 13374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.7     |
|    ep_rew_mean      | 1.4      |
|    exploration_rate | 0.729    |
| time/               |          |
|    episodes         | 752      |
|    fps              | 192      |
|    time_elapsed     | 329      |
|    total_timesteps  | 63525    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00356  |
|    n_updates        | 13381    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.2     |
|    ep_rew_mean      | 1.4      |
|    exploration_rate | 0.727    |
| time/               |          |
|    episodes         | 756      |
|    fps              | 193      |
|    time_elapsed     | 329      |
|    total_timesteps  | 63880    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00488  |
|    n_updates        | 13469    |
----------------------------------
Eval num_timesteps=64000, episode_reward=2.48 +/- 1.77
Episode length: 92.58 +/- 27.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.6     |
|    mean_reward      | 2.48     |
| rollout/            |          |
|    exploration_rate | 0.726    |
| time/               |          |
|    total_timesteps  | 64000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0169   |
|    n_updates        | 13499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.5     |
|    ep_rew_mean      | 1.46     |
|    exploration_rate | 0.724    |
| time/               |          |
|    episodes         | 760      |
|    fps              | 193      |
|    time_elapsed     | 332      |
|    total_timesteps  | 64271    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00194  |
|    n_updates        | 13567    |
----------------------------------
Eval num_timesteps=64500, episode_reward=1.24 +/- 1.41
Episode length: 79.82 +/- 19.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.8     |
|    mean_reward      | 1.24     |
| rollout/            |          |
|    exploration_rate | 0.723    |
| time/               |          |
|    total_timesteps  | 64500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 13624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.5     |
|    ep_rew_mean      | 1.48     |
|    exploration_rate | 0.722    |
| time/               |          |
|    episodes         | 764      |
|    fps              | 192      |
|    time_elapsed     | 335      |
|    total_timesteps  | 64632    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0111   |
|    n_updates        | 13657    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.4     |
|    ep_rew_mean      | 1.43     |
|    exploration_rate | 0.72     |
| time/               |          |
|    episodes         | 768      |
|    fps              | 193      |
|    time_elapsed     | 335      |
|    total_timesteps  | 64901    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 13725    |
----------------------------------
Eval num_timesteps=65000, episode_reward=1.86 +/- 1.59
Episode length: 87.02 +/- 23.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87       |
|    mean_reward      | 1.86     |
| rollout/            |          |
|    exploration_rate | 0.719    |
| time/               |          |
|    total_timesteps  | 65000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0153   |
|    n_updates        | 13749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.2     |
|    ep_rew_mean      | 1.4      |
|    exploration_rate | 0.718    |
| time/               |          |
|    episodes         | 772      |
|    fps              | 193      |
|    time_elapsed     | 337      |
|    total_timesteps  | 65256    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00151  |
|    n_updates        | 13813    |
----------------------------------
Eval num_timesteps=65500, episode_reward=2.94 +/- 1.46
Episode length: 106.46 +/- 22.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 2.94     |
| rollout/            |          |
|    exploration_rate | 0.716    |
| time/               |          |
|    total_timesteps  | 65500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00289  |
|    n_updates        | 13874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.3     |
|    ep_rew_mean      | 1.39     |
|    exploration_rate | 0.715    |
| time/               |          |
|    episodes         | 776      |
|    fps              | 192      |
|    time_elapsed     | 341      |
|    total_timesteps  | 65597    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00934  |
|    n_updates        | 13899    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 90.6     |
|    ep_rew_mean      | 1.35     |
|    exploration_rate | 0.713    |
| time/               |          |
|    episodes         | 780      |
|    fps              | 193      |
|    time_elapsed     | 341      |
|    total_timesteps  | 65928    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00698  |
|    n_updates        | 13981    |
----------------------------------
Eval num_timesteps=66000, episode_reward=3.40 +/- 2.24
Episode length: 109.68 +/- 37.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.713    |
| time/               |          |
|    total_timesteps  | 66000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00387  |
|    n_updates        | 13999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.7     |
|    ep_rew_mean      | 1.32     |
|    exploration_rate | 0.711    |
| time/               |          |
|    episodes         | 784      |
|    fps              | 192      |
|    time_elapsed     | 344      |
|    total_timesteps  | 66223    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 14055    |
----------------------------------
Eval num_timesteps=66500, episode_reward=0.06 +/- 1.12
Episode length: 77.96 +/- 16.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78       |
|    mean_reward      | 0.06     |
| rollout/            |          |
|    exploration_rate | 0.71     |
| time/               |          |
|    total_timesteps  | 66500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0185   |
|    n_updates        | 14124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.5     |
|    ep_rew_mean      | 1.31     |
|    exploration_rate | 0.709    |
| time/               |          |
|    episodes         | 788      |
|    fps              | 191      |
|    time_elapsed     | 347      |
|    total_timesteps  | 66578    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.000744 |
|    n_updates        | 14144    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.1     |
|    ep_rew_mean      | 1.28     |
|    exploration_rate | 0.707    |
| time/               |          |
|    episodes         | 792      |
|    fps              | 192      |
|    time_elapsed     | 347      |
|    total_timesteps  | 66922    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00799  |
|    n_updates        | 14230    |
----------------------------------
Eval num_timesteps=67000, episode_reward=2.30 +/- 2.12
Episode length: 94.74 +/- 28.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.7     |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.706    |
| time/               |          |
|    total_timesteps  | 67000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00643  |
|    n_updates        | 14249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88       |
|    ep_rew_mean      | 1.21     |
|    exploration_rate | 0.705    |
| time/               |          |
|    episodes         | 796      |
|    fps              | 192      |
|    time_elapsed     | 350      |
|    total_timesteps  | 67252    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 14312    |
----------------------------------
Eval num_timesteps=67500, episode_reward=3.78 +/- 2.16
Episode length: 117.10 +/- 34.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.78     |
| rollout/            |          |
|    exploration_rate | 0.703    |
| time/               |          |
|    total_timesteps  | 67500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 14374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.1     |
|    ep_rew_mean      | 1.22     |
|    exploration_rate | 0.702    |
| time/               |          |
|    episodes         | 800      |
|    fps              | 191      |
|    time_elapsed     | 353      |
|    total_timesteps  | 67634    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00747  |
|    n_updates        | 14408    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.5     |
|    ep_rew_mean      | 1.27     |
|    exploration_rate | 0.7      |
| time/               |          |
|    episodes         | 804      |
|    fps              | 192      |
|    time_elapsed     | 354      |
|    total_timesteps  | 67988    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00303  |
|    n_updates        | 14496    |
----------------------------------
Eval num_timesteps=68000, episode_reward=4.04 +/- 2.21
Episode length: 119.82 +/- 36.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.04     |
| rollout/            |          |
|    exploration_rate | 0.7      |
| time/               |          |
|    total_timesteps  | 68000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0174   |
|    n_updates        | 14499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.2     |
|    ep_rew_mean      | 1.35     |
|    exploration_rate | 0.697    |
| time/               |          |
|    episodes         | 808      |
|    fps              | 191      |
|    time_elapsed     | 357      |
|    total_timesteps  | 68424    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0237   |
|    n_updates        | 14605    |
----------------------------------
Eval num_timesteps=68500, episode_reward=0.54 +/- 1.37
Episode length: 83.98 +/- 24.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84       |
|    mean_reward      | 0.54     |
| rollout/            |          |
|    exploration_rate | 0.696    |
| time/               |          |
|    total_timesteps  | 68500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00371  |
|    n_updates        | 14624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.6     |
|    ep_rew_mean      | 1.28     |
|    exploration_rate | 0.695    |
| time/               |          |
|    episodes         | 812      |
|    fps              | 190      |
|    time_elapsed     | 360      |
|    total_timesteps  | 68759    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0317   |
|    n_updates        | 14689    |
----------------------------------
Eval num_timesteps=69000, episode_reward=3.00 +/- 1.99
Episode length: 102.68 +/- 33.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.693    |
| time/               |          |
|    total_timesteps  | 69000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0073   |
|    n_updates        | 14749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88       |
|    ep_rew_mean      | 1.24     |
|    exploration_rate | 0.692    |
| time/               |          |
|    episodes         | 816      |
|    fps              | 190      |
|    time_elapsed     | 363      |
|    total_timesteps  | 69096    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00537  |
|    n_updates        | 14773    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.5     |
|    ep_rew_mean      | 1.18     |
|    exploration_rate | 0.691    |
| time/               |          |
|    episodes         | 820      |
|    fps              | 190      |
|    time_elapsed     | 363      |
|    total_timesteps  | 69372    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00343  |
|    n_updates        | 14842    |
----------------------------------
Eval num_timesteps=69500, episode_reward=2.26 +/- 1.57
Episode length: 94.56 +/- 21.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.6     |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 0.69     |
| time/               |          |
|    total_timesteps  | 69500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00364  |
|    n_updates        | 14874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.5     |
|    ep_rew_mean      | 1.2      |
|    exploration_rate | 0.688    |
| time/               |          |
|    episodes         | 824      |
|    fps              | 190      |
|    time_elapsed     | 366      |
|    total_timesteps  | 69713    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.019    |
|    n_updates        | 14928    |
----------------------------------
Eval num_timesteps=70000, episode_reward=2.28 +/- 1.81
Episode length: 86.80 +/- 30.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.8     |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 0.686    |
| time/               |          |
|    total_timesteps  | 70000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0052   |
|    n_updates        | 14999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.8     |
|    ep_rew_mean      | 1.18     |
|    exploration_rate | 0.686    |
| time/               |          |
|    episodes         | 828      |
|    fps              | 189      |
|    time_elapsed     | 369      |
|    total_timesteps  | 70031    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00113  |
|    n_updates        | 15007    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.5     |
|    ep_rew_mean      | 1.22     |
|    exploration_rate | 0.684    |
| time/               |          |
|    episodes         | 832      |
|    fps              | 190      |
|    time_elapsed     | 369      |
|    total_timesteps  | 70390    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0314   |
|    n_updates        | 15097    |
----------------------------------
Eval num_timesteps=70500, episode_reward=2.34 +/- 1.81
Episode length: 92.28 +/- 26.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.3     |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.683    |
| time/               |          |
|    total_timesteps  | 70500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 15124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 85.8     |
|    ep_rew_mean      | 1.25     |
|    exploration_rate | 0.681    |
| time/               |          |
|    episodes         | 836      |
|    fps              | 189      |
|    time_elapsed     | 372      |
|    total_timesteps  | 70734    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 15183    |
----------------------------------
Eval num_timesteps=71000, episode_reward=0.82 +/- 0.97
Episode length: 77.96 +/- 16.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 78       |
|    mean_reward      | 0.82     |
| rollout/            |          |
|    exploration_rate | 0.68     |
| time/               |          |
|    total_timesteps  | 71000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00864  |
|    n_updates        | 15249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.3     |
|    ep_rew_mean      | 1.31     |
|    exploration_rate | 0.679    |
| time/               |          |
|    episodes         | 840      |
|    fps              | 189      |
|    time_elapsed     | 374      |
|    total_timesteps  | 71150    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00598  |
|    n_updates        | 15287    |
----------------------------------
Eval num_timesteps=71500, episode_reward=0.80 +/- 0.60
Episode length: 79.30 +/- 15.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.3     |
|    mean_reward      | 0.8      |
| rollout/            |          |
|    exploration_rate | 0.676    |
| time/               |          |
|    total_timesteps  | 71500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 15374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87       |
|    ep_rew_mean      | 1.35     |
|    exploration_rate | 0.676    |
| time/               |          |
|    episodes         | 844      |
|    fps              | 189      |
|    time_elapsed     | 377      |
|    total_timesteps  | 71534    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00395  |
|    n_updates        | 15383    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.3     |
|    ep_rew_mean      | 1.34     |
|    exploration_rate | 0.673    |
| time/               |          |
|    episodes         | 848      |
|    fps              | 190      |
|    time_elapsed     | 377      |
|    total_timesteps  | 71899    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00944  |
|    n_updates        | 15474    |
----------------------------------
Eval num_timesteps=72000, episode_reward=2.66 +/- 2.00
Episode length: 98.70 +/- 28.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.7     |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.673    |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00693  |
|    n_updates        | 15499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 1.33     |
|    exploration_rate | 0.671    |
| time/               |          |
|    episodes         | 852      |
|    fps              | 189      |
|    time_elapsed     | 380      |
|    total_timesteps  | 72247    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00608  |
|    n_updates        | 15561    |
----------------------------------
Eval num_timesteps=72500, episode_reward=3.24 +/- 2.10
Episode length: 101.88 +/- 33.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.669    |
| time/               |          |
|    total_timesteps  | 72500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00597  |
|    n_updates        | 15624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 1.36     |
|    exploration_rate | 0.669    |
| time/               |          |
|    episodes         | 856      |
|    fps              | 189      |
|    time_elapsed     | 383      |
|    total_timesteps  | 72603    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0198   |
|    n_updates        | 15650    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.7     |
|    ep_rew_mean      | 1.31     |
|    exploration_rate | 0.666    |
| time/               |          |
|    episodes         | 860      |
|    fps              | 189      |
|    time_elapsed     | 384      |
|    total_timesteps  | 72945    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0146   |
|    n_updates        | 15736    |
----------------------------------
Eval num_timesteps=73000, episode_reward=2.82 +/- 1.85
Episode length: 101.58 +/- 26.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.82     |
| rollout/            |          |
|    exploration_rate | 0.666    |
| time/               |          |
|    total_timesteps  | 73000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00418  |
|    n_updates        | 15749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 1.36     |
|    exploration_rate | 0.664    |
| time/               |          |
|    episodes         | 864      |
|    fps              | 189      |
|    time_elapsed     | 387      |
|    total_timesteps  | 73314    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00168  |
|    n_updates        | 15828    |
----------------------------------
Eval num_timesteps=73500, episode_reward=1.44 +/- 1.37
Episode length: 84.92 +/- 21.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.9     |
|    mean_reward      | 1.44     |
| rollout/            |          |
|    exploration_rate | 0.662    |
| time/               |          |
|    total_timesteps  | 73500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0174   |
|    n_updates        | 15874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.2     |
|    ep_rew_mean      | 1.39     |
|    exploration_rate | 0.662    |
| time/               |          |
|    episodes         | 868      |
|    fps              | 188      |
|    time_elapsed     | 389      |
|    total_timesteps  | 73625    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 15906    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.8     |
|    ep_rew_mean      | 1.34     |
|    exploration_rate | 0.66     |
| time/               |          |
|    episodes         | 872      |
|    fps              | 189      |
|    time_elapsed     | 390      |
|    total_timesteps  | 73931    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 15982    |
----------------------------------
Eval num_timesteps=74000, episode_reward=3.66 +/- 2.27
Episode length: 108.24 +/- 36.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.66     |
| rollout/            |          |
|    exploration_rate | 0.659    |
| time/               |          |
|    total_timesteps  | 74000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00738  |
|    n_updates        | 15999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 86.7     |
|    ep_rew_mean      | 1.37     |
|    exploration_rate | 0.657    |
| time/               |          |
|    episodes         | 876      |
|    fps              | 188      |
|    time_elapsed     | 393      |
|    total_timesteps  | 74271    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00448  |
|    n_updates        | 16067    |
----------------------------------
Eval num_timesteps=74500, episode_reward=2.86 +/- 2.47
Episode length: 97.18 +/- 35.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.2     |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.656    |
| time/               |          |
|    total_timesteps  | 74500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00436  |
|    n_updates        | 16124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87       |
|    ep_rew_mean      | 1.4      |
|    exploration_rate | 0.655    |
| time/               |          |
|    episodes         | 880      |
|    fps              | 188      |
|    time_elapsed     | 396      |
|    total_timesteps  | 74627    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0205   |
|    n_updates        | 16156    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 87.4     |
|    ep_rew_mean      | 1.43     |
|    exploration_rate | 0.652    |
| time/               |          |
|    episodes         | 884      |
|    fps              | 189      |
|    time_elapsed     | 396      |
|    total_timesteps  | 74967    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00593  |
|    n_updates        | 16241    |
----------------------------------
Eval num_timesteps=75000, episode_reward=1.38 +/- 2.63
Episode length: 91.34 +/- 39.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.3     |
|    mean_reward      | 1.38     |
| rollout/            |          |
|    exploration_rate | 0.652    |
| time/               |          |
|    total_timesteps  | 75000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00391  |
|    n_updates        | 16249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.9     |
|    ep_rew_mean      | 1.54     |
|    exploration_rate | 0.649    |
| time/               |          |
|    episodes         | 888      |
|    fps              | 188      |
|    time_elapsed     | 399      |
|    total_timesteps  | 75471    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.021    |
|    n_updates        | 16367    |
----------------------------------
Eval num_timesteps=75500, episode_reward=4.34 +/- 2.12
Episode length: 118.28 +/- 33.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.34     |
| rollout/            |          |
|    exploration_rate | 0.649    |
| time/               |          |
|    total_timesteps  | 75500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00505  |
|    n_updates        | 16374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.8     |
|    ep_rew_mean      | 1.57     |
|    exploration_rate | 0.647    |
| time/               |          |
|    episodes         | 892      |
|    fps              | 188      |
|    time_elapsed     | 403      |
|    total_timesteps  | 75797    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0096   |
|    n_updates        | 16449    |
----------------------------------
Eval num_timesteps=76000, episode_reward=3.94 +/- 2.99
Episode length: 104.40 +/- 47.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 104      |
|    mean_reward      | 3.94     |
| rollout/            |          |
|    exploration_rate | 0.645    |
| time/               |          |
|    total_timesteps  | 76000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0146   |
|    n_updates        | 16499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.9     |
|    ep_rew_mean      | 1.59     |
|    exploration_rate | 0.644    |
| time/               |          |
|    episodes         | 896      |
|    fps              | 187      |
|    time_elapsed     | 406      |
|    total_timesteps  | 76141    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0226   |
|    n_updates        | 16535    |
----------------------------------
Eval num_timesteps=76500, episode_reward=1.88 +/- 1.49
Episode length: 89.50 +/- 21.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89.5     |
|    mean_reward      | 1.88     |
| rollout/            |          |
|    exploration_rate | 0.642    |
| time/               |          |
|    total_timesteps  | 76500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 16624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.1     |
|    ep_rew_mean      | 1.6      |
|    exploration_rate | 0.641    |
| time/               |          |
|    episodes         | 900      |
|    fps              | 187      |
|    time_elapsed     | 409      |
|    total_timesteps  | 76543    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00659  |
|    n_updates        | 16635    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.8     |
|    ep_rew_mean      | 1.58     |
|    exploration_rate | 0.639    |
| time/               |          |
|    episodes         | 904      |
|    fps              | 187      |
|    time_elapsed     | 409      |
|    total_timesteps  | 76867    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0136   |
|    n_updates        | 16716    |
----------------------------------
Eval num_timesteps=77000, episode_reward=1.28 +/- 1.59
Episode length: 79.58 +/- 22.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.6     |
|    mean_reward      | 1.28     |
| rollout/            |          |
|    exploration_rate | 0.638    |
| time/               |          |
|    total_timesteps  | 77000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0296   |
|    n_updates        | 16749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 88.4     |
|    ep_rew_mean      | 1.58     |
|    exploration_rate | 0.636    |
| time/               |          |
|    episodes         | 908      |
|    fps              | 187      |
|    time_elapsed     | 411      |
|    total_timesteps  | 77261    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00317  |
|    n_updates        | 16815    |
----------------------------------
Eval num_timesteps=77500, episode_reward=3.36 +/- 1.77
Episode length: 107.46 +/- 29.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 3.36     |
| rollout/            |          |
|    exploration_rate | 0.635    |
| time/               |          |
|    total_timesteps  | 77500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00279  |
|    n_updates        | 16874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.1     |
|    ep_rew_mean      | 1.66     |
|    exploration_rate | 0.633    |
| time/               |          |
|    episodes         | 912      |
|    fps              | 187      |
|    time_elapsed     | 415      |
|    total_timesteps  | 77673    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.018    |
|    n_updates        | 16918    |
----------------------------------
Eval num_timesteps=78000, episode_reward=3.90 +/- 2.07
Episode length: 110.14 +/- 32.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.9      |
| rollout/            |          |
|    exploration_rate | 0.631    |
| time/               |          |
|    total_timesteps  | 78000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0013   |
|    n_updates        | 16999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 89.4     |
|    ep_rew_mean      | 1.69     |
|    exploration_rate | 0.631    |
| time/               |          |
|    episodes         | 916      |
|    fps              | 186      |
|    time_elapsed     | 418      |
|    total_timesteps  | 78040    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0146   |
|    n_updates        | 17009    |
----------------------------------
Eval num_timesteps=78500, episode_reward=1.16 +/- 1.29
Episode length: 77.74 +/- 16.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 77.7     |
|    mean_reward      | 1.16     |
| rollout/            |          |
|    exploration_rate | 0.627    |
| time/               |          |
|    total_timesteps  | 78500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 17124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 91.4     |
|    ep_rew_mean      | 1.8      |
|    exploration_rate | 0.627    |
| time/               |          |
|    episodes         | 920      |
|    fps              | 186      |
|    time_elapsed     | 421      |
|    total_timesteps  | 78514    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 17128    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92       |
|    ep_rew_mean      | 1.81     |
|    exploration_rate | 0.625    |
| time/               |          |
|    episodes         | 924      |
|    fps              | 187      |
|    time_elapsed     | 421      |
|    total_timesteps  | 78911    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00791  |
|    n_updates        | 17227    |
----------------------------------
Eval num_timesteps=79000, episode_reward=2.80 +/- 2.01
Episode length: 90.68 +/- 25.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.7     |
|    mean_reward      | 2.8      |
| rollout/            |          |
|    exploration_rate | 0.624    |
| time/               |          |
|    total_timesteps  | 79000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0608   |
|    n_updates        | 17249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 92.2     |
|    ep_rew_mean      | 1.81     |
|    exploration_rate | 0.622    |
| time/               |          |
|    episodes         | 928      |
|    fps              | 186      |
|    time_elapsed     | 424      |
|    total_timesteps  | 79253    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0195   |
|    n_updates        | 17313    |
----------------------------------
Eval num_timesteps=79500, episode_reward=4.38 +/- 2.23
Episode length: 119.74 +/- 38.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.38     |
| rollout/            |          |
|    exploration_rate | 0.62     |
| time/               |          |
|    total_timesteps  | 79500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0062   |
|    n_updates        | 17374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.7     |
|    ep_rew_mean      | 1.87     |
|    exploration_rate | 0.618    |
| time/               |          |
|    episodes         | 932      |
|    fps              | 186      |
|    time_elapsed     | 428      |
|    total_timesteps  | 79759    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0352   |
|    n_updates        | 17439    |
----------------------------------
Eval num_timesteps=80000, episode_reward=2.30 +/- 2.57
Episode length: 90.16 +/- 35.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.2     |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.617    |
| time/               |          |
|    total_timesteps  | 80000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00953  |
|    n_updates        | 17499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.5     |
|    ep_rew_mean      | 1.93     |
|    exploration_rate | 0.615    |
| time/               |          |
|    episodes         | 936      |
|    fps              | 185      |
|    time_elapsed     | 431      |
|    total_timesteps  | 80187    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0273   |
|    n_updates        | 17546    |
----------------------------------
Eval num_timesteps=80500, episode_reward=2.30 +/- 1.88
Episode length: 94.72 +/- 26.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.7     |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.613    |
| time/               |          |
|    total_timesteps  | 80500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0058   |
|    n_updates        | 17624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.1     |
|    ep_rew_mean      | 1.94     |
|    exploration_rate | 0.613    |
| time/               |          |
|    episodes         | 940      |
|    fps              | 185      |
|    time_elapsed     | 434      |
|    total_timesteps  | 80560    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0133   |
|    n_updates        | 17639    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.8     |
|    ep_rew_mean      | 1.91     |
|    exploration_rate | 0.61     |
| time/               |          |
|    episodes         | 944      |
|    fps              | 186      |
|    time_elapsed     | 434      |
|    total_timesteps  | 80918    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00344  |
|    n_updates        | 17729    |
----------------------------------
Eval num_timesteps=81000, episode_reward=3.14 +/- 2.54
Episode length: 99.38 +/- 40.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.4     |
|    mean_reward      | 3.14     |
| rollout/            |          |
|    exploration_rate | 0.61     |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0204   |
|    n_updates        | 17749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 93.9     |
|    ep_rew_mean      | 1.95     |
|    exploration_rate | 0.607    |
| time/               |          |
|    episodes         | 948      |
|    fps              | 185      |
|    time_elapsed     | 437      |
|    total_timesteps  | 81290    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 17822    |
----------------------------------
Eval num_timesteps=81500, episode_reward=3.08 +/- 1.95
Episode length: 99.32 +/- 26.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.3     |
|    mean_reward      | 3.08     |
| rollout/            |          |
|    exploration_rate | 0.606    |
| time/               |          |
|    total_timesteps  | 81500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00522  |
|    n_updates        | 17874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.7     |
|    ep_rew_mean      | 2.01     |
|    exploration_rate | 0.604    |
| time/               |          |
|    episodes         | 952      |
|    fps              | 185      |
|    time_elapsed     | 440      |
|    total_timesteps  | 81713    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0316   |
|    n_updates        | 17928    |
----------------------------------
Eval num_timesteps=82000, episode_reward=1.62 +/- 1.59
Episode length: 82.80 +/- 23.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.8     |
|    mean_reward      | 1.62     |
| rollout/            |          |
|    exploration_rate | 0.602    |
| time/               |          |
|    total_timesteps  | 82000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00417  |
|    n_updates        | 17999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.4     |
|    ep_rew_mean      | 1.99     |
|    exploration_rate | 0.602    |
| time/               |          |
|    episodes         | 956      |
|    fps              | 185      |
|    time_elapsed     | 443      |
|    total_timesteps  | 82043    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 18010    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.3     |
|    ep_rew_mean      | 1.98     |
|    exploration_rate | 0.6      |
| time/               |          |
|    episodes         | 960      |
|    fps              | 185      |
|    time_elapsed     | 443      |
|    total_timesteps  | 82374    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 18093    |
----------------------------------
Eval num_timesteps=82500, episode_reward=2.76 +/- 2.83
Episode length: 92.88 +/- 38.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.9     |
|    mean_reward      | 2.76     |
| rollout/            |          |
|    exploration_rate | 0.599    |
| time/               |          |
|    total_timesteps  | 82500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0341   |
|    n_updates        | 18124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.4     |
|    ep_rew_mean      | 1.94     |
|    exploration_rate | 0.597    |
| time/               |          |
|    episodes         | 964      |
|    fps              | 185      |
|    time_elapsed     | 446      |
|    total_timesteps  | 82753    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00316  |
|    n_updates        | 18188    |
----------------------------------
Eval num_timesteps=83000, episode_reward=2.48 +/- 2.31
Episode length: 83.08 +/- 32.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 83.1     |
|    mean_reward      | 2.48     |
| rollout/            |          |
|    exploration_rate | 0.595    |
| time/               |          |
|    total_timesteps  | 83000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 18249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 94.5     |
|    ep_rew_mean      | 1.98     |
|    exploration_rate | 0.594    |
| time/               |          |
|    episodes         | 968      |
|    fps              | 185      |
|    time_elapsed     | 448      |
|    total_timesteps  | 83079    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 18269    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.4     |
|    ep_rew_mean      | 2.09     |
|    exploration_rate | 0.592    |
| time/               |          |
|    episodes         | 972      |
|    fps              | 185      |
|    time_elapsed     | 449      |
|    total_timesteps  | 83468    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 18366    |
----------------------------------
Eval num_timesteps=83500, episode_reward=3.40 +/- 2.82
Episode length: 99.76 +/- 49.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.8     |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.591    |
| time/               |          |
|    total_timesteps  | 83500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0357   |
|    n_updates        | 18374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.8     |
|    ep_rew_mean      | 2.17     |
|    exploration_rate | 0.588    |
| time/               |          |
|    episodes         | 976      |
|    fps              | 185      |
|    time_elapsed     | 452      |
|    total_timesteps  | 83949    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00449  |
|    n_updates        | 18487    |
----------------------------------
Eval num_timesteps=84000, episode_reward=2.56 +/- 2.55
Episode length: 87.00 +/- 40.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87       |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.588    |
| time/               |          |
|    total_timesteps  | 84000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00347  |
|    n_updates        | 18499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.2     |
|    ep_rew_mean      | 2.22     |
|    exploration_rate | 0.585    |
| time/               |          |
|    episodes         | 980      |
|    fps              | 185      |
|    time_elapsed     | 455      |
|    total_timesteps  | 84350    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0266   |
|    n_updates        | 18587    |
----------------------------------
Eval num_timesteps=84500, episode_reward=2.34 +/- 2.12
Episode length: 95.04 +/- 34.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95       |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.584    |
| time/               |          |
|    total_timesteps  | 84500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0291   |
|    n_updates        | 18624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98       |
|    ep_rew_mean      | 2.26     |
|    exploration_rate | 0.582    |
| time/               |          |
|    episodes         | 984      |
|    fps              | 185      |
|    time_elapsed     | 458      |
|    total_timesteps  | 84765    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00876  |
|    n_updates        | 18691    |
----------------------------------
Eval num_timesteps=85000, episode_reward=2.10 +/- 1.55
Episode length: 91.04 +/- 21.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91       |
|    mean_reward      | 2.1      |
| rollout/            |          |
|    exploration_rate | 0.58     |
| time/               |          |
|    total_timesteps  | 85000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0242   |
|    n_updates        | 18749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.9     |
|    ep_rew_mean      | 2.2      |
|    exploration_rate | 0.579    |
| time/               |          |
|    episodes         | 988      |
|    fps              | 184      |
|    time_elapsed     | 460      |
|    total_timesteps  | 85161    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00742  |
|    n_updates        | 18790    |
----------------------------------
Eval num_timesteps=85500, episode_reward=2.04 +/- 1.82
Episode length: 85.28 +/- 28.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.3     |
|    mean_reward      | 2.04     |
| rollout/            |          |
|    exploration_rate | 0.577    |
| time/               |          |
|    total_timesteps  | 85500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00977  |
|    n_updates        | 18874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.3     |
|    ep_rew_mean      | 2.29     |
|    exploration_rate | 0.576    |
| time/               |          |
|    episodes         | 992      |
|    fps              | 184      |
|    time_elapsed     | 463      |
|    total_timesteps  | 85624    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0046   |
|    n_updates        | 18905    |
----------------------------------
Eval num_timesteps=86000, episode_reward=1.92 +/- 1.34
Episode length: 91.64 +/- 19.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.6     |
|    mean_reward      | 1.92     |
| rollout/            |          |
|    exploration_rate | 0.573    |
| time/               |          |
|    total_timesteps  | 86000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0118   |
|    n_updates        | 18999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 2.38     |
|    exploration_rate | 0.572    |
| time/               |          |
|    episodes         | 996      |
|    fps              | 184      |
|    time_elapsed     | 466      |
|    total_timesteps  | 86110    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00885  |
|    n_updates        | 19027    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 2.3      |
|    exploration_rate | 0.57     |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 185      |
|    time_elapsed     | 466      |
|    total_timesteps  | 86386    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00406  |
|    n_updates        | 19096    |
----------------------------------
Eval num_timesteps=86500, episode_reward=4.20 +/- 2.88
Episode length: 117.04 +/- 46.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.569    |
| time/               |          |
|    total_timesteps  | 86500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 19124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 2.36     |
|    exploration_rate | 0.567    |
| time/               |          |
|    episodes         | 1004     |
|    fps              | 184      |
|    time_elapsed     | 470      |
|    total_timesteps  | 86784    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0043   |
|    n_updates        | 19195    |
----------------------------------
Eval num_timesteps=87000, episode_reward=2.80 +/- 1.82
Episode length: 97.30 +/- 30.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.3     |
|    mean_reward      | 2.8      |
| rollout/            |          |
|    exploration_rate | 0.565    |
| time/               |          |
|    total_timesteps  | 87000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0135   |
|    n_updates        | 19249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.4     |
|    ep_rew_mean      | 2.31     |
|    exploration_rate | 0.565    |
| time/               |          |
|    episodes         | 1008     |
|    fps              | 183      |
|    time_elapsed     | 473      |
|    total_timesteps  | 87103    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0125   |
|    n_updates        | 19275    |
----------------------------------
Eval num_timesteps=87500, episode_reward=1.58 +/- 1.31
Episode length: 82.66 +/- 19.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.7     |
|    mean_reward      | 1.58     |
| rollout/            |          |
|    exploration_rate | 0.562    |
| time/               |          |
|    total_timesteps  | 87500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0412   |
|    n_updates        | 19374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.2     |
|    ep_rew_mean      | 2.33     |
|    exploration_rate | 0.561    |
| time/               |          |
|    episodes         | 1012     |
|    fps              | 183      |
|    time_elapsed     | 476      |
|    total_timesteps  | 87592    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 19397    |
----------------------------------
Eval num_timesteps=88000, episode_reward=1.26 +/- 1.37
Episode length: 86.34 +/- 24.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.3     |
|    mean_reward      | 1.26     |
| rollout/            |          |
|    exploration_rate | 0.558    |
| time/               |          |
|    total_timesteps  | 88000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0157   |
|    n_updates        | 19499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.7     |
|    ep_rew_mean      | 2.35     |
|    exploration_rate | 0.558    |
| time/               |          |
|    episodes         | 1016     |
|    fps              | 183      |
|    time_elapsed     | 478      |
|    total_timesteps  | 88008    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0274   |
|    n_updates        | 19501    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.5     |
|    ep_rew_mean      | 2.27     |
|    exploration_rate | 0.555    |
| time/               |          |
|    episodes         | 1020     |
|    fps              | 184      |
|    time_elapsed     | 479      |
|    total_timesteps  | 88365    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00551  |
|    n_updates        | 19591    |
----------------------------------
Eval num_timesteps=88500, episode_reward=3.14 +/- 2.38
Episode length: 104.98 +/- 44.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.14     |
| rollout/            |          |
|    exploration_rate | 0.554    |
| time/               |          |
|    total_timesteps  | 88500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 19624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.1     |
|    ep_rew_mean      | 2.27     |
|    exploration_rate | 0.552    |
| time/               |          |
|    episodes         | 1024     |
|    fps              | 183      |
|    time_elapsed     | 482      |
|    total_timesteps  | 88719    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0339   |
|    n_updates        | 19679    |
----------------------------------
Eval num_timesteps=89000, episode_reward=-0.22 +/- 0.99
Episode length: 82.14 +/- 16.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.1     |
|    mean_reward      | -0.22    |
| rollout/            |          |
|    exploration_rate | 0.55     |
| time/               |          |
|    total_timesteps  | 89000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00514  |
|    n_updates        | 19749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 98.1     |
|    ep_rew_mean      | 2.29     |
|    exploration_rate | 0.55     |
| time/               |          |
|    episodes         | 1028     |
|    fps              | 183      |
|    time_elapsed     | 484      |
|    total_timesteps  | 89060    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0133   |
|    n_updates        | 19764    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 2.23     |
|    exploration_rate | 0.547    |
| time/               |          |
|    episodes         | 1032     |
|    fps              | 184      |
|    time_elapsed     | 485      |
|    total_timesteps  | 89410    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00385  |
|    n_updates        | 19852    |
----------------------------------
Eval num_timesteps=89500, episode_reward=2.18 +/- 1.66
Episode length: 86.16 +/- 22.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.2     |
|    mean_reward      | 2.18     |
| rollout/            |          |
|    exploration_rate | 0.546    |
| time/               |          |
|    total_timesteps  | 89500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 19874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95       |
|    ep_rew_mean      | 2.15     |
|    exploration_rate | 0.545    |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 183      |
|    time_elapsed     | 487      |
|    total_timesteps  | 89684    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00638  |
|    n_updates        | 19920    |
----------------------------------
Eval num_timesteps=90000, episode_reward=2.36 +/- 1.83
Episode length: 89.04 +/- 26.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89       |
|    mean_reward      | 2.36     |
| rollout/            |          |
|    exploration_rate | 0.543    |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0208   |
|    n_updates        | 19999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.1     |
|    ep_rew_mean      | 2.14     |
|    exploration_rate | 0.542    |
| time/               |          |
|    episodes         | 1040     |
|    fps              | 183      |
|    time_elapsed     | 490      |
|    total_timesteps  | 90068    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00403  |
|    n_updates        | 20016    |
----------------------------------
Eval num_timesteps=90500, episode_reward=6.30 +/- 3.29
Episode length: 148.66 +/- 50.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 6.3      |
| rollout/            |          |
|    exploration_rate | 0.539    |
| time/               |          |
|    total_timesteps  | 90500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 20124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.2     |
|    ep_rew_mean      | 2.24     |
|    exploration_rate | 0.539    |
| time/               |          |
|    episodes         | 1044     |
|    fps              | 182      |
|    time_elapsed     | 495      |
|    total_timesteps  | 90534    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0043   |
|    n_updates        | 20133    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 2.3      |
|    exploration_rate | 0.535    |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 183      |
|    time_elapsed     | 495      |
|    total_timesteps  | 90943    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00555  |
|    n_updates        | 20235    |
----------------------------------
Eval num_timesteps=91000, episode_reward=2.32 +/- 1.97
Episode length: 91.96 +/- 28.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92       |
|    mean_reward      | 2.32     |
| rollout/            |          |
|    exploration_rate | 0.535    |
| time/               |          |
|    total_timesteps  | 91000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 20249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.3     |
|    ep_rew_mean      | 2.21     |
|    exploration_rate | 0.533    |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 183      |
|    time_elapsed     | 498      |
|    total_timesteps  | 91240    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00715  |
|    n_updates        | 20309    |
----------------------------------
Eval num_timesteps=91500, episode_reward=3.96 +/- 2.68
Episode length: 113.40 +/- 40.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 3.96     |
| rollout/            |          |
|    exploration_rate | 0.531    |
| time/               |          |
|    total_timesteps  | 91500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00309  |
|    n_updates        | 20374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.1     |
|    ep_rew_mean      | 2.27     |
|    exploration_rate | 0.53     |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 182      |
|    time_elapsed     | 501      |
|    total_timesteps  | 91655    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00515  |
|    n_updates        | 20413    |
----------------------------------
Eval num_timesteps=92000, episode_reward=10.18 +/- 3.79
Episode length: 197.82 +/- 57.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | 10.2     |
| rollout/            |          |
|    exploration_rate | 0.527    |
| time/               |          |
|    total_timesteps  | 92000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00204  |
|    n_updates        | 20499    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 2.37     |
|    exploration_rate | 0.527    |
| time/               |          |
|    episodes         | 1060     |
|    fps              | 181      |
|    time_elapsed     | 507      |
|    total_timesteps  | 92110    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00804  |
|    n_updates        | 20527    |
----------------------------------
Eval num_timesteps=92500, episode_reward=1.02 +/- 1.05
Episode length: 79.38 +/- 16.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 79.4     |
|    mean_reward      | 1.02     |
| rollout/            |          |
|    exploration_rate | 0.523    |
| time/               |          |
|    total_timesteps  | 92500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0125   |
|    n_updates        | 20624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.6     |
|    ep_rew_mean      | 2.36     |
|    exploration_rate | 0.523    |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 181      |
|    time_elapsed     | 510      |
|    total_timesteps  | 92517    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00923  |
|    n_updates        | 20629    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.4     |
|    ep_rew_mean      | 2.32     |
|    exploration_rate | 0.521    |
| time/               |          |
|    episodes         | 1068     |
|    fps              | 181      |
|    time_elapsed     | 510      |
|    total_timesteps  | 92821    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00149  |
|    n_updates        | 20705    |
----------------------------------
Eval num_timesteps=93000, episode_reward=3.74 +/- 2.15
Episode length: 117.22 +/- 38.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 3.74     |
| rollout/            |          |
|    exploration_rate | 0.52     |
| time/               |          |
|    total_timesteps  | 93000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00385  |
|    n_updates        | 20749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.9     |
|    ep_rew_mean      | 2.31     |
|    exploration_rate | 0.518    |
| time/               |          |
|    episodes         | 1072     |
|    fps              | 181      |
|    time_elapsed     | 513      |
|    total_timesteps  | 93257    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00617  |
|    n_updates        | 20814    |
----------------------------------
Eval num_timesteps=93500, episode_reward=-0.56 +/- 0.64
Episode length: 71.60 +/- 10.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 71.6     |
|    mean_reward      | -0.56    |
| rollout/            |          |
|    exploration_rate | 0.516    |
| time/               |          |
|    total_timesteps  | 93500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0114   |
|    n_updates        | 20874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96       |
|    ep_rew_mean      | 2.18     |
|    exploration_rate | 0.515    |
| time/               |          |
|    episodes         | 1076     |
|    fps              | 181      |
|    time_elapsed     | 516      |
|    total_timesteps  | 93544    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 20885    |
----------------------------------
Eval num_timesteps=94000, episode_reward=1.44 +/- 1.37
Episode length: 86.32 +/- 22.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.3     |
|    mean_reward      | 1.44     |
| rollout/            |          |
|    exploration_rate | 0.512    |
| time/               |          |
|    total_timesteps  | 94000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00328  |
|    n_updates        | 20999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 2.24     |
|    exploration_rate | 0.512    |
| time/               |          |
|    episodes         | 1080     |
|    fps              | 181      |
|    time_elapsed     | 519      |
|    total_timesteps  | 94004    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0137   |
|    n_updates        | 21000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 95.8     |
|    ep_rew_mean      | 2.21     |
|    exploration_rate | 0.509    |
| time/               |          |
|    episodes         | 1084     |
|    fps              | 181      |
|    time_elapsed     | 519      |
|    total_timesteps  | 94349    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00631  |
|    n_updates        | 21087    |
----------------------------------
Eval num_timesteps=94500, episode_reward=3.24 +/- 2.29
Episode length: 102.26 +/- 35.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.24     |
| rollout/            |          |
|    exploration_rate | 0.508    |
| time/               |          |
|    total_timesteps  | 94500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 21124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.7     |
|    ep_rew_mean      | 2.3      |
|    exploration_rate | 0.505    |
| time/               |          |
|    episodes         | 1088     |
|    fps              | 181      |
|    time_elapsed     | 522      |
|    total_timesteps  | 94827    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0264   |
|    n_updates        | 21206    |
----------------------------------
Eval num_timesteps=95000, episode_reward=2.28 +/- 1.55
Episode length: 86.90 +/- 21.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.9     |
|    mean_reward      | 2.28     |
| rollout/            |          |
|    exploration_rate | 0.504    |
| time/               |          |
|    total_timesteps  | 95000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0208   |
|    n_updates        | 21249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.4     |
|    ep_rew_mean      | 2.31     |
|    exploration_rate | 0.502    |
| time/               |          |
|    episodes         | 1092     |
|    fps              | 181      |
|    time_elapsed     | 525      |
|    total_timesteps  | 95261    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0107   |
|    n_updates        | 21315    |
----------------------------------
Eval num_timesteps=95500, episode_reward=1.90 +/- 1.40
Episode length: 87.52 +/- 20.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.5     |
|    mean_reward      | 1.9      |
| rollout/            |          |
|    exploration_rate | 0.5      |
| time/               |          |
|    total_timesteps  | 95500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0274   |
|    n_updates        | 21374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 96.5     |
|    ep_rew_mean      | 2.35     |
|    exploration_rate | 0.498    |
| time/               |          |
|    episodes         | 1096     |
|    fps              | 181      |
|    time_elapsed     | 528      |
|    total_timesteps  | 95764    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0418   |
|    n_updates        | 21440    |
----------------------------------
Eval num_timesteps=96000, episode_reward=4.82 +/- 2.97
Episode length: 122.30 +/- 43.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.82     |
| rollout/            |          |
|    exploration_rate | 0.496    |
| time/               |          |
|    total_timesteps  | 96000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0398   |
|    n_updates        | 21499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.7     |
|    ep_rew_mean      | 2.46     |
|    exploration_rate | 0.495    |
| time/               |          |
|    episodes         | 1100     |
|    fps              | 180      |
|    time_elapsed     | 532      |
|    total_timesteps  | 96152    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00904  |
|    n_updates        | 21537    |
----------------------------------
Eval num_timesteps=96500, episode_reward=2.54 +/- 2.24
Episode length: 94.66 +/- 31.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.7     |
|    mean_reward      | 2.54     |
| rollout/            |          |
|    exploration_rate | 0.492    |
| time/               |          |
|    total_timesteps  | 96500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00655  |
|    n_updates        | 21624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 97.8     |
|    ep_rew_mean      | 2.47     |
|    exploration_rate | 0.492    |
| time/               |          |
|    episodes         | 1104     |
|    fps              | 180      |
|    time_elapsed     | 535      |
|    total_timesteps  | 96559    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00364  |
|    n_updates        | 21639    |
----------------------------------
Eval num_timesteps=97000, episode_reward=1.30 +/- 1.37
Episode length: 85.16 +/- 20.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.2     |
|    mean_reward      | 1.3      |
| rollout/            |          |
|    exploration_rate | 0.488    |
| time/               |          |
|    total_timesteps  | 97000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00261  |
|    n_updates        | 21749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99       |
|    ep_rew_mean      | 2.58     |
|    exploration_rate | 0.488    |
| time/               |          |
|    episodes         | 1108     |
|    fps              | 180      |
|    time_elapsed     | 537      |
|    total_timesteps  | 97003    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00979  |
|    n_updates        | 21750    |
----------------------------------
Eval num_timesteps=97500, episode_reward=2.38 +/- 2.00
Episode length: 101.32 +/- 36.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.38     |
| rollout/            |          |
|    exploration_rate | 0.484    |
| time/               |          |
|    total_timesteps  | 97500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.038    |
|    n_updates        | 21874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 99.1     |
|    ep_rew_mean      | 2.64     |
|    exploration_rate | 0.484    |
| time/               |          |
|    episodes         | 1112     |
|    fps              | 180      |
|    time_elapsed     | 541      |
|    total_timesteps  | 97502    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00548  |
|    n_updates        | 21875    |
----------------------------------
Eval num_timesteps=98000, episode_reward=2.66 +/- 2.29
Episode length: 90.76 +/- 35.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.8     |
|    mean_reward      | 2.66     |
| rollout/            |          |
|    exploration_rate | 0.48     |
| time/               |          |
|    total_timesteps  | 98000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0167   |
|    n_updates        | 21999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 101      |
|    ep_rew_mean      | 2.81     |
|    exploration_rate | 0.48     |
| time/               |          |
|    episodes         | 1116     |
|    fps              | 180      |
|    time_elapsed     | 544      |
|    total_timesteps  | 98079    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0093   |
|    n_updates        | 22019    |
----------------------------------
Eval num_timesteps=98500, episode_reward=5.98 +/- 3.06
Episode length: 147.16 +/- 44.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 147      |
|    mean_reward      | 5.98     |
| rollout/            |          |
|    exploration_rate | 0.476    |
| time/               |          |
|    total_timesteps  | 98500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 22124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 102      |
|    ep_rew_mean      | 2.95     |
|    exploration_rate | 0.476    |
| time/               |          |
|    episodes         | 1120     |
|    fps              | 179      |
|    time_elapsed     | 548      |
|    total_timesteps  | 98592    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0199   |
|    n_updates        | 22147    |
----------------------------------
Eval num_timesteps=99000, episode_reward=1.56 +/- 1.40
Episode length: 84.16 +/- 19.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 84.2     |
|    mean_reward      | 1.56     |
| rollout/            |          |
|    exploration_rate | 0.472    |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0321   |
|    n_updates        | 22249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 103      |
|    ep_rew_mean      | 2.98     |
|    exploration_rate | 0.472    |
| time/               |          |
|    episodes         | 1124     |
|    fps              | 179      |
|    time_elapsed     | 551      |
|    total_timesteps  | 99007    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00671  |
|    n_updates        | 22251    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 104      |
|    ep_rew_mean      | 3.08     |
|    exploration_rate | 0.469    |
| time/               |          |
|    episodes         | 1128     |
|    fps              | 180      |
|    time_elapsed     | 551      |
|    total_timesteps  | 99462    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0247   |
|    n_updates        | 22365    |
----------------------------------
Eval num_timesteps=99500, episode_reward=2.56 +/- 2.65
Episode length: 94.00 +/- 39.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94       |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.468    |
| time/               |          |
|    total_timesteps  | 99500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00888  |
|    n_updates        | 22374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 105      |
|    ep_rew_mean      | 3.09     |
|    exploration_rate | 0.465    |
| time/               |          |
|    episodes         | 1132     |
|    fps              | 180      |
|    time_elapsed     | 554      |
|    total_timesteps  | 99886    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 22471    |
----------------------------------
Eval num_timesteps=100000, episode_reward=1.84 +/- 1.72
Episode length: 85.32 +/- 26.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 85.3     |
|    mean_reward      | 1.84     |
| rollout/            |          |
|    exploration_rate | 0.464    |
| time/               |          |
|    total_timesteps  | 100000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0494   |
|    n_updates        | 22499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.28     |
|    exploration_rate | 0.461    |
| time/               |          |
|    episodes         | 1136     |
|    fps              | 180      |
|    time_elapsed     | 557      |
|    total_timesteps  | 100439   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0125   |
|    n_updates        | 22609    |
----------------------------------
Eval num_timesteps=100500, episode_reward=1.42 +/- 1.36
Episode length: 81.22 +/- 20.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.2     |
|    mean_reward      | 1.42     |
| rollout/            |          |
|    exploration_rate | 0.46     |
| time/               |          |
|    total_timesteps  | 100500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00541  |
|    n_updates        | 22624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.31     |
|    exploration_rate | 0.458    |
| time/               |          |
|    episodes         | 1140     |
|    fps              | 180      |
|    time_elapsed     | 559      |
|    total_timesteps  | 100820   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0134   |
|    n_updates        | 22704    |
----------------------------------
Eval num_timesteps=101000, episode_reward=5.98 +/- 2.69
Episode length: 137.82 +/- 43.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 5.98     |
| rollout/            |          |
|    exploration_rate | 0.456    |
| time/               |          |
|    total_timesteps  | 101000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00787  |
|    n_updates        | 22749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 107      |
|    ep_rew_mean      | 3.29     |
|    exploration_rate | 0.454    |
| time/               |          |
|    episodes         | 1144     |
|    fps              | 179      |
|    time_elapsed     | 564      |
|    total_timesteps  | 101234   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 22808    |
----------------------------------
Eval num_timesteps=101500, episode_reward=3.84 +/- 2.21
Episode length: 107.56 +/- 34.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 3.84     |
| rollout/            |          |
|    exploration_rate | 0.452    |
| time/               |          |
|    total_timesteps  | 101500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00845  |
|    n_updates        | 22874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 106      |
|    ep_rew_mean      | 3.25     |
|    exploration_rate | 0.452    |
| time/               |          |
|    episodes         | 1148     |
|    fps              | 179      |
|    time_elapsed     | 567      |
|    total_timesteps  | 101577   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.024    |
|    n_updates        | 22894    |
----------------------------------
Eval num_timesteps=102000, episode_reward=2.30 +/- 1.97
Episode length: 81.32 +/- 29.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 81.3     |
|    mean_reward      | 2.3      |
| rollout/            |          |
|    exploration_rate | 0.448    |
| time/               |          |
|    total_timesteps  | 102000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0355   |
|    n_updates        | 22999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.41     |
|    exploration_rate | 0.448    |
| time/               |          |
|    episodes         | 1152     |
|    fps              | 179      |
|    time_elapsed     | 570      |
|    total_timesteps  | 102084   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0315   |
|    n_updates        | 23020    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.39     |
|    exploration_rate | 0.445    |
| time/               |          |
|    episodes         | 1156     |
|    fps              | 179      |
|    time_elapsed     | 570      |
|    total_timesteps  | 102433   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0312   |
|    n_updates        | 23108    |
----------------------------------
Eval num_timesteps=102500, episode_reward=4.20 +/- 2.68
Episode length: 119.64 +/- 40.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.444    |
| time/               |          |
|    total_timesteps  | 102500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00548  |
|    n_updates        | 23124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 109      |
|    ep_rew_mean      | 3.48     |
|    exploration_rate | 0.44     |
| time/               |          |
|    episodes         | 1160     |
|    fps              | 179      |
|    time_elapsed     | 574      |
|    total_timesteps  | 102982   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0143   |
|    n_updates        | 23245    |
----------------------------------
Eval num_timesteps=103000, episode_reward=2.92 +/- 2.19
Episode length: 95.82 +/- 29.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.8     |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.44     |
| time/               |          |
|    total_timesteps  | 103000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.004    |
|    n_updates        | 23249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 108      |
|    ep_rew_mean      | 3.51     |
|    exploration_rate | 0.437    |
| time/               |          |
|    episodes         | 1164     |
|    fps              | 179      |
|    time_elapsed     | 577      |
|    total_timesteps  | 103360   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0173   |
|    n_updates        | 23339    |
----------------------------------
Eval num_timesteps=103500, episode_reward=0.28 +/- 1.11
Episode length: 82.54 +/- 17.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 82.5     |
|    mean_reward      | 0.28     |
| rollout/            |          |
|    exploration_rate | 0.436    |
| time/               |          |
|    total_timesteps  | 103500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0116   |
|    n_updates        | 23374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.68     |
|    exploration_rate | 0.433    |
| time/               |          |
|    episodes         | 1168     |
|    fps              | 179      |
|    time_elapsed     | 579      |
|    total_timesteps  | 103888   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00384  |
|    n_updates        | 23471    |
----------------------------------
Eval num_timesteps=104000, episode_reward=2.34 +/- 1.74
Episode length: 86.98 +/- 28.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87       |
|    mean_reward      | 2.34     |
| rollout/            |          |
|    exploration_rate | 0.432    |
| time/               |          |
|    total_timesteps  | 104000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00854  |
|    n_updates        | 23499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 111      |
|    ep_rew_mean      | 3.73     |
|    exploration_rate | 0.429    |
| time/               |          |
|    episodes         | 1172     |
|    fps              | 179      |
|    time_elapsed     | 582      |
|    total_timesteps  | 104359   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00885  |
|    n_updates        | 23589    |
----------------------------------
Eval num_timesteps=104500, episode_reward=5.66 +/- 3.14
Episode length: 134.72 +/- 49.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 5.66     |
| rollout/            |          |
|    exploration_rate | 0.428    |
| time/               |          |
|    total_timesteps  | 104500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 23624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.79     |
|    exploration_rate | 0.426    |
| time/               |          |
|    episodes         | 1176     |
|    fps              | 178      |
|    time_elapsed     | 586      |
|    total_timesteps  | 104740   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00696  |
|    n_updates        | 23684    |
----------------------------------
Eval num_timesteps=105000, episode_reward=2.04 +/- 1.96
Episode length: 88.96 +/- 26.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 89       |
|    mean_reward      | 2.04     |
| rollout/            |          |
|    exploration_rate | 0.424    |
| time/               |          |
|    total_timesteps  | 105000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0089   |
|    n_updates        | 23749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 112      |
|    ep_rew_mean      | 3.79     |
|    exploration_rate | 0.422    |
| time/               |          |
|    episodes         | 1180     |
|    fps              | 178      |
|    time_elapsed     | 589      |
|    total_timesteps  | 105243   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00556  |
|    n_updates        | 23810    |
----------------------------------
Eval num_timesteps=105500, episode_reward=2.98 +/- 1.87
Episode length: 101.22 +/- 28.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 2.98     |
| rollout/            |          |
|    exploration_rate | 0.42     |
| time/               |          |
|    total_timesteps  | 105500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 23874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.84     |
|    exploration_rate | 0.418    |
| time/               |          |
|    episodes         | 1184     |
|    fps              | 178      |
|    time_elapsed     | 592      |
|    total_timesteps  | 105651   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 23912    |
----------------------------------
Eval num_timesteps=106000, episode_reward=4.42 +/- 2.09
Episode length: 118.80 +/- 34.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.415    |
| time/               |          |
|    total_timesteps  | 106000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0179   |
|    n_updates        | 23999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 113      |
|    ep_rew_mean      | 3.85     |
|    exploration_rate | 0.414    |
| time/               |          |
|    episodes         | 1188     |
|    fps              | 177      |
|    time_elapsed     | 596      |
|    total_timesteps  | 106139   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00621  |
|    n_updates        | 24034    |
----------------------------------
Eval num_timesteps=106500, episode_reward=3.84 +/- 2.53
Episode length: 110.50 +/- 39.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.84     |
| rollout/            |          |
|    exploration_rate | 0.411    |
| time/               |          |
|    total_timesteps  | 106500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0305   |
|    n_updates        | 24124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 3.88     |
|    exploration_rate | 0.41     |
| time/               |          |
|    episodes         | 1192     |
|    fps              | 177      |
|    time_elapsed     | 599      |
|    total_timesteps  | 106611   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0137   |
|    n_updates        | 24152    |
----------------------------------
Eval num_timesteps=107000, episode_reward=2.06 +/- 1.55
Episode length: 87.02 +/- 21.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87       |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 0.407    |
| time/               |          |
|    total_timesteps  | 107000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00636  |
|    n_updates        | 24249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 115      |
|    ep_rew_mean      | 3.93     |
|    exploration_rate | 0.405    |
| time/               |          |
|    episodes         | 1196     |
|    fps              | 177      |
|    time_elapsed     | 602      |
|    total_timesteps  | 107271   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0114   |
|    n_updates        | 24317    |
----------------------------------
Eval num_timesteps=107500, episode_reward=5.48 +/- 2.89
Episode length: 134.64 +/- 51.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 5.48     |
| rollout/            |          |
|    exploration_rate | 0.403    |
| time/               |          |
|    total_timesteps  | 107500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00722  |
|    n_updates        | 24374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 117      |
|    ep_rew_mean      | 4.03     |
|    exploration_rate | 0.4      |
| time/               |          |
|    episodes         | 1200     |
|    fps              | 177      |
|    time_elapsed     | 607      |
|    total_timesteps  | 107881   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00231  |
|    n_updates        | 24470    |
----------------------------------
Eval num_timesteps=108000, episode_reward=4.48 +/- 2.65
Episode length: 120.58 +/- 39.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.48     |
| rollout/            |          |
|    exploration_rate | 0.399    |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00728  |
|    n_updates        | 24499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.08     |
|    exploration_rate | 0.396    |
| time/               |          |
|    episodes         | 1204     |
|    fps              | 177      |
|    time_elapsed     | 610      |
|    total_timesteps  | 108355   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00413  |
|    n_updates        | 24588    |
----------------------------------
Eval num_timesteps=108500, episode_reward=4.68 +/- 2.13
Episode length: 124.80 +/- 36.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.68     |
| rollout/            |          |
|    exploration_rate | 0.395    |
| time/               |          |
|    total_timesteps  | 108500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00364  |
|    n_updates        | 24624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 118      |
|    ep_rew_mean      | 4.06     |
|    exploration_rate | 0.392    |
| time/               |          |
|    episodes         | 1208     |
|    fps              | 177      |
|    time_elapsed     | 614      |
|    total_timesteps  | 108809   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0093   |
|    n_updates        | 24702    |
----------------------------------
Eval num_timesteps=109000, episode_reward=2.06 +/- 1.63
Episode length: 91.14 +/- 23.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.1     |
|    mean_reward      | 2.06     |
| rollout/            |          |
|    exploration_rate | 0.39     |
| time/               |          |
|    total_timesteps  | 109000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 24749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.13     |
|    exploration_rate | 0.387    |
| time/               |          |
|    episodes         | 1212     |
|    fps              | 177      |
|    time_elapsed     | 617      |
|    total_timesteps  | 109456   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00454  |
|    n_updates        | 24863    |
----------------------------------
Eval num_timesteps=109500, episode_reward=3.80 +/- 2.21
Episode length: 110.22 +/- 39.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 110      |
|    mean_reward      | 3.8      |
| rollout/            |          |
|    exploration_rate | 0.386    |
| time/               |          |
|    total_timesteps  | 109500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0216   |
|    n_updates        | 24874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.09     |
|    exploration_rate | 0.382    |
| time/               |          |
|    episodes         | 1216     |
|    fps              | 177      |
|    time_elapsed     | 621      |
|    total_timesteps  | 109986   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00518  |
|    n_updates        | 24996    |
----------------------------------
Eval num_timesteps=110000, episode_reward=3.02 +/- 2.08
Episode length: 96.54 +/- 32.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.5     |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.382    |
| time/               |          |
|    total_timesteps  | 110000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0113   |
|    n_updates        | 24999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 119      |
|    ep_rew_mean      | 4.13     |
|    exploration_rate | 0.378    |
| time/               |          |
|    episodes         | 1220     |
|    fps              | 176      |
|    time_elapsed     | 624      |
|    total_timesteps  | 110468   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 25116    |
----------------------------------
Eval num_timesteps=110500, episode_reward=12.98 +/- 4.47
Episode length: 249.96 +/- 70.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 250      |
|    mean_reward      | 13       |
| rollout/            |          |
|    exploration_rate | 0.378    |
| time/               |          |
|    total_timesteps  | 110500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0158   |
|    n_updates        | 25124    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 120      |
|    ep_rew_mean      | 4.24     |
|    exploration_rate | 0.374    |
| time/               |          |
|    episodes         | 1224     |
|    fps              | 175      |
|    time_elapsed     | 631      |
|    total_timesteps  | 110980   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00581  |
|    n_updates        | 25244    |
----------------------------------
Eval num_timesteps=111000, episode_reward=5.16 +/- 3.00
Episode length: 135.02 +/- 54.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 5.16     |
| rollout/            |          |
|    exploration_rate | 0.374    |
| time/               |          |
|    total_timesteps  | 111000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00915  |
|    n_updates        | 25249    |
----------------------------------
Eval num_timesteps=111500, episode_reward=14.84 +/- 3.32
Episode length: 257.32 +/- 51.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 257      |
|    mean_reward      | 14.8     |
| rollout/            |          |
|    exploration_rate | 0.369    |
| time/               |          |
|    total_timesteps  | 111500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0175   |
|    n_updates        | 25374    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.32     |
|    exploration_rate | 0.369    |
| time/               |          |
|    episodes         | 1228     |
|    fps              | 173      |
|    time_elapsed     | 643      |
|    total_timesteps  | 111544   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00954  |
|    n_updates        | 25385    |
----------------------------------
Eval num_timesteps=112000, episode_reward=5.68 +/- 2.47
Episode length: 132.06 +/- 39.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 5.68     |
| rollout/            |          |
|    exploration_rate | 0.365    |
| time/               |          |
|    total_timesteps  | 112000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 25499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.37     |
|    exploration_rate | 0.365    |
| time/               |          |
|    episodes         | 1232     |
|    fps              | 173      |
|    time_elapsed     | 647      |
|    total_timesteps  | 112004   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0255   |
|    n_updates        | 25500    |
----------------------------------
Eval num_timesteps=112500, episode_reward=6.48 +/- 2.54
Episode length: 149.54 +/- 42.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 6.48     |
| rollout/            |          |
|    exploration_rate | 0.361    |
| time/               |          |
|    total_timesteps  | 112500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0127   |
|    n_updates        | 25624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 121      |
|    ep_rew_mean      | 4.35     |
|    exploration_rate | 0.361    |
| time/               |          |
|    episodes         | 1236     |
|    fps              | 172      |
|    time_elapsed     | 651      |
|    total_timesteps  | 112540   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0163   |
|    n_updates        | 25634    |
----------------------------------
Eval num_timesteps=113000, episode_reward=2.78 +/- 2.14
Episode length: 92.70 +/- 32.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.7     |
|    mean_reward      | 2.78     |
| rollout/            |          |
|    exploration_rate | 0.357    |
| time/               |          |
|    total_timesteps  | 113000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0125   |
|    n_updates        | 25749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.41     |
|    exploration_rate | 0.357    |
| time/               |          |
|    episodes         | 1240     |
|    fps              | 172      |
|    time_elapsed     | 654      |
|    total_timesteps  | 113007   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 25751    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 122      |
|    ep_rew_mean      | 4.41     |
|    exploration_rate | 0.353    |
| time/               |          |
|    episodes         | 1244     |
|    fps              | 173      |
|    time_elapsed     | 655      |
|    total_timesteps  | 113457   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00521  |
|    n_updates        | 25864    |
----------------------------------
Eval num_timesteps=113500, episode_reward=4.24 +/- 1.97
Episode length: 114.14 +/- 27.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 4.24     |
| rollout/            |          |
|    exploration_rate | 0.352    |
| time/               |          |
|    total_timesteps  | 113500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.023    |
|    n_updates        | 25874    |
----------------------------------
Eval num_timesteps=114000, episode_reward=3.62 +/- 2.65
Episode length: 103.30 +/- 41.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.62     |
| rollout/            |          |
|    exploration_rate | 0.348    |
| time/               |          |
|    total_timesteps  | 114000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 25999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 125      |
|    ep_rew_mean      | 4.64     |
|    exploration_rate | 0.347    |
| time/               |          |
|    episodes         | 1248     |
|    fps              | 172      |
|    time_elapsed     | 661      |
|    total_timesteps  | 114125   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00732  |
|    n_updates        | 26031    |
----------------------------------
Eval num_timesteps=114500, episode_reward=5.70 +/- 3.94
Episode length: 130.46 +/- 59.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 130      |
|    mean_reward      | 5.7      |
| rollout/            |          |
|    exploration_rate | 0.344    |
| time/               |          |
|    total_timesteps  | 114500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00753  |
|    n_updates        | 26124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 128      |
|    ep_rew_mean      | 4.83     |
|    exploration_rate | 0.34     |
| time/               |          |
|    episodes         | 1252     |
|    fps              | 172      |
|    time_elapsed     | 666      |
|    total_timesteps  | 114890   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00593  |
|    n_updates        | 26222    |
----------------------------------
Eval num_timesteps=115000, episode_reward=3.12 +/- 3.10
Episode length: 100.12 +/- 45.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 3.12     |
| rollout/            |          |
|    exploration_rate | 0.339    |
| time/               |          |
|    total_timesteps  | 115000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0246   |
|    n_updates        | 26249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 129      |
|    ep_rew_mean      | 4.94     |
|    exploration_rate | 0.336    |
| time/               |          |
|    episodes         | 1256     |
|    fps              | 172      |
|    time_elapsed     | 669      |
|    total_timesteps  | 115368   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 26341    |
----------------------------------
Eval num_timesteps=115500, episode_reward=2.88 +/- 2.66
Episode length: 96.68 +/- 35.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 96.7     |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.335    |
| time/               |          |
|    total_timesteps  | 115500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 26374    |
----------------------------------
Eval num_timesteps=116000, episode_reward=5.96 +/- 3.91
Episode length: 138.54 +/- 59.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 5.96     |
| rollout/            |          |
|    exploration_rate | 0.331    |
| time/               |          |
|    total_timesteps  | 116000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 26499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 131      |
|    ep_rew_mean      | 5.05     |
|    exploration_rate | 0.33     |
| time/               |          |
|    episodes         | 1260     |
|    fps              | 171      |
|    time_elapsed     | 676      |
|    total_timesteps  | 116118   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00726  |
|    n_updates        | 26529    |
----------------------------------
Eval num_timesteps=116500, episode_reward=3.80 +/- 2.92
Episode length: 111.66 +/- 43.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 3.8      |
| rollout/            |          |
|    exploration_rate | 0.326    |
| time/               |          |
|    total_timesteps  | 116500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0202   |
|    n_updates        | 26624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 133      |
|    ep_rew_mean      | 5.21     |
|    exploration_rate | 0.325    |
| time/               |          |
|    episodes         | 1264     |
|    fps              | 171      |
|    time_elapsed     | 679      |
|    total_timesteps  | 116624   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00731  |
|    n_updates        | 26655    |
----------------------------------
Eval num_timesteps=117000, episode_reward=3.62 +/- 2.81
Episode length: 105.52 +/- 43.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.62     |
| rollout/            |          |
|    exploration_rate | 0.322    |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 26749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 132      |
|    ep_rew_mean      | 5.23     |
|    exploration_rate | 0.321    |
| time/               |          |
|    episodes         | 1268     |
|    fps              | 171      |
|    time_elapsed     | 683      |
|    total_timesteps  | 117116   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00927  |
|    n_updates        | 26778    |
----------------------------------
Eval num_timesteps=117500, episode_reward=5.06 +/- 3.16
Episode length: 131.56 +/- 49.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 5.06     |
| rollout/            |          |
|    exploration_rate | 0.318    |
| time/               |          |
|    total_timesteps  | 117500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0201   |
|    n_updates        | 26874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.34     |
|    exploration_rate | 0.316    |
| time/               |          |
|    episodes         | 1272     |
|    fps              | 171      |
|    time_elapsed     | 687      |
|    total_timesteps  | 117737   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0032   |
|    n_updates        | 26934    |
----------------------------------
Eval num_timesteps=118000, episode_reward=6.24 +/- 3.35
Episode length: 141.00 +/- 55.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 141      |
|    mean_reward      | 6.24     |
| rollout/            |          |
|    exploration_rate | 0.313    |
| time/               |          |
|    total_timesteps  | 118000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00637  |
|    n_updates        | 26999    |
----------------------------------
Eval num_timesteps=118500, episode_reward=4.98 +/- 2.75
Episode length: 125.02 +/- 43.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.98     |
| rollout/            |          |
|    exploration_rate | 0.309    |
| time/               |          |
|    total_timesteps  | 118500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0168   |
|    n_updates        | 27124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 5.64     |
|    exploration_rate | 0.309    |
| time/               |          |
|    episodes         | 1276     |
|    fps              | 170      |
|    time_elapsed     | 695      |
|    total_timesteps  | 118504   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0419   |
|    n_updates        | 27125    |
----------------------------------
Eval num_timesteps=119000, episode_reward=8.82 +/- 5.23
Episode length: 180.68 +/- 75.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 8.82     |
| rollout/            |          |
|    exploration_rate | 0.305    |
| time/               |          |
|    total_timesteps  | 119000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0364   |
|    n_updates        | 27249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 5.73     |
|    exploration_rate | 0.304    |
| time/               |          |
|    episodes         | 1280     |
|    fps              | 169      |
|    time_elapsed     | 700      |
|    total_timesteps  | 119038   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0595   |
|    n_updates        | 27259    |
----------------------------------
Eval num_timesteps=119500, episode_reward=4.18 +/- 2.90
Episode length: 118.18 +/- 43.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.18     |
| rollout/            |          |
|    exploration_rate | 0.3      |
| time/               |          |
|    total_timesteps  | 119500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0198   |
|    n_updates        | 27374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 5.84     |
|    exploration_rate | 0.3      |
| time/               |          |
|    episodes         | 1284     |
|    fps              | 169      |
|    time_elapsed     | 704      |
|    total_timesteps  | 119582   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0181   |
|    n_updates        | 27395    |
----------------------------------
Eval num_timesteps=120000, episode_reward=5.26 +/- 2.78
Episode length: 127.44 +/- 43.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 5.26     |
| rollout/            |          |
|    exploration_rate | 0.296    |
| time/               |          |
|    total_timesteps  | 120000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 27499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 5.79     |
|    exploration_rate | 0.296    |
| time/               |          |
|    episodes         | 1288     |
|    fps              | 169      |
|    time_elapsed     | 708      |
|    total_timesteps  | 120026   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00936  |
|    n_updates        | 27506    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 5.72     |
|    exploration_rate | 0.292    |
| time/               |          |
|    episodes         | 1292     |
|    fps              | 169      |
|    time_elapsed     | 708      |
|    total_timesteps  | 120406   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0237   |
|    n_updates        | 27601    |
----------------------------------
Eval num_timesteps=120500, episode_reward=3.50 +/- 2.39
Episode length: 101.18 +/- 33.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 101      |
|    mean_reward      | 3.5      |
| rollout/            |          |
|    exploration_rate | 0.292    |
| time/               |          |
|    total_timesteps  | 120500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00703  |
|    n_updates        | 27624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 5.59     |
|    exploration_rate | 0.289    |
| time/               |          |
|    episodes         | 1296     |
|    fps              | 169      |
|    time_elapsed     | 712      |
|    total_timesteps  | 120816   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00584  |
|    n_updates        | 27703    |
----------------------------------
Eval num_timesteps=121000, episode_reward=2.96 +/- 2.14
Episode length: 101.78 +/- 30.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.287    |
| time/               |          |
|    total_timesteps  | 121000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00915  |
|    n_updates        | 27749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.56     |
|    exploration_rate | 0.285    |
| time/               |          |
|    episodes         | 1300     |
|    fps              | 169      |
|    time_elapsed     | 715      |
|    total_timesteps  | 121278   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 27819    |
----------------------------------
Eval num_timesteps=121500, episode_reward=16.58 +/- 4.78
Episode length: 280.28 +/- 66.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 280      |
|    mean_reward      | 16.6     |
| rollout/            |          |
|    exploration_rate | 0.283    |
| time/               |          |
|    total_timesteps  | 121500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0449   |
|    n_updates        | 27874    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.6      |
|    exploration_rate | 0.28     |
| time/               |          |
|    episodes         | 1304     |
|    fps              | 168      |
|    time_elapsed     | 723      |
|    total_timesteps  | 121780   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0258   |
|    n_updates        | 27944    |
----------------------------------
Eval num_timesteps=122000, episode_reward=12.96 +/- 3.78
Episode length: 231.08 +/- 55.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 13       |
| rollout/            |          |
|    exploration_rate | 0.278    |
| time/               |          |
|    total_timesteps  | 122000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0136   |
|    n_updates        | 27999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 5.69     |
|    exploration_rate | 0.275    |
| time/               |          |
|    episodes         | 1308     |
|    fps              | 167      |
|    time_elapsed     | 730      |
|    total_timesteps  | 122376   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0192   |
|    n_updates        | 28093    |
----------------------------------
Eval num_timesteps=122500, episode_reward=2.52 +/- 1.85
Episode length: 90.60 +/- 28.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.6     |
|    mean_reward      | 2.52     |
| rollout/            |          |
|    exploration_rate | 0.274    |
| time/               |          |
|    total_timesteps  | 122500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 28124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 5.6      |
|    exploration_rate | 0.271    |
| time/               |          |
|    episodes         | 1312     |
|    fps              | 167      |
|    time_elapsed     | 733      |
|    total_timesteps  | 122839   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 28209    |
----------------------------------
Eval num_timesteps=123000, episode_reward=4.78 +/- 2.42
Episode length: 115.22 +/- 35.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.269    |
| time/               |          |
|    total_timesteps  | 123000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0125   |
|    n_updates        | 28249    |
----------------------------------
Eval num_timesteps=123500, episode_reward=4.26 +/- 2.36
Episode length: 111.38 +/- 37.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 4.26     |
| rollout/            |          |
|    exploration_rate | 0.265    |
| time/               |          |
|    total_timesteps  | 123500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00522  |
|    n_updates        | 28374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 5.75     |
|    exploration_rate | 0.264    |
| time/               |          |
|    episodes         | 1316     |
|    fps              | 166      |
|    time_elapsed     | 740      |
|    total_timesteps  | 123590   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 28397    |
----------------------------------
Eval num_timesteps=124000, episode_reward=4.32 +/- 3.11
Episode length: 118.66 +/- 46.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.32     |
| rollout/            |          |
|    exploration_rate | 0.26     |
| time/               |          |
|    total_timesteps  | 124000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00875  |
|    n_updates        | 28499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 5.72     |
|    exploration_rate | 0.26     |
| time/               |          |
|    episodes         | 1320     |
|    fps              | 166      |
|    time_elapsed     | 743      |
|    total_timesteps  | 124095   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.023    |
|    n_updates        | 28523    |
----------------------------------
Eval num_timesteps=124500, episode_reward=6.32 +/- 2.38
Episode length: 142.72 +/- 38.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 143      |
|    mean_reward      | 6.32     |
| rollout/            |          |
|    exploration_rate | 0.256    |
| time/               |          |
|    total_timesteps  | 124500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0135   |
|    n_updates        | 28624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 5.73     |
|    exploration_rate | 0.255    |
| time/               |          |
|    episodes         | 1324     |
|    fps              | 166      |
|    time_elapsed     | 748      |
|    total_timesteps  | 124592   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00399  |
|    n_updates        | 28647    |
----------------------------------
Eval num_timesteps=125000, episode_reward=5.40 +/- 2.81
Episode length: 132.26 +/- 48.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 5.4      |
| rollout/            |          |
|    exploration_rate | 0.251    |
| time/               |          |
|    total_timesteps  | 125000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0125   |
|    n_updates        | 28749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 5.75     |
|    exploration_rate | 0.25     |
| time/               |          |
|    episodes         | 1328     |
|    fps              | 166      |
|    time_elapsed     | 752      |
|    total_timesteps  | 125188   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0196   |
|    n_updates        | 28796    |
----------------------------------
Eval num_timesteps=125500, episode_reward=8.56 +/- 4.69
Episode length: 174.20 +/- 65.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 174      |
|    mean_reward      | 8.56     |
| rollout/            |          |
|    exploration_rate | 0.247    |
| time/               |          |
|    total_timesteps  | 125500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00993  |
|    n_updates        | 28874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 5.89     |
|    exploration_rate | 0.245    |
| time/               |          |
|    episodes         | 1332     |
|    fps              | 165      |
|    time_elapsed     | 757      |
|    total_timesteps  | 125762   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0121   |
|    n_updates        | 28940    |
----------------------------------
Eval num_timesteps=126000, episode_reward=5.02 +/- 2.95
Episode length: 124.30 +/- 42.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.02     |
| rollout/            |          |
|    exploration_rate | 0.242    |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0101   |
|    n_updates        | 28999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 5.91     |
|    exploration_rate | 0.24     |
| time/               |          |
|    episodes         | 1336     |
|    fps              | 165      |
|    time_elapsed     | 761      |
|    total_timesteps  | 126284   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00777  |
|    n_updates        | 29070    |
----------------------------------
Eval num_timesteps=126500, episode_reward=4.62 +/- 3.04
Episode length: 123.56 +/- 47.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 4.62     |
| rollout/            |          |
|    exploration_rate | 0.238    |
| time/               |          |
|    total_timesteps  | 126500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 29124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 6.02     |
|    exploration_rate | 0.235    |
| time/               |          |
|    episodes         | 1340     |
|    fps              | 165      |
|    time_elapsed     | 765      |
|    total_timesteps  | 126854   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00926  |
|    n_updates        | 29213    |
----------------------------------
Eval num_timesteps=127000, episode_reward=4.46 +/- 2.89
Episode length: 120.38 +/- 40.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 4.46     |
| rollout/            |          |
|    exploration_rate | 0.233    |
| time/               |          |
|    total_timesteps  | 127000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 29249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 6.09     |
|    exploration_rate | 0.23     |
| time/               |          |
|    episodes         | 1344     |
|    fps              | 165      |
|    time_elapsed     | 769      |
|    total_timesteps  | 127342   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0189   |
|    n_updates        | 29335    |
----------------------------------
Eval num_timesteps=127500, episode_reward=4.78 +/- 2.52
Episode length: 106.88 +/- 38.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.229    |
| time/               |          |
|    total_timesteps  | 127500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0276   |
|    n_updates        | 29374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 5.98     |
|    exploration_rate | 0.226    |
| time/               |          |
|    episodes         | 1348     |
|    fps              | 165      |
|    time_elapsed     | 772      |
|    total_timesteps  | 127871   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 29467    |
----------------------------------
Eval num_timesteps=128000, episode_reward=5.06 +/- 2.84
Episode length: 118.94 +/- 43.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 5.06     |
| rollout/            |          |
|    exploration_rate | 0.224    |
| time/               |          |
|    total_timesteps  | 128000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0326   |
|    n_updates        | 29499    |
----------------------------------
Eval num_timesteps=128500, episode_reward=4.60 +/- 3.91
Episode length: 122.42 +/- 55.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 4.6      |
| rollout/            |          |
|    exploration_rate | 0.22     |
| time/               |          |
|    total_timesteps  | 128500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0294   |
|    n_updates        | 29624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 6.03     |
|    exploration_rate | 0.218    |
| time/               |          |
|    episodes         | 1352     |
|    fps              | 164      |
|    time_elapsed     | 780      |
|    total_timesteps  | 128673   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 29668    |
----------------------------------
Eval num_timesteps=129000, episode_reward=4.66 +/- 2.05
Episode length: 121.34 +/- 36.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 4.66     |
| rollout/            |          |
|    exploration_rate | 0.215    |
| time/               |          |
|    total_timesteps  | 129000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00582  |
|    n_updates        | 29749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 6.11     |
|    exploration_rate | 0.213    |
| time/               |          |
|    episodes         | 1356     |
|    fps              | 164      |
|    time_elapsed     | 784      |
|    total_timesteps  | 129275   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0194   |
|    n_updates        | 29818    |
----------------------------------
Eval num_timesteps=129500, episode_reward=3.02 +/- 1.98
Episode length: 94.72 +/- 26.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.7     |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.211    |
| time/               |          |
|    total_timesteps  | 129500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00927  |
|    n_updates        | 29874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 5.97     |
|    exploration_rate | 0.208    |
| time/               |          |
|    episodes         | 1360     |
|    fps              | 164      |
|    time_elapsed     | 787      |
|    total_timesteps  | 129771   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 29942    |
----------------------------------
Eval num_timesteps=130000, episode_reward=2.84 +/- 1.91
Episode length: 94.06 +/- 29.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.1     |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.206    |
| time/               |          |
|    total_timesteps  | 130000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0119   |
|    n_updates        | 29999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 5.93     |
|    exploration_rate | 0.204    |
| time/               |          |
|    episodes         | 1364     |
|    fps              | 164      |
|    time_elapsed     | 790      |
|    total_timesteps  | 130232   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0194   |
|    n_updates        | 30057    |
----------------------------------
Eval num_timesteps=130500, episode_reward=4.64 +/- 2.66
Episode length: 111.46 +/- 41.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 111      |
|    mean_reward      | 4.64     |
| rollout/            |          |
|    exploration_rate | 0.202    |
| time/               |          |
|    total_timesteps  | 130500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0137   |
|    n_updates        | 30124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 5.99     |
|    exploration_rate | 0.199    |
| time/               |          |
|    episodes         | 1368     |
|    fps              | 164      |
|    time_elapsed     | 793      |
|    total_timesteps  | 130805   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0129   |
|    n_updates        | 30201    |
----------------------------------
Eval num_timesteps=131000, episode_reward=2.88 +/- 2.05
Episode length: 93.72 +/- 31.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.7     |
|    mean_reward      | 2.88     |
| rollout/            |          |
|    exploration_rate | 0.197    |
| time/               |          |
|    total_timesteps  | 131000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 30249    |
----------------------------------
Eval num_timesteps=131500, episode_reward=5.22 +/- 2.98
Episode length: 125.44 +/- 43.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 5.22     |
| rollout/            |          |
|    exploration_rate | 0.192    |
| time/               |          |
|    total_timesteps  | 131500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00693  |
|    n_updates        | 30374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 6.07     |
|    exploration_rate | 0.192    |
| time/               |          |
|    episodes         | 1372     |
|    fps              | 164      |
|    time_elapsed     | 800      |
|    total_timesteps  | 131516   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 30378    |
----------------------------------
Eval num_timesteps=132000, episode_reward=2.82 +/- 1.95
Episode length: 98.14 +/- 28.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.1     |
|    mean_reward      | 2.82     |
| rollout/            |          |
|    exploration_rate | 0.188    |
| time/               |          |
|    total_timesteps  | 132000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00928  |
|    n_updates        | 30499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 6.06     |
|    exploration_rate | 0.186    |
| time/               |          |
|    episodes         | 1376     |
|    fps              | 164      |
|    time_elapsed     | 803      |
|    total_timesteps  | 132206   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0202   |
|    n_updates        | 30551    |
----------------------------------
Eval num_timesteps=132500, episode_reward=9.84 +/- 2.74
Episode length: 188.12 +/- 37.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 188      |
|    mean_reward      | 9.84     |
| rollout/            |          |
|    exploration_rate | 0.183    |
| time/               |          |
|    total_timesteps  | 132500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0195   |
|    n_updates        | 30624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 6.02     |
|    exploration_rate | 0.181    |
| time/               |          |
|    episodes         | 1380     |
|    fps              | 163      |
|    time_elapsed     | 809      |
|    total_timesteps  | 132739   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 30684    |
----------------------------------
Eval num_timesteps=133000, episode_reward=6.16 +/- 3.43
Episode length: 144.00 +/- 54.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 6.16     |
| rollout/            |          |
|    exploration_rate | 0.178    |
| time/               |          |
|    total_timesteps  | 133000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 30749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 6.03     |
|    exploration_rate | 0.176    |
| time/               |          |
|    episodes         | 1384     |
|    fps              | 163      |
|    time_elapsed     | 814      |
|    total_timesteps  | 133280   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0107   |
|    n_updates        | 30819    |
----------------------------------
Eval num_timesteps=133500, episode_reward=5.24 +/- 4.24
Episode length: 127.60 +/- 62.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 5.24     |
| rollout/            |          |
|    exploration_rate | 0.174    |
| time/               |          |
|    total_timesteps  | 133500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00669  |
|    n_updates        | 30874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 6.21     |
|    exploration_rate | 0.17     |
| time/               |          |
|    episodes         | 1388     |
|    fps              | 163      |
|    time_elapsed     | 818      |
|    total_timesteps  | 133932   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 30982    |
----------------------------------
Eval num_timesteps=134000, episode_reward=4.68 +/- 2.85
Episode length: 118.96 +/- 43.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 4.68     |
| rollout/            |          |
|    exploration_rate | 0.169    |
| time/               |          |
|    total_timesteps  | 134000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 30999    |
----------------------------------
Eval num_timesteps=134500, episode_reward=8.26 +/- 5.28
Episode length: 170.16 +/- 75.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 8.26     |
| rollout/            |          |
|    exploration_rate | 0.165    |
| time/               |          |
|    total_timesteps  | 134500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00942  |
|    n_updates        | 31124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 6.37     |
|    exploration_rate | 0.164    |
| time/               |          |
|    episodes         | 1392     |
|    fps              | 162      |
|    time_elapsed     | 826      |
|    total_timesteps  | 134563   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 31140    |
----------------------------------
Eval num_timesteps=135000, episode_reward=6.84 +/- 3.60
Episode length: 143.76 +/- 55.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 6.84     |
| rollout/            |          |
|    exploration_rate | 0.16     |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 31249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 6.55     |
|    exploration_rate | 0.158    |
| time/               |          |
|    episodes         | 1396     |
|    fps              | 162      |
|    time_elapsed     | 831      |
|    total_timesteps  | 135197   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00527  |
|    n_updates        | 31299    |
----------------------------------
Eval num_timesteps=135500, episode_reward=4.94 +/- 2.43
Episode length: 126.28 +/- 42.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 4.94     |
| rollout/            |          |
|    exploration_rate | 0.155    |
| time/               |          |
|    total_timesteps  | 135500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 31374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 147      |
|    ep_rew_mean      | 6.74     |
|    exploration_rate | 0.151    |
| time/               |          |
|    episodes         | 1400     |
|    fps              | 162      |
|    time_elapsed     | 835      |
|    total_timesteps  | 135979   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00901  |
|    n_updates        | 31494    |
----------------------------------
Eval num_timesteps=136000, episode_reward=13.10 +/- 5.58
Episode length: 222.28 +/- 74.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 13.1     |
| rollout/            |          |
|    exploration_rate | 0.151    |
| time/               |          |
|    total_timesteps  | 136000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 31499    |
----------------------------------
Eval num_timesteps=136500, episode_reward=8.04 +/- 1.98
Episode length: 169.96 +/- 32.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 8.04     |
| rollout/            |          |
|    exploration_rate | 0.146    |
| time/               |          |
|    total_timesteps  | 136500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00652  |
|    n_updates        | 31624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 148      |
|    ep_rew_mean      | 6.84     |
|    exploration_rate | 0.145    |
| time/               |          |
|    episodes         | 1404     |
|    fps              | 161      |
|    time_elapsed     | 846      |
|    total_timesteps  | 136577   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 31644    |
----------------------------------
Eval num_timesteps=137000, episode_reward=5.76 +/- 3.46
Episode length: 133.56 +/- 51.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 5.76     |
| rollout/            |          |
|    exploration_rate | 0.141    |
| time/               |          |
|    total_timesteps  | 137000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0274   |
|    n_updates        | 31749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 147      |
|    ep_rew_mean      | 6.85     |
|    exploration_rate | 0.14     |
| time/               |          |
|    episodes         | 1408     |
|    fps              | 161      |
|    time_elapsed     | 851      |
|    total_timesteps  | 137123   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0189   |
|    n_updates        | 31780    |
----------------------------------
Eval num_timesteps=137500, episode_reward=11.44 +/- 2.96
Episode length: 203.82 +/- 40.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 11.4     |
| rollout/            |          |
|    exploration_rate | 0.136    |
| time/               |          |
|    total_timesteps  | 137500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00803  |
|    n_updates        | 31874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 148      |
|    ep_rew_mean      | 6.93     |
|    exploration_rate | 0.135    |
| time/               |          |
|    episodes         | 1412     |
|    fps              | 160      |
|    time_elapsed     | 857      |
|    total_timesteps  | 137687   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0175   |
|    n_updates        | 31921    |
----------------------------------
Eval num_timesteps=138000, episode_reward=6.60 +/- 3.26
Episode length: 149.72 +/- 51.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 6.6      |
| rollout/            |          |
|    exploration_rate | 0.132    |
| time/               |          |
|    total_timesteps  | 138000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 31999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 149      |
|    ep_rew_mean      | 6.95     |
|    exploration_rate | 0.127    |
| time/               |          |
|    episodes         | 1416     |
|    fps              | 160      |
|    time_elapsed     | 862      |
|    total_timesteps  | 138447   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0107   |
|    n_updates        | 32111    |
----------------------------------
Eval num_timesteps=138500, episode_reward=15.46 +/- 4.01
Episode length: 272.60 +/- 58.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 273      |
|    mean_reward      | 15.5     |
| rollout/            |          |
|    exploration_rate | 0.127    |
| time/               |          |
|    total_timesteps  | 138500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00593  |
|    n_updates        | 32124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 149      |
|    ep_rew_mean      | 6.97     |
|    exploration_rate | 0.122    |
| time/               |          |
|    episodes         | 1420     |
|    fps              | 159      |
|    time_elapsed     | 870      |
|    total_timesteps  | 138982   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00468  |
|    n_updates        | 32245    |
----------------------------------
Eval num_timesteps=139000, episode_reward=14.58 +/- 5.09
Episode length: 259.04 +/- 76.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 259      |
|    mean_reward      | 14.6     |
| rollout/            |          |
|    exploration_rate | 0.122    |
| time/               |          |
|    total_timesteps  | 139000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.059    |
|    n_updates        | 32249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 149      |
|    ep_rew_mean      | 7.01     |
|    exploration_rate | 0.118    |
| time/               |          |
|    episodes         | 1424     |
|    fps              | 158      |
|    time_elapsed     | 877      |
|    total_timesteps  | 139482   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0131   |
|    n_updates        | 32370    |
----------------------------------
Eval num_timesteps=139500, episode_reward=14.38 +/- 2.55
Episode length: 250.10 +/- 33.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 250      |
|    mean_reward      | 14.4     |
| rollout/            |          |
|    exploration_rate | 0.117    |
| time/               |          |
|    total_timesteps  | 139500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00551  |
|    n_updates        | 32374    |
----------------------------------
Eval num_timesteps=140000, episode_reward=9.60 +/- 3.83
Episode length: 178.32 +/- 54.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 178      |
|    mean_reward      | 9.6      |
| rollout/            |          |
|    exploration_rate | 0.113    |
| time/               |          |
|    total_timesteps  | 140000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00763  |
|    n_updates        | 32499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 148      |
|    ep_rew_mean      | 7.01     |
|    exploration_rate | 0.113    |
| time/               |          |
|    episodes         | 1428     |
|    fps              | 157      |
|    time_elapsed     | 890      |
|    total_timesteps  | 140020   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0117   |
|    n_updates        | 32504    |
----------------------------------
Eval num_timesteps=140500, episode_reward=4.38 +/- 2.47
Episode length: 117.92 +/- 35.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.38     |
| rollout/            |          |
|    exploration_rate | 0.108    |
| time/               |          |
|    total_timesteps  | 140500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 32624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 148      |
|    ep_rew_mean      | 6.99     |
|    exploration_rate | 0.108    |
| time/               |          |
|    episodes         | 1432     |
|    fps              | 157      |
|    time_elapsed     | 893      |
|    total_timesteps  | 140522   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00985  |
|    n_updates        | 32630    |
----------------------------------
Eval num_timesteps=141000, episode_reward=12.88 +/- 3.36
Episode length: 232.70 +/- 51.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 233      |
|    mean_reward      | 12.9     |
| rollout/            |          |
|    exploration_rate | 0.103    |
| time/               |          |
|    total_timesteps  | 141000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0153   |
|    n_updates        | 32749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 148      |
|    ep_rew_mean      | 7.08     |
|    exploration_rate | 0.102    |
| time/               |          |
|    episodes         | 1436     |
|    fps              | 156      |
|    time_elapsed     | 900      |
|    total_timesteps  | 141118   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00827  |
|    n_updates        | 32779    |
----------------------------------
Eval num_timesteps=141500, episode_reward=4.30 +/- 2.28
Episode length: 107.80 +/- 32.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 108      |
|    mean_reward      | 4.3      |
| rollout/            |          |
|    exploration_rate | 0.0985   |
| time/               |          |
|    total_timesteps  | 141500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0226   |
|    n_updates        | 32874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 150      |
|    ep_rew_mean      | 7.18     |
|    exploration_rate | 0.0948   |
| time/               |          |
|    episodes         | 1440     |
|    fps              | 156      |
|    time_elapsed     | 904      |
|    total_timesteps  | 141877   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00504  |
|    n_updates        | 32969    |
----------------------------------
Eval num_timesteps=142000, episode_reward=11.62 +/- 3.79
Episode length: 210.22 +/- 52.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 11.6     |
| rollout/            |          |
|    exploration_rate | 0.0937   |
| time/               |          |
|    total_timesteps  | 142000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 32999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 151      |
|    ep_rew_mean      | 7.25     |
|    exploration_rate | 0.0897   |
| time/               |          |
|    episodes         | 1444     |
|    fps              | 156      |
|    time_elapsed     | 910      |
|    total_timesteps  | 142417   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0098   |
|    n_updates        | 33104    |
----------------------------------
Eval num_timesteps=142500, episode_reward=7.50 +/- 4.15
Episode length: 163.50 +/- 62.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 164      |
|    mean_reward      | 7.5      |
| rollout/            |          |
|    exploration_rate | 0.0889   |
| time/               |          |
|    total_timesteps  | 142500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00494  |
|    n_updates        | 33124    |
----------------------------------
Eval num_timesteps=143000, episode_reward=6.88 +/- 3.51
Episode length: 139.36 +/- 50.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 6.88     |
| rollout/            |          |
|    exploration_rate | 0.0841   |
| time/               |          |
|    total_timesteps  | 143000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 33249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 152      |
|    ep_rew_mean      | 7.34     |
|    exploration_rate | 0.0838   |
| time/               |          |
|    episodes         | 1448     |
|    fps              | 155      |
|    time_elapsed     | 919      |
|    total_timesteps  | 143033   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 33258    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 148      |
|    ep_rew_mean      | 7.12     |
|    exploration_rate | 0.0798   |
| time/               |          |
|    episodes         | 1452     |
|    fps              | 155      |
|    time_elapsed     | 920      |
|    total_timesteps  | 143449   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00866  |
|    n_updates        | 33362    |
----------------------------------
Eval num_timesteps=143500, episode_reward=7.26 +/- 1.80
Episode length: 156.86 +/- 28.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 157      |
|    mean_reward      | 7.26     |
| rollout/            |          |
|    exploration_rate | 0.0793   |
| time/               |          |
|    total_timesteps  | 143500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 33374    |
----------------------------------
Eval num_timesteps=144000, episode_reward=12.20 +/- 3.02
Episode length: 219.18 +/- 46.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 12.2     |
| rollout/            |          |
|    exploration_rate | 0.0745   |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0136   |
|    n_updates        | 33499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 149      |
|    ep_rew_mean      | 7.15     |
|    exploration_rate | 0.0731   |
| time/               |          |
|    episodes         | 1456     |
|    fps              | 154      |
|    time_elapsed     | 931      |
|    total_timesteps  | 144143   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0242   |
|    n_updates        | 33535    |
----------------------------------
Eval num_timesteps=144500, episode_reward=12.96 +/- 2.71
Episode length: 228.50 +/- 40.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 13       |
| rollout/            |          |
|    exploration_rate | 0.0696   |
| time/               |          |
|    total_timesteps  | 144500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00851  |
|    n_updates        | 33624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 150      |
|    ep_rew_mean      | 7.3      |
|    exploration_rate | 0.0666   |
| time/               |          |
|    episodes         | 1460     |
|    fps              | 154      |
|    time_elapsed     | 938      |
|    total_timesteps  | 144812   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0117   |
|    n_updates        | 33702    |
----------------------------------
Eval num_timesteps=145000, episode_reward=5.94 +/- 1.68
Episode length: 136.40 +/- 29.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 5.94     |
| rollout/            |          |
|    exploration_rate | 0.0648   |
| time/               |          |
|    total_timesteps  | 145000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00933  |
|    n_updates        | 33749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 152      |
|    ep_rew_mean      | 7.42     |
|    exploration_rate | 0.0602   |
| time/               |          |
|    episodes         | 1464     |
|    fps              | 154      |
|    time_elapsed     | 942      |
|    total_timesteps  | 145477   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 33869    |
----------------------------------
Eval num_timesteps=145500, episode_reward=5.40 +/- 2.73
Episode length: 120.52 +/- 43.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 121      |
|    mean_reward      | 5.4      |
| rollout/            |          |
|    exploration_rate | 0.06     |
| time/               |          |
|    total_timesteps  | 145500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0135   |
|    n_updates        | 33874    |
----------------------------------
Eval num_timesteps=146000, episode_reward=13.06 +/- 3.62
Episode length: 233.02 +/- 51.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 233      |
|    mean_reward      | 13.1     |
| rollout/            |          |
|    exploration_rate | 0.0551   |
| time/               |          |
|    total_timesteps  | 146000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00729  |
|    n_updates        | 33999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 154      |
|    ep_rew_mean      | 7.48     |
|    exploration_rate | 0.0532   |
| time/               |          |
|    episodes         | 1468     |
|    fps              | 153      |
|    time_elapsed     | 953      |
|    total_timesteps  | 146197   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00916  |
|    n_updates        | 34049    |
----------------------------------
Eval num_timesteps=146500, episode_reward=6.12 +/- 3.42
Episode length: 135.08 +/- 51.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 6.12     |
| rollout/            |          |
|    exploration_rate | 0.0502   |
| time/               |          |
|    total_timesteps  | 146500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 34124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 152      |
|    ep_rew_mean      | 7.4      |
|    exploration_rate | 0.0477   |
| time/               |          |
|    episodes         | 1472     |
|    fps              | 153      |
|    time_elapsed     | 957      |
|    total_timesteps  | 146761   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0049   |
|    n_updates        | 34190    |
----------------------------------
Eval num_timesteps=147000, episode_reward=14.74 +/- 2.99
Episode length: 244.04 +/- 49.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 244      |
|    mean_reward      | 14.7     |
| rollout/            |          |
|    exploration_rate | 0.0454   |
| time/               |          |
|    total_timesteps  | 147000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0149   |
|    n_updates        | 34249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 152      |
|    ep_rew_mean      | 7.37     |
|    exploration_rate | 0.0415   |
| time/               |          |
|    episodes         | 1476     |
|    fps              | 152      |
|    time_elapsed     | 964      |
|    total_timesteps  | 147399   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0056   |
|    n_updates        | 34349    |
----------------------------------
Eval num_timesteps=147500, episode_reward=15.32 +/- 3.33
Episode length: 254.52 +/- 49.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 255      |
|    mean_reward      | 15.3     |
| rollout/            |          |
|    exploration_rate | 0.0405   |
| time/               |          |
|    total_timesteps  | 147500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 34374    |
----------------------------------
Eval num_timesteps=148000, episode_reward=4.84 +/- 3.05
Episode length: 113.98 +/- 41.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 4.84     |
| rollout/            |          |
|    exploration_rate | 0.0356   |
| time/               |          |
|    total_timesteps  | 148000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 34499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 153      |
|    ep_rew_mean      | 7.47     |
|    exploration_rate | 0.0352   |
| time/               |          |
|    episodes         | 1480     |
|    fps              | 151      |
|    time_elapsed     | 975      |
|    total_timesteps  | 148039   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 34509    |
----------------------------------
Eval num_timesteps=148500, episode_reward=9.68 +/- 3.48
Episode length: 190.18 +/- 45.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 9.68     |
| rollout/            |          |
|    exploration_rate | 0.0307   |
| time/               |          |
|    total_timesteps  | 148500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 34624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 155      |
|    ep_rew_mean      | 7.6      |
|    exploration_rate | 0.0285   |
| time/               |          |
|    episodes         | 1484     |
|    fps              | 151      |
|    time_elapsed     | 981      |
|    total_timesteps  | 148731   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00558  |
|    n_updates        | 34682    |
----------------------------------
Eval num_timesteps=149000, episode_reward=5.32 +/- 3.22
Episode length: 124.00 +/- 50.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.32     |
| rollout/            |          |
|    exploration_rate | 0.0258   |
| time/               |          |
|    total_timesteps  | 149000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00329  |
|    n_updates        | 34749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 155      |
|    ep_rew_mean      | 7.68     |
|    exploration_rate | 0.0215   |
| time/               |          |
|    episodes         | 1488     |
|    fps              | 151      |
|    time_elapsed     | 985      |
|    total_timesteps  | 149443   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0222   |
|    n_updates        | 34860    |
----------------------------------
Eval num_timesteps=149500, episode_reward=10.98 +/- 3.16
Episode length: 199.84 +/- 47.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 200      |
|    mean_reward      | 11       |
| rollout/            |          |
|    exploration_rate | 0.0209   |
| time/               |          |
|    total_timesteps  | 149500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 34874    |
----------------------------------
Eval num_timesteps=150000, episode_reward=10.24 +/- 4.01
Episode length: 187.18 +/- 61.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 187      |
|    mean_reward      | 10.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 150000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0373   |
|    n_updates        | 34999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 155      |
|    ep_rew_mean      | 7.69     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1492     |
|    fps              | 150      |
|    time_elapsed     | 996      |
|    total_timesteps  | 150034   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 35008    |
----------------------------------
Eval num_timesteps=150500, episode_reward=10.64 +/- 3.76
Episode length: 200.12 +/- 62.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 200      |
|    mean_reward      | 10.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 150500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0185   |
|    n_updates        | 35124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 155      |
|    ep_rew_mean      | 7.71     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1496     |
|    fps              | 150      |
|    time_elapsed     | 1003     |
|    total_timesteps  | 150655   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.006    |
|    n_updates        | 35163    |
----------------------------------
Eval num_timesteps=151000, episode_reward=15.10 +/- 4.04
Episode length: 258.28 +/- 57.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 258      |
|    mean_reward      | 15.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 151000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00867  |
|    n_updates        | 35249    |
----------------------------------
Eval num_timesteps=151500, episode_reward=8.56 +/- 4.04
Episode length: 163.76 +/- 53.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 164      |
|    mean_reward      | 8.56     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 151500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0168   |
|    n_updates        | 35374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 156      |
|    ep_rew_mean      | 7.82     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1500     |
|    fps              | 149      |
|    time_elapsed     | 1015     |
|    total_timesteps  | 151573   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 35393    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 154      |
|    ep_rew_mean      | 7.66     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1504     |
|    fps              | 149      |
|    time_elapsed     | 1015     |
|    total_timesteps  | 151959   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0106   |
|    n_updates        | 35489    |
----------------------------------
Eval num_timesteps=152000, episode_reward=6.14 +/- 3.17
Episode length: 131.64 +/- 49.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 6.14     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 152000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0314   |
|    n_updates        | 35499    |
----------------------------------
Eval num_timesteps=152500, episode_reward=5.48 +/- 3.04
Episode length: 126.36 +/- 41.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 5.48     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 152500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00874  |
|    n_updates        | 35624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 156      |
|    ep_rew_mean      | 7.83     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1508     |
|    fps              | 149      |
|    time_elapsed     | 1023     |
|    total_timesteps  | 152759   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 35689    |
----------------------------------
Eval num_timesteps=153000, episode_reward=6.82 +/- 3.58
Episode length: 151.00 +/- 52.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 6.82     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0151   |
|    n_updates        | 35749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 158      |
|    ep_rew_mean      | 7.97     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1512     |
|    fps              | 149      |
|    time_elapsed     | 1028     |
|    total_timesteps  | 153485   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 35871    |
----------------------------------
Eval num_timesteps=153500, episode_reward=10.82 +/- 2.56
Episode length: 198.34 +/- 37.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | 10.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 153500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 35874    |
----------------------------------
Eval num_timesteps=154000, episode_reward=7.16 +/- 2.58
Episode length: 155.60 +/- 40.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 7.16     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 154000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0202   |
|    n_updates        | 35999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 157      |
|    ep_rew_mean      | 7.91     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1516     |
|    fps              | 148      |
|    time_elapsed     | 1039     |
|    total_timesteps  | 154146   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00532  |
|    n_updates        | 36036    |
----------------------------------
Eval num_timesteps=154500, episode_reward=7.86 +/- 3.61
Episode length: 156.56 +/- 54.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 157      |
|    mean_reward      | 7.86     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 154500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0111   |
|    n_updates        | 36124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 159      |
|    ep_rew_mean      | 8        |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1520     |
|    fps              | 148      |
|    time_elapsed     | 1044     |
|    total_timesteps  | 154857   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 36214    |
----------------------------------
Eval num_timesteps=155000, episode_reward=10.08 +/- 4.23
Episode length: 194.76 +/- 62.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 195      |
|    mean_reward      | 10.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 155000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 36249    |
----------------------------------
Eval num_timesteps=155500, episode_reward=5.94 +/- 3.15
Episode length: 125.18 +/- 43.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 5.94     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 155500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00509  |
|    n_updates        | 36374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 162      |
|    ep_rew_mean      | 8.19     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1524     |
|    fps              | 147      |
|    time_elapsed     | 1053     |
|    total_timesteps  | 155699   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 36424    |
----------------------------------
Eval num_timesteps=156000, episode_reward=8.00 +/- 3.97
Episode length: 165.36 +/- 53.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 165      |
|    mean_reward      | 8        |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 156000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 36499    |
----------------------------------
Eval num_timesteps=156500, episode_reward=8.14 +/- 3.99
Episode length: 144.76 +/- 51.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 8.14     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 156500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0104   |
|    n_updates        | 36624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 165      |
|    ep_rew_mean      | 8.39     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1528     |
|    fps              | 147      |
|    time_elapsed     | 1063     |
|    total_timesteps  | 156541   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00564  |
|    n_updates        | 36635    |
----------------------------------
Eval num_timesteps=157000, episode_reward=5.62 +/- 2.81
Episode length: 122.52 +/- 43.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 5.62     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 157000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0149   |
|    n_updates        | 36749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 168      |
|    ep_rew_mean      | 8.52     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1532     |
|    fps              | 147      |
|    time_elapsed     | 1067     |
|    total_timesteps  | 157314   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 36828    |
----------------------------------
Eval num_timesteps=157500, episode_reward=6.16 +/- 3.18
Episode length: 124.50 +/- 49.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 6.16     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 157500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00443  |
|    n_updates        | 36874    |
----------------------------------
Eval num_timesteps=158000, episode_reward=9.20 +/- 3.30
Episode length: 179.52 +/- 49.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 180      |
|    mean_reward      | 9.2      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 158000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00774  |
|    n_updates        | 36999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 169      |
|    ep_rew_mean      | 8.65     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1536     |
|    fps              | 146      |
|    time_elapsed     | 1076     |
|    total_timesteps  | 158000   |
----------------------------------
Eval num_timesteps=158500, episode_reward=10.10 +/- 4.21
Episode length: 187.50 +/- 61.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 188      |
|    mean_reward      | 10.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 158500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00992  |
|    n_updates        | 37124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 168      |
|    ep_rew_mean      | 8.71     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1540     |
|    fps              | 146      |
|    time_elapsed     | 1082     |
|    total_timesteps  | 158698   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00575  |
|    n_updates        | 37174    |
----------------------------------
Eval num_timesteps=159000, episode_reward=6.06 +/- 3.95
Episode length: 122.44 +/- 51.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 122      |
|    mean_reward      | 6.06     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 159000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 37249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 170      |
|    ep_rew_mean      | 8.83     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1544     |
|    fps              | 146      |
|    time_elapsed     | 1086     |
|    total_timesteps  | 159427   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0115   |
|    n_updates        | 37356    |
----------------------------------
Eval num_timesteps=159500, episode_reward=12.72 +/- 5.25
Episode length: 210.50 +/- 70.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 12.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 159500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00928  |
|    n_updates        | 37374    |
----------------------------------
Eval num_timesteps=160000, episode_reward=11.90 +/- 4.28
Episode length: 217.32 +/- 62.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 11.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 160000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 37499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 172      |
|    ep_rew_mean      | 8.91     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1548     |
|    fps              | 145      |
|    time_elapsed     | 1098     |
|    total_timesteps  | 160215   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0124   |
|    n_updates        | 37553    |
----------------------------------
Eval num_timesteps=160500, episode_reward=4.72 +/- 1.98
Episode length: 118.20 +/- 30.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 4.72     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 160500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00551  |
|    n_updates        | 37624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 175      |
|    ep_rew_mean      | 9.06     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1552     |
|    fps              | 145      |
|    time_elapsed     | 1102     |
|    total_timesteps  | 160919   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0319   |
|    n_updates        | 37729    |
----------------------------------
Eval num_timesteps=161000, episode_reward=12.82 +/- 4.28
Episode length: 219.44 +/- 60.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 12.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 161000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00531  |
|    n_updates        | 37749    |
----------------------------------
Eval num_timesteps=161500, episode_reward=11.50 +/- 3.40
Episode length: 204.88 +/- 52.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 11.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 161500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0126   |
|    n_updates        | 37874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 177      |
|    ep_rew_mean      | 9.24     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1556     |
|    fps              | 145      |
|    time_elapsed     | 1115     |
|    total_timesteps  | 161794   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00796  |
|    n_updates        | 37948    |
----------------------------------
Eval num_timesteps=162000, episode_reward=8.92 +/- 3.55
Episode length: 166.38 +/- 51.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 166      |
|    mean_reward      | 8.92     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0169   |
|    n_updates        | 37999    |
----------------------------------
Eval num_timesteps=162500, episode_reward=9.94 +/- 4.40
Episode length: 190.00 +/- 69.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 9.94     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 162500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0188   |
|    n_updates        | 38124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 177      |
|    ep_rew_mean      | 9.31     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1560     |
|    fps              | 144      |
|    time_elapsed     | 1126     |
|    total_timesteps  | 162554   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00674  |
|    n_updates        | 38138    |
----------------------------------
Eval num_timesteps=163000, episode_reward=3.56 +/- 2.18
Episode length: 95.22 +/- 29.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.2     |
|    mean_reward      | 3.56     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 163000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00767  |
|    n_updates        | 38249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 179      |
|    ep_rew_mean      | 9.39     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1564     |
|    fps              | 144      |
|    time_elapsed     | 1129     |
|    total_timesteps  | 163329   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 38332    |
----------------------------------
Eval num_timesteps=163500, episode_reward=10.16 +/- 2.40
Episode length: 192.96 +/- 46.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 193      |
|    mean_reward      | 10.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 163500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.017    |
|    n_updates        | 38374    |
----------------------------------
Eval num_timesteps=164000, episode_reward=8.78 +/- 3.18
Episode length: 168.38 +/- 47.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 168      |
|    mean_reward      | 8.78     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 164000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 38499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 9.45     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1568     |
|    fps              | 143      |
|    time_elapsed     | 1140     |
|    total_timesteps  | 164011   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00852  |
|    n_updates        | 38502    |
----------------------------------
Eval num_timesteps=164500, episode_reward=6.86 +/- 3.07
Episode length: 138.04 +/- 39.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 6.86     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 164500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00903  |
|    n_updates        | 38624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 9.64     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1572     |
|    fps              | 143      |
|    time_elapsed     | 1144     |
|    total_timesteps  | 164786   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00915  |
|    n_updates        | 38696    |
----------------------------------
Eval num_timesteps=165000, episode_reward=12.54 +/- 4.02
Episode length: 215.78 +/- 59.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 12.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 165000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0104   |
|    n_updates        | 38749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 9.62     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1576     |
|    fps              | 143      |
|    time_elapsed     | 1151     |
|    total_timesteps  | 165372   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00695  |
|    n_updates        | 38842    |
----------------------------------
Eval num_timesteps=165500, episode_reward=8.32 +/- 2.23
Episode length: 169.32 +/- 40.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 169      |
|    mean_reward      | 8.32     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 165500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0173   |
|    n_updates        | 38874    |
----------------------------------
Eval num_timesteps=166000, episode_reward=13.48 +/- 4.17
Episode length: 219.92 +/- 60.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 13.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 166000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00758  |
|    n_updates        | 38999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 9.61     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1580     |
|    fps              | 142      |
|    time_elapsed     | 1162     |
|    total_timesteps  | 166011   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00404  |
|    n_updates        | 39002    |
----------------------------------
Eval num_timesteps=166500, episode_reward=9.30 +/- 3.47
Episode length: 181.36 +/- 48.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 9.3      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 166500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00699  |
|    n_updates        | 39124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 9.61     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1584     |
|    fps              | 142      |
|    time_elapsed     | 1168     |
|    total_timesteps  | 166762   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 39190    |
----------------------------------
Eval num_timesteps=167000, episode_reward=10.86 +/- 4.45
Episode length: 200.96 +/- 70.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 201      |
|    mean_reward      | 10.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 167000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 39249    |
----------------------------------
Eval num_timesteps=167500, episode_reward=7.60 +/- 4.14
Episode length: 148.30 +/- 66.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 7.6      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 167500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0121   |
|    n_updates        | 39374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 182      |
|    ep_rew_mean      | 9.68     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1588     |
|    fps              | 142      |
|    time_elapsed     | 1178     |
|    total_timesteps  | 167631   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00844  |
|    n_updates        | 39407    |
----------------------------------
Eval num_timesteps=168000, episode_reward=6.80 +/- 2.12
Episode length: 152.34 +/- 32.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 152      |
|    mean_reward      | 6.8      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 168000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 39499    |
----------------------------------
Eval num_timesteps=168500, episode_reward=10.54 +/- 4.28
Episode length: 193.82 +/- 63.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | 10.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 168500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00767  |
|    n_updates        | 39624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 186      |
|    ep_rew_mean      | 9.97     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1592     |
|    fps              | 141      |
|    time_elapsed     | 1189     |
|    total_timesteps  | 168632   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00664  |
|    n_updates        | 39657    |
----------------------------------
Eval num_timesteps=169000, episode_reward=10.10 +/- 2.38
Episode length: 180.70 +/- 40.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 10.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 169000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00719  |
|    n_updates        | 39749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 10       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1596     |
|    fps              | 141      |
|    time_elapsed     | 1194     |
|    total_timesteps  | 169375   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0101   |
|    n_updates        | 39843    |
----------------------------------
Eval num_timesteps=169500, episode_reward=10.10 +/- 4.51
Episode length: 186.48 +/- 68.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 186      |
|    mean_reward      | 10.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 169500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0125   |
|    n_updates        | 39874    |
----------------------------------
Eval num_timesteps=170000, episode_reward=8.66 +/- 3.73
Episode length: 163.74 +/- 54.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 164      |
|    mean_reward      | 8.66     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 170000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00911  |
|    n_updates        | 39999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 186      |
|    ep_rew_mean      | 10       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1600     |
|    fps              | 141      |
|    time_elapsed     | 1205     |
|    total_timesteps  | 170191   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00769  |
|    n_updates        | 40047    |
----------------------------------
Eval num_timesteps=170500, episode_reward=5.56 +/- 3.16
Episode length: 119.18 +/- 45.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 5.56     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 170500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00927  |
|    n_updates        | 40124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 10.2     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1604     |
|    fps              | 141      |
|    time_elapsed     | 1209     |
|    total_timesteps  | 170675   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00933  |
|    n_updates        | 40168    |
----------------------------------
Eval num_timesteps=171000, episode_reward=9.18 +/- 3.31
Episode length: 175.34 +/- 48.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 175      |
|    mean_reward      | 9.18     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0107   |
|    n_updates        | 40249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 185      |
|    ep_rew_mean      | 10       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1608     |
|    fps              | 141      |
|    time_elapsed     | 1214     |
|    total_timesteps  | 171259   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0158   |
|    n_updates        | 40314    |
----------------------------------
Eval num_timesteps=171500, episode_reward=2.92 +/- 1.55
Episode length: 96.96 +/- 25.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97       |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 171500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0145   |
|    n_updates        | 40374    |
----------------------------------
Eval num_timesteps=172000, episode_reward=2.96 +/- 2.26
Episode length: 95.44 +/- 34.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 95.4     |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 172000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0106   |
|    n_updates        | 40499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 185      |
|    ep_rew_mean      | 10.1     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1612     |
|    fps              | 140      |
|    time_elapsed     | 1220     |
|    total_timesteps  | 172003   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0292   |
|    n_updates        | 40500    |
----------------------------------
Eval num_timesteps=172500, episode_reward=6.48 +/- 3.13
Episode length: 132.48 +/- 48.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 6.48     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 172500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00994  |
|    n_updates        | 40624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 184      |
|    ep_rew_mean      | 10       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1616     |
|    fps              | 140      |
|    time_elapsed     | 1224     |
|    total_timesteps  | 172551   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 40637    |
----------------------------------
Eval num_timesteps=173000, episode_reward=4.42 +/- 3.24
Episode length: 117.16 +/- 45.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.42     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 173000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00929  |
|    n_updates        | 40749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 182      |
|    ep_rew_mean      | 9.91     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1620     |
|    fps              | 140      |
|    time_elapsed     | 1228     |
|    total_timesteps  | 173018   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00985  |
|    n_updates        | 40754    |
----------------------------------
Eval num_timesteps=173500, episode_reward=10.36 +/- 3.58
Episode length: 189.64 +/- 56.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 10.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 173500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00781  |
|    n_updates        | 40874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 9.68     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1624     |
|    fps              | 140      |
|    time_elapsed     | 1234     |
|    total_timesteps  | 173527   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 40881    |
----------------------------------
Eval num_timesteps=174000, episode_reward=5.34 +/- 3.03
Episode length: 120.36 +/- 43.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 120      |
|    mean_reward      | 5.34     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 174000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.043    |
|    n_updates        | 40999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 176      |
|    ep_rew_mean      | 9.46     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1628     |
|    fps              | 140      |
|    time_elapsed     | 1238     |
|    total_timesteps  | 174091   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0251   |
|    n_updates        | 41022    |
----------------------------------
Eval num_timesteps=174500, episode_reward=16.60 +/- 4.46
Episode length: 277.82 +/- 63.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 278      |
|    mean_reward      | 16.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 174500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 41124    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 174      |
|    ep_rew_mean      | 9.39     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1632     |
|    fps              | 140      |
|    time_elapsed     | 1246     |
|    total_timesteps  | 174718   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0133   |
|    n_updates        | 41179    |
----------------------------------
Eval num_timesteps=175000, episode_reward=9.40 +/- 3.07
Episode length: 174.54 +/- 57.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 175      |
|    mean_reward      | 9.4      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 175000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 41249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 173      |
|    ep_rew_mean      | 9.24     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1636     |
|    fps              | 140      |
|    time_elapsed     | 1251     |
|    total_timesteps  | 175288   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0201   |
|    n_updates        | 41321    |
----------------------------------
Eval num_timesteps=175500, episode_reward=13.66 +/- 3.98
Episode length: 223.06 +/- 57.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 13.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 175500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0131   |
|    n_updates        | 41374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 172      |
|    ep_rew_mean      | 9.07     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1640     |
|    fps              | 139      |
|    time_elapsed     | 1258     |
|    total_timesteps  | 175892   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00402  |
|    n_updates        | 41472    |
----------------------------------
Eval num_timesteps=176000, episode_reward=9.02 +/- 3.91
Episode length: 169.64 +/- 51.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 9.02     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 176000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 41499    |
----------------------------------
Eval num_timesteps=176500, episode_reward=6.04 +/- 2.93
Episode length: 133.54 +/- 48.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 6.04     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 176500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0116   |
|    n_updates        | 41624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 174      |
|    ep_rew_mean      | 9.17     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1644     |
|    fps              | 139      |
|    time_elapsed     | 1267     |
|    total_timesteps  | 176832   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0209   |
|    n_updates        | 41707    |
----------------------------------
Eval num_timesteps=177000, episode_reward=7.04 +/- 2.31
Episode length: 148.90 +/- 33.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 7.04     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 177000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0206   |
|    n_updates        | 41749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 172      |
|    ep_rew_mean      | 9.02     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1648     |
|    fps              | 139      |
|    time_elapsed     | 1272     |
|    total_timesteps  | 177431   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0129   |
|    n_updates        | 41857    |
----------------------------------
Eval num_timesteps=177500, episode_reward=7.12 +/- 2.72
Episode length: 151.36 +/- 38.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 7.12     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 177500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 41874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 170      |
|    ep_rew_mean      | 8.93     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1652     |
|    fps              | 139      |
|    time_elapsed     | 1277     |
|    total_timesteps  | 177917   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0382   |
|    n_updates        | 41979    |
----------------------------------
Eval num_timesteps=178000, episode_reward=11.78 +/- 3.76
Episode length: 201.26 +/- 51.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 201      |
|    mean_reward      | 11.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 178000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 41999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 166      |
|    ep_rew_mean      | 8.69     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1656     |
|    fps              | 139      |
|    time_elapsed     | 1283     |
|    total_timesteps  | 178414   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0177   |
|    n_updates        | 42103    |
----------------------------------
Eval num_timesteps=178500, episode_reward=6.86 +/- 2.60
Episode length: 151.20 +/- 41.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 6.86     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 178500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00793  |
|    n_updates        | 42124    |
----------------------------------
Eval num_timesteps=179000, episode_reward=3.28 +/- 2.18
Episode length: 99.98 +/- 33.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 179000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0115   |
|    n_updates        | 42249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 166      |
|    ep_rew_mean      | 8.65     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1660     |
|    fps              | 138      |
|    time_elapsed     | 1290     |
|    total_timesteps  | 179124   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00859  |
|    n_updates        | 42280    |
----------------------------------
Eval num_timesteps=179500, episode_reward=7.38 +/- 3.72
Episode length: 148.62 +/- 55.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 149      |
|    mean_reward      | 7.38     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 179500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0208   |
|    n_updates        | 42374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 164      |
|    ep_rew_mean      | 8.54     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1664     |
|    fps              | 138      |
|    time_elapsed     | 1295     |
|    total_timesteps  | 179721   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0113   |
|    n_updates        | 42430    |
----------------------------------
Eval num_timesteps=180000, episode_reward=9.44 +/- 3.53
Episode length: 184.22 +/- 49.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 184      |
|    mean_reward      | 9.44     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00952  |
|    n_updates        | 42499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 163      |
|    ep_rew_mean      | 8.45     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1668     |
|    fps              | 138      |
|    time_elapsed     | 1301     |
|    total_timesteps  | 180301   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00975  |
|    n_updates        | 42575    |
----------------------------------
Eval num_timesteps=180500, episode_reward=13.92 +/- 4.93
Episode length: 236.36 +/- 73.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 236      |
|    mean_reward      | 13.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 180500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0145   |
|    n_updates        | 42624    |
----------------------------------
Eval num_timesteps=181000, episode_reward=14.20 +/- 5.39
Episode length: 231.54 +/- 73.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 14.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 181000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 42749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 163      |
|    ep_rew_mean      | 8.47     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1672     |
|    fps              | 137      |
|    time_elapsed     | 1314     |
|    total_timesteps  | 181089   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0155   |
|    n_updates        | 42772    |
----------------------------------
Eval num_timesteps=181500, episode_reward=7.62 +/- 3.25
Episode length: 145.96 +/- 45.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 7.62     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 181500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00568  |
|    n_updates        | 42874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 163      |
|    ep_rew_mean      | 8.4      |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1676     |
|    fps              | 137      |
|    time_elapsed     | 1319     |
|    total_timesteps  | 181638   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00642  |
|    n_updates        | 42909    |
----------------------------------
Eval num_timesteps=182000, episode_reward=12.94 +/- 4.36
Episode length: 222.46 +/- 61.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 12.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 182000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0119   |
|    n_updates        | 42999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 164      |
|    ep_rew_mean      | 8.48     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1680     |
|    fps              | 137      |
|    time_elapsed     | 1326     |
|    total_timesteps  | 182364   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 43090    |
----------------------------------
Eval num_timesteps=182500, episode_reward=9.60 +/- 3.49
Episode length: 180.26 +/- 53.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 180      |
|    mean_reward      | 9.6      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 182500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 43124    |
----------------------------------
Eval num_timesteps=183000, episode_reward=12.28 +/- 4.44
Episode length: 225.26 +/- 68.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 12.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 183000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0125   |
|    n_updates        | 43249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 164      |
|    ep_rew_mean      | 8.56     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1684     |
|    fps              | 136      |
|    time_elapsed     | 1338     |
|    total_timesteps  | 183199   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.01     |
|    n_updates        | 43299    |
----------------------------------
Eval num_timesteps=183500, episode_reward=9.72 +/- 3.12
Episode length: 186.82 +/- 46.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 187      |
|    mean_reward      | 9.72     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 183500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 43374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 161      |
|    ep_rew_mean      | 8.27     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1688     |
|    fps              | 136      |
|    time_elapsed     | 1344     |
|    total_timesteps  | 183682   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00785  |
|    n_updates        | 43420    |
----------------------------------
Eval num_timesteps=184000, episode_reward=10.34 +/- 3.70
Episode length: 189.16 +/- 50.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 189      |
|    mean_reward      | 10.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 184000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 43499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 154      |
|    ep_rew_mean      | 7.85     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1692     |
|    fps              | 136      |
|    time_elapsed     | 1349     |
|    total_timesteps  | 184075   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 43518    |
----------------------------------
Eval num_timesteps=184500, episode_reward=13.98 +/- 4.13
Episode length: 229.86 +/- 57.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 14       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 184500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00681  |
|    n_updates        | 43624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 154      |
|    ep_rew_mean      | 7.81     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1696     |
|    fps              | 136      |
|    time_elapsed     | 1356     |
|    total_timesteps  | 184729   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0486   |
|    n_updates        | 43682    |
----------------------------------
Eval num_timesteps=185000, episode_reward=9.16 +/- 2.78
Episode length: 171.62 +/- 41.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 172      |
|    mean_reward      | 9.16     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 185000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00762  |
|    n_updates        | 43749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 152      |
|    ep_rew_mean      | 7.69     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1700     |
|    fps              | 136      |
|    time_elapsed     | 1362     |
|    total_timesteps  | 185357   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00725  |
|    n_updates        | 43839    |
----------------------------------
Eval num_timesteps=185500, episode_reward=7.96 +/- 3.46
Episode length: 153.84 +/- 52.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 154      |
|    mean_reward      | 7.96     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 185500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 43874    |
----------------------------------
Eval num_timesteps=186000, episode_reward=15.66 +/- 2.82
Episode length: 259.96 +/- 43.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 260      |
|    mean_reward      | 15.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 186000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 43999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 156      |
|    ep_rew_mean      | 7.92     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1704     |
|    fps              | 135      |
|    time_elapsed     | 1374     |
|    total_timesteps  | 186246   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0224   |
|    n_updates        | 44061    |
----------------------------------
Eval num_timesteps=186500, episode_reward=5.84 +/- 2.84
Episode length: 133.96 +/- 41.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 5.84     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 186500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 44124    |
----------------------------------
Eval num_timesteps=187000, episode_reward=10.78 +/- 4.14
Episode length: 193.58 +/- 57.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | 10.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 187000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0427   |
|    n_updates        | 44249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 158      |
|    ep_rew_mean      | 8.09     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1708     |
|    fps              | 135      |
|    time_elapsed     | 1385     |
|    total_timesteps  | 187074   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0052   |
|    n_updates        | 44268    |
----------------------------------
Eval num_timesteps=187500, episode_reward=12.08 +/- 5.08
Episode length: 209.72 +/- 82.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 12.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 187500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 44374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 159      |
|    ep_rew_mean      | 8.22     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1712     |
|    fps              | 135      |
|    time_elapsed     | 1392     |
|    total_timesteps  | 187952   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00514  |
|    n_updates        | 44487    |
----------------------------------
Eval num_timesteps=188000, episode_reward=11.28 +/- 3.62
Episode length: 194.38 +/- 52.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | 11.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 188000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00494  |
|    n_updates        | 44499    |
----------------------------------
Eval num_timesteps=188500, episode_reward=7.96 +/- 2.20
Episode length: 158.96 +/- 34.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 159      |
|    mean_reward      | 7.96     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 188500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 44624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 160      |
|    ep_rew_mean      | 8.28     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1716     |
|    fps              | 134      |
|    time_elapsed     | 1402     |
|    total_timesteps  | 188541   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0153   |
|    n_updates        | 44635    |
----------------------------------
Eval num_timesteps=189000, episode_reward=7.94 +/- 3.49
Episode length: 151.30 +/- 50.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 7.94     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0378   |
|    n_updates        | 44749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 162      |
|    ep_rew_mean      | 8.4      |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1720     |
|    fps              | 134      |
|    time_elapsed     | 1407     |
|    total_timesteps  | 189172   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 44792    |
----------------------------------
Eval num_timesteps=189500, episode_reward=11.24 +/- 2.44
Episode length: 200.86 +/- 39.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 201      |
|    mean_reward      | 11.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 189500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 44874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 162      |
|    ep_rew_mean      | 8.46     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1724     |
|    fps              | 134      |
|    time_elapsed     | 1413     |
|    total_timesteps  | 189764   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 44940    |
----------------------------------
Eval num_timesteps=190000, episode_reward=9.06 +/- 3.34
Episode length: 174.10 +/- 46.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 174      |
|    mean_reward      | 9.06     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 190000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0181   |
|    n_updates        | 44999    |
----------------------------------
Eval num_timesteps=190500, episode_reward=11.50 +/- 4.69
Episode length: 198.90 +/- 60.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 199      |
|    mean_reward      | 11.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 190500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0162   |
|    n_updates        | 45124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 165      |
|    ep_rew_mean      | 8.62     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1728     |
|    fps              | 133      |
|    time_elapsed     | 1424     |
|    total_timesteps  | 190558   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00901  |
|    n_updates        | 45139    |
----------------------------------
Eval num_timesteps=191000, episode_reward=10.18 +/- 3.82
Episode length: 190.22 +/- 56.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 10.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 191000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 45249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 165      |
|    ep_rew_mean      | 8.69     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1732     |
|    fps              | 133      |
|    time_elapsed     | 1430     |
|    total_timesteps  | 191243   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0139   |
|    n_updates        | 45310    |
----------------------------------
Eval num_timesteps=191500, episode_reward=13.10 +/- 4.40
Episode length: 215.40 +/- 64.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 215      |
|    mean_reward      | 13.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 191500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00909  |
|    n_updates        | 45374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 166      |
|    ep_rew_mean      | 8.76     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1736     |
|    fps              | 133      |
|    time_elapsed     | 1436     |
|    total_timesteps  | 191928   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00849  |
|    n_updates        | 45481    |
----------------------------------
Eval num_timesteps=192000, episode_reward=8.44 +/- 3.12
Episode length: 164.04 +/- 46.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 164      |
|    mean_reward      | 8.44     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 192000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 45499    |
----------------------------------
Eval num_timesteps=192500, episode_reward=5.84 +/- 3.38
Episode length: 129.12 +/- 49.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 5.84     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 192500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00633  |
|    n_updates        | 45624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 168      |
|    ep_rew_mean      | 8.94     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1740     |
|    fps              | 133      |
|    time_elapsed     | 1445     |
|    total_timesteps  | 192705   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0306   |
|    n_updates        | 45676    |
----------------------------------
Eval num_timesteps=193000, episode_reward=11.56 +/- 3.40
Episode length: 206.40 +/- 56.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | 11.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 193000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 45749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 166      |
|    ep_rew_mean      | 8.85     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1744     |
|    fps              | 133      |
|    time_elapsed     | 1451     |
|    total_timesteps  | 193400   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00846  |
|    n_updates        | 45849    |
----------------------------------
Eval num_timesteps=193500, episode_reward=5.66 +/- 3.31
Episode length: 114.86 +/- 46.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 5.66     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 193500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.012    |
|    n_updates        | 45874    |
----------------------------------
Eval num_timesteps=194000, episode_reward=14.44 +/- 3.38
Episode length: 242.42 +/- 48.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 242      |
|    mean_reward      | 14.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 194000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0308   |
|    n_updates        | 45999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 168      |
|    ep_rew_mean      | 9.1      |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1748     |
|    fps              | 132      |
|    time_elapsed     | 1462     |
|    total_timesteps  | 194198   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 46049    |
----------------------------------
Eval num_timesteps=194500, episode_reward=10.98 +/- 4.51
Episode length: 185.36 +/- 65.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 185      |
|    mean_reward      | 11       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 194500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0191   |
|    n_updates        | 46124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 170      |
|    ep_rew_mean      | 9.2      |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1752     |
|    fps              | 132      |
|    time_elapsed     | 1468     |
|    total_timesteps  | 194900   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0312   |
|    n_updates        | 46224    |
----------------------------------
Eval num_timesteps=195000, episode_reward=9.66 +/- 3.05
Episode length: 190.88 +/- 51.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 191      |
|    mean_reward      | 9.66     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 195000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0114   |
|    n_updates        | 46249    |
----------------------------------
Eval num_timesteps=195500, episode_reward=8.88 +/- 4.56
Episode length: 161.88 +/- 60.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 162      |
|    mean_reward      | 8.88     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 195500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0395   |
|    n_updates        | 46374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 172      |
|    ep_rew_mean      | 9.3      |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1756     |
|    fps              | 132      |
|    time_elapsed     | 1478     |
|    total_timesteps  | 195594   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00582  |
|    n_updates        | 46398    |
----------------------------------
Eval num_timesteps=196000, episode_reward=7.10 +/- 3.58
Episode length: 151.58 +/- 52.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 152      |
|    mean_reward      | 7.1      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 196000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0261   |
|    n_updates        | 46499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 171      |
|    ep_rew_mean      | 9.31     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1760     |
|    fps              | 132      |
|    time_elapsed     | 1483     |
|    total_timesteps  | 196226   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 46556    |
----------------------------------
Eval num_timesteps=196500, episode_reward=13.30 +/- 3.37
Episode length: 228.18 +/- 53.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 13.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 196500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0198   |
|    n_updates        | 46624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 171      |
|    ep_rew_mean      | 9.32     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1764     |
|    fps              | 132      |
|    time_elapsed     | 1490     |
|    total_timesteps  | 196821   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0104   |
|    n_updates        | 46705    |
----------------------------------
Eval num_timesteps=197000, episode_reward=6.74 +/- 3.94
Episode length: 144.02 +/- 55.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 144      |
|    mean_reward      | 6.74     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 197000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 46749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 170      |
|    ep_rew_mean      | 9.26     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1768     |
|    fps              | 131      |
|    time_elapsed     | 1494     |
|    total_timesteps  | 197286   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00347  |
|    n_updates        | 46821    |
----------------------------------
Eval num_timesteps=197500, episode_reward=6.32 +/- 3.93
Episode length: 128.22 +/- 50.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 6.32     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 197500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0139   |
|    n_updates        | 46874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 169      |
|    ep_rew_mean      | 9.18     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1772     |
|    fps              | 132      |
|    time_elapsed     | 1499     |
|    total_timesteps  | 197970   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00539  |
|    n_updates        | 46992    |
----------------------------------
Eval num_timesteps=198000, episode_reward=11.08 +/- 3.74
Episode length: 188.82 +/- 54.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 189      |
|    mean_reward      | 11.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0202   |
|    n_updates        | 46999    |
----------------------------------
Eval num_timesteps=198500, episode_reward=11.34 +/- 3.43
Episode length: 192.28 +/- 48.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 192      |
|    mean_reward      | 11.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 198500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00429  |
|    n_updates        | 47124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 171      |
|    ep_rew_mean      | 9.29     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1776     |
|    fps              | 131      |
|    time_elapsed     | 1510     |
|    total_timesteps  | 198745   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0212   |
|    n_updates        | 47186    |
----------------------------------
Eval num_timesteps=199000, episode_reward=9.24 +/- 4.32
Episode length: 179.72 +/- 64.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 180      |
|    mean_reward      | 9.24     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 199000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0077   |
|    n_updates        | 47249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 171      |
|    ep_rew_mean      | 9.33     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1780     |
|    fps              | 131      |
|    time_elapsed     | 1515     |
|    total_timesteps  | 199460   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0142   |
|    n_updates        | 47364    |
----------------------------------
Eval num_timesteps=199500, episode_reward=13.42 +/- 3.09
Episode length: 217.66 +/- 46.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 13.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 199500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 47374    |
----------------------------------
Eval num_timesteps=200000, episode_reward=10.38 +/- 5.02
Episode length: 178.82 +/- 67.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 179      |
|    mean_reward      | 10.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 200000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00401  |
|    n_updates        | 47499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 169      |
|    ep_rew_mean      | 9.24     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1784     |
|    fps              | 130      |
|    time_elapsed     | 1527     |
|    total_timesteps  | 200075   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.012    |
|    n_updates        | 47518    |
----------------------------------
Eval num_timesteps=200500, episode_reward=15.18 +/- 3.05
Episode length: 251.84 +/- 54.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 252      |
|    mean_reward      | 15.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 200500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00721  |
|    n_updates        | 47624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 170      |
|    ep_rew_mean      | 9.39     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1788     |
|    fps              | 130      |
|    time_elapsed     | 1535     |
|    total_timesteps  | 200676   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0246   |
|    n_updates        | 47668    |
----------------------------------
Eval num_timesteps=201000, episode_reward=16.82 +/- 3.91
Episode length: 265.70 +/- 56.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 266      |
|    mean_reward      | 16.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 201000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 47749    |
----------------------------------
New best mean reward!
Eval num_timesteps=201500, episode_reward=16.68 +/- 4.26
Episode length: 268.52 +/- 58.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 269      |
|    mean_reward      | 16.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 201500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0127   |
|    n_updates        | 47874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 176      |
|    ep_rew_mean      | 9.86     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1792     |
|    fps              | 130      |
|    time_elapsed     | 1550     |
|    total_timesteps  | 201716   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 47928    |
----------------------------------
Eval num_timesteps=202000, episode_reward=13.22 +/- 3.05
Episode length: 218.00 +/- 42.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 13.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 202000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.015    |
|    n_updates        | 47999    |
----------------------------------
Eval num_timesteps=202500, episode_reward=12.92 +/- 4.53
Episode length: 216.46 +/- 59.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 12.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 202500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00852  |
|    n_updates        | 48124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 10.1     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1796     |
|    fps              | 129      |
|    time_elapsed     | 1563     |
|    total_timesteps  | 202500   |
----------------------------------
Eval num_timesteps=203000, episode_reward=5.54 +/- 3.89
Episode length: 127.30 +/- 56.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 5.54     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 203000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0332   |
|    n_updates        | 48249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 10       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1800     |
|    fps              | 129      |
|    time_elapsed     | 1567     |
|    total_timesteps  | 203180   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00815  |
|    n_updates        | 48294    |
----------------------------------
Eval num_timesteps=203500, episode_reward=12.32 +/- 3.95
Episode length: 214.34 +/- 56.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 12.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 203500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0273   |
|    n_updates        | 48374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 177      |
|    ep_rew_mean      | 9.95     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1804     |
|    fps              | 129      |
|    time_elapsed     | 1574     |
|    total_timesteps  | 203924   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0242   |
|    n_updates        | 48480    |
----------------------------------
Eval num_timesteps=204000, episode_reward=11.56 +/- 4.06
Episode length: 201.26 +/- 58.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 201      |
|    mean_reward      | 11.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 204000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00447  |
|    n_updates        | 48499    |
----------------------------------
Eval num_timesteps=204500, episode_reward=10.52 +/- 3.73
Episode length: 197.18 +/- 71.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 197      |
|    mean_reward      | 10.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 204500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00504  |
|    n_updates        | 48624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 10.1     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1808     |
|    fps              | 129      |
|    time_elapsed     | 1586     |
|    total_timesteps  | 204848   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 48711    |
----------------------------------
Eval num_timesteps=205000, episode_reward=11.30 +/- 3.37
Episode length: 197.62 +/- 45.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | 11.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 205000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00977  |
|    n_updates        | 48749    |
----------------------------------
Eval num_timesteps=205500, episode_reward=14.28 +/- 4.62
Episode length: 239.36 +/- 65.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 239      |
|    mean_reward      | 14.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 205500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0081   |
|    n_updates        | 48874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 176      |
|    ep_rew_mean      | 9.96     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1812     |
|    fps              | 128      |
|    time_elapsed     | 1599     |
|    total_timesteps  | 205567   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0226   |
|    n_updates        | 48891    |
----------------------------------
Eval num_timesteps=206000, episode_reward=9.66 +/- 3.65
Episode length: 186.48 +/- 53.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 186      |
|    mean_reward      | 9.66     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 206000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00729  |
|    n_updates        | 48999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 176      |
|    ep_rew_mean      | 9.88     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1816     |
|    fps              | 128      |
|    time_elapsed     | 1605     |
|    total_timesteps  | 206118   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 49029    |
----------------------------------
Eval num_timesteps=206500, episode_reward=5.34 +/- 3.01
Episode length: 126.82 +/- 45.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 127      |
|    mean_reward      | 5.34     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 206500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 49124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 176      |
|    ep_rew_mean      | 9.89     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1820     |
|    fps              | 128      |
|    time_elapsed     | 1609     |
|    total_timesteps  | 206787   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0133   |
|    n_updates        | 49196    |
----------------------------------
Eval num_timesteps=207000, episode_reward=12.32 +/- 4.25
Episode length: 219.16 +/- 62.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 12.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0111   |
|    n_updates        | 49249    |
----------------------------------
Eval num_timesteps=207500, episode_reward=9.00 +/- 3.75
Episode length: 177.54 +/- 57.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 178      |
|    mean_reward      | 9        |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 207500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00725  |
|    n_updates        | 49374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 10.1     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1824     |
|    fps              | 128      |
|    time_elapsed     | 1621     |
|    total_timesteps  | 207599   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 49399    |
----------------------------------
Eval num_timesteps=208000, episode_reward=12.74 +/- 3.59
Episode length: 212.46 +/- 51.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 12.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 208000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 49499    |
----------------------------------
Eval num_timesteps=208500, episode_reward=9.78 +/- 3.80
Episode length: 180.46 +/- 54.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 180      |
|    mean_reward      | 9.78     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 208500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0028   |
|    n_updates        | 49624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 10.2     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1828     |
|    fps              | 127      |
|    time_elapsed     | 1633     |
|    total_timesteps  | 208515   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0119   |
|    n_updates        | 49628    |
----------------------------------
Eval num_timesteps=209000, episode_reward=14.26 +/- 3.78
Episode length: 237.64 +/- 60.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 238      |
|    mean_reward      | 14.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 209000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0113   |
|    n_updates        | 49749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 10.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1832     |
|    fps              | 127      |
|    time_elapsed     | 1640     |
|    total_timesteps  | 209276   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00957  |
|    n_updates        | 49818    |
----------------------------------
Eval num_timesteps=209500, episode_reward=14.92 +/- 4.46
Episode length: 242.38 +/- 66.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 242      |
|    mean_reward      | 14.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 209500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 49874    |
----------------------------------
Eval num_timesteps=210000, episode_reward=3.86 +/- 2.76
Episode length: 101.50 +/- 39.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 210000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00952  |
|    n_updates        | 49999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 182      |
|    ep_rew_mean      | 10.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1836     |
|    fps              | 127      |
|    time_elapsed     | 1651     |
|    total_timesteps  | 210158   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00524  |
|    n_updates        | 50039    |
----------------------------------
Eval num_timesteps=210500, episode_reward=6.48 +/- 3.66
Episode length: 130.84 +/- 52.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 6.48     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 210500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.021    |
|    n_updates        | 50124    |
----------------------------------
Eval num_timesteps=211000, episode_reward=5.78 +/- 2.93
Episode length: 129.18 +/- 46.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 129      |
|    mean_reward      | 5.78     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 211000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0184   |
|    n_updates        | 50249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 183      |
|    ep_rew_mean      | 10.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1840     |
|    fps              | 127      |
|    time_elapsed     | 1659     |
|    total_timesteps  | 211044   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 50260    |
----------------------------------
Eval num_timesteps=211500, episode_reward=15.40 +/- 4.30
Episode length: 245.56 +/- 55.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 246      |
|    mean_reward      | 15.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 211500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00628  |
|    n_updates        | 50374    |
----------------------------------
Eval num_timesteps=212000, episode_reward=12.50 +/- 4.46
Episode length: 215.00 +/- 62.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 215      |
|    mean_reward      | 12.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 212000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0198   |
|    n_updates        | 50499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 186      |
|    ep_rew_mean      | 10.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1844     |
|    fps              | 126      |
|    time_elapsed     | 1673     |
|    total_timesteps  | 212038   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00941  |
|    n_updates        | 50509    |
----------------------------------
Eval num_timesteps=212500, episode_reward=5.88 +/- 4.57
Episode length: 128.00 +/- 62.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 5.88     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 212500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00586  |
|    n_updates        | 50624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 185      |
|    ep_rew_mean      | 10.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1848     |
|    fps              | 126      |
|    time_elapsed     | 1677     |
|    total_timesteps  | 212677   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00746  |
|    n_updates        | 50669    |
----------------------------------
Eval num_timesteps=213000, episode_reward=5.28 +/- 3.44
Episode length: 125.26 +/- 51.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 5.28     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 213000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0239   |
|    n_updates        | 50749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 186      |
|    ep_rew_mean      | 10.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1852     |
|    fps              | 126      |
|    time_elapsed     | 1681     |
|    total_timesteps  | 213487   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0127   |
|    n_updates        | 50871    |
----------------------------------
Eval num_timesteps=213500, episode_reward=10.40 +/- 2.65
Episode length: 190.22 +/- 43.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 10.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 213500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0144   |
|    n_updates        | 50874    |
----------------------------------
Eval num_timesteps=214000, episode_reward=12.40 +/- 2.86
Episode length: 208.70 +/- 44.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 12.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 214000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 50999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 185      |
|    ep_rew_mean      | 10.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1856     |
|    fps              | 126      |
|    time_elapsed     | 1693     |
|    total_timesteps  | 214092   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00831  |
|    n_updates        | 51022    |
----------------------------------
Eval num_timesteps=214500, episode_reward=9.74 +/- 3.89
Episode length: 175.38 +/- 59.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 175      |
|    mean_reward      | 9.74     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 214500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0212   |
|    n_updates        | 51124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 183      |
|    ep_rew_mean      | 10.2     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1860     |
|    fps              | 126      |
|    time_elapsed     | 1698     |
|    total_timesteps  | 214552   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00352  |
|    n_updates        | 51137    |
----------------------------------
Eval num_timesteps=215000, episode_reward=12.42 +/- 2.93
Episode length: 209.70 +/- 40.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 12.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 215000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0076   |
|    n_updates        | 51249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 184      |
|    ep_rew_mean      | 10.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1864     |
|    fps              | 126      |
|    time_elapsed     | 1704     |
|    total_timesteps  | 215242   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00409  |
|    n_updates        | 51310    |
----------------------------------
Eval num_timesteps=215500, episode_reward=10.86 +/- 2.33
Episode length: 189.44 +/- 35.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 189      |
|    mean_reward      | 10.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 215500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 51374    |
----------------------------------
Eval num_timesteps=216000, episode_reward=5.14 +/- 3.50
Episode length: 123.78 +/- 47.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.14     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 51499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 10.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1868     |
|    fps              | 126      |
|    time_elapsed     | 1714     |
|    total_timesteps  | 216006   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0144   |
|    n_updates        | 51501    |
----------------------------------
Eval num_timesteps=216500, episode_reward=13.78 +/- 4.86
Episode length: 239.00 +/- 69.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 239      |
|    mean_reward      | 13.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 216500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.031    |
|    n_updates        | 51624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 10.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1872     |
|    fps              | 125      |
|    time_elapsed     | 1721     |
|    total_timesteps  | 216904   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0135   |
|    n_updates        | 51725    |
----------------------------------
Eval num_timesteps=217000, episode_reward=14.42 +/- 2.58
Episode length: 238.14 +/- 43.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 238      |
|    mean_reward      | 14.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 217000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00265  |
|    n_updates        | 51749    |
----------------------------------
Eval num_timesteps=217500, episode_reward=4.84 +/- 2.98
Episode length: 114.68 +/- 44.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 4.84     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 217500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 51874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 10.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1876     |
|    fps              | 125      |
|    time_elapsed     | 1732     |
|    total_timesteps  | 217675   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00609  |
|    n_updates        | 51918    |
----------------------------------
Eval num_timesteps=218000, episode_reward=11.70 +/- 3.81
Episode length: 203.36 +/- 54.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | 11.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 218000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00675  |
|    n_updates        | 51999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 10.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1880     |
|    fps              | 125      |
|    time_elapsed     | 1738     |
|    total_timesteps  | 218375   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0101   |
|    n_updates        | 52093    |
----------------------------------
Eval num_timesteps=218500, episode_reward=12.86 +/- 2.77
Episode length: 216.18 +/- 43.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 12.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 218500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0163   |
|    n_updates        | 52124    |
----------------------------------
Eval num_timesteps=219000, episode_reward=12.54 +/- 3.66
Episode length: 211.12 +/- 54.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 12.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 219000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0062   |
|    n_updates        | 52249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 10.7     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1884     |
|    fps              | 125      |
|    time_elapsed     | 1751     |
|    total_timesteps  | 219200   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0133   |
|    n_updates        | 52299    |
----------------------------------
Eval num_timesteps=219500, episode_reward=13.10 +/- 3.37
Episode length: 212.26 +/- 48.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 13.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 219500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0571   |
|    n_updates        | 52374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 10.7     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1888     |
|    fps              | 125      |
|    time_elapsed     | 1757     |
|    total_timesteps  | 219891   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 52472    |
----------------------------------
Eval num_timesteps=220000, episode_reward=4.16 +/- 3.12
Episode length: 106.98 +/- 44.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 4.16     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 220000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00828  |
|    n_updates        | 52499    |
----------------------------------
Eval num_timesteps=220500, episode_reward=14.96 +/- 3.36
Episode length: 241.86 +/- 42.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 242      |
|    mean_reward      | 15       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 220500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 52624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 190      |
|    ep_rew_mean      | 10.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1892     |
|    fps              | 124      |
|    time_elapsed     | 1767     |
|    total_timesteps  | 220736   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00768  |
|    n_updates        | 52683    |
----------------------------------
Eval num_timesteps=221000, episode_reward=11.94 +/- 3.25
Episode length: 210.76 +/- 59.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 11.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 221000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 52749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 10.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1896     |
|    fps              | 124      |
|    time_elapsed     | 1774     |
|    total_timesteps  | 221342   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00834  |
|    n_updates        | 52835    |
----------------------------------
Eval num_timesteps=221500, episode_reward=9.34 +/- 2.89
Episode length: 180.64 +/- 46.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 9.34     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 221500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00767  |
|    n_updates        | 52874    |
----------------------------------
Eval num_timesteps=222000, episode_reward=14.86 +/- 4.91
Episode length: 243.82 +/- 72.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 244      |
|    mean_reward      | 14.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 222000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0169   |
|    n_updates        | 52999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 10.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1900     |
|    fps              | 124      |
|    time_elapsed     | 1786     |
|    total_timesteps  | 222060   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00435  |
|    n_updates        | 53014    |
----------------------------------
Eval num_timesteps=222500, episode_reward=11.44 +/- 2.85
Episode length: 196.14 +/- 46.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 196      |
|    mean_reward      | 11.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 222500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 53124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 10.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1904     |
|    fps              | 124      |
|    time_elapsed     | 1792     |
|    total_timesteps  | 222785   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0129   |
|    n_updates        | 53196    |
----------------------------------
Eval num_timesteps=223000, episode_reward=18.02 +/- 3.44
Episode length: 282.84 +/- 47.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 283      |
|    mean_reward      | 18       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 223000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00847  |
|    n_updates        | 53249    |
----------------------------------
New best mean reward!
Eval num_timesteps=223500, episode_reward=5.76 +/- 3.67
Episode length: 123.74 +/- 57.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 124      |
|    mean_reward      | 5.76     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 223500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 53374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 10.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1908     |
|    fps              | 123      |
|    time_elapsed     | 1804     |
|    total_timesteps  | 223538   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00806  |
|    n_updates        | 53384    |
----------------------------------
Eval num_timesteps=224000, episode_reward=10.78 +/- 2.93
Episode length: 189.50 +/- 48.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 10.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 224000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.012    |
|    n_updates        | 53499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 10.2     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1912     |
|    fps              | 123      |
|    time_elapsed     | 1810     |
|    total_timesteps  | 224260   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 53564    |
----------------------------------
Eval num_timesteps=224500, episode_reward=15.44 +/- 3.85
Episode length: 245.10 +/- 53.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 245      |
|    mean_reward      | 15.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 224500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00994  |
|    n_updates        | 53624    |
----------------------------------
Eval num_timesteps=225000, episode_reward=13.66 +/- 4.16
Episode length: 232.20 +/- 60.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 13.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 225000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0111   |
|    n_updates        | 53749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 10.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1916     |
|    fps              | 123      |
|    time_elapsed     | 1824     |
|    total_timesteps  | 225056   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0241   |
|    n_updates        | 53763    |
----------------------------------
Eval num_timesteps=225500, episode_reward=5.48 +/- 4.37
Episode length: 130.60 +/- 62.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 5.48     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 225500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0169   |
|    n_updates        | 53874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 10.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1920     |
|    fps              | 123      |
|    time_elapsed     | 1829     |
|    total_timesteps  | 225879   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 53969    |
----------------------------------
Eval num_timesteps=226000, episode_reward=8.10 +/- 4.12
Episode length: 154.26 +/- 54.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 154      |
|    mean_reward      | 8.1      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 226000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00814  |
|    n_updates        | 53999    |
----------------------------------
Eval num_timesteps=226500, episode_reward=14.22 +/- 2.76
Episode length: 226.62 +/- 36.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 227      |
|    mean_reward      | 14.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 226500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0139   |
|    n_updates        | 54124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 10.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1924     |
|    fps              | 123      |
|    time_elapsed     | 1840     |
|    total_timesteps  | 226663   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00717  |
|    n_updates        | 54165    |
----------------------------------
Eval num_timesteps=227000, episode_reward=16.46 +/- 5.02
Episode length: 267.86 +/- 68.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 268      |
|    mean_reward      | 16.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 227000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0188   |
|    n_updates        | 54249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 10.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1928     |
|    fps              | 122      |
|    time_elapsed     | 1848     |
|    total_timesteps  | 227341   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00336  |
|    n_updates        | 54335    |
----------------------------------
Eval num_timesteps=227500, episode_reward=10.72 +/- 3.07
Episode length: 193.42 +/- 46.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 193      |
|    mean_reward      | 10.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 227500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 54374    |
----------------------------------
Eval num_timesteps=228000, episode_reward=10.82 +/- 3.62
Episode length: 194.30 +/- 54.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | 10.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 228000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00613  |
|    n_updates        | 54499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 10.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1932     |
|    fps              | 122      |
|    time_elapsed     | 1859     |
|    total_timesteps  | 228108   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 54526    |
----------------------------------
Eval num_timesteps=228500, episode_reward=15.36 +/- 4.92
Episode length: 243.38 +/- 66.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 243      |
|    mean_reward      | 15.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 228500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0143   |
|    n_updates        | 54624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 10.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1936     |
|    fps              | 122      |
|    time_elapsed     | 1867     |
|    total_timesteps  | 228920   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 54729    |
----------------------------------
Eval num_timesteps=229000, episode_reward=17.62 +/- 3.73
Episode length: 269.28 +/- 55.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 269      |
|    mean_reward      | 17.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 229000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 54749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 184      |
|    ep_rew_mean      | 10.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1940     |
|    fps              | 122      |
|    time_elapsed     | 1875     |
|    total_timesteps  | 229489   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 54872    |
----------------------------------
Eval num_timesteps=229500, episode_reward=9.30 +/- 3.57
Episode length: 171.00 +/- 53.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 171      |
|    mean_reward      | 9.3      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 229500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 54874    |
----------------------------------
Eval num_timesteps=230000, episode_reward=8.80 +/- 2.95
Episode length: 170.22 +/- 46.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 8.8      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 230000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0229   |
|    n_updates        | 54999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 181      |
|    ep_rew_mean      | 10.2     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1944     |
|    fps              | 122      |
|    time_elapsed     | 1885     |
|    total_timesteps  | 230106   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0149   |
|    n_updates        | 55026    |
----------------------------------
Eval num_timesteps=230500, episode_reward=13.78 +/- 3.90
Episode length: 229.48 +/- 53.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 13.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 230500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0264   |
|    n_updates        | 55124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 182      |
|    ep_rew_mean      | 10.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1948     |
|    fps              | 122      |
|    time_elapsed     | 1892     |
|    total_timesteps  | 230915   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0047   |
|    n_updates        | 55228    |
----------------------------------
Eval num_timesteps=231000, episode_reward=9.10 +/- 3.66
Episode length: 168.18 +/- 45.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 168      |
|    mean_reward      | 9.1      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 231000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0101   |
|    n_updates        | 55249    |
----------------------------------
Eval num_timesteps=231500, episode_reward=13.78 +/- 3.44
Episode length: 220.74 +/- 48.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 13.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 231500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00889  |
|    n_updates        | 55374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 183      |
|    ep_rew_mean      | 10.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1952     |
|    fps              | 121      |
|    time_elapsed     | 1904     |
|    total_timesteps  | 231747   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0108   |
|    n_updates        | 55436    |
----------------------------------
Eval num_timesteps=232000, episode_reward=7.08 +/- 2.46
Episode length: 150.00 +/- 41.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 7.08     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 232000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00571  |
|    n_updates        | 55499    |
----------------------------------
Eval num_timesteps=232500, episode_reward=9.40 +/- 3.22
Episode length: 179.30 +/- 47.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 179      |
|    mean_reward      | 9.4      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 232500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 55624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 185      |
|    ep_rew_mean      | 10.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1956     |
|    fps              | 121      |
|    time_elapsed     | 1914     |
|    total_timesteps  | 232577   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0106   |
|    n_updates        | 55644    |
----------------------------------
Eval num_timesteps=233000, episode_reward=14.00 +/- 3.49
Episode length: 234.58 +/- 46.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 235      |
|    mean_reward      | 14       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 233000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00726  |
|    n_updates        | 55749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 10.8     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1960     |
|    fps              | 121      |
|    time_elapsed     | 1921     |
|    total_timesteps  | 233294   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00826  |
|    n_updates        | 55823    |
----------------------------------
Eval num_timesteps=233500, episode_reward=7.54 +/- 3.77
Episode length: 146.34 +/- 51.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 7.54     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 233500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 55874    |
----------------------------------
Eval num_timesteps=234000, episode_reward=11.38 +/- 4.44
Episode length: 202.80 +/- 61.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | 11.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 234000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 55999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 10.9     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1964     |
|    fps              | 121      |
|    time_elapsed     | 1931     |
|    total_timesteps  | 234079   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 56019    |
----------------------------------
Eval num_timesteps=234500, episode_reward=5.92 +/- 3.34
Episode length: 134.98 +/- 44.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 135      |
|    mean_reward      | 5.92     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 234500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00782  |
|    n_updates        | 56124    |
----------------------------------
Eval num_timesteps=235000, episode_reward=13.30 +/- 4.49
Episode length: 215.84 +/- 50.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 13.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 235000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0293   |
|    n_updates        | 56249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 11.1     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1968     |
|    fps              | 121      |
|    time_elapsed     | 1942     |
|    total_timesteps  | 235095   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0156   |
|    n_updates        | 56273    |
----------------------------------
Eval num_timesteps=235500, episode_reward=12.46 +/- 3.66
Episode length: 203.74 +/- 48.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 12.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 235500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 56374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 11       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1972     |
|    fps              | 120      |
|    time_elapsed     | 1948     |
|    total_timesteps  | 235689   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00523  |
|    n_updates        | 56422    |
----------------------------------
Eval num_timesteps=236000, episode_reward=13.78 +/- 3.52
Episode length: 228.78 +/- 54.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 13.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 236000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 56499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 11       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1976     |
|    fps              | 120      |
|    time_elapsed     | 1955     |
|    total_timesteps  | 236423   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0169   |
|    n_updates        | 56605    |
----------------------------------
Eval num_timesteps=236500, episode_reward=15.94 +/- 4.75
Episode length: 265.00 +/- 65.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 265      |
|    mean_reward      | 15.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 236500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 56624    |
----------------------------------
Eval num_timesteps=237000, episode_reward=5.06 +/- 2.76
Episode length: 122.62 +/- 39.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 123      |
|    mean_reward      | 5.06     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 237000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00442  |
|    n_updates        | 56749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 11       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1980     |
|    fps              | 120      |
|    time_elapsed     | 1967     |
|    total_timesteps  | 237289   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00976  |
|    n_updates        | 56822    |
----------------------------------
Eval num_timesteps=237500, episode_reward=15.96 +/- 3.75
Episode length: 246.42 +/- 50.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 246      |
|    mean_reward      | 16       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 237500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00687  |
|    n_updates        | 56874    |
----------------------------------
Eval num_timesteps=238000, episode_reward=18.04 +/- 4.20
Episode length: 281.42 +/- 58.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 281      |
|    mean_reward      | 18       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 238000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00999  |
|    n_updates        | 56999    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 11.1     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1984     |
|    fps              | 120      |
|    time_elapsed     | 1982     |
|    total_timesteps  | 238093   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0135   |
|    n_updates        | 57023    |
----------------------------------
Eval num_timesteps=238500, episode_reward=8.46 +/- 2.99
Episode length: 165.86 +/- 45.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 166      |
|    mean_reward      | 8.46     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 238500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0198   |
|    n_updates        | 57124    |
----------------------------------
Eval num_timesteps=239000, episode_reward=16.86 +/- 4.02
Episode length: 261.46 +/- 55.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 261      |
|    mean_reward      | 16.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 239000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 57249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 11.2     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1988     |
|    fps              | 119      |
|    time_elapsed     | 1995     |
|    total_timesteps  | 239007   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0136   |
|    n_updates        | 57251    |
----------------------------------
Eval num_timesteps=239500, episode_reward=12.64 +/- 3.55
Episode length: 216.72 +/- 51.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 12.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 239500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0143   |
|    n_updates        | 57374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 190      |
|    ep_rew_mean      | 11.1     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1992     |
|    fps              | 119      |
|    time_elapsed     | 2001     |
|    total_timesteps  | 239768   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0124   |
|    n_updates        | 57441    |
----------------------------------
Eval num_timesteps=240000, episode_reward=14.06 +/- 4.31
Episode length: 242.18 +/- 56.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 242      |
|    mean_reward      | 14.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 240000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00527  |
|    n_updates        | 57499    |
----------------------------------
Eval num_timesteps=240500, episode_reward=13.62 +/- 3.48
Episode length: 226.94 +/- 51.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 227      |
|    mean_reward      | 13.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 240500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0312   |
|    n_updates        | 57624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 11.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 1996     |
|    fps              | 119      |
|    time_elapsed     | 2015     |
|    total_timesteps  | 240564   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0137   |
|    n_updates        | 57640    |
----------------------------------
Eval num_timesteps=241000, episode_reward=14.40 +/- 5.33
Episode length: 251.04 +/- 80.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 251      |
|    mean_reward      | 14.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 241000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00594  |
|    n_updates        | 57749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | 11.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2000     |
|    fps              | 119      |
|    time_elapsed     | 2023     |
|    total_timesteps  | 241494   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00908  |
|    n_updates        | 57873    |
----------------------------------
Eval num_timesteps=241500, episode_reward=14.46 +/- 4.30
Episode length: 236.42 +/- 51.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 236      |
|    mean_reward      | 14.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 241500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0127   |
|    n_updates        | 57874    |
----------------------------------
Eval num_timesteps=242000, episode_reward=14.92 +/- 2.86
Episode length: 237.64 +/- 42.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 238      |
|    mean_reward      | 14.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 242000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00277  |
|    n_updates        | 57999    |
----------------------------------
Eval num_timesteps=242500, episode_reward=11.36 +/- 3.60
Episode length: 203.36 +/- 54.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | 11.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 242500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00817  |
|    n_updates        | 58124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | 11.8     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2004     |
|    fps              | 118      |
|    time_elapsed     | 2043     |
|    total_timesteps  | 242651   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00879  |
|    n_updates        | 58162    |
----------------------------------
Eval num_timesteps=243000, episode_reward=11.40 +/- 3.89
Episode length: 201.88 +/- 62.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | 11.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 243000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 58249    |
----------------------------------
Eval num_timesteps=243500, episode_reward=14.26 +/- 3.25
Episode length: 245.74 +/- 55.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 246      |
|    mean_reward      | 14.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 243500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00601  |
|    n_updates        | 58374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | 11.9     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2008     |
|    fps              | 118      |
|    time_elapsed     | 2056     |
|    total_timesteps  | 243608   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00948  |
|    n_updates        | 58401    |
----------------------------------
Eval num_timesteps=244000, episode_reward=11.46 +/- 3.23
Episode length: 206.36 +/- 42.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | 11.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 244000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0121   |
|    n_updates        | 58499    |
----------------------------------
Eval num_timesteps=244500, episode_reward=15.44 +/- 4.44
Episode length: 256.12 +/- 67.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 256      |
|    mean_reward      | 15.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 244500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0128   |
|    n_updates        | 58624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | 12       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2012     |
|    fps              | 118      |
|    time_elapsed     | 2070     |
|    total_timesteps  | 244538   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 58634    |
----------------------------------
Eval num_timesteps=245000, episode_reward=9.32 +/- 3.22
Episode length: 172.46 +/- 45.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 172      |
|    mean_reward      | 9.32     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 245000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00874  |
|    n_updates        | 58749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | 12       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2016     |
|    fps              | 118      |
|    time_elapsed     | 2075     |
|    total_timesteps  | 245116   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00911  |
|    n_updates        | 58778    |
----------------------------------
Eval num_timesteps=245500, episode_reward=14.36 +/- 4.29
Episode length: 239.46 +/- 68.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 239      |
|    mean_reward      | 14.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 245500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00815  |
|    n_updates        | 58874    |
----------------------------------
Eval num_timesteps=246000, episode_reward=13.64 +/- 5.05
Episode length: 236.50 +/- 81.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 236      |
|    mean_reward      | 13.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 246000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0151   |
|    n_updates        | 58999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 201      |
|    ep_rew_mean      | 12       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2020     |
|    fps              | 117      |
|    time_elapsed     | 2089     |
|    total_timesteps  | 246010   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00769  |
|    n_updates        | 59002    |
----------------------------------
Eval num_timesteps=246500, episode_reward=13.46 +/- 3.75
Episode length: 224.26 +/- 54.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 13.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 246500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00664  |
|    n_updates        | 59124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | 12       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2024     |
|    fps              | 117      |
|    time_elapsed     | 2096     |
|    total_timesteps  | 246835   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0193   |
|    n_updates        | 59208    |
----------------------------------
Eval num_timesteps=247000, episode_reward=13.72 +/- 3.99
Episode length: 231.00 +/- 57.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 13.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 247000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0143   |
|    n_updates        | 59249    |
----------------------------------
Eval num_timesteps=247500, episode_reward=13.06 +/- 2.64
Episode length: 223.14 +/- 45.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 13.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 247500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00437  |
|    n_updates        | 59374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | 12       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2028     |
|    fps              | 117      |
|    time_elapsed     | 2109     |
|    total_timesteps  | 247664   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00923  |
|    n_updates        | 59415    |
----------------------------------
Eval num_timesteps=248000, episode_reward=9.86 +/- 3.08
Episode length: 184.62 +/- 45.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 185      |
|    mean_reward      | 9.86     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 248000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00706  |
|    n_updates        | 59499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | 11.8     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2032     |
|    fps              | 117      |
|    time_elapsed     | 2115     |
|    total_timesteps  | 248489   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0167   |
|    n_updates        | 59622    |
----------------------------------
Eval num_timesteps=248500, episode_reward=9.48 +/- 3.94
Episode length: 172.32 +/- 52.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 172      |
|    mean_reward      | 9.48     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 248500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00734  |
|    n_updates        | 59624    |
----------------------------------
Eval num_timesteps=249000, episode_reward=13.14 +/- 2.64
Episode length: 221.98 +/- 50.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 13.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 249000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 59749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 205      |
|    ep_rew_mean      | 11.9     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2036     |
|    fps              | 117      |
|    time_elapsed     | 2127     |
|    total_timesteps  | 249452   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00524  |
|    n_updates        | 59862    |
----------------------------------
Eval num_timesteps=249500, episode_reward=14.60 +/- 4.05
Episode length: 241.70 +/- 58.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 242      |
|    mean_reward      | 14.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 249500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0101   |
|    n_updates        | 59874    |
----------------------------------
Eval num_timesteps=250000, episode_reward=11.88 +/- 4.10
Episode length: 220.76 +/- 64.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 11.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 250000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00808  |
|    n_updates        | 59999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 12       |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2040     |
|    fps              | 116      |
|    time_elapsed     | 2140     |
|    total_timesteps  | 250122   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 60030    |
----------------------------------
Eval num_timesteps=250500, episode_reward=16.62 +/- 3.89
Episode length: 268.64 +/- 59.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 269      |
|    mean_reward      | 16.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 250500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0128   |
|    n_updates        | 60124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | 12.1     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2044     |
|    fps              | 116      |
|    time_elapsed     | 2149     |
|    total_timesteps  | 250888   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00704  |
|    n_updates        | 60221    |
----------------------------------
Eval num_timesteps=251000, episode_reward=15.06 +/- 4.45
Episode length: 244.48 +/- 67.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 244      |
|    mean_reward      | 15.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 251000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0107   |
|    n_updates        | 60249    |
----------------------------------
Eval num_timesteps=251500, episode_reward=16.56 +/- 2.93
Episode length: 262.90 +/- 42.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 263      |
|    mean_reward      | 16.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 251500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00842  |
|    n_updates        | 60374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | 12.2     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2048     |
|    fps              | 116      |
|    time_elapsed     | 2163     |
|    total_timesteps  | 251851   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00501  |
|    n_updates        | 60462    |
----------------------------------
Eval num_timesteps=252000, episode_reward=13.84 +/- 3.31
Episode length: 227.82 +/- 48.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 13.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 252000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.033    |
|    n_updates        | 60499    |
----------------------------------
Eval num_timesteps=252500, episode_reward=15.22 +/- 4.63
Episode length: 249.86 +/- 59.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 250      |
|    mean_reward      | 15.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 252500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 60624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 12.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2052     |
|    fps              | 116      |
|    time_elapsed     | 2178     |
|    total_timesteps  | 252879   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 60719    |
----------------------------------
Eval num_timesteps=253000, episode_reward=12.72 +/- 4.30
Episode length: 218.52 +/- 63.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 12.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 253000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 60749    |
----------------------------------
Eval num_timesteps=253500, episode_reward=14.48 +/- 4.05
Episode length: 238.86 +/- 61.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 239      |
|    mean_reward      | 14.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 253500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00881  |
|    n_updates        | 60874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 12.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2056     |
|    fps              | 115      |
|    time_elapsed     | 2191     |
|    total_timesteps  | 253723   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00834  |
|    n_updates        | 60930    |
----------------------------------
Eval num_timesteps=254000, episode_reward=13.86 +/- 3.72
Episode length: 236.44 +/- 57.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 236      |
|    mean_reward      | 13.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 254000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0287   |
|    n_updates        | 60999    |
----------------------------------
Eval num_timesteps=254500, episode_reward=12.20 +/- 2.04
Episode length: 204.00 +/- 31.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 12.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 254500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00229  |
|    n_updates        | 61124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 12.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2060     |
|    fps              | 115      |
|    time_elapsed     | 2204     |
|    total_timesteps  | 254577   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0253   |
|    n_updates        | 61144    |
----------------------------------
Eval num_timesteps=255000, episode_reward=10.28 +/- 3.80
Episode length: 188.38 +/- 57.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 188      |
|    mean_reward      | 10.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 255000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00988  |
|    n_updates        | 61249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 12.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2064     |
|    fps              | 115      |
|    time_elapsed     | 2210     |
|    total_timesteps  | 255184   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00728  |
|    n_updates        | 61295    |
----------------------------------
Eval num_timesteps=255500, episode_reward=12.68 +/- 4.43
Episode length: 231.30 +/- 69.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 12.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 255500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0104   |
|    n_updates        | 61374    |
----------------------------------
Eval num_timesteps=256000, episode_reward=13.50 +/- 3.35
Episode length: 224.30 +/- 50.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 13.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 256000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00976  |
|    n_updates        | 61499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 12.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2068     |
|    fps              | 115      |
|    time_elapsed     | 2223     |
|    total_timesteps  | 256224   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0099   |
|    n_updates        | 61555    |
----------------------------------
Eval num_timesteps=256500, episode_reward=11.96 +/- 3.45
Episode length: 209.28 +/- 54.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 12       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 256500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00759  |
|    n_updates        | 61624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 12.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2072     |
|    fps              | 115      |
|    time_elapsed     | 2230     |
|    total_timesteps  | 256996   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0179   |
|    n_updates        | 61748    |
----------------------------------
Eval num_timesteps=257000, episode_reward=12.76 +/- 3.68
Episode length: 220.70 +/- 68.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 12.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 257000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00581  |
|    n_updates        | 61749    |
----------------------------------
Eval num_timesteps=257500, episode_reward=13.08 +/- 3.83
Episode length: 216.56 +/- 47.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 13.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 257500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00405  |
|    n_updates        | 61874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 214      |
|    ep_rew_mean      | 12.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2076     |
|    fps              | 114      |
|    time_elapsed     | 2243     |
|    total_timesteps  | 257859   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 61964    |
----------------------------------
Eval num_timesteps=258000, episode_reward=13.50 +/- 2.78
Episode length: 222.70 +/- 44.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 13.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 258000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 61999    |
----------------------------------
Eval num_timesteps=258500, episode_reward=14.18 +/- 4.30
Episode length: 234.20 +/- 59.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 234      |
|    mean_reward      | 14.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 258500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0201   |
|    n_updates        | 62124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 214      |
|    ep_rew_mean      | 12.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2080     |
|    fps              | 114      |
|    time_elapsed     | 2256     |
|    total_timesteps  | 258676   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0134   |
|    n_updates        | 62168    |
----------------------------------
Eval num_timesteps=259000, episode_reward=14.98 +/- 4.31
Episode length: 241.88 +/- 57.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 242      |
|    mean_reward      | 15       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 259000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00685  |
|    n_updates        | 62249    |
----------------------------------
Eval num_timesteps=259500, episode_reward=13.32 +/- 4.16
Episode length: 218.68 +/- 61.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 13.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 259500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0146   |
|    n_updates        | 62374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 215      |
|    ep_rew_mean      | 12.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2084     |
|    fps              | 114      |
|    time_elapsed     | 2270     |
|    total_timesteps  | 259638   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00615  |
|    n_updates        | 62409    |
----------------------------------
Eval num_timesteps=260000, episode_reward=13.02 +/- 4.08
Episode length: 220.52 +/- 53.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 13       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 260000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00581  |
|    n_updates        | 62499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 214      |
|    ep_rew_mean      | 12.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2088     |
|    fps              | 114      |
|    time_elapsed     | 2276     |
|    total_timesteps  | 260382   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0086   |
|    n_updates        | 62595    |
----------------------------------
Eval num_timesteps=260500, episode_reward=12.42 +/- 3.53
Episode length: 218.56 +/- 58.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 12.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 260500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0237   |
|    n_updates        | 62624    |
----------------------------------
Eval num_timesteps=261000, episode_reward=13.62 +/- 4.35
Episode length: 231.18 +/- 48.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 13.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 261000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00681  |
|    n_updates        | 62749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 12.7     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2092     |
|    fps              | 114      |
|    time_elapsed     | 2290     |
|    total_timesteps  | 261324   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0105   |
|    n_updates        | 62830    |
----------------------------------
Eval num_timesteps=261500, episode_reward=13.50 +/- 3.41
Episode length: 231.74 +/- 49.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 13.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 261500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0112   |
|    n_updates        | 62874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 214      |
|    ep_rew_mean      | 12.7     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2096     |
|    fps              | 114      |
|    time_elapsed     | 2297     |
|    total_timesteps  | 261997   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0168   |
|    n_updates        | 62999    |
----------------------------------
Eval num_timesteps=262000, episode_reward=7.26 +/- 4.25
Episode length: 153.92 +/- 65.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 154      |
|    mean_reward      | 7.26     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 262000   |
----------------------------------
Eval num_timesteps=262500, episode_reward=12.32 +/- 2.57
Episode length: 209.26 +/- 43.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 12.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 262500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0244   |
|    n_updates        | 63124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 12.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2100     |
|    fps              | 113      |
|    time_elapsed     | 2308     |
|    total_timesteps  | 262745   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.022    |
|    n_updates        | 63186    |
----------------------------------
Eval num_timesteps=263000, episode_reward=13.58 +/- 3.85
Episode length: 228.00 +/- 56.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 13.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 263000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 63249    |
----------------------------------
Eval num_timesteps=263500, episode_reward=12.36 +/- 3.43
Episode length: 202.12 +/- 47.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | 12.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 263500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00711  |
|    n_updates        | 63374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 212      |
|    ep_rew_mean      | 12.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2104     |
|    fps              | 113      |
|    time_elapsed     | 2321     |
|    total_timesteps  | 263850   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00847  |
|    n_updates        | 63462    |
----------------------------------
Eval num_timesteps=264000, episode_reward=13.04 +/- 2.86
Episode length: 209.00 +/- 44.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 13       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 264000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0238   |
|    n_updates        | 63499    |
----------------------------------
Eval num_timesteps=264500, episode_reward=14.68 +/- 4.23
Episode length: 237.22 +/- 65.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 237      |
|    mean_reward      | 14.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 264500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 63624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 12.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2108     |
|    fps              | 113      |
|    time_elapsed     | 2335     |
|    total_timesteps  | 264687   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 63671    |
----------------------------------
Eval num_timesteps=265000, episode_reward=15.28 +/- 3.21
Episode length: 243.90 +/- 48.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 244      |
|    mean_reward      | 15.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 265000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 63749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | 12.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2112     |
|    fps              | 113      |
|    time_elapsed     | 2342     |
|    total_timesteps  | 265458   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00587  |
|    n_updates        | 63864    |
----------------------------------
Eval num_timesteps=265500, episode_reward=12.96 +/- 3.57
Episode length: 210.38 +/- 53.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 13       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 265500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00659  |
|    n_updates        | 63874    |
----------------------------------
Eval num_timesteps=266000, episode_reward=13.82 +/- 4.34
Episode length: 233.28 +/- 63.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 233      |
|    mean_reward      | 13.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 266000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0149   |
|    n_updates        | 63999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 12.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2116     |
|    fps              | 113      |
|    time_elapsed     | 2355     |
|    total_timesteps  | 266410   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0189   |
|    n_updates        | 64102    |
----------------------------------
Eval num_timesteps=266500, episode_reward=11.60 +/- 3.98
Episode length: 204.92 +/- 62.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 11.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 266500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00697  |
|    n_updates        | 64124    |
----------------------------------
Eval num_timesteps=267000, episode_reward=12.94 +/- 3.34
Episode length: 218.02 +/- 48.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 12.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 267000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0163   |
|    n_updates        | 64249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 214      |
|    ep_rew_mean      | 12.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2120     |
|    fps              | 112      |
|    time_elapsed     | 2368     |
|    total_timesteps  | 267402   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0114   |
|    n_updates        | 64350    |
----------------------------------
Eval num_timesteps=267500, episode_reward=14.12 +/- 3.99
Episode length: 237.48 +/- 59.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 237      |
|    mean_reward      | 14.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 267500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0121   |
|    n_updates        | 64374    |
----------------------------------
Eval num_timesteps=268000, episode_reward=15.54 +/- 4.41
Episode length: 255.04 +/- 65.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 255      |
|    mean_reward      | 15.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 268000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00732  |
|    n_updates        | 64499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 12.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2124     |
|    fps              | 112      |
|    time_elapsed     | 2382     |
|    total_timesteps  | 268164   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00635  |
|    n_updates        | 64540    |
----------------------------------
Eval num_timesteps=268500, episode_reward=12.86 +/- 3.87
Episode length: 224.10 +/- 63.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 12.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 268500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0352   |
|    n_updates        | 64624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 12.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2128     |
|    fps              | 112      |
|    time_elapsed     | 2389     |
|    total_timesteps  | 268778   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00764  |
|    n_updates        | 64694    |
----------------------------------
Eval num_timesteps=269000, episode_reward=11.46 +/- 2.40
Episode length: 193.98 +/- 35.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | 11.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 269000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0163   |
|    n_updates        | 64749    |
----------------------------------
Eval num_timesteps=269500, episode_reward=8.54 +/- 3.38
Episode length: 176.02 +/- 72.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 176      |
|    mean_reward      | 8.54     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 269500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.058    |
|    n_updates        | 64874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | 12.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2132     |
|    fps              | 112      |
|    time_elapsed     | 2400     |
|    total_timesteps  | 269525   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0153   |
|    n_updates        | 64881    |
----------------------------------
Eval num_timesteps=270000, episode_reward=13.24 +/- 3.90
Episode length: 224.74 +/- 61.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 13.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 270000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0114   |
|    n_updates        | 64999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | 12.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2136     |
|    fps              | 112      |
|    time_elapsed     | 2407     |
|    total_timesteps  | 270419   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0161   |
|    n_updates        | 65104    |
----------------------------------
Eval num_timesteps=270500, episode_reward=12.96 +/- 3.19
Episode length: 214.86 +/- 47.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 215      |
|    mean_reward      | 13       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 270500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0223   |
|    n_updates        | 65124    |
----------------------------------
Eval num_timesteps=271000, episode_reward=13.32 +/- 2.34
Episode length: 214.28 +/- 34.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 13.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 271000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0337   |
|    n_updates        | 65249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 12.7     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2140     |
|    fps              | 112      |
|    time_elapsed     | 2420     |
|    total_timesteps  | 271422   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 65355    |
----------------------------------
Eval num_timesteps=271500, episode_reward=13.48 +/- 3.20
Episode length: 212.32 +/- 44.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 13.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 271500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0107   |
|    n_updates        | 65374    |
----------------------------------
Eval num_timesteps=272000, episode_reward=12.00 +/- 2.39
Episode length: 199.72 +/- 37.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 200      |
|    mean_reward      | 12       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 272000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 65499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 213      |
|    ep_rew_mean      | 12.8     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2144     |
|    fps              | 111      |
|    time_elapsed     | 2432     |
|    total_timesteps  | 272165   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00697  |
|    n_updates        | 65541    |
----------------------------------
Eval num_timesteps=272500, episode_reward=13.44 +/- 2.89
Episode length: 228.62 +/- 51.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 13.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 272500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00602  |
|    n_updates        | 65624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 211      |
|    ep_rew_mean      | 12.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2148     |
|    fps              | 111      |
|    time_elapsed     | 2439     |
|    total_timesteps  | 272922   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00973  |
|    n_updates        | 65730    |
----------------------------------
Eval num_timesteps=273000, episode_reward=3.96 +/- 2.41
Episode length: 106.04 +/- 36.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 3.96     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 273000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0145   |
|    n_updates        | 65749    |
----------------------------------
Eval num_timesteps=273500, episode_reward=12.08 +/- 3.19
Episode length: 200.62 +/- 47.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 201      |
|    mean_reward      | 12.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 273500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00784  |
|    n_updates        | 65874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | 12.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2152     |
|    fps              | 111      |
|    time_elapsed     | 2448     |
|    total_timesteps  | 273862   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00959  |
|    n_updates        | 65965    |
----------------------------------
Eval num_timesteps=274000, episode_reward=14.84 +/- 2.61
Episode length: 237.22 +/- 39.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 237      |
|    mean_reward      | 14.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 274000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0127   |
|    n_updates        | 65999    |
----------------------------------
Eval num_timesteps=274500, episode_reward=12.96 +/- 2.89
Episode length: 213.54 +/- 41.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 13       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 274500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00681  |
|    n_updates        | 66124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | 12.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2156     |
|    fps              | 111      |
|    time_elapsed     | 2461     |
|    total_timesteps  | 274647   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00465  |
|    n_updates        | 66161    |
----------------------------------
Eval num_timesteps=275000, episode_reward=11.14 +/- 2.88
Episode length: 189.50 +/- 39.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 11.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 275000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 66249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | 12.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2160     |
|    fps              | 111      |
|    time_elapsed     | 2467     |
|    total_timesteps  | 275332   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0163   |
|    n_updates        | 66332    |
----------------------------------
Eval num_timesteps=275500, episode_reward=13.72 +/- 3.70
Episode length: 230.66 +/- 58.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 13.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 275500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00576  |
|    n_updates        | 66374    |
----------------------------------
Eval num_timesteps=276000, episode_reward=10.00 +/- 1.55
Episode length: 178.28 +/- 25.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 178      |
|    mean_reward      | 10       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 276000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0129   |
|    n_updates        | 66499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | 12.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2164     |
|    fps              | 111      |
|    time_elapsed     | 2479     |
|    total_timesteps  | 276033   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 66508    |
----------------------------------
Eval num_timesteps=276500, episode_reward=10.74 +/- 2.24
Episode length: 191.64 +/- 43.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 192      |
|    mean_reward      | 10.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 276500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 66624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | 12.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2168     |
|    fps              | 111      |
|    time_elapsed     | 2486     |
|    total_timesteps  | 276879   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 66719    |
----------------------------------
Eval num_timesteps=277000, episode_reward=13.28 +/- 2.49
Episode length: 219.36 +/- 42.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 13.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 277000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00484  |
|    n_updates        | 66749    |
----------------------------------
Eval num_timesteps=277500, episode_reward=13.70 +/- 3.11
Episode length: 220.06 +/- 48.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 13.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 277500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0114   |
|    n_updates        | 66874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | 12.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2172     |
|    fps              | 111      |
|    time_elapsed     | 2498     |
|    total_timesteps  | 277681   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.022    |
|    n_updates        | 66920    |
----------------------------------
Eval num_timesteps=278000, episode_reward=12.96 +/- 2.82
Episode length: 215.60 +/- 43.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 13       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 278000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0247   |
|    n_updates        | 66999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 12.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2176     |
|    fps              | 111      |
|    time_elapsed     | 2505     |
|    total_timesteps  | 278471   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0217   |
|    n_updates        | 67117    |
----------------------------------
Eval num_timesteps=278500, episode_reward=9.92 +/- 1.73
Episode length: 174.00 +/- 26.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 174      |
|    mean_reward      | 9.92     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 278500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0103   |
|    n_updates        | 67124    |
----------------------------------
Eval num_timesteps=279000, episode_reward=11.94 +/- 2.42
Episode length: 194.66 +/- 37.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 195      |
|    mean_reward      | 11.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 279000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0186   |
|    n_updates        | 67249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | 12.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2180     |
|    fps              | 111      |
|    time_elapsed     | 2516     |
|    total_timesteps  | 279381   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0254   |
|    n_updates        | 67345    |
----------------------------------
Eval num_timesteps=279500, episode_reward=12.66 +/- 3.96
Episode length: 205.46 +/- 54.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 12.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 279500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 67374    |
----------------------------------
Eval num_timesteps=280000, episode_reward=7.50 +/- 1.98
Episode length: 152.86 +/- 32.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 153      |
|    mean_reward      | 7.5      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 280000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0142   |
|    n_updates        | 67499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 12.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2184     |
|    fps              | 110      |
|    time_elapsed     | 2527     |
|    total_timesteps  | 280275   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00905  |
|    n_updates        | 67568    |
----------------------------------
Eval num_timesteps=280500, episode_reward=10.36 +/- 3.40
Episode length: 181.02 +/- 46.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 10.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 280500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 67624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | 12.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2188     |
|    fps              | 110      |
|    time_elapsed     | 2533     |
|    total_timesteps  | 280829   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0199   |
|    n_updates        | 67707    |
----------------------------------
Eval num_timesteps=281000, episode_reward=12.02 +/- 2.49
Episode length: 210.46 +/- 44.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 12       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 281000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 67749    |
----------------------------------
Eval num_timesteps=281500, episode_reward=11.38 +/- 2.46
Episode length: 201.82 +/- 40.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | 11.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 281500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0285   |
|    n_updates        | 67874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | 12.2     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2192     |
|    fps              | 110      |
|    time_elapsed     | 2545     |
|    total_timesteps  | 281531   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0162   |
|    n_updates        | 67882    |
----------------------------------
Eval num_timesteps=282000, episode_reward=11.42 +/- 2.74
Episode length: 194.62 +/- 42.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 195      |
|    mean_reward      | 11.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 282000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00814  |
|    n_updates        | 67999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | 12.2     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2196     |
|    fps              | 110      |
|    time_elapsed     | 2551     |
|    total_timesteps  | 282198   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0063   |
|    n_updates        | 68049    |
----------------------------------
Eval num_timesteps=282500, episode_reward=10.64 +/- 2.04
Episode length: 191.76 +/- 34.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 192      |
|    mean_reward      | 10.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 282500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 68124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | 12.2     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2200     |
|    fps              | 110      |
|    time_elapsed     | 2557     |
|    total_timesteps  | 282997   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0155   |
|    n_updates        | 68249    |
----------------------------------
Eval num_timesteps=283000, episode_reward=13.76 +/- 3.41
Episode length: 229.62 +/- 49.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 13.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 283000   |
----------------------------------
Eval num_timesteps=283500, episode_reward=9.82 +/- 2.30
Episode length: 178.14 +/- 33.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 178      |
|    mean_reward      | 9.82     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 283500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0335   |
|    n_updates        | 68374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 11.9     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2204     |
|    fps              | 110      |
|    time_elapsed     | 2569     |
|    total_timesteps  | 283630   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00415  |
|    n_updates        | 68407    |
----------------------------------
Eval num_timesteps=284000, episode_reward=13.00 +/- 3.05
Episode length: 210.16 +/- 47.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 13       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 284000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 68499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | 11.9     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2208     |
|    fps              | 110      |
|    time_elapsed     | 2575     |
|    total_timesteps  | 284407   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0124   |
|    n_updates        | 68601    |
----------------------------------
Eval num_timesteps=284500, episode_reward=11.02 +/- 2.46
Episode length: 186.36 +/- 36.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 186      |
|    mean_reward      | 11       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 284500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0223   |
|    n_updates        | 68624    |
----------------------------------
Eval num_timesteps=285000, episode_reward=10.20 +/- 2.71
Episode length: 184.36 +/- 40.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 184      |
|    mean_reward      | 10.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 285000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0143   |
|    n_updates        | 68749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 11.8     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2212     |
|    fps              | 110      |
|    time_elapsed     | 2586     |
|    total_timesteps  | 285219   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0156   |
|    n_updates        | 68804    |
----------------------------------
Eval num_timesteps=285500, episode_reward=14.60 +/- 4.05
Episode length: 242.92 +/- 67.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 243      |
|    mean_reward      | 14.6     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 285500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 68874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 196      |
|    ep_rew_mean      | 11.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2216     |
|    fps              | 110      |
|    time_elapsed     | 2594     |
|    total_timesteps  | 285975   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 68993    |
----------------------------------
Eval num_timesteps=286000, episode_reward=11.20 +/- 2.00
Episode length: 189.60 +/- 32.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 11.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 286000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00505  |
|    n_updates        | 68999    |
----------------------------------
Eval num_timesteps=286500, episode_reward=12.76 +/- 3.33
Episode length: 213.44 +/- 49.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 213      |
|    mean_reward      | 12.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 286500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00428  |
|    n_updates        | 69124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 11.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2220     |
|    fps              | 110      |
|    time_elapsed     | 2605     |
|    total_timesteps  | 286727   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00713  |
|    n_updates        | 69181    |
----------------------------------
Eval num_timesteps=287000, episode_reward=10.46 +/- 1.85
Episode length: 186.06 +/- 32.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 186      |
|    mean_reward      | 10.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 287000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0117   |
|    n_updates        | 69249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 11.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2224     |
|    fps              | 110      |
|    time_elapsed     | 2611     |
|    total_timesteps  | 287338   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0156   |
|    n_updates        | 69334    |
----------------------------------
Eval num_timesteps=287500, episode_reward=3.86 +/- 2.15
Episode length: 109.38 +/- 35.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 109      |
|    mean_reward      | 3.86     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 287500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0217   |
|    n_updates        | 69374    |
----------------------------------
Eval num_timesteps=288000, episode_reward=12.66 +/- 3.27
Episode length: 215.96 +/- 45.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 12.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 288000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 69499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 11.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2228     |
|    fps              | 109      |
|    time_elapsed     | 2621     |
|    total_timesteps  | 288123   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00958  |
|    n_updates        | 69530    |
----------------------------------
Eval num_timesteps=288500, episode_reward=11.32 +/- 2.42
Episode length: 200.26 +/- 39.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 200      |
|    mean_reward      | 11.3     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 288500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 69624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 11.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2232     |
|    fps              | 109      |
|    time_elapsed     | 2627     |
|    total_timesteps  | 288832   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0131   |
|    n_updates        | 69707    |
----------------------------------
Eval num_timesteps=289000, episode_reward=10.38 +/- 2.22
Episode length: 174.12 +/- 29.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 174      |
|    mean_reward      | 10.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 289000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0121   |
|    n_updates        | 69749    |
----------------------------------
Eval num_timesteps=289500, episode_reward=10.16 +/- 2.06
Episode length: 174.86 +/- 28.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 175      |
|    mean_reward      | 10.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 289500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0234   |
|    n_updates        | 69874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 11.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2236     |
|    fps              | 109      |
|    time_elapsed     | 2637     |
|    total_timesteps  | 289527   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0106   |
|    n_updates        | 69881    |
----------------------------------
Eval num_timesteps=290000, episode_reward=11.82 +/- 2.90
Episode length: 202.58 +/- 47.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | 11.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 290000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 69999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 11.2     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2240     |
|    fps              | 109      |
|    time_elapsed     | 2644     |
|    total_timesteps  | 290343   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00556  |
|    n_updates        | 70085    |
----------------------------------
Eval num_timesteps=290500, episode_reward=13.10 +/- 3.05
Episode length: 217.10 +/- 56.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 13.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 290500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0129   |
|    n_updates        | 70124    |
----------------------------------
Eval num_timesteps=291000, episode_reward=12.20 +/- 2.12
Episode length: 206.60 +/- 32.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | 12.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 291000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00984  |
|    n_updates        | 70249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 11.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2244     |
|    fps              | 109      |
|    time_elapsed     | 2656     |
|    total_timesteps  | 291386   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0239   |
|    n_updates        | 70346    |
----------------------------------
Eval num_timesteps=291500, episode_reward=12.94 +/- 2.82
Episode length: 223.82 +/- 38.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 12.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 291500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0126   |
|    n_updates        | 70374    |
----------------------------------
Eval num_timesteps=292000, episode_reward=13.22 +/- 4.73
Episode length: 231.82 +/- 68.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 13.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 292000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0184   |
|    n_updates        | 70499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 11.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2248     |
|    fps              | 109      |
|    time_elapsed     | 2670     |
|    total_timesteps  | 292233   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 70558    |
----------------------------------
Eval num_timesteps=292500, episode_reward=13.16 +/- 2.99
Episode length: 216.12 +/- 44.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 13.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 292500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 70624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 190      |
|    ep_rew_mean      | 11.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2252     |
|    fps              | 109      |
|    time_elapsed     | 2676     |
|    total_timesteps  | 292853   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0238   |
|    n_updates        | 70713    |
----------------------------------
Eval num_timesteps=293000, episode_reward=15.42 +/- 4.01
Episode length: 245.82 +/- 51.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 246      |
|    mean_reward      | 15.4     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 293000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0194   |
|    n_updates        | 70749    |
----------------------------------
Eval num_timesteps=293500, episode_reward=13.72 +/- 3.58
Episode length: 221.58 +/- 54.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 13.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 293500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0333   |
|    n_updates        | 70874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 190      |
|    ep_rew_mean      | 11.2     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2256     |
|    fps              | 109      |
|    time_elapsed     | 2690     |
|    total_timesteps  | 293615   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00421  |
|    n_updates        | 70903    |
----------------------------------
Eval num_timesteps=294000, episode_reward=14.00 +/- 4.50
Episode length: 232.68 +/- 67.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 233      |
|    mean_reward      | 14       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 294000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 70999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 11.3     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2260     |
|    fps              | 109      |
|    time_elapsed     | 2697     |
|    total_timesteps  | 294492   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0338   |
|    n_updates        | 71122    |
----------------------------------
Eval num_timesteps=294500, episode_reward=12.00 +/- 2.70
Episode length: 206.36 +/- 43.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | 12       |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 294500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.017    |
|    n_updates        | 71124    |
----------------------------------
Eval num_timesteps=295000, episode_reward=10.06 +/- 2.20
Episode length: 176.62 +/- 39.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 177      |
|    mean_reward      | 10.1     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 295000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0139   |
|    n_updates        | 71249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 11.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2264     |
|    fps              | 109      |
|    time_elapsed     | 2709     |
|    total_timesteps  | 295343   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00609  |
|    n_updates        | 71335    |
----------------------------------
Eval num_timesteps=295500, episode_reward=12.90 +/- 2.91
Episode length: 221.98 +/- 59.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 12.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 295500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00886  |
|    n_updates        | 71374    |
----------------------------------
Eval num_timesteps=296000, episode_reward=12.92 +/- 4.29
Episode length: 217.90 +/- 53.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 12.9     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 296000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0117   |
|    n_updates        | 71499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | 11.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2268     |
|    fps              | 108      |
|    time_elapsed     | 2722     |
|    total_timesteps  | 296328   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0367   |
|    n_updates        | 71581    |
----------------------------------
Eval num_timesteps=296500, episode_reward=13.48 +/- 3.37
Episode length: 232.60 +/- 48.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 233      |
|    mean_reward      | 13.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 296500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00317  |
|    n_updates        | 71624    |
----------------------------------
Eval num_timesteps=297000, episode_reward=8.50 +/- 1.90
Episode length: 161.36 +/- 23.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 161      |
|    mean_reward      | 8.5      |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 297000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0194   |
|    n_updates        | 71749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | 11.4     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2272     |
|    fps              | 108      |
|    time_elapsed     | 2734     |
|    total_timesteps  | 297124   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0115   |
|    n_updates        | 71780    |
----------------------------------
Eval num_timesteps=297500, episode_reward=13.70 +/- 3.49
Episode length: 220.74 +/- 52.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 13.7     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 297500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 71874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | 11.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2276     |
|    fps              | 108      |
|    time_elapsed     | 2740     |
|    total_timesteps  | 297931   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00384  |
|    n_updates        | 71982    |
----------------------------------
Eval num_timesteps=298000, episode_reward=11.50 +/- 2.50
Episode length: 189.56 +/- 40.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 11.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 298000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0156   |
|    n_updates        | 71999    |
----------------------------------
Eval num_timesteps=298500, episode_reward=10.22 +/- 2.32
Episode length: 177.54 +/- 34.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 178      |
|    mean_reward      | 10.2     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 298500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0168   |
|    n_updates        | 72124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 11.5     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2280     |
|    fps              | 108      |
|    time_elapsed     | 2751     |
|    total_timesteps  | 298700   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00615  |
|    n_updates        | 72174    |
----------------------------------
Eval num_timesteps=299000, episode_reward=13.46 +/- 3.37
Episode length: 216.74 +/- 53.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 13.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 299000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0331   |
|    n_updates        | 72249    |
----------------------------------
Eval num_timesteps=299500, episode_reward=11.52 +/- 2.85
Episode length: 198.94 +/- 45.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 199      |
|    mean_reward      | 11.5     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 299500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 72374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | 11.6     |
|    exploration_rate | 0.016    |
| time/               |          |
|    episodes         | 2284     |
|    fps              | 108      |
|    time_elapsed     | 2764     |
|    total_timesteps  | 299671   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00849  |
|    n_updates        | 72417    |
----------------------------------
Eval num_timesteps=300000, episode_reward=12.82 +/- 4.07
Episode length: 213.44 +/- 58.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 213      |
|    mean_reward      | 12.8     |
| rollout/            |          |
|    exploration_rate | 0.016    |
| time/               |          |
|    total_timesteps  | 300000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0081   |
|    n_updates        | 72499    |
----------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/defend-center/dqn-1/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'batch_size': 32, 'learning_rate': 0.0001, 'buffer_size': 15000, 'gamma': 0.92, 'exploration_fraction': 0.28, 'exploration_final_eps': 0.016, 'learning_starts': 10000.0, 'decay_start_steps': 0, 'decay_end_steps': 150000.0}
