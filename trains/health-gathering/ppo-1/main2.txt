/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
Eval num_timesteps=500, episode_reward=317.92 +/- 55.21
Episode length: 42.26 +/- 5.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 500      |
---------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=312.16 +/- 46.87
Episode length: 41.68 +/- 4.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
Eval num_timesteps=1500, episode_reward=314.72 +/- 48.72
Episode length: 41.92 +/- 4.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 1500     |
---------------------------------
Eval num_timesteps=2000, episode_reward=324.32 +/- 59.97
Episode length: 42.86 +/- 5.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.9     |
|    mean_reward     | 324      |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 46.2     |
|    ep_rew_mean     | 358      |
| time/              |          |
|    fps             | 272      |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 2048     |
---------------------------------
Eval num_timesteps=2500, episode_reward=320.48 +/- 39.97
Episode length: 42.44 +/- 3.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.4        |
|    mean_reward          | 320         |
| time/                   |             |
|    total_timesteps      | 2500        |
| train/                  |             |
|    approx_kl            | 0.006502302 |
|    clip_fraction        | 0.00747     |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | -0.00122    |
|    learning_rate        | 1e-05       |
|    loss                 | 4.93e+03    |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00161    |
|    value_loss           | 1.05e+04    |
-----------------------------------------
Eval num_timesteps=3000, episode_reward=314.72 +/- 46.58
Episode length: 41.94 +/- 4.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 3000     |
---------------------------------
Eval num_timesteps=3500, episode_reward=312.80 +/- 35.20
Episode length: 41.70 +/- 3.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 3500     |
---------------------------------
Eval num_timesteps=4000, episode_reward=313.44 +/- 35.54
Episode length: 41.76 +/- 3.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 48.1     |
|    ep_rew_mean     | 377      |
| time/              |          |
|    fps             | 262      |
|    iterations      | 2        |
|    time_elapsed    | 15       |
|    total_timesteps | 4096     |
---------------------------------
Eval num_timesteps=4500, episode_reward=314.08 +/- 33.51
Episode length: 41.82 +/- 3.14
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 41.8       |
|    mean_reward          | 314        |
| time/                   |            |
|    total_timesteps      | 4500       |
| train/                  |            |
|    approx_kl            | 0.00545605 |
|    clip_fraction        | 0.0771     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.36      |
|    explained_variance   | -0.0359    |
|    learning_rate        | 1e-05      |
|    loss                 | 4.23e+03   |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00351   |
|    value_loss           | 9.4e+03    |
----------------------------------------
Eval num_timesteps=5000, episode_reward=316.64 +/- 37.04
Episode length: 42.06 +/- 3.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
Eval num_timesteps=5500, episode_reward=311.52 +/- 36.21
Episode length: 41.58 +/- 3.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 5500     |
---------------------------------
Eval num_timesteps=6000, episode_reward=298.72 +/- 30.83
Episode length: 40.40 +/- 2.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.4     |
|    mean_reward     | 299      |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | 412      |
| time/              |          |
|    fps             | 260      |
|    iterations      | 3        |
|    time_elapsed    | 23       |
|    total_timesteps | 6144     |
---------------------------------
Eval num_timesteps=6500, episode_reward=305.12 +/- 35.41
Episode length: 41.00 +/- 3.38
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41           |
|    mean_reward          | 305          |
| time/                   |              |
|    total_timesteps      | 6500         |
| train/                  |              |
|    approx_kl            | 0.0041395244 |
|    clip_fraction        | 0.0388       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.36        |
|    explained_variance   | -0.152       |
|    learning_rate        | 1e-05        |
|    loss                 | 4.15e+03     |
|    n_updates            | 30           |
|    policy_gradient_loss | 0.00172      |
|    value_loss           | 1.06e+04     |
------------------------------------------
Eval num_timesteps=7000, episode_reward=314.08 +/- 44.99
Episode length: 41.86 +/- 4.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 7000     |
---------------------------------
Eval num_timesteps=7500, episode_reward=317.28 +/- 35.61
Episode length: 42.12 +/- 3.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 7500     |
---------------------------------
Eval num_timesteps=8000, episode_reward=308.96 +/- 34.64
Episode length: 41.34 +/- 3.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.6     |
|    ep_rew_mean     | 423      |
| time/              |          |
|    fps             | 259      |
|    iterations      | 4        |
|    time_elapsed    | 31       |
|    total_timesteps | 8192     |
---------------------------------
Eval num_timesteps=8500, episode_reward=305.12 +/- 31.10
Episode length: 40.98 +/- 2.92
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41          |
|    mean_reward          | 305         |
| time/                   |             |
|    total_timesteps      | 8500        |
| train/                  |             |
|    approx_kl            | 0.003197958 |
|    clip_fraction        | 0.0294      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.37       |
|    explained_variance   | -0.208      |
|    learning_rate        | 1e-05       |
|    loss                 | 5.84e+03    |
|    n_updates            | 40          |
|    policy_gradient_loss | 0.00106     |
|    value_loss           | 1.22e+04    |
-----------------------------------------
Eval num_timesteps=9000, episode_reward=319.84 +/- 44.64
Episode length: 42.42 +/- 4.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 9000     |
---------------------------------
Eval num_timesteps=9500, episode_reward=314.08 +/- 40.18
Episode length: 41.86 +/- 3.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 9500     |
---------------------------------
Eval num_timesteps=10000, episode_reward=314.72 +/- 37.30
Episode length: 41.88 +/- 3.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 259      |
|    iterations      | 5        |
|    time_elapsed    | 39       |
|    total_timesteps | 10240    |
---------------------------------
Eval num_timesteps=10500, episode_reward=304.48 +/- 32.48
Episode length: 40.92 +/- 3.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.9        |
|    mean_reward          | 304         |
| time/                   |             |
|    total_timesteps      | 10500       |
| train/                  |             |
|    approx_kl            | 0.008584007 |
|    clip_fraction        | 0.066       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.36       |
|    explained_variance   | -0.233      |
|    learning_rate        | 1e-05       |
|    loss                 | 5.42e+03    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00324    |
|    value_loss           | 1.31e+04    |
-----------------------------------------
Eval num_timesteps=11000, episode_reward=312.16 +/- 39.26
Episode length: 41.66 +/- 3.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 11000    |
---------------------------------
Eval num_timesteps=11500, episode_reward=314.72 +/- 39.95
Episode length: 41.90 +/- 3.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 11500    |
---------------------------------
Eval num_timesteps=12000, episode_reward=316.64 +/- 37.59
Episode length: 42.08 +/- 3.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | 404      |
| time/              |          |
|    fps             | 259      |
|    iterations      | 6        |
|    time_elapsed    | 47       |
|    total_timesteps | 12288    |
---------------------------------
Eval num_timesteps=12500, episode_reward=314.72 +/- 35.61
Episode length: 41.88 +/- 3.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 315         |
| time/                   |             |
|    total_timesteps      | 12500       |
| train/                  |             |
|    approx_kl            | 0.012166135 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | -0.154      |
|    learning_rate        | 1e-05       |
|    loss                 | 5.38e+03    |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0036     |
|    value_loss           | 1.2e+04     |
-----------------------------------------
Eval num_timesteps=13000, episode_reward=318.56 +/- 34.96
Episode length: 42.24 +/- 3.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 13000    |
---------------------------------
Eval num_timesteps=13500, episode_reward=317.28 +/- 36.18
Episode length: 42.12 +/- 3.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 13500    |
---------------------------------
Eval num_timesteps=14000, episode_reward=316.00 +/- 39.97
Episode length: 42.02 +/- 3.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 7        |
|    time_elapsed    | 55       |
|    total_timesteps | 14336    |
---------------------------------
Eval num_timesteps=14500, episode_reward=316.64 +/- 38.66
Episode length: 42.08 +/- 3.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 317         |
| time/                   |             |
|    total_timesteps      | 14500       |
| train/                  |             |
|    approx_kl            | 0.006179874 |
|    clip_fraction        | 0.0488      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.085       |
|    learning_rate        | 1e-05       |
|    loss                 | 3.83e+03    |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.000551   |
|    value_loss           | 1.04e+04    |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=307.68 +/- 32.51
Episode length: 41.22 +/- 3.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
Eval num_timesteps=15500, episode_reward=316.64 +/- 50.19
Episode length: 42.10 +/- 4.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 15500    |
---------------------------------
Eval num_timesteps=16000, episode_reward=307.04 +/- 35.08
Episode length: 41.18 +/- 3.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.5     |
|    ep_rew_mean     | 402      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 8        |
|    time_elapsed    | 63       |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=16500, episode_reward=317.92 +/- 41.18
Episode length: 42.22 +/- 3.97
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 318          |
| time/                   |              |
|    total_timesteps      | 16500        |
| train/                  |              |
|    approx_kl            | 0.0076700435 |
|    clip_fraction        | 0.0389       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.31        |
|    explained_variance   | 0.204        |
|    learning_rate        | 1e-05        |
|    loss                 | 5.76e+03     |
|    n_updates            | 80           |
|    policy_gradient_loss | -4.19e-06    |
|    value_loss           | 1.03e+04     |
------------------------------------------
Eval num_timesteps=17000, episode_reward=310.88 +/- 35.84
Episode length: 41.54 +/- 3.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 17000    |
---------------------------------
Eval num_timesteps=17500, episode_reward=314.72 +/- 36.74
Episode length: 41.88 +/- 3.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 17500    |
---------------------------------
Eval num_timesteps=18000, episode_reward=305.12 +/- 31.10
Episode length: 40.98 +/- 2.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 305      |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.2     |
|    ep_rew_mean     | 418      |
| time/              |          |
|    fps             | 258      |
|    iterations      | 9        |
|    time_elapsed    | 71       |
|    total_timesteps | 18432    |
---------------------------------
Eval num_timesteps=18500, episode_reward=436.32 +/- 141.17
Episode length: 54.02 +/- 14.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 54          |
|    mean_reward          | 436         |
| time/                   |             |
|    total_timesteps      | 18500       |
| train/                  |             |
|    approx_kl            | 0.010160117 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.326       |
|    learning_rate        | 1e-05       |
|    loss                 | 4.77e+03    |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 8.97e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=19000, episode_reward=453.60 +/- 202.61
Episode length: 55.72 +/- 20.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.7     |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 19000    |
---------------------------------
New best mean reward!
Eval num_timesteps=19500, episode_reward=486.24 +/- 169.38
Episode length: 59.02 +/- 16.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59       |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 19500    |
---------------------------------
New best mean reward!
Eval num_timesteps=20000, episode_reward=495.20 +/- 223.82
Episode length: 59.84 +/- 22.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.8     |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.9     |
|    ep_rew_mean     | 415      |
| time/              |          |
|    fps             | 251      |
|    iterations      | 10       |
|    time_elapsed    | 81       |
|    total_timesteps | 20480    |
---------------------------------
Eval num_timesteps=20500, episode_reward=1492.48 +/- 736.55
Episode length: 154.02 +/- 69.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 154         |
|    mean_reward          | 1.49e+03    |
| time/                   |             |
|    total_timesteps      | 20500       |
| train/                  |             |
|    approx_kl            | 0.010938307 |
|    clip_fraction        | 0.0912      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.331       |
|    learning_rate        | 1e-05       |
|    loss                 | 6.14e+03    |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00339    |
|    value_loss           | 9.59e+03    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=21000, episode_reward=1207.04 +/- 819.29
Episode length: 127.20 +/- 77.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 127      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 21000    |
---------------------------------
Eval num_timesteps=21500, episode_reward=1339.04 +/- 754.40
Episode length: 139.76 +/- 70.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 140      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 21500    |
---------------------------------
Eval num_timesteps=22000, episode_reward=1349.28 +/- 783.88
Episode length: 140.76 +/- 74.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 141      |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=22500, episode_reward=1045.12 +/- 735.78
Episode length: 111.80 +/- 69.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 22500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52.3     |
|    ep_rew_mean     | 419      |
| time/              |          |
|    fps             | 207      |
|    iterations      | 11       |
|    time_elapsed    | 108      |
|    total_timesteps | 22528    |
---------------------------------
Eval num_timesteps=23000, episode_reward=288.48 +/- 18.11
Episode length: 39.42 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.4        |
|    mean_reward          | 288         |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.011292396 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.204       |
|    learning_rate        | 1e-05       |
|    loss                 | 8.94e+03    |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00369    |
|    value_loss           | 1.15e+04    |
-----------------------------------------
Eval num_timesteps=23500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 23500    |
---------------------------------
Eval num_timesteps=24000, episode_reward=287.84 +/- 18.81
Episode length: 39.36 +/- 1.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.4     |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=24500, episode_reward=287.84 +/- 15.20
Episode length: 39.36 +/- 1.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39.4     |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 24500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.3     |
|    ep_rew_mean     | 439      |
| time/              |          |
|    fps             | 205      |
|    iterations      | 12       |
|    time_elapsed    | 119      |
|    total_timesteps | 24576    |
---------------------------------
Eval num_timesteps=25000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.011604872 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.245       |
|    learning_rate        | 1e-05       |
|    loss                 | 3.87e+03    |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00436    |
|    value_loss           | 1.11e+04    |
-----------------------------------------
Eval num_timesteps=25500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 25500    |
---------------------------------
Eval num_timesteps=26000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=26500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 26500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.2     |
|    ep_rew_mean     | 438      |
| time/              |          |
|    fps             | 204      |
|    iterations      | 13       |
|    time_elapsed    | 129      |
|    total_timesteps | 26624    |
---------------------------------
Eval num_timesteps=27000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 27000       |
| train/                  |             |
|    approx_kl            | 0.011118398 |
|    clip_fraction        | 0.0936      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.32       |
|    explained_variance   | 0.209       |
|    learning_rate        | 1e-05       |
|    loss                 | 6.5e+03     |
|    n_updates            | 130         |
|    policy_gradient_loss | 0.00157     |
|    value_loss           | 1.21e+04    |
-----------------------------------------
Eval num_timesteps=27500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 27500    |
---------------------------------
Eval num_timesteps=28000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=28500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 28500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53       |
|    ep_rew_mean     | 427      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 14       |
|    time_elapsed    | 140      |
|    total_timesteps | 28672    |
---------------------------------
Eval num_timesteps=29000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 29000       |
| train/                  |             |
|    approx_kl            | 0.010853072 |
|    clip_fraction        | 0.0746      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.132       |
|    learning_rate        | 1e-05       |
|    loss                 | 7.65e+03    |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0017     |
|    value_loss           | 1.33e+04    |
-----------------------------------------
Eval num_timesteps=29500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 29500    |
---------------------------------
Eval num_timesteps=30000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=30500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 30500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.6     |
|    ep_rew_mean     | 412      |
| time/              |          |
|    fps             | 203      |
|    iterations      | 15       |
|    time_elapsed    | 151      |
|    total_timesteps | 30720    |
---------------------------------
Eval num_timesteps=31000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 39           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0049949354 |
|    clip_fraction        | 0.0579       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.115        |
|    learning_rate        | 1e-05        |
|    loss                 | 6.28e+03     |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000167    |
|    value_loss           | 1.26e+04     |
------------------------------------------
Eval num_timesteps=31500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 31500    |
---------------------------------
Eval num_timesteps=32000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=32500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 32500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.8     |
|    ep_rew_mean     | 415      |
| time/              |          |
|    fps             | 202      |
|    iterations      | 16       |
|    time_elapsed    | 162      |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=33000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 33000       |
| train/                  |             |
|    approx_kl            | 0.010513706 |
|    clip_fraction        | 0.0729      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.33       |
|    explained_variance   | 0.0716      |
|    learning_rate        | 1e-05       |
|    loss                 | 4.57e+03    |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00178    |
|    value_loss           | 1.28e+04    |
-----------------------------------------
Eval num_timesteps=33500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 33500    |
---------------------------------
Eval num_timesteps=34000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=34500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 34500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54       |
|    ep_rew_mean     | 436      |
| time/              |          |
|    fps             | 201      |
|    iterations      | 17       |
|    time_elapsed    | 172      |
|    total_timesteps | 34816    |
---------------------------------
Eval num_timesteps=35000, episode_reward=307.04 +/- 39.47
Episode length: 41.20 +/- 3.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.2        |
|    mean_reward          | 307         |
| time/                   |             |
|    total_timesteps      | 35000       |
| train/                  |             |
|    approx_kl            | 0.008404595 |
|    clip_fraction        | 0.0829      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.23        |
|    learning_rate        | 1e-05       |
|    loss                 | 6.15e+03    |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00337    |
|    value_loss           | 1.08e+04    |
-----------------------------------------
Eval num_timesteps=35500, episode_reward=300.00 +/- 30.86
Episode length: 40.50 +/- 2.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.5     |
|    mean_reward     | 300      |
| time/              |          |
|    total_timesteps | 35500    |
---------------------------------
Eval num_timesteps=36000, episode_reward=298.08 +/- 29.44
Episode length: 40.34 +/- 2.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.3     |
|    mean_reward     | 298      |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=36500, episode_reward=296.80 +/- 24.79
Episode length: 40.20 +/- 2.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.2     |
|    mean_reward     | 297      |
| time/              |          |
|    total_timesteps | 36500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.3     |
|    ep_rew_mean     | 430      |
| time/              |          |
|    fps             | 200      |
|    iterations      | 18       |
|    time_elapsed    | 183      |
|    total_timesteps | 36864    |
---------------------------------
Eval num_timesteps=37000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.008085636 |
|    clip_fraction        | 0.059       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.178       |
|    learning_rate        | 1e-05       |
|    loss                 | 5.41e+03    |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00169    |
|    value_loss           | 1.07e+04    |
-----------------------------------------
Eval num_timesteps=37500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 37500    |
---------------------------------
Eval num_timesteps=38000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=38500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 38500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.5     |
|    ep_rew_mean     | 410      |
| time/              |          |
|    fps             | 200      |
|    iterations      | 19       |
|    time_elapsed    | 194      |
|    total_timesteps | 38912    |
---------------------------------
Eval num_timesteps=39000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 39           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 39000        |
| train/                  |              |
|    approx_kl            | 0.0031374495 |
|    clip_fraction        | 0.0542       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.3         |
|    explained_variance   | 0.171        |
|    learning_rate        | 1e-05        |
|    loss                 | 5.13e+03     |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.000191    |
|    value_loss           | 1.15e+04     |
------------------------------------------
Eval num_timesteps=39500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 39500    |
---------------------------------
Eval num_timesteps=40000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=40500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 40500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 49.4     |
|    ep_rew_mean     | 390      |
| time/              |          |
|    fps             | 200      |
|    iterations      | 20       |
|    time_elapsed    | 204      |
|    total_timesteps | 40960    |
---------------------------------
Eval num_timesteps=41000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.009989736 |
|    clip_fraction        | 0.047       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.261       |
|    learning_rate        | 1e-05       |
|    loss                 | 4.3e+03     |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00187    |
|    value_loss           | 9.79e+03    |
-----------------------------------------
Eval num_timesteps=41500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 41500    |
---------------------------------
Eval num_timesteps=42000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=42500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 42500    |
---------------------------------
Eval num_timesteps=43000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | 404      |
| time/              |          |
|    fps             | 198      |
|    iterations      | 21       |
|    time_elapsed    | 216      |
|    total_timesteps | 43008    |
---------------------------------
Eval num_timesteps=43500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 43500       |
| train/                  |             |
|    approx_kl            | 0.009071499 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.29       |
|    explained_variance   | 0.214       |
|    learning_rate        | 1e-05       |
|    loss                 | 5.24e+03    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00335    |
|    value_loss           | 1.06e+04    |
-----------------------------------------
Eval num_timesteps=44000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 44500    |
---------------------------------
Eval num_timesteps=45000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.8     |
|    ep_rew_mean     | 435      |
| time/              |          |
|    fps             | 198      |
|    iterations      | 22       |
|    time_elapsed    | 227      |
|    total_timesteps | 45056    |
---------------------------------
Eval num_timesteps=45500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 45500       |
| train/                  |             |
|    approx_kl            | 0.013032962 |
|    clip_fraction        | 0.0901      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.118       |
|    learning_rate        | 1e-05       |
|    loss                 | 5.67e+03    |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.000979   |
|    value_loss           | 1.19e+04    |
-----------------------------------------
Eval num_timesteps=46000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 46000    |
---------------------------------
Eval num_timesteps=46500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 46500    |
---------------------------------
Eval num_timesteps=47000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 54.6     |
|    ep_rew_mean     | 442      |
| time/              |          |
|    fps             | 198      |
|    iterations      | 23       |
|    time_elapsed    | 237      |
|    total_timesteps | 47104    |
---------------------------------
Eval num_timesteps=47500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 47500       |
| train/                  |             |
|    approx_kl            | 0.005367659 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.128       |
|    learning_rate        | 1e-05       |
|    loss                 | 7.55e+03    |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.0017     |
|    value_loss           | 1.25e+04    |
-----------------------------------------
Eval num_timesteps=48000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=48500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 48500    |
---------------------------------
Eval num_timesteps=49000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.1     |
|    ep_rew_mean     | 408      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 24       |
|    time_elapsed    | 248      |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 39           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 49500        |
| train/                  |              |
|    approx_kl            | 0.0044581615 |
|    clip_fraction        | 0.0626       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.23        |
|    explained_variance   | 0.164        |
|    learning_rate        | 1e-05        |
|    loss                 | 6.4e+03      |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00126     |
|    value_loss           | 1.23e+04     |
------------------------------------------
Eval num_timesteps=50000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 50000    |
---------------------------------
Eval num_timesteps=50500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 50500    |
---------------------------------
Eval num_timesteps=51000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50       |
|    ep_rew_mean     | 396      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 25       |
|    time_elapsed    | 258      |
|    total_timesteps | 51200    |
---------------------------------
Eval num_timesteps=51500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 39           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 51500        |
| train/                  |              |
|    approx_kl            | 0.0058143986 |
|    clip_fraction        | 0.0472       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.19        |
|    explained_variance   | 0.151        |
|    learning_rate        | 1e-05        |
|    loss                 | 5.04e+03     |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00105     |
|    value_loss           | 1.16e+04     |
------------------------------------------
Eval num_timesteps=52000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=52500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 52500    |
---------------------------------
Eval num_timesteps=53000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 50.8     |
|    ep_rew_mean     | 403      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 26       |
|    time_elapsed    | 269      |
|    total_timesteps | 53248    |
---------------------------------
Eval num_timesteps=53500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 39           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 53500        |
| train/                  |              |
|    approx_kl            | 0.0037584854 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.17        |
|    explained_variance   | 0.194        |
|    learning_rate        | 1e-05        |
|    loss                 | 3.77e+03     |
|    n_updates            | 260          |
|    policy_gradient_loss | 0.0012       |
|    value_loss           | 1.07e+04     |
------------------------------------------
Eval num_timesteps=54000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=54500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 54500    |
---------------------------------
Eval num_timesteps=55000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 51.3     |
|    ep_rew_mean     | 409      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 27       |
|    time_elapsed    | 279      |
|    total_timesteps | 55296    |
---------------------------------
Eval num_timesteps=55500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 39         |
|    mean_reward          | 284        |
| time/                   |            |
|    total_timesteps      | 55500      |
| train/                  |            |
|    approx_kl            | 0.00602182 |
|    clip_fraction        | 0.0801     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.22      |
|    explained_variance   | 0.154      |
|    learning_rate        | 1e-05      |
|    loss                 | 4.79e+03   |
|    n_updates            | 270        |
|    policy_gradient_loss | -0.00271   |
|    value_loss           | 1.01e+04   |
----------------------------------------
Eval num_timesteps=56000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=56500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 56500    |
---------------------------------
Eval num_timesteps=57000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 52       |
|    ep_rew_mean     | 416      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 28       |
|    time_elapsed    | 290      |
|    total_timesteps | 57344    |
---------------------------------
Eval num_timesteps=57500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 57500       |
| train/                  |             |
|    approx_kl            | 0.008232796 |
|    clip_fraction        | 0.0728      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.2        |
|    explained_variance   | 0.144       |
|    learning_rate        | 1e-05       |
|    loss                 | 4.41e+03    |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00236    |
|    value_loss           | 1.12e+04    |
-----------------------------------------
Eval num_timesteps=58000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 58000    |
---------------------------------
Eval num_timesteps=58500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 58500    |
---------------------------------
Eval num_timesteps=59000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 53.8     |
|    ep_rew_mean     | 434      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 29       |
|    time_elapsed    | 300      |
|    total_timesteps | 59392    |
---------------------------------
Eval num_timesteps=59500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39          |
|    mean_reward          | 284         |
| time/                   |             |
|    total_timesteps      | 59500       |
| train/                  |             |
|    approx_kl            | 0.008933846 |
|    clip_fraction        | 0.0778      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.25       |
|    explained_variance   | 0.105       |
|    learning_rate        | 1e-05       |
|    loss                 | 4.98e+03    |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00332    |
|    value_loss           | 1.18e+04    |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=60500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 60500    |
---------------------------------
Eval num_timesteps=61000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 57.1     |
|    ep_rew_mean     | 469      |
| time/              |          |
|    fps             | 197      |
|    iterations      | 30       |
|    time_elapsed    | 311      |
|    total_timesteps | 61440    |
---------------------------------
Eval num_timesteps=61500, episode_reward=1198.40 +/- 660.31
Episode length: 127.58 +/- 62.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 128          |
|    mean_reward          | 1.2e+03      |
| time/                   |              |
|    total_timesteps      | 61500        |
| train/                  |              |
|    approx_kl            | 0.0041115894 |
|    clip_fraction        | 0.0764       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.22        |
|    explained_variance   | 0.0827       |
|    learning_rate        | 1e-05        |
|    loss                 | 6.54e+03     |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00186     |
|    value_loss           | 1.14e+04     |
------------------------------------------
Eval num_timesteps=62000, episode_reward=1216.16 +/- 704.44
Episode length: 129.08 +/- 66.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 129      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 62000    |
---------------------------------
Eval num_timesteps=62500, episode_reward=1226.88 +/- 705.30
Episode length: 129.92 +/- 66.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 62500    |
---------------------------------
Eval num_timesteps=63000, episode_reward=1124.80 +/- 625.46
Episode length: 121.06 +/- 59.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 61       |
|    ep_rew_mean     | 507      |
| time/              |          |
|    fps             | 187      |
|    iterations      | 31       |
|    time_elapsed    | 338      |
|    total_timesteps | 63488    |
---------------------------------
Eval num_timesteps=63500, episode_reward=1794.72 +/- 592.03
Episode length: 181.98 +/- 55.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 182         |
|    mean_reward          | 1.79e+03    |
| time/                   |             |
|    total_timesteps      | 63500       |
| train/                  |             |
|    approx_kl            | 0.010782092 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.0983      |
|    learning_rate        | 1e-05       |
|    loss                 | 5.6e+03     |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00244    |
|    value_loss           | 1.26e+04    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=64000, episode_reward=1770.40 +/- 612.12
Episode length: 179.52 +/- 57.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 180      |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=64500, episode_reward=1835.68 +/- 521.73
Episode length: 186.06 +/- 48.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 186      |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 64500    |
---------------------------------
New best mean reward!
Eval num_timesteps=65000, episode_reward=1612.16 +/- 686.56
Episode length: 165.18 +/- 64.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=65500, episode_reward=1646.72 +/- 660.37
Episode length: 168.62 +/- 61.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 169      |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 65500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 63.6     |
|    ep_rew_mean     | 535      |
| time/              |          |
|    fps             | 171      |
|    iterations      | 32       |
|    time_elapsed    | 381      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=66000, episode_reward=1146.56 +/- 663.61
Episode length: 122.36 +/- 62.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 122         |
|    mean_reward          | 1.15e+03    |
| time/                   |             |
|    total_timesteps      | 66000       |
| train/                  |             |
|    approx_kl            | 0.011459744 |
|    clip_fraction        | 0.0982      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.042       |
|    learning_rate        | 1e-05       |
|    loss                 | 5.08e+03    |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00498    |
|    value_loss           | 1.34e+04    |
-----------------------------------------
Eval num_timesteps=66500, episode_reward=1173.92 +/- 582.03
Episode length: 125.68 +/- 54.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 126      |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 66500    |
---------------------------------
Eval num_timesteps=67000, episode_reward=1100.96 +/- 564.37
Episode length: 119.18 +/- 54.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=67500, episode_reward=1213.60 +/- 557.86
Episode length: 129.72 +/- 52.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 67500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.1     |
|    ep_rew_mean     | 600      |
| time/              |          |
|    fps             | 165      |
|    iterations      | 33       |
|    time_elapsed    | 407      |
|    total_timesteps | 67584    |
---------------------------------
Eval num_timesteps=68000, episode_reward=1675.04 +/- 587.72
Episode length: 171.70 +/- 54.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 172         |
|    mean_reward          | 1.68e+03    |
| time/                   |             |
|    total_timesteps      | 68000       |
| train/                  |             |
|    approx_kl            | 0.008137802 |
|    clip_fraction        | 0.0588      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | 0.0733      |
|    learning_rate        | 1e-05       |
|    loss                 | 7.48e+03    |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00257    |
|    value_loss           | 1.3e+04     |
-----------------------------------------
Eval num_timesteps=68500, episode_reward=1540.96 +/- 680.82
Episode length: 158.68 +/- 63.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 159      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 68500    |
---------------------------------
Eval num_timesteps=69000, episode_reward=1625.28 +/- 690.40
Episode length: 166.08 +/- 64.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=69500, episode_reward=1674.40 +/- 623.79
Episode length: 171.62 +/- 58.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 172      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 69500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 78.4     |
|    ep_rew_mean     | 686      |
| time/              |          |
|    fps             | 157      |
|    iterations      | 34       |
|    time_elapsed    | 441      |
|    total_timesteps | 69632    |
---------------------------------
Eval num_timesteps=70000, episode_reward=1464.16 +/- 679.93
Episode length: 152.70 +/- 64.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 153         |
|    mean_reward          | 1.46e+03    |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.011129184 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.08       |
|    explained_variance   | 0.0826      |
|    learning_rate        | 1e-05       |
|    loss                 | 6.32e+03    |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.003      |
|    value_loss           | 1.39e+04    |
-----------------------------------------
Eval num_timesteps=70500, episode_reward=1565.92 +/- 647.10
Episode length: 162.00 +/- 60.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 162      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 70500    |
---------------------------------
Eval num_timesteps=71000, episode_reward=1728.00 +/- 579.14
Episode length: 176.78 +/- 53.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 177      |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=71500, episode_reward=1673.12 +/- 578.88
Episode length: 172.30 +/- 54.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 172      |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 71500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 80.7     |
|    ep_rew_mean     | 710      |
| time/              |          |
|    fps             | 150      |
|    iterations      | 35       |
|    time_elapsed    | 475      |
|    total_timesteps | 71680    |
---------------------------------
Eval num_timesteps=72000, episode_reward=1034.24 +/- 596.43
Episode length: 112.32 +/- 57.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 112         |
|    mean_reward          | 1.03e+03    |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.010493832 |
|    clip_fraction        | 0.0826      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | 0.12        |
|    learning_rate        | 1e-05       |
|    loss                 | 6.55e+03    |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00298    |
|    value_loss           | 1.76e+04    |
-----------------------------------------
Eval num_timesteps=72500, episode_reward=1143.52 +/- 641.18
Episode length: 122.28 +/- 60.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 72500    |
---------------------------------
Eval num_timesteps=73000, episode_reward=1296.32 +/- 634.90
Episode length: 137.34 +/- 60.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 137      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=73500, episode_reward=1257.28 +/- 637.64
Episode length: 133.42 +/- 60.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 133      |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 73500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 79.4     |
|    ep_rew_mean     | 696      |
| time/              |          |
|    fps             | 146      |
|    iterations      | 36       |
|    time_elapsed    | 501      |
|    total_timesteps | 73728    |
---------------------------------
Eval num_timesteps=74000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 39           |
|    mean_reward          | 284          |
| time/                   |              |
|    total_timesteps      | 74000        |
| train/                  |              |
|    approx_kl            | 0.0069322437 |
|    clip_fraction        | 0.0491       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.0941       |
|    learning_rate        | 1e-05        |
|    loss                 | 8.7e+03      |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.000253    |
|    value_loss           | 1.84e+04     |
------------------------------------------
Eval num_timesteps=74500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 74500    |
---------------------------------
Eval num_timesteps=75000, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=75500, episode_reward=284.00 +/- 0.00
Episode length: 39.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 39       |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 75500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | 607      |
| time/              |          |
|    fps             | 147      |
|    iterations      | 37       |
|    time_elapsed    | 512      |
|    total_timesteps | 75776    |
---------------------------------
Eval num_timesteps=76000, episode_reward=1423.52 +/- 644.11
Episode length: 148.96 +/- 60.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 149         |
|    mean_reward          | 1.42e+03    |
| time/                   |             |
|    total_timesteps      | 76000       |
| train/                  |             |
|    approx_kl            | 0.005329688 |
|    clip_fraction        | 0.0527      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.13       |
|    explained_variance   | 0.094       |
|    learning_rate        | 1e-05       |
|    loss                 | 1.13e+04    |
|    n_updates            | 370         |
|    policy_gradient_loss | 4.44e-05    |
|    value_loss           | 1.78e+04    |
-----------------------------------------
Eval num_timesteps=76500, episode_reward=1498.40 +/- 661.92
Episode length: 155.66 +/- 62.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 156      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 76500    |
---------------------------------
Eval num_timesteps=77000, episode_reward=1299.84 +/- 645.31
Episode length: 137.26 +/- 60.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 137      |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=77500, episode_reward=1319.52 +/- 638.50
Episode length: 139.08 +/- 59.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 77500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.3     |
|    ep_rew_mean     | 570      |
| time/              |          |
|    fps             | 143      |
|    iterations      | 38       |
|    time_elapsed    | 541      |
|    total_timesteps | 77824    |
---------------------------------
Eval num_timesteps=78000, episode_reward=1594.24 +/- 619.07
Episode length: 164.22 +/- 57.34
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 164         |
|    mean_reward          | 1.59e+03    |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.011230436 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.1        |
|    explained_variance   | 0.046       |
|    learning_rate        | 1e-05       |
|    loss                 | 1.22e+04    |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0026     |
|    value_loss           | 1.68e+04    |
-----------------------------------------
Eval num_timesteps=78500, episode_reward=1787.04 +/- 520.67
Episode length: 182.02 +/- 47.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 182      |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 78500    |
---------------------------------
Eval num_timesteps=79000, episode_reward=1424.16 +/- 674.32
Episode length: 148.24 +/- 62.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 148      |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=79500, episode_reward=1701.12 +/- 621.83
Episode length: 174.00 +/- 58.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 174      |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 79500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.4     |
|    ep_rew_mean     | 572      |
| time/              |          |
|    fps             | 138      |
|    iterations      | 39       |
|    time_elapsed    | 575      |
|    total_timesteps | 79872    |
---------------------------------
Eval num_timesteps=80000, episode_reward=1363.84 +/- 658.74
Episode length: 143.62 +/- 62.53
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 144         |
|    mean_reward          | 1.36e+03    |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.010526863 |
|    clip_fraction        | 0.0819      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.06       |
|    explained_variance   | 0.0215      |
|    learning_rate        | 1e-05       |
|    loss                 | 8.81e+03    |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00214    |
|    value_loss           | 1.68e+04    |
-----------------------------------------
Eval num_timesteps=80500, episode_reward=1321.76 +/- 644.17
Episode length: 139.68 +/- 60.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 140      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 80500    |
---------------------------------
Eval num_timesteps=81000, episode_reward=1408.80 +/- 663.91
Episode length: 147.54 +/- 62.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 148      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=81500, episode_reward=1042.40 +/- 619.23
Episode length: 113.04 +/- 59.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 113      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 81500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 74.7     |
|    ep_rew_mean     | 648      |
| time/              |          |
|    fps             | 135      |
|    iterations      | 40       |
|    time_elapsed    | 603      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=82000, episode_reward=1121.76 +/- 517.42
Episode length: 121.76 +/- 50.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 122         |
|    mean_reward          | 1.12e+03    |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.012598304 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.977      |
|    explained_variance   | -0.0225     |
|    learning_rate        | 1e-05       |
|    loss                 | 5e+03       |
|    n_updates            | 400         |
|    policy_gradient_loss | 0.000582    |
|    value_loss           | 1.22e+04    |
-----------------------------------------
Eval num_timesteps=82500, episode_reward=1125.92 +/- 570.24
Episode length: 121.68 +/- 54.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 82500    |
---------------------------------
Eval num_timesteps=83000, episode_reward=1238.72 +/- 605.13
Episode length: 132.40 +/- 57.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 132      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=83500, episode_reward=1091.36 +/- 576.58
Episode length: 118.24 +/- 55.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 118      |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 83500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 83.1     |
|    ep_rew_mean     | 736      |
| time/              |          |
|    fps             | 133      |
|    iterations      | 41       |
|    time_elapsed    | 629      |
|    total_timesteps | 83968    |
---------------------------------
Eval num_timesteps=84000, episode_reward=1364.32 +/- 765.10
Episode length: 141.88 +/- 71.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 142          |
|    mean_reward          | 1.36e+03     |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0046069426 |
|    clip_fraction        | 0.101        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.924       |
|    explained_variance   | 0.0191       |
|    learning_rate        | 1e-05        |
|    loss                 | 7.72e+03     |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.000807    |
|    value_loss           | 1.49e+04     |
------------------------------------------
Eval num_timesteps=84500, episode_reward=1397.92 +/- 729.09
Episode length: 145.62 +/- 68.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 146      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 84500    |
---------------------------------
Eval num_timesteps=85000, episode_reward=1358.88 +/- 783.77
Episode length: 141.72 +/- 74.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=85500, episode_reward=1398.88 +/- 755.15
Episode length: 145.30 +/- 70.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 145      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 85500    |
---------------------------------
Eval num_timesteps=86000, episode_reward=1376.80 +/- 747.76
Episode length: 143.44 +/- 70.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 143      |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 88.6     |
|    ep_rew_mean     | 794      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 42       |
|    time_elapsed    | 665      |
|    total_timesteps | 86016    |
---------------------------------
Eval num_timesteps=86500, episode_reward=557.92 +/- 274.54
Episode length: 66.22 +/- 27.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 66.2        |
|    mean_reward          | 558         |
| time/                   |             |
|    total_timesteps      | 86500       |
| train/                  |             |
|    approx_kl            | 0.007229138 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.891      |
|    explained_variance   | -0.0267     |
|    learning_rate        | 1e-05       |
|    loss                 | 7.84e+03    |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.0021     |
|    value_loss           | 1.73e+04    |
-----------------------------------------
Eval num_timesteps=87000, episode_reward=459.36 +/- 183.52
Episode length: 56.30 +/- 18.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.3     |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 87000    |
---------------------------------
Eval num_timesteps=87500, episode_reward=454.24 +/- 190.55
Episode length: 55.78 +/- 19.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 87500    |
---------------------------------
Eval num_timesteps=88000, episode_reward=441.44 +/- 193.47
Episode length: 54.56 +/- 19.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.6     |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 97.4     |
|    ep_rew_mean     | 888      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 43       |
|    time_elapsed    | 680      |
|    total_timesteps | 88064    |
---------------------------------
Eval num_timesteps=88500, episode_reward=474.08 +/- 247.82
Episode length: 57.84 +/- 24.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 57.8        |
|    mean_reward          | 474         |
| time/                   |             |
|    total_timesteps      | 88500       |
| train/                  |             |
|    approx_kl            | 0.010149339 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.825      |
|    explained_variance   | 0.000289    |
|    learning_rate        | 1e-05       |
|    loss                 | 1.25e+04    |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00241    |
|    value_loss           | 1.83e+04    |
-----------------------------------------
Eval num_timesteps=89000, episode_reward=424.16 +/- 125.40
Episode length: 52.76 +/- 12.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 89000    |
---------------------------------
Eval num_timesteps=89500, episode_reward=461.92 +/- 190.95
Episode length: 56.54 +/- 19.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.5     |
|    mean_reward     | 462      |
| time/              |          |
|    total_timesteps | 89500    |
---------------------------------
Eval num_timesteps=90000, episode_reward=422.24 +/- 198.24
Episode length: 52.66 +/- 19.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 105      |
|    ep_rew_mean     | 970      |
| time/              |          |
|    fps             | 129      |
|    iterations      | 44       |
|    time_elapsed    | 693      |
|    total_timesteps | 90112    |
---------------------------------
Eval num_timesteps=90500, episode_reward=315.36 +/- 36.48
Episode length: 41.94 +/- 3.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 315          |
| time/                   |              |
|    total_timesteps      | 90500        |
| train/                  |              |
|    approx_kl            | 0.0048562437 |
|    clip_fraction        | 0.0616       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.815       |
|    explained_variance   | -0.0204      |
|    learning_rate        | 1e-05        |
|    loss                 | 9.57e+03     |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.000701    |
|    value_loss           | 1.78e+04     |
------------------------------------------
Eval num_timesteps=91000, episode_reward=312.16 +/- 33.03
Episode length: 41.64 +/- 3.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 91000    |
---------------------------------
Eval num_timesteps=91500, episode_reward=318.56 +/- 39.89
Episode length: 42.28 +/- 3.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 91500    |
---------------------------------
Eval num_timesteps=92000, episode_reward=311.52 +/- 39.46
Episode length: 41.60 +/- 3.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 1.04e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 45       |
|    time_elapsed    | 704      |
|    total_timesteps | 92160    |
---------------------------------
Eval num_timesteps=92500, episode_reward=468.32 +/- 202.14
Episode length: 57.22 +/- 20.22
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 57.2         |
|    mean_reward          | 468          |
| time/                   |              |
|    total_timesteps      | 92500        |
| train/                  |              |
|    approx_kl            | 0.0070014717 |
|    clip_fraction        | 0.0855       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.808       |
|    explained_variance   | -0.00327     |
|    learning_rate        | 1e-05        |
|    loss                 | 9.03e+03     |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00103     |
|    value_loss           | 1.8e+04      |
------------------------------------------
Eval num_timesteps=93000, episode_reward=440.80 +/- 194.88
Episode length: 54.44 +/- 19.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 93000    |
---------------------------------
Eval num_timesteps=93500, episode_reward=472.16 +/- 233.02
Episode length: 57.62 +/- 23.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.6     |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 93500    |
---------------------------------
Eval num_timesteps=94000, episode_reward=430.56 +/- 156.78
Episode length: 53.46 +/- 15.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.5     |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 1.04e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 46       |
|    time_elapsed    | 717      |
|    total_timesteps | 94208    |
---------------------------------
Eval num_timesteps=94500, episode_reward=335.20 +/- 64.95
Episode length: 44.00 +/- 6.44
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 44           |
|    mean_reward          | 335          |
| time/                   |              |
|    total_timesteps      | 94500        |
| train/                  |              |
|    approx_kl            | 0.0040764892 |
|    clip_fraction        | 0.092        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.827       |
|    explained_variance   | 0.00973      |
|    learning_rate        | 1e-05        |
|    loss                 | 8.71e+03     |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.000387    |
|    value_loss           | 1.82e+04     |
------------------------------------------
Eval num_timesteps=95000, episode_reward=353.76 +/- 91.56
Episode length: 45.76 +/- 9.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 45.8     |
|    mean_reward     | 354      |
| time/              |          |
|    total_timesteps | 95000    |
---------------------------------
Eval num_timesteps=95500, episode_reward=340.32 +/- 46.84
Episode length: 44.36 +/- 4.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 95500    |
---------------------------------
Eval num_timesteps=96000, episode_reward=340.32 +/- 79.82
Episode length: 44.44 +/- 7.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 47       |
|    time_elapsed    | 729      |
|    total_timesteps | 96256    |
---------------------------------
Eval num_timesteps=96500, episode_reward=316.00 +/- 33.87
Episode length: 42.00 +/- 3.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 316          |
| time/                   |              |
|    total_timesteps      | 96500        |
| train/                  |              |
|    approx_kl            | 0.0033428767 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.793       |
|    explained_variance   | -0.0245      |
|    learning_rate        | 1e-05        |
|    loss                 | 6.49e+03     |
|    n_updates            | 470          |
|    policy_gradient_loss | 0.000874     |
|    value_loss           | 2.06e+04     |
------------------------------------------
Eval num_timesteps=97000, episode_reward=319.20 +/- 37.46
Episode length: 42.30 +/- 3.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 97000    |
---------------------------------
Eval num_timesteps=97500, episode_reward=308.96 +/- 32.19
Episode length: 41.34 +/- 3.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 97500    |
---------------------------------
Eval num_timesteps=98000, episode_reward=318.56 +/- 36.68
Episode length: 42.24 +/- 3.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    fps             | 132      |
|    iterations      | 48       |
|    time_elapsed    | 740      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=98500, episode_reward=311.52 +/- 38.93
Episode length: 41.60 +/- 3.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 41.6       |
|    mean_reward          | 312        |
| time/                   |            |
|    total_timesteps      | 98500      |
| train/                  |            |
|    approx_kl            | 0.00999459 |
|    clip_fraction        | 0.0742     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.794     |
|    explained_variance   | 0.0244     |
|    learning_rate        | 1e-05      |
|    loss                 | 1.24e+04   |
|    n_updates            | 480        |
|    policy_gradient_loss | -7.31e-05  |
|    value_loss           | 2.24e+04   |
----------------------------------------
Eval num_timesteps=99000, episode_reward=314.72 +/- 35.61
Episode length: 41.88 +/- 3.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 99000    |
---------------------------------
Eval num_timesteps=99500, episode_reward=316.00 +/- 36.77
Episode length: 42.00 +/- 3.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 99500    |
---------------------------------
Eval num_timesteps=100000, episode_reward=311.52 +/- 33.26
Episode length: 41.58 +/- 3.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 133      |
|    iterations      | 49       |
|    time_elapsed    | 751      |
|    total_timesteps | 100352   |
---------------------------------
Eval num_timesteps=100500, episode_reward=315.36 +/- 34.16
Episode length: 41.94 +/- 3.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 315         |
| time/                   |             |
|    total_timesteps      | 100500      |
| train/                  |             |
|    approx_kl            | 0.012019668 |
|    clip_fraction        | 0.0704      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.765      |
|    explained_variance   | 0.0293      |
|    learning_rate        | 1e-05       |
|    loss                 | 8.32e+03    |
|    n_updates            | 490         |
|    policy_gradient_loss | 0.000883    |
|    value_loss           | 1.7e+04     |
-----------------------------------------
Eval num_timesteps=101000, episode_reward=318.56 +/- 46.08
Episode length: 42.28 +/- 4.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 101000   |
---------------------------------
Eval num_timesteps=101500, episode_reward=305.12 +/- 35.41
Episode length: 41.00 +/- 3.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 305      |
| time/              |          |
|    total_timesteps | 101500   |
---------------------------------
Eval num_timesteps=102000, episode_reward=314.08 +/- 38.08
Episode length: 41.82 +/- 3.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    fps             | 134      |
|    iterations      | 50       |
|    time_elapsed    | 762      |
|    total_timesteps | 102400   |
---------------------------------
Eval num_timesteps=102500, episode_reward=303.84 +/- 32.56
Episode length: 40.86 +/- 3.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.9        |
|    mean_reward          | 304         |
| time/                   |             |
|    total_timesteps      | 102500      |
| train/                  |             |
|    approx_kl            | 0.008643134 |
|    clip_fraction        | 0.0869      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.741      |
|    explained_variance   | 0.011       |
|    learning_rate        | 1e-05       |
|    loss                 | 1.09e+04    |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.000672   |
|    value_loss           | 2.23e+04    |
-----------------------------------------
Eval num_timesteps=103000, episode_reward=321.76 +/- 43.26
Episode length: 42.56 +/- 4.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 322      |
| time/              |          |
|    total_timesteps | 103000   |
---------------------------------
Eval num_timesteps=103500, episode_reward=303.84 +/- 33.19
Episode length: 40.86 +/- 3.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.9     |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 103500   |
---------------------------------
Eval num_timesteps=104000, episode_reward=319.84 +/- 39.26
Episode length: 42.38 +/- 3.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 115      |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    fps             | 135      |
|    iterations      | 51       |
|    time_elapsed    | 772      |
|    total_timesteps | 104448   |
---------------------------------
Eval num_timesteps=104500, episode_reward=319.20 +/- 38.53
Episode length: 42.32 +/- 3.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 42.3       |
|    mean_reward          | 319        |
| time/                   |            |
|    total_timesteps      | 104500     |
| train/                  |            |
|    approx_kl            | 0.00802959 |
|    clip_fraction        | 0.096      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.703     |
|    explained_variance   | 0.0343     |
|    learning_rate        | 1e-05      |
|    loss                 | 8.32e+03   |
|    n_updates            | 510        |
|    policy_gradient_loss | -0.00342   |
|    value_loss           | 2.29e+04   |
----------------------------------------
Eval num_timesteps=105000, episode_reward=313.44 +/- 36.68
Episode length: 41.76 +/- 3.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
Eval num_timesteps=105500, episode_reward=316.00 +/- 40.48
Episode length: 42.02 +/- 3.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 105500   |
---------------------------------
Eval num_timesteps=106000, episode_reward=322.40 +/- 38.93
Episode length: 42.60 +/- 3.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.6     |
|    mean_reward     | 322      |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 108      |
|    ep_rew_mean     | 1e+03    |
| time/              |          |
|    fps             | 135      |
|    iterations      | 52       |
|    time_elapsed    | 783      |
|    total_timesteps | 106496   |
---------------------------------
Eval num_timesteps=106500, episode_reward=308.32 +/- 37.08
Episode length: 41.30 +/- 3.53
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.3         |
|    mean_reward          | 308          |
| time/                   |              |
|    total_timesteps      | 106500       |
| train/                  |              |
|    approx_kl            | 0.0046422863 |
|    clip_fraction        | 0.0739       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.707       |
|    explained_variance   | 0.0284       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.21e+04     |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 2.32e+04     |
------------------------------------------
Eval num_timesteps=107000, episode_reward=317.92 +/- 43.60
Episode length: 42.22 +/- 4.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 107000   |
---------------------------------
Eval num_timesteps=107500, episode_reward=310.24 +/- 31.80
Episode length: 41.46 +/- 2.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 107500   |
---------------------------------
Eval num_timesteps=108000, episode_reward=321.12 +/- 37.52
Episode length: 42.50 +/- 3.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=108500, episode_reward=316.64 +/- 37.04
Episode length: 42.06 +/- 3.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 108500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 109      |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    fps             | 136      |
|    iterations      | 53       |
|    time_elapsed    | 796      |
|    total_timesteps | 108544   |
---------------------------------
Eval num_timesteps=109000, episode_reward=305.76 +/- 34.73
Episode length: 41.06 +/- 3.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.1        |
|    mean_reward          | 306         |
| time/                   |             |
|    total_timesteps      | 109000      |
| train/                  |             |
|    approx_kl            | 0.002800878 |
|    clip_fraction        | 0.0777      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.689      |
|    explained_variance   | 0.0214      |
|    learning_rate        | 1e-05       |
|    loss                 | 1.33e+04    |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00075    |
|    value_loss           | 2.1e+04     |
-----------------------------------------
Eval num_timesteps=109500, episode_reward=314.72 +/- 40.96
Episode length: 41.88 +/- 3.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 109500   |
---------------------------------
Eval num_timesteps=110000, episode_reward=316.64 +/- 39.19
Episode length: 42.06 +/- 3.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=110500, episode_reward=310.24 +/- 33.68
Episode length: 41.46 +/- 3.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 110500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 104      |
|    ep_rew_mean     | 959      |
| time/              |          |
|    fps             | 136      |
|    iterations      | 54       |
|    time_elapsed    | 807      |
|    total_timesteps | 110592   |
---------------------------------
Eval num_timesteps=111000, episode_reward=317.28 +/- 38.38
Episode length: 42.14 +/- 3.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 317         |
| time/                   |             |
|    total_timesteps      | 111000      |
| train/                  |             |
|    approx_kl            | 0.003373892 |
|    clip_fraction        | 0.0429      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.691      |
|    explained_variance   | 0.0545      |
|    learning_rate        | 1e-05       |
|    loss                 | 1.17e+04    |
|    n_updates            | 540         |
|    policy_gradient_loss | 0.000831    |
|    value_loss           | 2.06e+04    |
-----------------------------------------
Eval num_timesteps=111500, episode_reward=312.16 +/- 37.67
Episode length: 41.66 +/- 3.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 111500   |
---------------------------------
Eval num_timesteps=112000, episode_reward=314.72 +/- 39.43
Episode length: 41.90 +/- 3.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=112500, episode_reward=313.44 +/- 40.40
Episode length: 41.78 +/- 3.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 112500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 101      |
|    ep_rew_mean     | 924      |
| time/              |          |
|    fps             | 137      |
|    iterations      | 55       |
|    time_elapsed    | 818      |
|    total_timesteps | 112640   |
---------------------------------
Eval num_timesteps=113000, episode_reward=447.20 +/- 202.01
Episode length: 55.16 +/- 20.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.2        |
|    mean_reward          | 447         |
| time/                   |             |
|    total_timesteps      | 113000      |
| train/                  |             |
|    approx_kl            | 0.008590585 |
|    clip_fraction        | 0.0565      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.661      |
|    explained_variance   | -0.0136     |
|    learning_rate        | 1e-05       |
|    loss                 | 1.12e+04    |
|    n_updates            | 550         |
|    policy_gradient_loss | 0.00193     |
|    value_loss           | 2.25e+04    |
-----------------------------------------
Eval num_timesteps=113500, episode_reward=504.16 +/- 230.55
Episode length: 60.80 +/- 23.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.8     |
|    mean_reward     | 504      |
| time/              |          |
|    total_timesteps | 113500   |
---------------------------------
Eval num_timesteps=114000, episode_reward=424.80 +/- 128.32
Episode length: 52.88 +/- 12.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.9     |
|    mean_reward     | 425      |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=114500, episode_reward=467.68 +/- 237.99
Episode length: 57.16 +/- 23.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.2     |
|    mean_reward     | 468      |
| time/              |          |
|    total_timesteps | 114500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 106      |
|    ep_rew_mean     | 980      |
| time/              |          |
|    fps             | 137      |
|    iterations      | 56       |
|    time_elapsed    | 831      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=115000, episode_reward=1303.04 +/- 736.13
Episode length: 136.78 +/- 69.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 137         |
|    mean_reward          | 1.3e+03     |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.010617802 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.768      |
|    explained_variance   | -0.0289     |
|    learning_rate        | 1e-05       |
|    loss                 | 4.96e+03    |
|    n_updates            | 560         |
|    policy_gradient_loss | 0.0017      |
|    value_loss           | 1.53e+04    |
-----------------------------------------
Eval num_timesteps=115500, episode_reward=1118.40 +/- 716.86
Episode length: 119.56 +/- 68.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 115500   |
---------------------------------
Eval num_timesteps=116000, episode_reward=1342.40 +/- 720.05
Episode length: 140.24 +/- 67.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 140      |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=116500, episode_reward=1097.92 +/- 719.17
Episode length: 117.44 +/- 68.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 116500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 112      |
|    ep_rew_mean     | 1.04e+03 |
| time/              |          |
|    fps             | 135      |
|    iterations      | 57       |
|    time_elapsed    | 858      |
|    total_timesteps | 116736   |
---------------------------------
Eval num_timesteps=117000, episode_reward=497.12 +/- 198.57
Episode length: 60.02 +/- 19.81
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 60          |
|    mean_reward          | 497         |
| time/                   |             |
|    total_timesteps      | 117000      |
| train/                  |             |
|    approx_kl            | 0.008921791 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.778      |
|    explained_variance   | -0.000644   |
|    learning_rate        | 1e-05       |
|    loss                 | 1.56e+04    |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.000349   |
|    value_loss           | 2.29e+04    |
-----------------------------------------
Eval num_timesteps=117500, episode_reward=473.44 +/- 178.38
Episode length: 57.76 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.8     |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 117500   |
---------------------------------
Eval num_timesteps=118000, episode_reward=468.96 +/- 222.93
Episode length: 57.32 +/- 22.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.3     |
|    mean_reward     | 469      |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=118500, episode_reward=520.16 +/- 264.72
Episode length: 62.42 +/- 26.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.4     |
|    mean_reward     | 520      |
| time/              |          |
|    total_timesteps | 118500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | 1.03e+03 |
| time/              |          |
|    fps             | 136      |
|    iterations      | 58       |
|    time_elapsed    | 872      |
|    total_timesteps | 118784   |
---------------------------------
Eval num_timesteps=119000, episode_reward=412.00 +/- 165.78
Episode length: 51.66 +/- 16.51
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.7         |
|    mean_reward          | 412          |
| time/                   |              |
|    total_timesteps      | 119000       |
| train/                  |              |
|    approx_kl            | 0.0030531823 |
|    clip_fraction        | 0.0827       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.742       |
|    explained_variance   | 0.0329       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.08e+04     |
|    n_updates            | 580          |
|    policy_gradient_loss | 1.17e-05     |
|    value_loss           | 2.07e+04     |
------------------------------------------
Eval num_timesteps=119500, episode_reward=467.04 +/- 230.94
Episode length: 57.10 +/- 23.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.1     |
|    mean_reward     | 467      |
| time/              |          |
|    total_timesteps | 119500   |
---------------------------------
Eval num_timesteps=120000, episode_reward=458.08 +/- 186.17
Episode length: 56.16 +/- 18.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.2     |
|    mean_reward     | 458      |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=120500, episode_reward=478.56 +/- 268.18
Episode length: 58.30 +/- 26.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.3     |
|    mean_reward     | 479      |
| time/              |          |
|    total_timesteps | 120500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    fps             | 136      |
|    iterations      | 59       |
|    time_elapsed    | 885      |
|    total_timesteps | 120832   |
---------------------------------
Eval num_timesteps=121000, episode_reward=307.68 +/- 30.57
Episode length: 41.22 +/- 2.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 41.2       |
|    mean_reward          | 308        |
| time/                   |            |
|    total_timesteps      | 121000     |
| train/                  |            |
|    approx_kl            | 0.01004061 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.709     |
|    explained_variance   | -0.0146    |
|    learning_rate        | 1e-05      |
|    loss                 | 6.49e+03   |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.00268   |
|    value_loss           | 1.59e+04   |
----------------------------------------
Eval num_timesteps=121500, episode_reward=307.04 +/- 32.03
Episode length: 41.16 +/- 3.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 121500   |
---------------------------------
Eval num_timesteps=122000, episode_reward=314.72 +/- 35.61
Episode length: 41.90 +/- 3.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=122500, episode_reward=319.84 +/- 40.80
Episode length: 42.38 +/- 3.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 122500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 137      |
|    iterations      | 60       |
|    time_elapsed    | 896      |
|    total_timesteps | 122880   |
---------------------------------
Eval num_timesteps=123000, episode_reward=452.96 +/- 136.37
Episode length: 55.76 +/- 13.70
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 55.8         |
|    mean_reward          | 453          |
| time/                   |              |
|    total_timesteps      | 123000       |
| train/                  |              |
|    approx_kl            | 0.0033071502 |
|    clip_fraction        | 0.0588       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.713       |
|    explained_variance   | -0.0239      |
|    learning_rate        | 1e-05        |
|    loss                 | 7.92e+03     |
|    n_updates            | 600          |
|    policy_gradient_loss | 0.00109      |
|    value_loss           | 1.74e+04     |
------------------------------------------
Eval num_timesteps=123500, episode_reward=395.36 +/- 116.13
Episode length: 50.00 +/- 11.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50       |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 123500   |
---------------------------------
Eval num_timesteps=124000, episode_reward=376.16 +/- 114.60
Episode length: 47.94 +/- 11.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.9     |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=124500, episode_reward=431.84 +/- 161.89
Episode length: 53.68 +/- 16.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 124500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 137      |
|    iterations      | 61       |
|    time_elapsed    | 908      |
|    total_timesteps | 124928   |
---------------------------------
Eval num_timesteps=125000, episode_reward=310.88 +/- 38.59
Episode length: 41.54 +/- 3.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.5         |
|    mean_reward          | 311          |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0066938163 |
|    clip_fraction        | 0.0527       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.688       |
|    explained_variance   | 0.0146       |
|    learning_rate        | 1e-05        |
|    loss                 | 6.14e+03     |
|    n_updates            | 610          |
|    policy_gradient_loss | 0.00209      |
|    value_loss           | 2.14e+04     |
------------------------------------------
Eval num_timesteps=125500, episode_reward=312.80 +/- 39.06
Episode length: 41.72 +/- 3.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 125500   |
---------------------------------
Eval num_timesteps=126000, episode_reward=308.32 +/- 35.95
Episode length: 41.30 +/- 3.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=126500, episode_reward=307.68 +/- 33.75
Episode length: 41.22 +/- 3.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 126500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 138      |
|    iterations      | 62       |
|    time_elapsed    | 919      |
|    total_timesteps | 126976   |
---------------------------------
Eval num_timesteps=127000, episode_reward=310.88 +/- 35.84
Episode length: 41.52 +/- 3.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.5         |
|    mean_reward          | 311          |
| time/                   |              |
|    total_timesteps      | 127000       |
| train/                  |              |
|    approx_kl            | 0.0037108208 |
|    clip_fraction        | 0.0621       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.724       |
|    explained_variance   | -0.00367     |
|    learning_rate        | 1e-05        |
|    loss                 | 1.14e+04     |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00105     |
|    value_loss           | 2.19e+04     |
------------------------------------------
Eval num_timesteps=127500, episode_reward=314.72 +/- 40.96
Episode length: 41.92 +/- 3.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 127500   |
---------------------------------
Eval num_timesteps=128000, episode_reward=307.04 +/- 36.79
Episode length: 41.16 +/- 3.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=128500, episode_reward=318.56 +/- 38.31
Episode length: 42.26 +/- 3.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 128500   |
---------------------------------
Eval num_timesteps=129000, episode_reward=317.28 +/- 43.86
Episode length: 42.14 +/- 4.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 1.25e+03 |
| time/              |          |
|    fps             | 138      |
|    iterations      | 63       |
|    time_elapsed    | 932      |
|    total_timesteps | 129024   |
---------------------------------
Eval num_timesteps=129500, episode_reward=431.20 +/- 138.60
Episode length: 53.54 +/- 13.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 53.5        |
|    mean_reward          | 431         |
| time/                   |             |
|    total_timesteps      | 129500      |
| train/                  |             |
|    approx_kl            | 0.006900774 |
|    clip_fraction        | 0.0735      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.713      |
|    explained_variance   | -0.00776    |
|    learning_rate        | 1e-05       |
|    loss                 | 1.12e+04    |
|    n_updates            | 630         |
|    policy_gradient_loss | -0.00143    |
|    value_loss           | 1.89e+04    |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=440.80 +/- 184.19
Episode length: 54.48 +/- 18.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
Eval num_timesteps=130500, episode_reward=445.92 +/- 164.59
Episode length: 54.96 +/- 16.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | 446      |
| time/              |          |
|    total_timesteps | 130500   |
---------------------------------
Eval num_timesteps=131000, episode_reward=426.72 +/- 160.67
Episode length: 53.14 +/- 16.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.1     |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 138      |
|    iterations      | 64       |
|    time_elapsed    | 945      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=131500, episode_reward=511.20 +/- 234.30
Episode length: 61.56 +/- 23.48
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 61.6        |
|    mean_reward          | 511         |
| time/                   |             |
|    total_timesteps      | 131500      |
| train/                  |             |
|    approx_kl            | 0.010650637 |
|    clip_fraction        | 0.0689      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.685      |
|    explained_variance   | 0.012       |
|    learning_rate        | 1e-05       |
|    loss                 | 1.12e+04    |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00126    |
|    value_loss           | 2.03e+04    |
-----------------------------------------
Eval num_timesteps=132000, episode_reward=416.48 +/- 137.27
Episode length: 51.98 +/- 13.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=132500, episode_reward=476.64 +/- 229.29
Episode length: 58.12 +/- 22.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.1     |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 132500   |
---------------------------------
Eval num_timesteps=133000, episode_reward=461.28 +/- 201.09
Episode length: 56.54 +/- 20.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.5     |
|    mean_reward     | 461      |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 138      |
|    iterations      | 65       |
|    time_elapsed    | 958      |
|    total_timesteps | 133120   |
---------------------------------
Eval num_timesteps=133500, episode_reward=307.04 +/- 32.03
Episode length: 41.16 +/- 3.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.2         |
|    mean_reward          | 307          |
| time/                   |              |
|    total_timesteps      | 133500       |
| train/                  |              |
|    approx_kl            | 0.0029763966 |
|    clip_fraction        | 0.0599       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.689       |
|    explained_variance   | -0.000998    |
|    learning_rate        | 1e-05        |
|    loss                 | 8.67e+03     |
|    n_updates            | 650          |
|    policy_gradient_loss | 0.0011       |
|    value_loss           | 2.22e+04     |
------------------------------------------
Eval num_timesteps=134000, episode_reward=317.28 +/- 41.46
Episode length: 42.16 +/- 4.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 134000   |
---------------------------------
Eval num_timesteps=134500, episode_reward=312.16 +/- 34.84
Episode length: 41.64 +/- 3.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 134500   |
---------------------------------
Eval num_timesteps=135000, episode_reward=309.60 +/- 34.47
Episode length: 41.40 +/- 3.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.4     |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 139      |
|    iterations      | 66       |
|    time_elapsed    | 969      |
|    total_timesteps | 135168   |
---------------------------------
Eval num_timesteps=135500, episode_reward=310.88 +/- 35.84
Episode length: 41.54 +/- 3.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.5         |
|    mean_reward          | 311          |
| time/                   |              |
|    total_timesteps      | 135500       |
| train/                  |              |
|    approx_kl            | 0.0020844443 |
|    clip_fraction        | 0.0832       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.603       |
|    explained_variance   | -0.0089      |
|    learning_rate        | 1e-05        |
|    loss                 | 1.49e+04     |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00139     |
|    value_loss           | 1.92e+04     |
------------------------------------------
Eval num_timesteps=136000, episode_reward=313.44 +/- 34.37
Episode length: 41.78 +/- 3.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=136500, episode_reward=321.76 +/- 36.59
Episode length: 42.54 +/- 3.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 322      |
| time/              |          |
|    total_timesteps | 136500   |
---------------------------------
Eval num_timesteps=137000, episode_reward=310.88 +/- 39.12
Episode length: 41.54 +/- 3.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 139      |
|    iterations      | 67       |
|    time_elapsed    | 980      |
|    total_timesteps | 137216   |
---------------------------------
Eval num_timesteps=137500, episode_reward=309.60 +/- 36.77
Episode length: 41.40 +/- 3.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.4        |
|    mean_reward          | 310         |
| time/                   |             |
|    total_timesteps      | 137500      |
| train/                  |             |
|    approx_kl            | 0.007782611 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.602      |
|    explained_variance   | -0.0489     |
|    learning_rate        | 1e-05       |
|    loss                 | 8.99e+03    |
|    n_updates            | 670         |
|    policy_gradient_loss | 0.00111     |
|    value_loss           | 2.15e+04    |
-----------------------------------------
Eval num_timesteps=138000, episode_reward=312.80 +/- 35.78
Episode length: 41.70 +/- 3.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=138500, episode_reward=315.36 +/- 34.16
Episode length: 41.94 +/- 3.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 138500   |
---------------------------------
Eval num_timesteps=139000, episode_reward=310.88 +/- 35.26
Episode length: 41.52 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 140      |
|    iterations      | 68       |
|    time_elapsed    | 991      |
|    total_timesteps | 139264   |
---------------------------------
Eval num_timesteps=139500, episode_reward=323.68 +/- 41.75
Episode length: 42.76 +/- 4.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.8         |
|    mean_reward          | 324          |
| time/                   |              |
|    total_timesteps      | 139500       |
| train/                  |              |
|    approx_kl            | 0.0061767567 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.611       |
|    explained_variance   | -0.00231     |
|    learning_rate        | 1e-05        |
|    loss                 | 1.6e+04      |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.000499    |
|    value_loss           | 2.94e+04     |
------------------------------------------
Eval num_timesteps=140000, episode_reward=306.40 +/- 38.53
Episode length: 41.12 +/- 3.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.1     |
|    mean_reward     | 306      |
| time/              |          |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=140500, episode_reward=310.88 +/- 32.86
Episode length: 41.52 +/- 3.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 140500   |
---------------------------------
Eval num_timesteps=141000, episode_reward=317.92 +/- 45.89
Episode length: 42.22 +/- 4.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 140      |
|    iterations      | 69       |
|    time_elapsed    | 1002     |
|    total_timesteps | 141312   |
---------------------------------
Eval num_timesteps=141500, episode_reward=312.16 +/- 37.67
Episode length: 41.66 +/- 3.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.7         |
|    mean_reward          | 312          |
| time/                   |              |
|    total_timesteps      | 141500       |
| train/                  |              |
|    approx_kl            | 0.0041681984 |
|    clip_fraction        | 0.0614       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.579       |
|    explained_variance   | 0.000708     |
|    learning_rate        | 1e-05        |
|    loss                 | 9.31e+03     |
|    n_updates            | 690          |
|    policy_gradient_loss | 0.0021       |
|    value_loss           | 1.84e+04     |
------------------------------------------
Eval num_timesteps=142000, episode_reward=315.36 +/- 37.59
Episode length: 41.96 +/- 3.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 142000   |
---------------------------------
Eval num_timesteps=142500, episode_reward=323.68 +/- 42.72
Episode length: 42.74 +/- 4.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 324      |
| time/              |          |
|    total_timesteps | 142500   |
---------------------------------
Eval num_timesteps=143000, episode_reward=314.08 +/- 36.44
Episode length: 41.84 +/- 3.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    fps             | 141      |
|    iterations      | 70       |
|    time_elapsed    | 1013     |
|    total_timesteps | 143360   |
---------------------------------
Eval num_timesteps=143500, episode_reward=312.80 +/- 32.79
Episode length: 41.70 +/- 3.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.7        |
|    mean_reward          | 313         |
| time/                   |             |
|    total_timesteps      | 143500      |
| train/                  |             |
|    approx_kl            | 0.005737962 |
|    clip_fraction        | 0.052       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.598      |
|    explained_variance   | -0.0051     |
|    learning_rate        | 1e-05       |
|    loss                 | 1.17e+04    |
|    n_updates            | 700         |
|    policy_gradient_loss | 0.00183     |
|    value_loss           | 2.13e+04    |
-----------------------------------------
Eval num_timesteps=144000, episode_reward=317.92 +/- 43.13
Episode length: 42.22 +/- 4.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=144500, episode_reward=314.08 +/- 36.44
Episode length: 41.82 +/- 3.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 144500   |
---------------------------------
Eval num_timesteps=145000, episode_reward=303.20 +/- 26.39
Episode length: 40.80 +/- 2.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.8     |
|    mean_reward     | 303      |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 142      |
|    iterations      | 71       |
|    time_elapsed    | 1023     |
|    total_timesteps | 145408   |
---------------------------------
Eval num_timesteps=145500, episode_reward=317.92 +/- 38.08
Episode length: 42.18 +/- 3.57
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.2         |
|    mean_reward          | 318          |
| time/                   |              |
|    total_timesteps      | 145500       |
| train/                  |              |
|    approx_kl            | 0.0028473767 |
|    clip_fraction        | 0.0532       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.557       |
|    explained_variance   | 0.000383     |
|    learning_rate        | 1e-05        |
|    loss                 | 1.36e+04     |
|    n_updates            | 710          |
|    policy_gradient_loss | -9.46e-05    |
|    value_loss           | 2.12e+04     |
------------------------------------------
Eval num_timesteps=146000, episode_reward=320.48 +/- 42.46
Episode length: 42.46 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 146000   |
---------------------------------
Eval num_timesteps=146500, episode_reward=307.04 +/- 33.89
Episode length: 41.18 +/- 3.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 146500   |
---------------------------------
Eval num_timesteps=147000, episode_reward=313.44 +/- 33.77
Episode length: 41.76 +/- 3.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 142      |
|    iterations      | 72       |
|    time_elapsed    | 1034     |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=147500, episode_reward=306.40 +/- 34.02
Episode length: 41.10 +/- 3.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.1         |
|    mean_reward          | 306          |
| time/                   |              |
|    total_timesteps      | 147500       |
| train/                  |              |
|    approx_kl            | 0.0045629786 |
|    clip_fraction        | 0.0744       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.548       |
|    explained_variance   | -0.00804     |
|    learning_rate        | 1e-05        |
|    loss                 | 1.23e+04     |
|    n_updates            | 720          |
|    policy_gradient_loss | 0.0027       |
|    value_loss           | 1.88e+04     |
------------------------------------------
Eval num_timesteps=148000, episode_reward=307.04 +/- 33.89
Episode length: 41.16 +/- 3.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=148500, episode_reward=317.92 +/- 39.66
Episode length: 42.22 +/- 3.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 148500   |
---------------------------------
Eval num_timesteps=149000, episode_reward=305.12 +/- 34.83
Episode length: 40.98 +/- 3.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 305      |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=149500, episode_reward=316.64 +/- 35.91
Episode length: 42.06 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 149500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    fps             | 142      |
|    iterations      | 73       |
|    time_elapsed    | 1047     |
|    total_timesteps | 149504   |
---------------------------------
Eval num_timesteps=150000, episode_reward=308.96 +/- 35.22
Episode length: 41.34 +/- 3.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.3         |
|    mean_reward          | 309          |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0010371581 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.535       |
|    explained_variance   | 0.0199       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.7e+04      |
|    n_updates            | 730          |
|    policy_gradient_loss | 0.000499     |
|    value_loss           | 2.74e+04     |
------------------------------------------
Eval num_timesteps=150500, episode_reward=318.56 +/- 42.86
Episode length: 42.26 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 150500   |
---------------------------------
Eval num_timesteps=151000, episode_reward=319.84 +/- 42.76
Episode length: 42.40 +/- 4.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=151500, episode_reward=319.84 +/- 36.00
Episode length: 42.36 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 151500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 143      |
|    iterations      | 74       |
|    time_elapsed    | 1058     |
|    total_timesteps | 151552   |
---------------------------------
Eval num_timesteps=152000, episode_reward=315.36 +/- 38.13
Episode length: 41.94 +/- 3.57
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 315         |
| time/                   |             |
|    total_timesteps      | 152000      |
| train/                  |             |
|    approx_kl            | 0.001709535 |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.575      |
|    explained_variance   | -0.00198    |
|    learning_rate        | 1e-05       |
|    loss                 | 9.84e+03    |
|    n_updates            | 740         |
|    policy_gradient_loss | 1.26e-05    |
|    value_loss           | 1.7e+04     |
-----------------------------------------
Eval num_timesteps=152500, episode_reward=307.68 +/- 34.35
Episode length: 41.22 +/- 3.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 152500   |
---------------------------------
Eval num_timesteps=153000, episode_reward=308.96 +/- 33.43
Episode length: 41.34 +/- 3.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=153500, episode_reward=308.32 +/- 33.60
Episode length: 41.28 +/- 3.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 153500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    fps             | 143      |
|    iterations      | 75       |
|    time_elapsed    | 1069     |
|    total_timesteps | 153600   |
---------------------------------
Eval num_timesteps=154000, episode_reward=312.16 +/- 40.80
Episode length: 41.66 +/- 3.89
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.7         |
|    mean_reward          | 312          |
| time/                   |              |
|    total_timesteps      | 154000       |
| train/                  |              |
|    approx_kl            | 0.0043485253 |
|    clip_fraction        | 0.036        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.599       |
|    explained_variance   | 0.0411       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.06e+04     |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00143     |
|    value_loss           | 2.37e+04     |
------------------------------------------
Eval num_timesteps=154500, episode_reward=308.96 +/- 32.19
Episode length: 41.34 +/- 3.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 154500   |
---------------------------------
Eval num_timesteps=155000, episode_reward=317.92 +/- 35.29
Episode length: 42.18 +/- 3.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=155500, episode_reward=316.00 +/- 38.93
Episode length: 42.04 +/- 3.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 155500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    fps             | 144      |
|    iterations      | 76       |
|    time_elapsed    | 1080     |
|    total_timesteps | 155648   |
---------------------------------
Eval num_timesteps=156000, episode_reward=374.88 +/- 98.60
Episode length: 47.86 +/- 9.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 47.9         |
|    mean_reward          | 375          |
| time/                   |              |
|    total_timesteps      | 156000       |
| train/                  |              |
|    approx_kl            | 0.0058120852 |
|    clip_fraction        | 0.0842       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.574       |
|    explained_variance   | 0.00373      |
|    learning_rate        | 1e-05        |
|    loss                 | 1.27e+04     |
|    n_updates            | 760          |
|    policy_gradient_loss | 0.00428      |
|    value_loss           | 2.06e+04     |
------------------------------------------
Eval num_timesteps=156500, episode_reward=373.60 +/- 115.38
Episode length: 47.78 +/- 11.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | 374      |
| time/              |          |
|    total_timesteps | 156500   |
---------------------------------
Eval num_timesteps=157000, episode_reward=408.16 +/- 123.71
Episode length: 51.24 +/- 12.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=157500, episode_reward=390.88 +/- 136.76
Episode length: 49.48 +/- 13.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 49.5     |
|    mean_reward     | 391      |
| time/              |          |
|    total_timesteps | 157500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    fps             | 144      |
|    iterations      | 77       |
|    time_elapsed    | 1092     |
|    total_timesteps | 157696   |
---------------------------------
Eval num_timesteps=158000, episode_reward=314.08 +/- 40.68
Episode length: 41.86 +/- 3.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.9         |
|    mean_reward          | 314          |
| time/                   |              |
|    total_timesteps      | 158000       |
| train/                  |              |
|    approx_kl            | 0.0073602223 |
|    clip_fraction        | 0.059        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.556       |
|    explained_variance   | 0.0101       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.06e+04     |
|    n_updates            | 770          |
|    policy_gradient_loss | 0.00146      |
|    value_loss           | 2.01e+04     |
------------------------------------------
Eval num_timesteps=158500, episode_reward=317.28 +/- 39.43
Episode length: 42.14 +/- 3.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 158500   |
---------------------------------
Eval num_timesteps=159000, episode_reward=314.08 +/- 35.87
Episode length: 41.84 +/- 3.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=159500, episode_reward=305.76 +/- 31.64
Episode length: 41.04 +/- 2.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 306      |
| time/              |          |
|    total_timesteps | 159500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 144      |
|    iterations      | 78       |
|    time_elapsed    | 1103     |
|    total_timesteps | 159744   |
---------------------------------
Eval num_timesteps=160000, episode_reward=315.36 +/- 40.72
Episode length: 41.96 +/- 3.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 315          |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0038112956 |
|    clip_fraction        | 0.0385       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.545       |
|    explained_variance   | 0.00803      |
|    learning_rate        | 1e-05        |
|    loss                 | 9.36e+03     |
|    n_updates            | 780          |
|    policy_gradient_loss | 0.000615     |
|    value_loss           | 1.45e+04     |
------------------------------------------
Eval num_timesteps=160500, episode_reward=316.00 +/- 37.86
Episode length: 42.00 +/- 3.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 160500   |
---------------------------------
Eval num_timesteps=161000, episode_reward=311.52 +/- 35.64
Episode length: 41.58 +/- 3.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=161500, episode_reward=310.24 +/- 37.15
Episode length: 41.46 +/- 3.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 161500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 145      |
|    iterations      | 79       |
|    time_elapsed    | 1114     |
|    total_timesteps | 161792   |
---------------------------------
Eval num_timesteps=162000, episode_reward=445.92 +/- 215.37
Episode length: 55.00 +/- 21.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55          |
|    mean_reward          | 446         |
| time/                   |             |
|    total_timesteps      | 162000      |
| train/                  |             |
|    approx_kl            | 0.010731715 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.553      |
|    explained_variance   | -0.00892    |
|    learning_rate        | 1e-05       |
|    loss                 | 8.25e+03    |
|    n_updates            | 790         |
|    policy_gradient_loss | 0.00221     |
|    value_loss           | 1.73e+04    |
-----------------------------------------
Eval num_timesteps=162500, episode_reward=436.32 +/- 169.27
Episode length: 54.06 +/- 16.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | 436      |
| time/              |          |
|    total_timesteps | 162500   |
---------------------------------
Eval num_timesteps=163000, episode_reward=426.08 +/- 152.82
Episode length: 52.96 +/- 15.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=163500, episode_reward=368.48 +/- 117.62
Episode length: 47.30 +/- 11.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 163500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 145      |
|    iterations      | 80       |
|    time_elapsed    | 1127     |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=164000, episode_reward=892.48 +/- 618.77
Episode length: 98.62 +/- 59.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 98.6         |
|    mean_reward          | 892          |
| time/                   |              |
|    total_timesteps      | 164000       |
| train/                  |              |
|    approx_kl            | 0.0144624505 |
|    clip_fraction        | 0.072        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.579       |
|    explained_variance   | 0.0111       |
|    learning_rate        | 1e-05        |
|    loss                 | 8.57e+03     |
|    n_updates            | 800          |
|    policy_gradient_loss | 0.000614     |
|    value_loss           | 2.42e+04     |
------------------------------------------
Eval num_timesteps=164500, episode_reward=911.52 +/- 576.68
Episode length: 101.16 +/- 56.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 164500   |
---------------------------------
Eval num_timesteps=165000, episode_reward=753.92 +/- 494.03
Episode length: 85.12 +/- 47.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 85.1     |
|    mean_reward     | 754      |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=165500, episode_reward=921.12 +/- 578.67
Episode length: 102.12 +/- 57.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 921      |
| time/              |          |
|    total_timesteps | 165500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 144      |
|    iterations      | 81       |
|    time_elapsed    | 1147     |
|    total_timesteps | 165888   |
---------------------------------
Eval num_timesteps=166000, episode_reward=476.64 +/- 208.71
Episode length: 58.06 +/- 20.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 58.1         |
|    mean_reward          | 477          |
| time/                   |              |
|    total_timesteps      | 166000       |
| train/                  |              |
|    approx_kl            | 0.0022780695 |
|    clip_fraction        | 0.0388       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.619       |
|    explained_variance   | 0.00627      |
|    learning_rate        | 1e-05        |
|    loss                 | 1.19e+04     |
|    n_updates            | 810          |
|    policy_gradient_loss | 0.00165      |
|    value_loss           | 2.15e+04     |
------------------------------------------
Eval num_timesteps=166500, episode_reward=499.04 +/- 262.79
Episode length: 60.28 +/- 26.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.3     |
|    mean_reward     | 499      |
| time/              |          |
|    total_timesteps | 166500   |
---------------------------------
Eval num_timesteps=167000, episode_reward=470.88 +/- 205.34
Episode length: 57.46 +/- 20.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.5     |
|    mean_reward     | 471      |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=167500, episode_reward=436.96 +/- 150.27
Episode length: 54.06 +/- 15.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 167500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 144      |
|    iterations      | 82       |
|    time_elapsed    | 1161     |
|    total_timesteps | 167936   |
---------------------------------
Eval num_timesteps=168000, episode_reward=473.44 +/- 192.94
Episode length: 57.76 +/- 19.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 57.8         |
|    mean_reward          | 473          |
| time/                   |              |
|    total_timesteps      | 168000       |
| train/                  |              |
|    approx_kl            | 0.0065801707 |
|    clip_fraction        | 0.0506       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.596       |
|    explained_variance   | 0.00988      |
|    learning_rate        | 1e-05        |
|    loss                 | 7.07e+03     |
|    n_updates            | 820          |
|    policy_gradient_loss | 0.000854     |
|    value_loss           | 1.85e+04     |
------------------------------------------
Eval num_timesteps=168500, episode_reward=433.12 +/- 199.29
Episode length: 53.74 +/- 19.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 168500   |
---------------------------------
Eval num_timesteps=169000, episode_reward=512.96 +/- 301.61
Episode length: 61.50 +/- 29.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.5     |
|    mean_reward     | 513      |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=169500, episode_reward=460.00 +/- 189.67
Episode length: 56.40 +/- 18.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.4     |
|    mean_reward     | 460      |
| time/              |          |
|    total_timesteps | 169500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 144      |
|    iterations      | 83       |
|    time_elapsed    | 1175     |
|    total_timesteps | 169984   |
---------------------------------
Eval num_timesteps=170000, episode_reward=471.52 +/- 166.03
Episode length: 57.52 +/- 16.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 57.5         |
|    mean_reward          | 472          |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0068753976 |
|    clip_fraction        | 0.0707       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.588       |
|    explained_variance   | -0.00843     |
|    learning_rate        | 1e-05        |
|    loss                 | 5.03e+03     |
|    n_updates            | 830          |
|    policy_gradient_loss | 0.00208      |
|    value_loss           | 1.62e+04     |
------------------------------------------
Eval num_timesteps=170500, episode_reward=456.16 +/- 224.99
Episode length: 56.02 +/- 22.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 170500   |
---------------------------------
Eval num_timesteps=171000, episode_reward=526.56 +/- 324.52
Episode length: 63.08 +/- 32.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.1     |
|    mean_reward     | 527      |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=171500, episode_reward=620.00 +/- 366.10
Episode length: 72.46 +/- 36.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.5     |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 171500   |
---------------------------------
Eval num_timesteps=172000, episode_reward=497.12 +/- 282.15
Episode length: 60.10 +/- 28.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.1     |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    fps             | 144      |
|    iterations      | 84       |
|    time_elapsed    | 1192     |
|    total_timesteps | 172032   |
---------------------------------
Eval num_timesteps=172500, episode_reward=524.00 +/- 250.77
Episode length: 62.76 +/- 25.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 62.8        |
|    mean_reward          | 524         |
| time/                   |             |
|    total_timesteps      | 172500      |
| train/                  |             |
|    approx_kl            | 0.003553127 |
|    clip_fraction        | 0.0587      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.00342     |
|    learning_rate        | 1e-05       |
|    loss                 | 1.5e+04     |
|    n_updates            | 840         |
|    policy_gradient_loss | 0.000276    |
|    value_loss           | 2.35e+04    |
-----------------------------------------
Eval num_timesteps=173000, episode_reward=510.56 +/- 221.87
Episode length: 61.38 +/- 22.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.4     |
|    mean_reward     | 511      |
| time/              |          |
|    total_timesteps | 173000   |
---------------------------------
Eval num_timesteps=173500, episode_reward=531.68 +/- 285.06
Episode length: 63.56 +/- 28.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.6     |
|    mean_reward     | 532      |
| time/              |          |
|    total_timesteps | 173500   |
---------------------------------
Eval num_timesteps=174000, episode_reward=531.04 +/- 289.78
Episode length: 63.50 +/- 29.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.5     |
|    mean_reward     | 531      |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    fps             | 144      |
|    iterations      | 85       |
|    time_elapsed    | 1207     |
|    total_timesteps | 174080   |
---------------------------------
Eval num_timesteps=174500, episode_reward=840.00 +/- 602.40
Episode length: 93.36 +/- 58.12
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 93.4        |
|    mean_reward          | 840         |
| time/                   |             |
|    total_timesteps      | 174500      |
| train/                  |             |
|    approx_kl            | 0.006405042 |
|    clip_fraction        | 0.0407      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.582      |
|    explained_variance   | -0.0099     |
|    learning_rate        | 1e-05       |
|    loss                 | 8.76e+03    |
|    n_updates            | 850         |
|    policy_gradient_loss | 0.000954    |
|    value_loss           | 1.9e+04     |
-----------------------------------------
Eval num_timesteps=175000, episode_reward=741.12 +/- 525.28
Episode length: 83.82 +/- 50.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.8     |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 175000   |
---------------------------------
Eval num_timesteps=175500, episode_reward=907.20 +/- 620.45
Episode length: 100.10 +/- 60.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 100      |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 175500   |
---------------------------------
Eval num_timesteps=176000, episode_reward=1001.12 +/- 634.55
Episode length: 109.24 +/- 61.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 109      |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    fps             | 143      |
|    iterations      | 86       |
|    time_elapsed    | 1228     |
|    total_timesteps | 176128   |
---------------------------------
Eval num_timesteps=176500, episode_reward=824.96 +/- 639.47
Episode length: 91.40 +/- 61.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 91.4        |
|    mean_reward          | 825         |
| time/                   |             |
|    total_timesteps      | 176500      |
| train/                  |             |
|    approx_kl            | 0.006147514 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.614      |
|    explained_variance   | -0.0197     |
|    learning_rate        | 1e-05       |
|    loss                 | 6.12e+03    |
|    n_updates            | 860         |
|    policy_gradient_loss | 0.00145     |
|    value_loss           | 1.68e+04    |
-----------------------------------------
Eval num_timesteps=177000, episode_reward=841.92 +/- 609.81
Episode length: 93.56 +/- 58.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | 842      |
| time/              |          |
|    total_timesteps | 177000   |
---------------------------------
Eval num_timesteps=177500, episode_reward=834.08 +/- 627.06
Episode length: 92.52 +/- 60.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 177500   |
---------------------------------
Eval num_timesteps=178000, episode_reward=840.48 +/- 595.75
Episode length: 93.18 +/- 56.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.2     |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 142      |
|    iterations      | 87       |
|    time_elapsed    | 1248     |
|    total_timesteps | 178176   |
---------------------------------
Eval num_timesteps=178500, episode_reward=540.48 +/- 306.48
Episode length: 64.22 +/- 29.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 64.2        |
|    mean_reward          | 540         |
| time/                   |             |
|    total_timesteps      | 178500      |
| train/                  |             |
|    approx_kl            | 0.008200233 |
|    clip_fraction        | 0.0603      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.594      |
|    explained_variance   | 0.00655     |
|    learning_rate        | 1e-05       |
|    loss                 | 8.74e+03    |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.000351   |
|    value_loss           | 2.38e+04    |
-----------------------------------------
Eval num_timesteps=179000, episode_reward=590.40 +/- 418.17
Episode length: 69.18 +/- 41.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.2     |
|    mean_reward     | 590      |
| time/              |          |
|    total_timesteps | 179000   |
---------------------------------
Eval num_timesteps=179500, episode_reward=592.48 +/- 409.04
Episode length: 69.66 +/- 40.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.7     |
|    mean_reward     | 592      |
| time/              |          |
|    total_timesteps | 179500   |
---------------------------------
Eval num_timesteps=180000, episode_reward=543.68 +/- 400.98
Episode length: 64.60 +/- 39.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.6     |
|    mean_reward     | 544      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 142      |
|    iterations      | 88       |
|    time_elapsed    | 1263     |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=180500, episode_reward=1289.44 +/- 678.69
Episode length: 136.04 +/- 63.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 136         |
|    mean_reward          | 1.29e+03    |
| time/                   |             |
|    total_timesteps      | 180500      |
| train/                  |             |
|    approx_kl            | 0.005533467 |
|    clip_fraction        | 0.0676      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.612      |
|    explained_variance   | 0.0132      |
|    learning_rate        | 1e-05       |
|    loss                 | 8.68e+03    |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00104    |
|    value_loss           | 2.07e+04    |
-----------------------------------------
Eval num_timesteps=181000, episode_reward=1193.76 +/- 713.87
Episode length: 126.82 +/- 67.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 127      |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 181000   |
---------------------------------
Eval num_timesteps=181500, episode_reward=1127.04 +/- 709.65
Episode length: 120.82 +/- 67.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 181500   |
---------------------------------
Eval num_timesteps=182000, episode_reward=1114.08 +/- 680.16
Episode length: 119.34 +/- 64.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 119      |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    fps             | 141      |
|    iterations      | 89       |
|    time_elapsed    | 1289     |
|    total_timesteps | 182272   |
---------------------------------
Eval num_timesteps=182500, episode_reward=765.28 +/- 490.05
Episode length: 86.90 +/- 48.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 86.9         |
|    mean_reward          | 765          |
| time/                   |              |
|    total_timesteps      | 182500       |
| train/                  |              |
|    approx_kl            | 0.0026616738 |
|    clip_fraction        | 0.0952       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.596       |
|    explained_variance   | -0.0124      |
|    learning_rate        | 1e-05        |
|    loss                 | 1.12e+04     |
|    n_updates            | 890          |
|    policy_gradient_loss | 0.00185      |
|    value_loss           | 1.85e+04     |
------------------------------------------
Eval num_timesteps=183000, episode_reward=922.08 +/- 540.03
Episode length: 101.80 +/- 52.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 922      |
| time/              |          |
|    total_timesteps | 183000   |
---------------------------------
Eval num_timesteps=183500, episode_reward=921.92 +/- 553.84
Episode length: 101.62 +/- 53.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 922      |
| time/              |          |
|    total_timesteps | 183500   |
---------------------------------
Eval num_timesteps=184000, episode_reward=799.52 +/- 469.83
Episode length: 89.92 +/- 45.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 89.9     |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 140      |
|    iterations      | 90       |
|    time_elapsed    | 1310     |
|    total_timesteps | 184320   |
---------------------------------
Eval num_timesteps=184500, episode_reward=511.20 +/- 212.58
Episode length: 61.50 +/- 21.23
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 61.5        |
|    mean_reward          | 511         |
| time/                   |             |
|    total_timesteps      | 184500      |
| train/                  |             |
|    approx_kl            | 0.003628353 |
|    clip_fraction        | 0.0403      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.588      |
|    explained_variance   | 0.00682     |
|    learning_rate        | 1e-05       |
|    loss                 | 1.03e+04    |
|    n_updates            | 900         |
|    policy_gradient_loss | 0.00139     |
|    value_loss           | 2.21e+04    |
-----------------------------------------
Eval num_timesteps=185000, episode_reward=488.80 +/- 152.26
Episode length: 59.24 +/- 15.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.2     |
|    mean_reward     | 489      |
| time/              |          |
|    total_timesteps | 185000   |
---------------------------------
Eval num_timesteps=185500, episode_reward=506.72 +/- 231.81
Episode length: 61.10 +/- 23.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.1     |
|    mean_reward     | 507      |
| time/              |          |
|    total_timesteps | 185500   |
---------------------------------
Eval num_timesteps=186000, episode_reward=499.04 +/- 212.75
Episode length: 60.26 +/- 21.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.3     |
|    mean_reward     | 499      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    fps             | 140      |
|    iterations      | 91       |
|    time_elapsed    | 1324     |
|    total_timesteps | 186368   |
---------------------------------
Eval num_timesteps=186500, episode_reward=953.60 +/- 685.31
Episode length: 104.24 +/- 66.16
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 104          |
|    mean_reward          | 954          |
| time/                   |              |
|    total_timesteps      | 186500       |
| train/                  |              |
|    approx_kl            | 0.0020129154 |
|    clip_fraction        | 0.0473       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.573       |
|    explained_variance   | -0.00333     |
|    learning_rate        | 1e-05        |
|    loss                 | 7.5e+03      |
|    n_updates            | 910          |
|    policy_gradient_loss | 0.000873     |
|    value_loss           | 2.09e+04     |
------------------------------------------
Eval num_timesteps=187000, episode_reward=939.36 +/- 616.33
Episode length: 103.50 +/- 60.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 104      |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 187000   |
---------------------------------
Eval num_timesteps=187500, episode_reward=1070.40 +/- 652.43
Episode length: 115.58 +/- 62.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 187500   |
---------------------------------
Eval num_timesteps=188000, episode_reward=1030.24 +/- 667.77
Episode length: 111.80 +/- 64.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 139      |
|    iterations      | 92       |
|    time_elapsed    | 1347     |
|    total_timesteps | 188416   |
---------------------------------
Eval num_timesteps=188500, episode_reward=1052.64 +/- 634.01
Episode length: 113.98 +/- 60.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 114         |
|    mean_reward          | 1.05e+03    |
| time/                   |             |
|    total_timesteps      | 188500      |
| train/                  |             |
|    approx_kl            | 0.001537713 |
|    clip_fraction        | 0.0413      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.604      |
|    explained_variance   | 0.00101     |
|    learning_rate        | 1e-05       |
|    loss                 | 1.37e+04    |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.000477   |
|    value_loss           | 2.64e+04    |
-----------------------------------------
Eval num_timesteps=189000, episode_reward=1429.44 +/- 628.61
Episode length: 149.82 +/- 59.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 189000   |
---------------------------------
Eval num_timesteps=189500, episode_reward=1310.08 +/- 727.48
Episode length: 137.50 +/- 68.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 138      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 189500   |
---------------------------------
Eval num_timesteps=190000, episode_reward=1366.24 +/- 709.59
Episode length: 142.86 +/- 66.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 143      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 138      |
|    iterations      | 93       |
|    time_elapsed    | 1375     |
|    total_timesteps | 190464   |
---------------------------------
Eval num_timesteps=190500, episode_reward=1286.24 +/- 678.55
Episode length: 135.72 +/- 63.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 136         |
|    mean_reward          | 1.29e+03    |
| time/                   |             |
|    total_timesteps      | 190500      |
| train/                  |             |
|    approx_kl            | 0.008604826 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.616      |
|    explained_variance   | 0.00597     |
|    learning_rate        | 1e-05       |
|    loss                 | 7.22e+03    |
|    n_updates            | 930         |
|    policy_gradient_loss | -3.58e-05   |
|    value_loss           | 2.05e+04    |
-----------------------------------------
Eval num_timesteps=191000, episode_reward=1316.48 +/- 595.71
Episode length: 139.76 +/- 56.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 140      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 191000   |
---------------------------------
Eval num_timesteps=191500, episode_reward=1492.32 +/- 656.63
Episode length: 154.62 +/- 61.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 191500   |
---------------------------------
Eval num_timesteps=192000, episode_reward=1372.64 +/- 674.36
Episode length: 143.56 +/- 63.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 144      |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=192500, episode_reward=1439.20 +/- 681.36
Episode length: 150.18 +/- 64.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 192500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 136      |
|    iterations      | 94       |
|    time_elapsed    | 1411     |
|    total_timesteps | 192512   |
---------------------------------
Eval num_timesteps=193000, episode_reward=1622.72 +/- 610.77
Episode length: 167.44 +/- 57.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 167         |
|    mean_reward          | 1.62e+03    |
| time/                   |             |
|    total_timesteps      | 193000      |
| train/                  |             |
|    approx_kl            | 0.009392894 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.017       |
|    learning_rate        | 1e-05       |
|    loss                 | 6.52e+03    |
|    n_updates            | 940         |
|    policy_gradient_loss | 0.000991    |
|    value_loss           | 2.15e+04    |
-----------------------------------------
Eval num_timesteps=193500, episode_reward=1624.80 +/- 551.92
Episode length: 167.88 +/- 50.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 193500   |
---------------------------------
Eval num_timesteps=194000, episode_reward=1548.48 +/- 649.20
Episode length: 160.02 +/- 60.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=194500, episode_reward=1553.28 +/- 629.34
Episode length: 160.90 +/- 58.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 161      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 194500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 131      |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 134      |
|    iterations      | 95       |
|    time_elapsed    | 1445     |
|    total_timesteps | 194560   |
---------------------------------
Eval num_timesteps=195000, episode_reward=1627.04 +/- 607.02
Episode length: 167.66 +/- 56.58
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 168          |
|    mean_reward          | 1.63e+03     |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 0.0027016697 |
|    clip_fraction        | 0.0801       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.655       |
|    explained_variance   | 0.0332       |
|    learning_rate        | 1e-05        |
|    loss                 | 7.88e+03     |
|    n_updates            | 950          |
|    policy_gradient_loss | 0.000395     |
|    value_loss           | 2.17e+04     |
------------------------------------------
Eval num_timesteps=195500, episode_reward=1555.20 +/- 683.29
Episode length: 160.32 +/- 63.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 195500   |
---------------------------------
Eval num_timesteps=196000, episode_reward=1468.16 +/- 671.84
Episode length: 152.42 +/- 62.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 152      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=196500, episode_reward=1462.08 +/- 630.16
Episode length: 152.24 +/- 58.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 152      |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 196500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 133      |
|    iterations      | 96       |
|    time_elapsed    | 1477     |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=197000, episode_reward=1426.72 +/- 661.90
Episode length: 149.28 +/- 62.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 149         |
|    mean_reward          | 1.43e+03    |
| time/                   |             |
|    total_timesteps      | 197000      |
| train/                  |             |
|    approx_kl            | 0.009564383 |
|    clip_fraction        | 0.0617      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.654      |
|    explained_variance   | 0.0411      |
|    learning_rate        | 1e-05       |
|    loss                 | 1.14e+04    |
|    n_updates            | 960         |
|    policy_gradient_loss | 6.69e-05    |
|    value_loss           | 1.87e+04    |
-----------------------------------------
Eval num_timesteps=197500, episode_reward=1444.16 +/- 659.13
Episode length: 150.46 +/- 61.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 197500   |
---------------------------------
Eval num_timesteps=198000, episode_reward=1497.28 +/- 626.62
Episode length: 155.74 +/- 58.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 156      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=198500, episode_reward=1395.36 +/- 655.22
Episode length: 146.20 +/- 61.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 146      |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 198500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 97       |
|    time_elapsed    | 1508     |
|    total_timesteps | 198656   |
---------------------------------
Eval num_timesteps=199000, episode_reward=1648.48 +/- 615.27
Episode length: 169.40 +/- 57.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 1.65e+03    |
| time/                   |             |
|    total_timesteps      | 199000      |
| train/                  |             |
|    approx_kl            | 0.009785054 |
|    clip_fraction        | 0.0535      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.67       |
|    explained_variance   | 0.013       |
|    learning_rate        | 1e-05       |
|    loss                 | 1.02e+04    |
|    n_updates            | 970         |
|    policy_gradient_loss | 0.00116     |
|    value_loss           | 2.36e+04    |
-----------------------------------------
Eval num_timesteps=199500, episode_reward=1628.80 +/- 637.16
Episode length: 167.62 +/- 59.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 168      |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 199500   |
---------------------------------
Eval num_timesteps=200000, episode_reward=1472.32 +/- 630.56
Episode length: 153.24 +/- 58.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 153      |
|    mean_reward     | 1.47e+03 |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=200500, episode_reward=1584.64 +/- 615.42
Episode length: 164.12 +/- 57.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 164      |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 200500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 98       |
|    time_elapsed    | 1541     |
|    total_timesteps | 200704   |
---------------------------------
Eval num_timesteps=201000, episode_reward=1616.32 +/- 650.52
Episode length: 165.98 +/- 60.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 166         |
|    mean_reward          | 1.62e+03    |
| time/                   |             |
|    total_timesteps      | 201000      |
| train/                  |             |
|    approx_kl            | 0.013659298 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.647      |
|    explained_variance   | 0.0535      |
|    learning_rate        | 1e-05       |
|    loss                 | 1e+04       |
|    n_updates            | 980         |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 2.75e+04    |
-----------------------------------------
Eval num_timesteps=201500, episode_reward=1609.76 +/- 623.83
Episode length: 165.94 +/- 58.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 166      |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 201500   |
---------------------------------
Eval num_timesteps=202000, episode_reward=1490.40 +/- 656.19
Episode length: 155.30 +/- 61.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 155      |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=202500, episode_reward=1500.80 +/- 654.55
Episode length: 155.68 +/- 61.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 156      |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 202500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 128      |
|    iterations      | 99       |
|    time_elapsed    | 1573     |
|    total_timesteps | 202752   |
---------------------------------
Eval num_timesteps=203000, episode_reward=1480.80 +/- 705.33
Episode length: 153.46 +/- 66.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 153          |
|    mean_reward          | 1.48e+03     |
| time/                   |              |
|    total_timesteps      | 203000       |
| train/                  |              |
|    approx_kl            | 0.0039912956 |
|    clip_fraction        | 0.0609       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.66        |
|    explained_variance   | -0.0283      |
|    learning_rate        | 1e-05        |
|    loss                 | 6.91e+03     |
|    n_updates            | 990          |
|    policy_gradient_loss | 0.00296      |
|    value_loss           | 2.12e+04     |
------------------------------------------
Eval num_timesteps=203500, episode_reward=1549.28 +/- 654.05
Episode length: 160.34 +/- 61.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 160      |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 203500   |
---------------------------------
Eval num_timesteps=204000, episode_reward=1438.56 +/- 653.68
Episode length: 150.10 +/- 61.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 150      |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=204500, episode_reward=1599.84 +/- 581.15
Episode length: 165.38 +/- 53.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 165      |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 204500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 127      |
|    iterations      | 100      |
|    time_elapsed    | 1605     |
|    total_timesteps | 204800   |
---------------------------------
Eval num_timesteps=205000, episode_reward=1641.12 +/- 608.02
Episode length: 169.06 +/- 56.79
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 169         |
|    mean_reward          | 1.64e+03    |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.004836359 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.66       |
|    explained_variance   | 0.0273      |
|    learning_rate        | 1e-05       |
|    loss                 | 9.64e+03    |
|    n_updates            | 1000        |
|    policy_gradient_loss | -0.00264    |
|    value_loss           | 1.8e+04     |
-----------------------------------------
Eval num_timesteps=205500, episode_reward=1519.36 +/- 629.75
Episode length: 157.54 +/- 58.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 158      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 205500   |
---------------------------------
Eval num_timesteps=206000, episode_reward=1535.84 +/- 624.92
Episode length: 158.98 +/- 58.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 159      |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=206500, episode_reward=1359.36 +/- 670.93
Episode length: 142.34 +/- 62.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 142      |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 206500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 126      |
|    iterations      | 101      |
|    time_elapsed    | 1637     |
|    total_timesteps | 206848   |
---------------------------------
Eval num_timesteps=207000, episode_reward=1566.56 +/- 675.47
Episode length: 161.20 +/- 63.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 161          |
|    mean_reward          | 1.57e+03     |
| time/                   |              |
|    total_timesteps      | 207000       |
| train/                  |              |
|    approx_kl            | 0.0068055606 |
|    clip_fraction        | 0.0291       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.677       |
|    explained_variance   | 0.00522      |
|    learning_rate        | 1e-05        |
|    loss                 | 1.22e+04     |
|    n_updates            | 1010         |
|    policy_gradient_loss | 0.000594     |
|    value_loss           | 2.1e+04      |
------------------------------------------
Eval num_timesteps=207500, episode_reward=1566.40 +/- 642.15
Episode length: 161.84 +/- 59.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 162      |
|    mean_reward     | 1.57e+03 |
| time/              |          |
|    total_timesteps | 207500   |
---------------------------------
Eval num_timesteps=208000, episode_reward=1414.72 +/- 674.54
Episode length: 147.48 +/- 63.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 147      |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=208500, episode_reward=1520.48 +/- 681.76
Episode length: 157.44 +/- 63.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 157      |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 208500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    fps             | 125      |
|    iterations      | 102      |
|    time_elapsed    | 1669     |
|    total_timesteps | 208896   |
---------------------------------
Eval num_timesteps=209000, episode_reward=1285.60 +/- 705.42
Episode length: 135.64 +/- 66.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 136       |
|    mean_reward          | 1.29e+03  |
| time/                   |           |
|    total_timesteps      | 209000    |
| train/                  |           |
|    approx_kl            | 0.0066002 |
|    clip_fraction        | 0.156     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.661    |
|    explained_variance   | 0.0144    |
|    learning_rate        | 1e-05     |
|    loss                 | 9.69e+03  |
|    n_updates            | 1020      |
|    policy_gradient_loss | 0.00297   |
|    value_loss           | 2.01e+04  |
---------------------------------------
Eval num_timesteps=209500, episode_reward=1319.36 +/- 690.46
Episode length: 138.84 +/- 65.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 139      |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 209500   |
---------------------------------
Eval num_timesteps=210000, episode_reward=1222.72 +/- 649.99
Episode length: 129.92 +/- 61.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=210500, episode_reward=1241.60 +/- 691.17
Episode length: 131.44 +/- 65.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 210500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    fps             | 124      |
|    iterations      | 103      |
|    time_elapsed    | 1697     |
|    total_timesteps | 210944   |
---------------------------------
Eval num_timesteps=211000, episode_reward=1299.04 +/- 681.70
Episode length: 136.98 +/- 64.31
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 137          |
|    mean_reward          | 1.3e+03      |
| time/                   |              |
|    total_timesteps      | 211000       |
| train/                  |              |
|    approx_kl            | 0.0043444214 |
|    clip_fraction        | 0.0768       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.599       |
|    explained_variance   | 0.0304       |
|    learning_rate        | 1e-05        |
|    loss                 | 9.81e+03     |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00293     |
|    value_loss           | 2.12e+04     |
------------------------------------------
Eval num_timesteps=211500, episode_reward=1162.08 +/- 658.18
Episode length: 124.08 +/- 62.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 124      |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 211500   |
---------------------------------
Eval num_timesteps=212000, episode_reward=1205.76 +/- 726.63
Episode length: 127.84 +/- 68.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 128      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=212500, episode_reward=1176.80 +/- 673.16
Episode length: 125.64 +/- 63.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 126      |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 212500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 123      |
|    iterations      | 104      |
|    time_elapsed    | 1724     |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=213000, episode_reward=703.52 +/- 533.04
Episode length: 80.34 +/- 52.22
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.3        |
|    mean_reward          | 704         |
| time/                   |             |
|    total_timesteps      | 213000      |
| train/                  |             |
|    approx_kl            | 0.005929431 |
|    clip_fraction        | 0.0593      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.598      |
|    explained_variance   | 0.0364      |
|    learning_rate        | 1e-05       |
|    loss                 | 1.3e+04     |
|    n_updates            | 1040        |
|    policy_gradient_loss | 0.000487    |
|    value_loss           | 1.94e+04    |
-----------------------------------------
Eval num_timesteps=213500, episode_reward=690.72 +/- 517.53
Episode length: 79.06 +/- 50.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.1     |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 213500   |
---------------------------------
Eval num_timesteps=214000, episode_reward=736.80 +/- 492.72
Episode length: 83.72 +/- 48.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 83.7     |
|    mean_reward     | 737      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=214500, episode_reward=844.00 +/- 622.77
Episode length: 93.92 +/- 60.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.9     |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 214500   |
---------------------------------
Eval num_timesteps=215000, episode_reward=762.40 +/- 555.73
Episode length: 86.22 +/- 54.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.2     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 123      |
|    iterations      | 105      |
|    time_elapsed    | 1746     |
|    total_timesteps | 215040   |
---------------------------------
Eval num_timesteps=215500, episode_reward=853.44 +/- 597.36
Episode length: 94.68 +/- 57.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 94.7        |
|    mean_reward          | 853         |
| time/                   |             |
|    total_timesteps      | 215500      |
| train/                  |             |
|    approx_kl            | 0.002592369 |
|    clip_fraction        | 0.0579      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.554      |
|    explained_variance   | 0.0219      |
|    learning_rate        | 1e-05       |
|    loss                 | 5.65e+03    |
|    n_updates            | 1050        |
|    policy_gradient_loss | 0.000396    |
|    value_loss           | 1.73e+04    |
-----------------------------------------
Eval num_timesteps=216000, episode_reward=607.84 +/- 345.58
Episode length: 71.22 +/- 34.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.2     |
|    mean_reward     | 608      |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=216500, episode_reward=816.16 +/- 505.18
Episode length: 91.60 +/- 49.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.6     |
|    mean_reward     | 816      |
| time/              |          |
|    total_timesteps | 216500   |
---------------------------------
Eval num_timesteps=217000, episode_reward=827.20 +/- 464.41
Episode length: 92.90 +/- 45.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.9     |
|    mean_reward     | 827      |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 124      |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 122      |
|    iterations      | 106      |
|    time_elapsed    | 1766     |
|    total_timesteps | 217088   |
---------------------------------
Eval num_timesteps=217500, episode_reward=628.16 +/- 410.25
Episode length: 73.04 +/- 40.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 73           |
|    mean_reward          | 628          |
| time/                   |              |
|    total_timesteps      | 217500       |
| train/                  |              |
|    approx_kl            | 0.0030969614 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.594       |
|    explained_variance   | 0.0175       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.59e+04     |
|    n_updates            | 1060         |
|    policy_gradient_loss | 0.00156      |
|    value_loss           | 2.41e+04     |
------------------------------------------
Eval num_timesteps=218000, episode_reward=624.96 +/- 396.23
Episode length: 72.66 +/- 38.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | 625      |
| time/              |          |
|    total_timesteps | 218000   |
---------------------------------
Eval num_timesteps=218500, episode_reward=549.44 +/- 340.05
Episode length: 65.06 +/- 33.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.1     |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 218500   |
---------------------------------
Eval num_timesteps=219000, episode_reward=610.24 +/- 411.81
Episode length: 71.22 +/- 40.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.2     |
|    mean_reward     | 610      |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 122      |
|    iterations      | 107      |
|    time_elapsed    | 1782     |
|    total_timesteps | 219136   |
---------------------------------
Eval num_timesteps=219500, episode_reward=1044.64 +/- 615.01
Episode length: 113.64 +/- 59.42
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 114          |
|    mean_reward          | 1.04e+03     |
| time/                   |              |
|    total_timesteps      | 219500       |
| train/                  |              |
|    approx_kl            | 0.0041769524 |
|    clip_fraction        | 0.0934       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.581       |
|    explained_variance   | 0.0183       |
|    learning_rate        | 1e-05        |
|    loss                 | 4.1e+03      |
|    n_updates            | 1070         |
|    policy_gradient_loss | 9.31e-06     |
|    value_loss           | 1.29e+04     |
------------------------------------------
Eval num_timesteps=220000, episode_reward=1290.88 +/- 639.44
Episode length: 137.26 +/- 61.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 137      |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=220500, episode_reward=1050.24 +/- 626.18
Episode length: 113.96 +/- 60.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 114      |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 220500   |
---------------------------------
Eval num_timesteps=221000, episode_reward=1131.68 +/- 697.61
Episode length: 121.46 +/- 66.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 121      |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 122      |
|    iterations      | 108      |
|    time_elapsed    | 1807     |
|    total_timesteps | 221184   |
---------------------------------
Eval num_timesteps=221500, episode_reward=494.56 +/- 266.05
Episode length: 59.86 +/- 26.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 59.9        |
|    mean_reward          | 495         |
| time/                   |             |
|    total_timesteps      | 221500      |
| train/                  |             |
|    approx_kl            | 0.013011183 |
|    clip_fraction        | 0.0815      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.00623     |
|    learning_rate        | 1e-05       |
|    loss                 | 5.82e+03    |
|    n_updates            | 1080        |
|    policy_gradient_loss | 0.00246     |
|    value_loss           | 1.76e+04    |
-----------------------------------------
Eval num_timesteps=222000, episode_reward=532.96 +/- 296.98
Episode length: 63.74 +/- 29.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.7     |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=222500, episode_reward=502.88 +/- 259.45
Episode length: 60.76 +/- 26.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.8     |
|    mean_reward     | 503      |
| time/              |          |
|    total_timesteps | 222500   |
---------------------------------
Eval num_timesteps=223000, episode_reward=527.84 +/- 263.09
Episode length: 63.20 +/- 26.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.2     |
|    mean_reward     | 528      |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    fps             | 122      |
|    iterations      | 109      |
|    time_elapsed    | 1822     |
|    total_timesteps | 223232   |
---------------------------------
Eval num_timesteps=223500, episode_reward=451.68 +/- 201.12
Episode length: 55.58 +/- 20.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.6        |
|    mean_reward          | 452         |
| time/                   |             |
|    total_timesteps      | 223500      |
| train/                  |             |
|    approx_kl            | 0.003265159 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.539      |
|    explained_variance   | 0.0112      |
|    learning_rate        | 1e-05       |
|    loss                 | 4.74e+03    |
|    n_updates            | 1090        |
|    policy_gradient_loss | 0.00258     |
|    value_loss           | 1.84e+04    |
-----------------------------------------
Eval num_timesteps=224000, episode_reward=488.16 +/- 220.30
Episode length: 59.18 +/- 22.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.2     |
|    mean_reward     | 488      |
| time/              |          |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=224500, episode_reward=502.24 +/- 258.60
Episode length: 60.58 +/- 25.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.6     |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 224500   |
---------------------------------
Eval num_timesteps=225000, episode_reward=519.52 +/- 291.52
Episode length: 62.34 +/- 29.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.3     |
|    mean_reward     | 520      |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 1.39e+03 |
| time/              |          |
|    fps             | 122      |
|    iterations      | 110      |
|    time_elapsed    | 1836     |
|    total_timesteps | 225280   |
---------------------------------
Eval num_timesteps=225500, episode_reward=410.72 +/- 138.89
Episode length: 51.46 +/- 13.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.5        |
|    mean_reward          | 411         |
| time/                   |             |
|    total_timesteps      | 225500      |
| train/                  |             |
|    approx_kl            | 0.004158034 |
|    clip_fraction        | 0.0668      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.508      |
|    explained_variance   | 0.00896     |
|    learning_rate        | 1e-05       |
|    loss                 | 1.02e+04    |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00123    |
|    value_loss           | 1.82e+04    |
-----------------------------------------
Eval num_timesteps=226000, episode_reward=449.76 +/- 200.93
Episode length: 55.42 +/- 20.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.4     |
|    mean_reward     | 450      |
| time/              |          |
|    total_timesteps | 226000   |
---------------------------------
Eval num_timesteps=226500, episode_reward=451.04 +/- 189.13
Episode length: 55.48 +/- 18.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.5     |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 226500   |
---------------------------------
Eval num_timesteps=227000, episode_reward=484.32 +/- 174.19
Episode length: 58.80 +/- 17.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.8     |
|    mean_reward     | 484      |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 149      |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    fps             | 122      |
|    iterations      | 111      |
|    time_elapsed    | 1849     |
|    total_timesteps | 227328   |
---------------------------------
Eval num_timesteps=227500, episode_reward=560.48 +/- 281.29
Episode length: 66.46 +/- 28.13
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 66.5        |
|    mean_reward          | 560         |
| time/                   |             |
|    total_timesteps      | 227500      |
| train/                  |             |
|    approx_kl            | 0.004129219 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.56       |
|    explained_variance   | 0.0152      |
|    learning_rate        | 1e-05       |
|    loss                 | 9.04e+03    |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.000181   |
|    value_loss           | 1.81e+04    |
-----------------------------------------
Eval num_timesteps=228000, episode_reward=583.36 +/- 369.53
Episode length: 68.50 +/- 36.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.5     |
|    mean_reward     | 583      |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=228500, episode_reward=591.04 +/- 365.37
Episode length: 69.24 +/- 35.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.2     |
|    mean_reward     | 591      |
| time/              |          |
|    total_timesteps | 228500   |
---------------------------------
Eval num_timesteps=229000, episode_reward=548.96 +/- 289.35
Episode length: 65.28 +/- 28.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.3     |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 1.44e+03 |
| time/              |          |
|    fps             | 122      |
|    iterations      | 112      |
|    time_elapsed    | 1865     |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229500, episode_reward=465.12 +/- 223.23
Episode length: 56.96 +/- 22.35
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 57           |
|    mean_reward          | 465          |
| time/                   |              |
|    total_timesteps      | 229500       |
| train/                  |              |
|    approx_kl            | 0.0068767914 |
|    clip_fraction        | 0.0663       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.532       |
|    explained_variance   | -0.00512     |
|    learning_rate        | 1e-05        |
|    loss                 | 7.31e+03     |
|    n_updates            | 1120         |
|    policy_gradient_loss | 0.00047      |
|    value_loss           | 1.92e+04     |
------------------------------------------
Eval num_timesteps=230000, episode_reward=490.08 +/- 212.09
Episode length: 59.42 +/- 21.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.4     |
|    mean_reward     | 490      |
| time/              |          |
|    total_timesteps | 230000   |
---------------------------------
Eval num_timesteps=230500, episode_reward=410.72 +/- 182.82
Episode length: 51.44 +/- 18.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.4     |
|    mean_reward     | 411      |
| time/              |          |
|    total_timesteps | 230500   |
---------------------------------
Eval num_timesteps=231000, episode_reward=417.12 +/- 141.72
Episode length: 52.12 +/- 14.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.1     |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 155      |
|    ep_rew_mean     | 1.51e+03 |
| time/              |          |
|    fps             | 123      |
|    iterations      | 113      |
|    time_elapsed    | 1878     |
|    total_timesteps | 231424   |
---------------------------------
Eval num_timesteps=231500, episode_reward=311.52 +/- 37.32
Episode length: 41.60 +/- 3.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 312          |
| time/                   |              |
|    total_timesteps      | 231500       |
| train/                  |              |
|    approx_kl            | 0.0058794846 |
|    clip_fraction        | 0.0734       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.494       |
|    explained_variance   | -0.00579     |
|    learning_rate        | 1e-05        |
|    loss                 | 5.61e+03     |
|    n_updates            | 1130         |
|    policy_gradient_loss | 0.00232      |
|    value_loss           | 1.85e+04     |
------------------------------------------
Eval num_timesteps=232000, episode_reward=315.36 +/- 35.91
Episode length: 41.94 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=232500, episode_reward=307.68 +/- 31.23
Episode length: 41.22 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 232500   |
---------------------------------
Eval num_timesteps=233000, episode_reward=317.28 +/- 42.43
Episode length: 42.16 +/- 4.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 1.46e+03 |
| time/              |          |
|    fps             | 123      |
|    iterations      | 114      |
|    time_elapsed    | 1889     |
|    total_timesteps | 233472   |
---------------------------------
Eval num_timesteps=233500, episode_reward=314.08 +/- 34.11
Episode length: 41.82 +/- 3.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.8         |
|    mean_reward          | 314          |
| time/                   |              |
|    total_timesteps      | 233500       |
| train/                  |              |
|    approx_kl            | 0.0008495066 |
|    clip_fraction        | 0.0309       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.48        |
|    explained_variance   | 0.00922      |
|    learning_rate        | 1e-05        |
|    loss                 | 1.16e+04     |
|    n_updates            | 1140         |
|    policy_gradient_loss | 0.000732     |
|    value_loss           | 2.77e+04     |
------------------------------------------
Eval num_timesteps=234000, episode_reward=317.28 +/- 35.03
Episode length: 42.12 +/- 3.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=234500, episode_reward=308.96 +/- 33.43
Episode length: 41.34 +/- 3.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.3     |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 234500   |
---------------------------------
Eval num_timesteps=235000, episode_reward=311.52 +/- 33.87
Episode length: 41.58 +/- 3.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=235500, episode_reward=320.48 +/- 45.71
Episode length: 42.46 +/- 4.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 235500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 1.43e+03 |
| time/              |          |
|    fps             | 123      |
|    iterations      | 115      |
|    time_elapsed    | 1902     |
|    total_timesteps | 235520   |
---------------------------------
Eval num_timesteps=236000, episode_reward=444.00 +/- 186.81
Episode length: 54.82 +/- 18.66
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 54.8         |
|    mean_reward          | 444          |
| time/                   |              |
|    total_timesteps      | 236000       |
| train/                  |              |
|    approx_kl            | 0.0052722376 |
|    clip_fraction        | 0.074        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.503       |
|    explained_variance   | 0.000362     |
|    learning_rate        | 1e-05        |
|    loss                 | 1.11e+04     |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00118     |
|    value_loss           | 2.19e+04     |
------------------------------------------
Eval num_timesteps=236500, episode_reward=437.60 +/- 134.86
Episode length: 54.20 +/- 13.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.2     |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 236500   |
---------------------------------
Eval num_timesteps=237000, episode_reward=443.36 +/- 191.41
Episode length: 54.72 +/- 19.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | 443      |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=237500, episode_reward=475.36 +/- 170.95
Episode length: 57.96 +/- 17.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58       |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 237500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 1.39e+03 |
| time/              |          |
|    fps             | 123      |
|    iterations      | 116      |
|    time_elapsed    | 1916     |
|    total_timesteps | 237568   |
---------------------------------
Eval num_timesteps=238000, episode_reward=307.04 +/- 31.38
Episode length: 41.16 +/- 2.94
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.2         |
|    mean_reward          | 307          |
| time/                   |              |
|    total_timesteps      | 238000       |
| train/                  |              |
|    approx_kl            | 0.0034148986 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.462       |
|    explained_variance   | 0.014        |
|    learning_rate        | 1e-05        |
|    loss                 | 4.95e+03     |
|    n_updates            | 1160         |
|    policy_gradient_loss | 0.000517     |
|    value_loss           | 2.13e+04     |
------------------------------------------
Eval num_timesteps=238500, episode_reward=307.68 +/- 34.94
Episode length: 41.22 +/- 3.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 238500   |
---------------------------------
Eval num_timesteps=239000, episode_reward=317.92 +/- 40.18
Episode length: 42.18 +/- 3.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=239500, episode_reward=315.36 +/- 37.04
Episode length: 41.94 +/- 3.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 239500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 124      |
|    iterations      | 117      |
|    time_elapsed    | 1927     |
|    total_timesteps | 239616   |
---------------------------------
Eval num_timesteps=240000, episode_reward=315.36 +/- 40.72
Episode length: 41.96 +/- 3.87
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42           |
|    mean_reward          | 315          |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0034858328 |
|    clip_fraction        | 0.0443       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.459       |
|    explained_variance   | -0.000654    |
|    learning_rate        | 1e-05        |
|    loss                 | 6.29e+03     |
|    n_updates            | 1170         |
|    policy_gradient_loss | 0.000739     |
|    value_loss           | 1.8e+04      |
------------------------------------------
Eval num_timesteps=240500, episode_reward=303.84 +/- 30.62
Episode length: 40.86 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.9     |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 240500   |
---------------------------------
Eval num_timesteps=241000, episode_reward=311.52 +/- 34.47
Episode length: 41.58 +/- 3.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=241500, episode_reward=323.04 +/- 38.55
Episode length: 42.66 +/- 3.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 323      |
| time/              |          |
|    total_timesteps | 241500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 124      |
|    iterations      | 118      |
|    time_elapsed    | 1937     |
|    total_timesteps | 241664   |
---------------------------------
Eval num_timesteps=242000, episode_reward=317.28 +/- 38.38
Episode length: 42.12 +/- 3.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 317          |
| time/                   |              |
|    total_timesteps      | 242000       |
| train/                  |              |
|    approx_kl            | 0.0033134134 |
|    clip_fraction        | 0.0423       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.472       |
|    explained_variance   | 0.00671      |
|    learning_rate        | 1e-05        |
|    loss                 | 1.27e+04     |
|    n_updates            | 1180         |
|    policy_gradient_loss | 0.000396     |
|    value_loss           | 2.65e+04     |
------------------------------------------
Eval num_timesteps=242500, episode_reward=307.68 +/- 37.76
Episode length: 41.24 +/- 3.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 242500   |
---------------------------------
Eval num_timesteps=243000, episode_reward=307.04 +/- 32.66
Episode length: 41.16 +/- 3.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=243500, episode_reward=303.84 +/- 31.28
Episode length: 40.86 +/- 2.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.9     |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 243500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 125      |
|    iterations      | 119      |
|    time_elapsed    | 1948     |
|    total_timesteps | 243712   |
---------------------------------
Eval num_timesteps=244000, episode_reward=344.16 +/- 54.55
Episode length: 44.72 +/- 5.29
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 44.7         |
|    mean_reward          | 344          |
| time/                   |              |
|    total_timesteps      | 244000       |
| train/                  |              |
|    approx_kl            | 0.0026931022 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.494       |
|    explained_variance   | 0.00957      |
|    learning_rate        | 1e-05        |
|    loss                 | 1.2e+04      |
|    n_updates            | 1190         |
|    policy_gradient_loss | 0.000854     |
|    value_loss           | 2.16e+04     |
------------------------------------------
Eval num_timesteps=244500, episode_reward=340.32 +/- 56.73
Episode length: 44.40 +/- 5.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.4     |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 244500   |
---------------------------------
Eval num_timesteps=245000, episode_reward=371.04 +/- 130.23
Episode length: 47.48 +/- 12.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.5     |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=245500, episode_reward=340.96 +/- 55.16
Episode length: 44.46 +/- 5.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.5     |
|    mean_reward     | 341      |
| time/              |          |
|    total_timesteps | 245500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 125      |
|    iterations      | 120      |
|    time_elapsed    | 1960     |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=246000, episode_reward=460.00 +/- 220.52
Episode length: 56.40 +/- 22.02
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 56.4         |
|    mean_reward          | 460          |
| time/                   |              |
|    total_timesteps      | 246000       |
| train/                  |              |
|    approx_kl            | 0.0039945496 |
|    clip_fraction        | 0.0962       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.5         |
|    explained_variance   | 0.0151       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.05e+04     |
|    n_updates            | 1200         |
|    policy_gradient_loss | 0.00074      |
|    value_loss           | 1.98e+04     |
------------------------------------------
Eval num_timesteps=246500, episode_reward=448.48 +/- 194.54
Episode length: 55.20 +/- 19.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | 448      |
| time/              |          |
|    total_timesteps | 246500   |
---------------------------------
Eval num_timesteps=247000, episode_reward=415.20 +/- 147.65
Episode length: 51.90 +/- 14.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=247500, episode_reward=477.28 +/- 223.36
Episode length: 58.14 +/- 22.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.1     |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 247500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 125      |
|    iterations      | 121      |
|    time_elapsed    | 1973     |
|    total_timesteps | 247808   |
---------------------------------
Eval num_timesteps=248000, episode_reward=452.96 +/- 157.03
Episode length: 55.68 +/- 15.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 55.7         |
|    mean_reward          | 453          |
| time/                   |              |
|    total_timesteps      | 248000       |
| train/                  |              |
|    approx_kl            | 0.0009342114 |
|    clip_fraction        | 0.0617       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.486       |
|    explained_variance   | 0.00298      |
|    learning_rate        | 1e-05        |
|    loss                 | 8.94e+03     |
|    n_updates            | 1210         |
|    policy_gradient_loss | 0.00302      |
|    value_loss           | 1.79e+04     |
------------------------------------------
Eval num_timesteps=248500, episode_reward=438.88 +/- 212.40
Episode length: 54.34 +/- 21.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 248500   |
---------------------------------
Eval num_timesteps=249000, episode_reward=500.32 +/- 228.49
Episode length: 60.40 +/- 22.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.4     |
|    mean_reward     | 500      |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=249500, episode_reward=471.52 +/- 277.35
Episode length: 57.64 +/- 27.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.6     |
|    mean_reward     | 472      |
| time/              |          |
|    total_timesteps | 249500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    fps             | 125      |
|    iterations      | 122      |
|    time_elapsed    | 1987     |
|    total_timesteps | 249856   |
---------------------------------
Eval num_timesteps=250000, episode_reward=440.16 +/- 183.56
Episode length: 54.42 +/- 18.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 54.4         |
|    mean_reward          | 440          |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0018674931 |
|    clip_fraction        | 0.0552       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.492       |
|    explained_variance   | -0.00277     |
|    learning_rate        | 1e-05        |
|    loss                 | 1.62e+04     |
|    n_updates            | 1220         |
|    policy_gradient_loss | 0.00246      |
|    value_loss           | 2.05e+04     |
------------------------------------------
Eval num_timesteps=250500, episode_reward=447.20 +/- 184.63
Episode length: 55.10 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 250500   |
---------------------------------
Eval num_timesteps=251000, episode_reward=416.48 +/- 182.82
Episode length: 52.04 +/- 18.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52       |
|    mean_reward     | 416      |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=251500, episode_reward=429.28 +/- 179.00
Episode length: 53.30 +/- 17.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | 429      |
| time/              |          |
|    total_timesteps | 251500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 125      |
|    iterations      | 123      |
|    time_elapsed    | 2000     |
|    total_timesteps | 251904   |
---------------------------------
Eval num_timesteps=252000, episode_reward=506.08 +/- 242.48
Episode length: 60.98 +/- 24.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 61           |
|    mean_reward          | 506          |
| time/                   |              |
|    total_timesteps      | 252000       |
| train/                  |              |
|    approx_kl            | 0.0014281722 |
|    clip_fraction        | 0.0588       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.49        |
|    explained_variance   | 0.0242       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.33e+04     |
|    n_updates            | 1230         |
|    policy_gradient_loss | 0.00219      |
|    value_loss           | 2.14e+04     |
------------------------------------------
Eval num_timesteps=252500, episode_reward=478.56 +/- 187.67
Episode length: 58.24 +/- 18.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.2     |
|    mean_reward     | 479      |
| time/              |          |
|    total_timesteps | 252500   |
---------------------------------
Eval num_timesteps=253000, episode_reward=499.68 +/- 252.03
Episode length: 60.32 +/- 25.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.3     |
|    mean_reward     | 500      |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=253500, episode_reward=484.96 +/- 281.02
Episode length: 58.90 +/- 28.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.9     |
|    mean_reward     | 485      |
| time/              |          |
|    total_timesteps | 253500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 1.35e+03 |
| time/              |          |
|    fps             | 126      |
|    iterations      | 124      |
|    time_elapsed    | 2015     |
|    total_timesteps | 253952   |
---------------------------------
Eval num_timesteps=254000, episode_reward=436.32 +/- 186.21
Episode length: 54.06 +/- 18.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 54.1         |
|    mean_reward          | 436          |
| time/                   |              |
|    total_timesteps      | 254000       |
| train/                  |              |
|    approx_kl            | 0.0064033163 |
|    clip_fraction        | 0.0907       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.481       |
|    explained_variance   | 0.00858      |
|    learning_rate        | 1e-05        |
|    loss                 | 6.05e+03     |
|    n_updates            | 1240         |
|    policy_gradient_loss | 0.000807     |
|    value_loss           | 1.59e+04     |
------------------------------------------
Eval num_timesteps=254500, episode_reward=456.16 +/- 160.11
Episode length: 56.02 +/- 16.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 254500   |
---------------------------------
Eval num_timesteps=255000, episode_reward=518.24 +/- 321.43
Episode length: 62.20 +/- 32.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.2     |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=255500, episode_reward=445.92 +/- 232.30
Episode length: 55.00 +/- 23.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55       |
|    mean_reward     | 446      |
| time/              |          |
|    total_timesteps | 255500   |
---------------------------------
Eval num_timesteps=256000, episode_reward=404.32 +/- 151.66
Episode length: 50.82 +/- 15.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.8     |
|    mean_reward     | 404      |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 126      |
|    iterations      | 125      |
|    time_elapsed    | 2031     |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=256500, episode_reward=321.12 +/- 42.14
Episode length: 42.52 +/- 4.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.5         |
|    mean_reward          | 321          |
| time/                   |              |
|    total_timesteps      | 256500       |
| train/                  |              |
|    approx_kl            | 0.0026337253 |
|    clip_fraction        | 0.0544       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.468       |
|    explained_variance   | 0.000977     |
|    learning_rate        | 1e-05        |
|    loss                 | 6.25e+03     |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.000501    |
|    value_loss           | 2.21e+04     |
------------------------------------------
Eval num_timesteps=257000, episode_reward=317.28 +/- 40.46
Episode length: 42.14 +/- 3.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 257000   |
---------------------------------
Eval num_timesteps=257500, episode_reward=324.96 +/- 36.23
Episode length: 42.84 +/- 3.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.8     |
|    mean_reward     | 325      |
| time/              |          |
|    total_timesteps | 257500   |
---------------------------------
Eval num_timesteps=258000, episode_reward=310.24 +/- 39.29
Episode length: 41.48 +/- 3.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 148      |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 126      |
|    iterations      | 126      |
|    time_elapsed    | 2042     |
|    total_timesteps | 258048   |
---------------------------------
Eval num_timesteps=258500, episode_reward=429.92 +/- 178.53
Episode length: 53.40 +/- 17.79
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.4         |
|    mean_reward          | 430          |
| time/                   |              |
|    total_timesteps      | 258500       |
| train/                  |              |
|    approx_kl            | 0.0018325935 |
|    clip_fraction        | 0.0618       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.463       |
|    explained_variance   | 0.0114       |
|    learning_rate        | 1e-05        |
|    loss                 | 6.7e+03      |
|    n_updates            | 1260         |
|    policy_gradient_loss | 0.00201      |
|    value_loss           | 2.12e+04     |
------------------------------------------
Eval num_timesteps=259000, episode_reward=420.96 +/- 135.47
Episode length: 52.50 +/- 13.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.5     |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 259000   |
---------------------------------
Eval num_timesteps=259500, episode_reward=452.96 +/- 207.19
Episode length: 55.64 +/- 20.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 259500   |
---------------------------------
Eval num_timesteps=260000, episode_reward=459.36 +/- 200.68
Episode length: 56.34 +/- 20.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.3     |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 126      |
|    iterations      | 127      |
|    time_elapsed    | 2055     |
|    total_timesteps | 260096   |
---------------------------------
Eval num_timesteps=260500, episode_reward=306.40 +/- 32.79
Episode length: 41.10 +/- 3.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.1         |
|    mean_reward          | 306          |
| time/                   |              |
|    total_timesteps      | 260500       |
| train/                  |              |
|    approx_kl            | 0.0074102166 |
|    clip_fraction        | 0.0613       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.468       |
|    explained_variance   | 0.0222       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.05e+04     |
|    n_updates            | 1270         |
|    policy_gradient_loss | 0.00224      |
|    value_loss           | 2.41e+04     |
------------------------------------------
Eval num_timesteps=261000, episode_reward=312.80 +/- 35.78
Episode length: 41.70 +/- 3.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 261000   |
---------------------------------
Eval num_timesteps=261500, episode_reward=311.52 +/- 33.87
Episode length: 41.58 +/- 3.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 261500   |
---------------------------------
Eval num_timesteps=262000, episode_reward=316.00 +/- 38.93
Episode length: 42.02 +/- 3.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    fps             | 126      |
|    iterations      | 128      |
|    time_elapsed    | 2065     |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=262500, episode_reward=307.04 +/- 38.42
Episode length: 41.18 +/- 3.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.2        |
|    mean_reward          | 307         |
| time/                   |             |
|    total_timesteps      | 262500      |
| train/                  |             |
|    approx_kl            | 0.006052438 |
|    clip_fraction        | 0.0514      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.461      |
|    explained_variance   | 0.0223      |
|    learning_rate        | 1e-05       |
|    loss                 | 1.31e+04    |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00164    |
|    value_loss           | 2.48e+04    |
-----------------------------------------
Eval num_timesteps=263000, episode_reward=315.36 +/- 40.72
Episode length: 41.96 +/- 3.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 263000   |
---------------------------------
Eval num_timesteps=263500, episode_reward=317.92 +/- 37.54
Episode length: 42.20 +/- 3.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 263500   |
---------------------------------
Eval num_timesteps=264000, episode_reward=307.68 +/- 30.57
Episode length: 41.22 +/- 2.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | 1.34e+03 |
| time/              |          |
|    fps             | 127      |
|    iterations      | 129      |
|    time_elapsed    | 2076     |
|    total_timesteps | 264192   |
---------------------------------
Eval num_timesteps=264500, episode_reward=388.96 +/- 126.07
Episode length: 49.34 +/- 12.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 49.3         |
|    mean_reward          | 389          |
| time/                   |              |
|    total_timesteps      | 264500       |
| train/                  |              |
|    approx_kl            | 0.0050761215 |
|    clip_fraction        | 0.0565       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.458       |
|    explained_variance   | 0.0122       |
|    learning_rate        | 1e-05        |
|    loss                 | 5.14e+03     |
|    n_updates            | 1290         |
|    policy_gradient_loss | 0.00179      |
|    value_loss           | 1.75e+04     |
------------------------------------------
Eval num_timesteps=265000, episode_reward=461.28 +/- 256.42
Episode length: 56.58 +/- 25.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.6     |
|    mean_reward     | 461      |
| time/              |          |
|    total_timesteps | 265000   |
---------------------------------
Eval num_timesteps=265500, episode_reward=376.80 +/- 97.74
Episode length: 48.06 +/- 9.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.1     |
|    mean_reward     | 377      |
| time/              |          |
|    total_timesteps | 265500   |
---------------------------------
Eval num_timesteps=266000, episode_reward=454.24 +/- 162.72
Episode length: 55.84 +/- 16.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 127      |
|    iterations      | 130      |
|    time_elapsed    | 2089     |
|    total_timesteps | 266240   |
---------------------------------
Eval num_timesteps=266500, episode_reward=323.04 +/- 36.37
Episode length: 42.66 +/- 3.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.7        |
|    mean_reward          | 323         |
| time/                   |             |
|    total_timesteps      | 266500      |
| train/                  |             |
|    approx_kl            | 0.005585646 |
|    clip_fraction        | 0.0534      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.476      |
|    explained_variance   | 0.00726     |
|    learning_rate        | 1e-05       |
|    loss                 | 1.01e+04    |
|    n_updates            | 1300        |
|    policy_gradient_loss | 0.00281     |
|    value_loss           | 2.5e+04     |
-----------------------------------------
Eval num_timesteps=267000, episode_reward=323.04 +/- 40.12
Episode length: 42.68 +/- 3.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.7     |
|    mean_reward     | 323      |
| time/              |          |
|    total_timesteps | 267000   |
---------------------------------
Eval num_timesteps=267500, episode_reward=305.76 +/- 32.28
Episode length: 41.04 +/- 3.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41       |
|    mean_reward     | 306      |
| time/              |          |
|    total_timesteps | 267500   |
---------------------------------
Eval num_timesteps=268000, episode_reward=319.20 +/- 38.00
Episode length: 42.32 +/- 3.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 127      |
|    iterations      | 131      |
|    time_elapsed    | 2100     |
|    total_timesteps | 268288   |
---------------------------------
Eval num_timesteps=268500, episode_reward=451.68 +/- 205.45
Episode length: 55.56 +/- 20.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 55.6        |
|    mean_reward          | 452         |
| time/                   |             |
|    total_timesteps      | 268500      |
| train/                  |             |
|    approx_kl            | 0.004728585 |
|    clip_fraction        | 0.0866      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.468      |
|    explained_variance   | 0.0168      |
|    learning_rate        | 1e-05       |
|    loss                 | 1.3e+04     |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00358    |
|    value_loss           | 2.45e+04    |
-----------------------------------------
Eval num_timesteps=269000, episode_reward=427.36 +/- 212.09
Episode length: 53.16 +/- 21.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 269000   |
---------------------------------
Eval num_timesteps=269500, episode_reward=412.00 +/- 131.00
Episode length: 51.56 +/- 13.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.6     |
|    mean_reward     | 412      |
| time/              |          |
|    total_timesteps | 269500   |
---------------------------------
Eval num_timesteps=270000, episode_reward=435.68 +/- 207.17
Episode length: 53.96 +/- 20.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54       |
|    mean_reward     | 436      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    fps             | 127      |
|    iterations      | 132      |
|    time_elapsed    | 2113     |
|    total_timesteps | 270336   |
---------------------------------
Eval num_timesteps=270500, episode_reward=428.00 +/- 198.12
Episode length: 53.18 +/- 19.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.2         |
|    mean_reward          | 428          |
| time/                   |              |
|    total_timesteps      | 270500       |
| train/                  |              |
|    approx_kl            | 0.0076010996 |
|    clip_fraction        | 0.0761       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.492       |
|    explained_variance   | 0.00176      |
|    learning_rate        | 1e-05        |
|    loss                 | 1.37e+04     |
|    n_updates            | 1320         |
|    policy_gradient_loss | 0.0015       |
|    value_loss           | 2.47e+04     |
------------------------------------------
Eval num_timesteps=271000, episode_reward=451.04 +/- 169.85
Episode length: 55.42 +/- 17.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.4     |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 271000   |
---------------------------------
Eval num_timesteps=271500, episode_reward=432.48 +/- 139.60
Episode length: 53.68 +/- 13.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 271500   |
---------------------------------
Eval num_timesteps=272000, episode_reward=436.96 +/- 211.13
Episode length: 54.10 +/- 21.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 121      |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 128      |
|    iterations      | 133      |
|    time_elapsed    | 2126     |
|    total_timesteps | 272384   |
---------------------------------
Eval num_timesteps=272500, episode_reward=463.20 +/- 197.16
Episode length: 56.74 +/- 19.68
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 56.7         |
|    mean_reward          | 463          |
| time/                   |              |
|    total_timesteps      | 272500       |
| train/                  |              |
|    approx_kl            | 0.0053276494 |
|    clip_fraction        | 0.0589       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.494       |
|    explained_variance   | 0.0147       |
|    learning_rate        | 1e-05        |
|    loss                 | 8.53e+03     |
|    n_updates            | 1330         |
|    policy_gradient_loss | -0.00129     |
|    value_loss           | 1.72e+04     |
------------------------------------------
Eval num_timesteps=273000, episode_reward=450.40 +/- 179.77
Episode length: 55.42 +/- 18.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.4     |
|    mean_reward     | 450      |
| time/              |          |
|    total_timesteps | 273000   |
---------------------------------
Eval num_timesteps=273500, episode_reward=469.60 +/- 201.27
Episode length: 57.40 +/- 20.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.4     |
|    mean_reward     | 470      |
| time/              |          |
|    total_timesteps | 273500   |
---------------------------------
Eval num_timesteps=274000, episode_reward=509.92 +/- 262.20
Episode length: 61.38 +/- 26.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.4     |
|    mean_reward     | 510      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 123      |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    fps             | 128      |
|    iterations      | 134      |
|    time_elapsed    | 2140     |
|    total_timesteps | 274432   |
---------------------------------
Eval num_timesteps=274500, episode_reward=315.36 +/- 35.34
Episode length: 41.94 +/- 3.31
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 41.9        |
|    mean_reward          | 315         |
| time/                   |             |
|    total_timesteps      | 274500      |
| train/                  |             |
|    approx_kl            | 0.013058769 |
|    clip_fraction        | 0.0757      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.479      |
|    explained_variance   | 0.00938     |
|    learning_rate        | 1e-05       |
|    loss                 | 1.18e+04    |
|    n_updates            | 1340        |
|    policy_gradient_loss | 0.00388     |
|    value_loss           | 1.77e+04    |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=314.72 +/- 36.18
Episode length: 41.88 +/- 3.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 275000   |
---------------------------------
Eval num_timesteps=275500, episode_reward=315.36 +/- 39.71
Episode length: 41.96 +/- 3.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 275500   |
---------------------------------
Eval num_timesteps=276000, episode_reward=314.08 +/- 34.71
Episode length: 41.82 +/- 3.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 128      |
|    iterations      | 135      |
|    time_elapsed    | 2151     |
|    total_timesteps | 276480   |
---------------------------------
Eval num_timesteps=276500, episode_reward=312.16 +/- 33.03
Episode length: 41.64 +/- 3.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.6         |
|    mean_reward          | 312          |
| time/                   |              |
|    total_timesteps      | 276500       |
| train/                  |              |
|    approx_kl            | 0.0032770936 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.446       |
|    explained_variance   | 0.00791      |
|    learning_rate        | 1e-05        |
|    loss                 | 9.55e+03     |
|    n_updates            | 1350         |
|    policy_gradient_loss | 0.00212      |
|    value_loss           | 2.65e+04     |
------------------------------------------
Eval num_timesteps=277000, episode_reward=326.24 +/- 42.18
Episode length: 42.98 +/- 4.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43       |
|    mean_reward     | 326      |
| time/              |          |
|    total_timesteps | 277000   |
---------------------------------
Eval num_timesteps=277500, episode_reward=312.80 +/- 34.61
Episode length: 41.70 +/- 3.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.7     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 277500   |
---------------------------------
Eval num_timesteps=278000, episode_reward=310.88 +/- 34.68
Episode length: 41.52 +/- 3.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 311      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=278500, episode_reward=315.36 +/- 37.04
Episode length: 41.94 +/- 3.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 278500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 128      |
|    iterations      | 136      |
|    time_elapsed    | 2164     |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279000, episode_reward=307.68 +/- 31.23
Episode length: 41.22 +/- 2.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.2         |
|    mean_reward          | 308          |
| time/                   |              |
|    total_timesteps      | 279000       |
| train/                  |              |
|    approx_kl            | 0.0017639966 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.455       |
|    explained_variance   | 0.00951      |
|    learning_rate        | 1e-05        |
|    loss                 | 1.01e+04     |
|    n_updates            | 1360         |
|    policy_gradient_loss | 0.000889     |
|    value_loss           | 1.89e+04     |
------------------------------------------
Eval num_timesteps=279500, episode_reward=300.64 +/- 30.18
Episode length: 40.58 +/- 2.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.6     |
|    mean_reward     | 301      |
| time/              |          |
|    total_timesteps | 279500   |
---------------------------------
Eval num_timesteps=280000, episode_reward=317.92 +/- 38.08
Episode length: 42.18 +/- 3.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 318      |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=280500, episode_reward=321.12 +/- 45.87
Episode length: 42.52 +/- 4.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.5     |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 280500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 1.14e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 137      |
|    time_elapsed    | 2174     |
|    total_timesteps | 280576   |
---------------------------------
Eval num_timesteps=281000, episode_reward=317.28 +/- 42.43
Episode length: 42.14 +/- 4.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 42.1        |
|    mean_reward          | 317         |
| time/                   |             |
|    total_timesteps      | 281000      |
| train/                  |             |
|    approx_kl            | 0.006555423 |
|    clip_fraction        | 0.053       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.48       |
|    explained_variance   | 0.0153      |
|    learning_rate        | 1e-05       |
|    loss                 | 1.27e+04    |
|    n_updates            | 1370        |
|    policy_gradient_loss | -0.000421   |
|    value_loss           | 2.26e+04    |
-----------------------------------------
Eval num_timesteps=281500, episode_reward=312.16 +/- 36.00
Episode length: 41.64 +/- 3.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.6     |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 281500   |
---------------------------------
Eval num_timesteps=282000, episode_reward=318.56 +/- 36.11
Episode length: 42.26 +/- 3.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.3     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=282500, episode_reward=319.84 +/- 37.12
Episode length: 42.36 +/- 3.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 320      |
| time/              |          |
|    total_timesteps | 282500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 119      |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 138      |
|    time_elapsed    | 2185     |
|    total_timesteps | 282624   |
---------------------------------
Eval num_timesteps=283000, episode_reward=333.92 +/- 60.09
Episode length: 43.78 +/- 5.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 43.8        |
|    mean_reward          | 334         |
| time/                   |             |
|    total_timesteps      | 283000      |
| train/                  |             |
|    approx_kl            | 0.008394857 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.434      |
|    explained_variance   | 0.0143      |
|    learning_rate        | 1e-05       |
|    loss                 | 1.41e+04    |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00229    |
|    value_loss           | 2.35e+04    |
-----------------------------------------
Eval num_timesteps=283500, episode_reward=332.64 +/- 67.50
Episode length: 43.66 +/- 6.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 43.7     |
|    mean_reward     | 333      |
| time/              |          |
|    total_timesteps | 283500   |
---------------------------------
Eval num_timesteps=284000, episode_reward=339.04 +/- 55.07
Episode length: 44.28 +/- 5.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.3     |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=284500, episode_reward=337.12 +/- 65.77
Episode length: 44.10 +/- 6.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 44.1     |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 284500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    fps             | 129      |
|    iterations      | 139      |
|    time_elapsed    | 2196     |
|    total_timesteps | 284672   |
---------------------------------
Eval num_timesteps=285000, episode_reward=383.84 +/- 116.20
Episode length: 48.76 +/- 11.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 48.8         |
|    mean_reward          | 384          |
| time/                   |              |
|    total_timesteps      | 285000       |
| train/                  |              |
|    approx_kl            | 0.0076060845 |
|    clip_fraction        | 0.0598       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.465       |
|    explained_variance   | 0.00399      |
|    learning_rate        | 1e-05        |
|    loss                 | 1.47e+04     |
|    n_updates            | 1390         |
|    policy_gradient_loss | 0.00165      |
|    value_loss           | 2.05e+04     |
------------------------------------------
Eval num_timesteps=285500, episode_reward=422.88 +/- 155.14
Episode length: 52.68 +/- 15.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.7     |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 285500   |
---------------------------------
Eval num_timesteps=286000, episode_reward=436.96 +/- 173.54
Episode length: 54.12 +/- 17.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=286500, episode_reward=436.96 +/- 163.83
Episode length: 54.10 +/- 16.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 286500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 140      |
|    time_elapsed    | 2209     |
|    total_timesteps | 286720   |
---------------------------------
Eval num_timesteps=287000, episode_reward=455.52 +/- 269.39
Episode length: 56.04 +/- 26.90
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 56           |
|    mean_reward          | 456          |
| time/                   |              |
|    total_timesteps      | 287000       |
| train/                  |              |
|    approx_kl            | 0.0066286903 |
|    clip_fraction        | 0.0766       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.442       |
|    explained_variance   | 0.00879      |
|    learning_rate        | 1e-05        |
|    loss                 | 4.43e+03     |
|    n_updates            | 1400         |
|    policy_gradient_loss | 0.0041       |
|    value_loss           | 1.2e+04      |
------------------------------------------
Eval num_timesteps=287500, episode_reward=479.84 +/- 197.33
Episode length: 58.34 +/- 19.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.3     |
|    mean_reward     | 480      |
| time/              |          |
|    total_timesteps | 287500   |
---------------------------------
Eval num_timesteps=288000, episode_reward=494.56 +/- 218.27
Episode length: 59.88 +/- 21.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.9     |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=288500, episode_reward=474.08 +/- 202.73
Episode length: 57.84 +/- 20.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.8     |
|    mean_reward     | 474      |
| time/              |          |
|    total_timesteps | 288500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 126      |
|    ep_rew_mean     | 1.2e+03  |
| time/              |          |
|    fps             | 129      |
|    iterations      | 141      |
|    time_elapsed    | 2223     |
|    total_timesteps | 288768   |
---------------------------------
Eval num_timesteps=289000, episode_reward=526.56 +/- 270.02
Episode length: 63.04 +/- 27.01
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 63           |
|    mean_reward          | 527          |
| time/                   |              |
|    total_timesteps      | 289000       |
| train/                  |              |
|    approx_kl            | 0.0010034973 |
|    clip_fraction        | 0.0674       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.472       |
|    explained_variance   | 0.0344       |
|    learning_rate        | 1e-05        |
|    loss                 | 8.77e+03     |
|    n_updates            | 1410         |
|    policy_gradient_loss | 0.0024       |
|    value_loss           | 1.77e+04     |
------------------------------------------
Eval num_timesteps=289500, episode_reward=476.00 +/- 239.72
Episode length: 58.00 +/- 23.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58       |
|    mean_reward     | 476      |
| time/              |          |
|    total_timesteps | 289500   |
---------------------------------
Eval num_timesteps=290000, episode_reward=449.76 +/- 199.50
Episode length: 55.36 +/- 19.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.4     |
|    mean_reward     | 450      |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=290500, episode_reward=564.00 +/- 437.43
Episode length: 66.44 +/- 42.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.4     |
|    mean_reward     | 564      |
| time/              |          |
|    total_timesteps | 290500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | 1.24e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 142      |
|    time_elapsed    | 2238     |
|    total_timesteps | 290816   |
---------------------------------
Eval num_timesteps=291000, episode_reward=626.72 +/- 474.68
Episode length: 72.62 +/- 46.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.6        |
|    mean_reward          | 627         |
| time/                   |             |
|    total_timesteps      | 291000      |
| train/                  |             |
|    approx_kl            | 0.003675276 |
|    clip_fraction        | 0.0655      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.47       |
|    explained_variance   | 0.00228     |
|    learning_rate        | 1e-05       |
|    loss                 | 7.91e+03    |
|    n_updates            | 1420        |
|    policy_gradient_loss | 0.00152     |
|    value_loss           | 2.07e+04    |
-----------------------------------------
Eval num_timesteps=291500, episode_reward=605.76 +/- 396.84
Episode length: 70.74 +/- 38.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.7     |
|    mean_reward     | 606      |
| time/              |          |
|    total_timesteps | 291500   |
---------------------------------
Eval num_timesteps=292000, episode_reward=620.64 +/- 496.60
Episode length: 71.62 +/- 47.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.6     |
|    mean_reward     | 621      |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=292500, episode_reward=623.68 +/- 487.55
Episode length: 72.52 +/- 48.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.5     |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 292500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 143      |
|    time_elapsed    | 2254     |
|    total_timesteps | 292864   |
---------------------------------
Eval num_timesteps=293000, episode_reward=625.76 +/- 394.60
Episode length: 72.98 +/- 39.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 73          |
|    mean_reward          | 626         |
| time/                   |             |
|    total_timesteps      | 293000      |
| train/                  |             |
|    approx_kl            | 0.003283327 |
|    clip_fraction        | 0.0591      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.515      |
|    explained_variance   | 0.00482     |
|    learning_rate        | 1e-05       |
|    loss                 | 1.19e+04    |
|    n_updates            | 1430        |
|    policy_gradient_loss | 0.00195     |
|    value_loss           | 1.59e+04    |
-----------------------------------------
Eval num_timesteps=293500, episode_reward=493.60 +/- 390.30
Episode length: 59.34 +/- 37.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.3     |
|    mean_reward     | 494      |
| time/              |          |
|    total_timesteps | 293500   |
---------------------------------
Eval num_timesteps=294000, episode_reward=552.80 +/- 341.73
Episode length: 65.68 +/- 34.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.7     |
|    mean_reward     | 553      |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=294500, episode_reward=522.08 +/- 340.35
Episode length: 62.58 +/- 33.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.6     |
|    mean_reward     | 522      |
| time/              |          |
|    total_timesteps | 294500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 1.36e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 144      |
|    time_elapsed    | 2269     |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=295000, episode_reward=485.60 +/- 236.99
Episode length: 59.00 +/- 23.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 59          |
|    mean_reward          | 486         |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.006493861 |
|    clip_fraction        | 0.0747      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.489      |
|    explained_variance   | 0.0147      |
|    learning_rate        | 1e-05       |
|    loss                 | 5.91e+03    |
|    n_updates            | 1440        |
|    policy_gradient_loss | 0.00197     |
|    value_loss           | 1.72e+04    |
-----------------------------------------
Eval num_timesteps=295500, episode_reward=474.08 +/- 230.44
Episode length: 57.84 +/- 23.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.8     |
|    mean_reward     | 474      |
| time/              |          |
|    total_timesteps | 295500   |
---------------------------------
Eval num_timesteps=296000, episode_reward=462.56 +/- 174.93
Episode length: 56.64 +/- 17.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.6     |
|    mean_reward     | 463      |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=296500, episode_reward=437.60 +/- 167.01
Episode length: 54.12 +/- 16.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.1     |
|    mean_reward     | 438      |
| time/              |          |
|    total_timesteps | 296500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 145      |
|    time_elapsed    | 2283     |
|    total_timesteps | 296960   |
---------------------------------
Eval num_timesteps=297000, episode_reward=472.80 +/- 216.77
Episode length: 57.70 +/- 21.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 57.7      |
|    mean_reward          | 473       |
| time/                   |           |
|    total_timesteps      | 297000    |
| train/                  |           |
|    approx_kl            | 0.0035802 |
|    clip_fraction        | 0.0463    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.483    |
|    explained_variance   | 0.0271    |
|    learning_rate        | 1e-05     |
|    loss                 | 1.68e+04  |
|    n_updates            | 1450      |
|    policy_gradient_loss | 0.00219   |
|    value_loss           | 2.32e+04  |
---------------------------------------
Eval num_timesteps=297500, episode_reward=447.84 +/- 181.43
Episode length: 55.20 +/- 18.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | 448      |
| time/              |          |
|    total_timesteps | 297500   |
---------------------------------
Eval num_timesteps=298000, episode_reward=478.56 +/- 247.36
Episode length: 58.30 +/- 24.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.3     |
|    mean_reward     | 479      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=298500, episode_reward=440.16 +/- 167.70
Episode length: 54.38 +/- 16.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.4     |
|    mean_reward     | 440      |
| time/              |          |
|    total_timesteps | 298500   |
---------------------------------
Eval num_timesteps=299000, episode_reward=428.00 +/- 153.63
Episode length: 53.26 +/- 15.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.3     |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 1.46e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 146      |
|    time_elapsed    | 2299     |
|    total_timesteps | 299008   |
---------------------------------
Eval num_timesteps=299500, episode_reward=308.96 +/- 35.80
Episode length: 41.34 +/- 3.36
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.3         |
|    mean_reward          | 309          |
| time/                   |              |
|    total_timesteps      | 299500       |
| train/                  |              |
|    approx_kl            | 0.0029519119 |
|    clip_fraction        | 0.0588       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.447       |
|    explained_variance   | 0.0249       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.13e+04     |
|    n_updates            | 1460         |
|    policy_gradient_loss | 0.00138      |
|    value_loss           | 2.23e+04     |
------------------------------------------
Eval num_timesteps=300000, episode_reward=314.08 +/- 38.08
Episode length: 41.82 +/- 3.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=300500, episode_reward=310.24 +/- 36.03
Episode length: 41.48 +/- 3.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.5     |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 300500   |
---------------------------------
Eval num_timesteps=301000, episode_reward=316.00 +/- 36.77
Episode length: 42.00 +/- 3.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42       |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 145      |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 130      |
|    iterations      | 147      |
|    time_elapsed    | 2310     |
|    total_timesteps | 301056   |
---------------------------------
Eval num_timesteps=301500, episode_reward=406.88 +/- 167.06
Episode length: 51.08 +/- 16.65
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 51.1         |
|    mean_reward          | 407          |
| time/                   |              |
|    total_timesteps      | 301500       |
| train/                  |              |
|    approx_kl            | 0.0017433236 |
|    clip_fraction        | 0.0462       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.459       |
|    explained_variance   | 0.0127       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.11e+04     |
|    n_updates            | 1470         |
|    policy_gradient_loss | 0.000631     |
|    value_loss           | 2.29e+04     |
------------------------------------------
Eval num_timesteps=302000, episode_reward=448.48 +/- 139.05
Episode length: 55.20 +/- 13.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.2     |
|    mean_reward     | 448      |
| time/              |          |
|    total_timesteps | 302000   |
---------------------------------
Eval num_timesteps=302500, episode_reward=401.12 +/- 127.46
Episode length: 50.52 +/- 12.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.5     |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 302500   |
---------------------------------
Eval num_timesteps=303000, episode_reward=426.72 +/- 192.34
Episode length: 53.04 +/- 19.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53       |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 148      |
|    time_elapsed    | 2323     |
|    total_timesteps | 303104   |
---------------------------------
Eval num_timesteps=303500, episode_reward=462.56 +/- 205.01
Episode length: 56.68 +/- 20.47
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 56.7         |
|    mean_reward          | 463          |
| time/                   |              |
|    total_timesteps      | 303500       |
| train/                  |              |
|    approx_kl            | 0.0029299187 |
|    clip_fraction        | 0.0732       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.461       |
|    explained_variance   | 0.0226       |
|    learning_rate        | 1e-05        |
|    loss                 | 7e+03        |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.000187    |
|    value_loss           | 2.04e+04     |
------------------------------------------
Eval num_timesteps=304000, episode_reward=489.44 +/- 188.24
Episode length: 59.34 +/- 18.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.3     |
|    mean_reward     | 489      |
| time/              |          |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=304500, episode_reward=438.88 +/- 245.49
Episode length: 54.32 +/- 24.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.3     |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 304500   |
---------------------------------
Eval num_timesteps=305000, episode_reward=453.60 +/- 218.00
Episode length: 55.78 +/- 21.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 144      |
|    ep_rew_mean     | 1.39e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 149      |
|    time_elapsed    | 2336     |
|    total_timesteps | 305152   |
---------------------------------
Eval num_timesteps=305500, episode_reward=463.84 +/- 202.48
Episode length: 56.74 +/- 20.28
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 56.7         |
|    mean_reward          | 464          |
| time/                   |              |
|    total_timesteps      | 305500       |
| train/                  |              |
|    approx_kl            | 0.0005785065 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.465       |
|    explained_variance   | 0.0261       |
|    learning_rate        | 1e-05        |
|    loss                 | 6.1e+03      |
|    n_updates            | 1490         |
|    policy_gradient_loss | 0.000842     |
|    value_loss           | 1.91e+04     |
------------------------------------------
Eval num_timesteps=306000, episode_reward=580.96 +/- 287.29
Episode length: 68.46 +/- 28.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.5     |
|    mean_reward     | 581      |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
Eval num_timesteps=306500, episode_reward=459.36 +/- 235.95
Episode length: 56.28 +/- 23.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.3     |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 306500   |
---------------------------------
Eval num_timesteps=307000, episode_reward=468.32 +/- 188.07
Episode length: 57.26 +/- 18.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.3     |
|    mean_reward     | 468      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 150      |
|    time_elapsed    | 2351     |
|    total_timesteps | 307200   |
---------------------------------
Eval num_timesteps=307500, episode_reward=300.64 +/- 26.57
Episode length: 40.56 +/- 2.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.6        |
|    mean_reward          | 301         |
| time/                   |             |
|    total_timesteps      | 307500      |
| train/                  |             |
|    approx_kl            | 0.003685952 |
|    clip_fraction        | 0.0977      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.438      |
|    explained_variance   | 0.00171     |
|    learning_rate        | 1e-05       |
|    loss                 | 6.83e+03    |
|    n_updates            | 1500        |
|    policy_gradient_loss | -0.00271    |
|    value_loss           | 2.27e+04    |
-----------------------------------------
Eval num_timesteps=308000, episode_reward=307.68 +/- 34.35
Episode length: 41.22 +/- 3.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=308500, episode_reward=317.28 +/- 39.43
Episode length: 42.16 +/- 3.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.2     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 308500   |
---------------------------------
Eval num_timesteps=309000, episode_reward=315.36 +/- 37.04
Episode length: 41.94 +/- 3.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.9     |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 130      |
|    iterations      | 151      |
|    time_elapsed    | 2361     |
|    total_timesteps | 309248   |
---------------------------------
Eval num_timesteps=309500, episode_reward=413.28 +/- 151.72
Episode length: 51.80 +/- 15.18
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 51.8        |
|    mean_reward          | 413         |
| time/                   |             |
|    total_timesteps      | 309500      |
| train/                  |             |
|    approx_kl            | 0.002172256 |
|    clip_fraction        | 0.0534      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.441      |
|    explained_variance   | 0.013       |
|    learning_rate        | 1e-05       |
|    loss                 | 8.54e+03    |
|    n_updates            | 1510        |
|    policy_gradient_loss | 0.000433    |
|    value_loss           | 1.86e+04    |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=384.48 +/- 151.59
Episode length: 48.92 +/- 15.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.9     |
|    mean_reward     | 384      |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
Eval num_timesteps=310500, episode_reward=373.60 +/- 110.85
Episode length: 47.82 +/- 11.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.8     |
|    mean_reward     | 374      |
| time/              |          |
|    total_timesteps | 310500   |
---------------------------------
Eval num_timesteps=311000, episode_reward=429.28 +/- 152.83
Episode length: 53.36 +/- 15.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.4     |
|    mean_reward     | 429      |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 152      |
|    time_elapsed    | 2373     |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=311500, episode_reward=416.48 +/- 164.54
Episode length: 52.02 +/- 16.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52           |
|    mean_reward          | 416          |
| time/                   |              |
|    total_timesteps      | 311500       |
| train/                  |              |
|    approx_kl            | 0.0066193943 |
|    clip_fraction        | 0.0413       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.455       |
|    explained_variance   | 0.0208       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.28e+04     |
|    n_updates            | 1520         |
|    policy_gradient_loss | 0.000959     |
|    value_loss           | 2.89e+04     |
------------------------------------------
Eval num_timesteps=312000, episode_reward=407.52 +/- 158.97
Episode length: 51.20 +/- 15.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.2     |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=312500, episode_reward=441.44 +/- 190.48
Episode length: 54.52 +/- 19.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.5     |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 312500   |
---------------------------------
Eval num_timesteps=313000, episode_reward=380.64 +/- 106.61
Episode length: 48.46 +/- 10.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 48.5     |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 127      |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 153      |
|    time_elapsed    | 2386     |
|    total_timesteps | 313344   |
---------------------------------
Eval num_timesteps=313500, episode_reward=399.84 +/- 162.40
Episode length: 50.42 +/- 16.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 50.4        |
|    mean_reward          | 400         |
| time/                   |             |
|    total_timesteps      | 313500      |
| train/                  |             |
|    approx_kl            | 0.008301757 |
|    clip_fraction        | 0.0979      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.451      |
|    explained_variance   | 0.0228      |
|    learning_rate        | 1e-05       |
|    loss                 | 8.71e+03    |
|    n_updates            | 1530        |
|    policy_gradient_loss | 0.0017      |
|    value_loss           | 2.14e+04    |
-----------------------------------------
Eval num_timesteps=314000, episode_reward=369.12 +/- 110.41
Episode length: 47.32 +/- 10.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 47.3     |
|    mean_reward     | 369      |
| time/              |          |
|    total_timesteps | 314000   |
---------------------------------
Eval num_timesteps=314500, episode_reward=402.40 +/- 132.29
Episode length: 50.56 +/- 13.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 50.6     |
|    mean_reward     | 402      |
| time/              |          |
|    total_timesteps | 314500   |
---------------------------------
Eval num_timesteps=315000, episode_reward=411.36 +/- 120.84
Episode length: 51.54 +/- 12.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.5     |
|    mean_reward     | 411      |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | 1.13e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 154      |
|    time_elapsed    | 2398     |
|    total_timesteps | 315392   |
---------------------------------
Eval num_timesteps=315500, episode_reward=312.80 +/- 34.02
Episode length: 41.70 +/- 3.19
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 41.7         |
|    mean_reward          | 313          |
| time/                   |              |
|    total_timesteps      | 315500       |
| train/                  |              |
|    approx_kl            | 0.0036707567 |
|    clip_fraction        | 0.0529       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.448       |
|    explained_variance   | 0.00472      |
|    learning_rate        | 1e-05        |
|    loss                 | 6.56e+03     |
|    n_updates            | 1540         |
|    policy_gradient_loss | 0.000535     |
|    value_loss           | 2.46e+04     |
------------------------------------------
Eval num_timesteps=316000, episode_reward=313.44 +/- 34.37
Episode length: 41.76 +/- 3.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=316500, episode_reward=319.20 +/- 44.91
Episode length: 42.38 +/- 4.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.4     |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 316500   |
---------------------------------
Eval num_timesteps=317000, episode_reward=317.28 +/- 40.46
Episode length: 42.14 +/- 3.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 42.1     |
|    mean_reward     | 317      |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 122      |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 155      |
|    time_elapsed    | 2409     |
|    total_timesteps | 317440   |
---------------------------------
Eval num_timesteps=317500, episode_reward=316.64 +/- 35.91
Episode length: 42.06 +/- 3.37
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 42.1         |
|    mean_reward          | 317          |
| time/                   |              |
|    total_timesteps      | 317500       |
| train/                  |              |
|    approx_kl            | 0.0018361821 |
|    clip_fraction        | 0.046        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.426       |
|    explained_variance   | 0.0173       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.13e+04     |
|    n_updates            | 1550         |
|    policy_gradient_loss | 0.00151      |
|    value_loss           | 1.67e+04     |
------------------------------------------
Eval num_timesteps=318000, episode_reward=313.44 +/- 34.37
Episode length: 41.76 +/- 3.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.8     |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=318500, episode_reward=303.84 +/- 32.56
Episode length: 40.86 +/- 3.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 40.9     |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 318500   |
---------------------------------
Eval num_timesteps=319000, episode_reward=307.04 +/- 33.28
Episode length: 41.16 +/- 3.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 41.2     |
|    mean_reward     | 307      |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 116      |
|    ep_rew_mean     | 1.09e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 156      |
|    time_elapsed    | 2420     |
|    total_timesteps | 319488   |
---------------------------------
Eval num_timesteps=319500, episode_reward=457.44 +/- 205.81
Episode length: 56.18 +/- 20.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.2        |
|    mean_reward          | 457         |
| time/                   |             |
|    total_timesteps      | 319500      |
| train/                  |             |
|    approx_kl            | 0.005026302 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.457      |
|    explained_variance   | 0.0302      |
|    learning_rate        | 1e-05       |
|    loss                 | 8.36e+03    |
|    n_updates            | 1560        |
|    policy_gradient_loss | -0.00108    |
|    value_loss           | 2.49e+04    |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=447.20 +/- 210.64
Episode length: 55.14 +/- 21.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.1     |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=320500, episode_reward=435.04 +/- 137.57
Episode length: 53.94 +/- 13.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | 435      |
| time/              |          |
|    total_timesteps | 320500   |
---------------------------------
Eval num_timesteps=321000, episode_reward=422.24 +/- 181.86
Episode length: 52.58 +/- 18.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=321500, episode_reward=476.64 +/- 234.50
Episode length: 58.04 +/- 23.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58       |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 321500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 117      |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    fps             | 131      |
|    iterations      | 157      |
|    time_elapsed    | 2436     |
|    total_timesteps | 321536   |
---------------------------------
Eval num_timesteps=322000, episode_reward=566.08 +/- 329.63
Episode length: 66.78 +/- 32.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 66.8         |
|    mean_reward          | 566          |
| time/                   |              |
|    total_timesteps      | 322000       |
| train/                  |              |
|    approx_kl            | 0.0031212778 |
|    clip_fraction        | 0.0424       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.469       |
|    explained_variance   | 0.0105       |
|    learning_rate        | 1e-05        |
|    loss                 | 5.95e+03     |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.000988    |
|    value_loss           | 1.74e+04     |
------------------------------------------
Eval num_timesteps=322500, episode_reward=452.96 +/- 194.65
Episode length: 55.72 +/- 19.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.7     |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 322500   |
---------------------------------
Eval num_timesteps=323000, episode_reward=516.96 +/- 286.36
Episode length: 62.08 +/- 28.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.1     |
|    mean_reward     | 517      |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=323500, episode_reward=481.12 +/- 228.92
Episode length: 58.50 +/- 22.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.5     |
|    mean_reward     | 481      |
| time/              |          |
|    total_timesteps | 323500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 125      |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    fps             | 132      |
|    iterations      | 158      |
|    time_elapsed    | 2450     |
|    total_timesteps | 323584   |
---------------------------------
Eval num_timesteps=324000, episode_reward=1180.96 +/- 667.87
Episode length: 126.40 +/- 63.99
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 126          |
|    mean_reward          | 1.18e+03     |
| time/                   |              |
|    total_timesteps      | 324000       |
| train/                  |              |
|    approx_kl            | 0.0068480414 |
|    clip_fraction        | 0.0863       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.461       |
|    explained_variance   | 0.00926      |
|    learning_rate        | 1e-05        |
|    loss                 | 5.27e+03     |
|    n_updates            | 1580         |
|    policy_gradient_loss | 0.00195      |
|    value_loss           | 1.67e+04     |
------------------------------------------
Eval num_timesteps=324500, episode_reward=1151.52 +/- 613.65
Episode length: 124.24 +/- 59.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 124      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 324500   |
---------------------------------
Eval num_timesteps=325000, episode_reward=922.56 +/- 618.83
Episode length: 101.68 +/- 59.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 102      |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=325500, episode_reward=1020.16 +/- 690.06
Episode length: 111.00 +/- 66.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 111      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 325500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 132      |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 159      |
|    time_elapsed    | 2475     |
|    total_timesteps | 325632   |
---------------------------------
Eval num_timesteps=326000, episode_reward=452.32 +/- 175.25
Episode length: 55.60 +/- 17.56
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 55.6         |
|    mean_reward          | 452          |
| time/                   |              |
|    total_timesteps      | 326000       |
| train/                  |              |
|    approx_kl            | 0.0063803317 |
|    clip_fraction        | 0.115        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.474       |
|    explained_variance   | 0.0151       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.09e+04     |
|    n_updates            | 1590         |
|    policy_gradient_loss | 0.00311      |
|    value_loss           | 1.48e+04     |
------------------------------------------
Eval num_timesteps=326500, episode_reward=453.60 +/- 236.99
Episode length: 55.74 +/- 23.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.7     |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 326500   |
---------------------------------
Eval num_timesteps=327000, episode_reward=467.04 +/- 200.25
Episode length: 57.06 +/- 19.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.1     |
|    mean_reward     | 467      |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=327500, episode_reward=486.88 +/- 232.31
Episode length: 59.06 +/- 23.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.1     |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 327500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 135      |
|    ep_rew_mean     | 1.29e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 160      |
|    time_elapsed    | 2488     |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=328000, episode_reward=509.28 +/- 219.75
Episode length: 61.30 +/- 22.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 61.3        |
|    mean_reward          | 509         |
| time/                   |             |
|    total_timesteps      | 328000      |
| train/                  |             |
|    approx_kl            | 0.003139888 |
|    clip_fraction        | 0.0175      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.462      |
|    explained_variance   | 0.00539     |
|    learning_rate        | 1e-05       |
|    loss                 | 1.02e+04    |
|    n_updates            | 1600        |
|    policy_gradient_loss | 0.000411    |
|    value_loss           | 2.5e+04     |
-----------------------------------------
Eval num_timesteps=328500, episode_reward=449.76 +/- 145.34
Episode length: 55.46 +/- 14.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.5     |
|    mean_reward     | 450      |
| time/              |          |
|    total_timesteps | 328500   |
---------------------------------
Eval num_timesteps=329000, episode_reward=514.40 +/- 265.58
Episode length: 61.82 +/- 26.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.8     |
|    mean_reward     | 514      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=329500, episode_reward=480.48 +/- 182.04
Episode length: 58.42 +/- 18.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.4     |
|    mean_reward     | 480      |
| time/              |          |
|    total_timesteps | 329500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 161      |
|    time_elapsed    | 2502     |
|    total_timesteps | 329728   |
---------------------------------
Eval num_timesteps=330000, episode_reward=493.92 +/- 210.34
Episode length: 59.80 +/- 21.01
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 59.8        |
|    mean_reward          | 494         |
| time/                   |             |
|    total_timesteps      | 330000      |
| train/                  |             |
|    approx_kl            | 0.004102587 |
|    clip_fraction        | 0.0544      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.465      |
|    explained_variance   | 0.015       |
|    learning_rate        | 1e-05       |
|    loss                 | 9.97e+03    |
|    n_updates            | 1610        |
|    policy_gradient_loss | -0.000744   |
|    value_loss           | 2.33e+04    |
-----------------------------------------
Eval num_timesteps=330500, episode_reward=449.12 +/- 183.75
Episode length: 55.26 +/- 18.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.3     |
|    mean_reward     | 449      |
| time/              |          |
|    total_timesteps | 330500   |
---------------------------------
Eval num_timesteps=331000, episode_reward=517.60 +/- 279.72
Episode length: 62.16 +/- 28.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.2     |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=331500, episode_reward=472.80 +/- 192.77
Episode length: 57.66 +/- 19.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.7     |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 331500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 162      |
|    time_elapsed    | 2516     |
|    total_timesteps | 331776   |
---------------------------------
Eval num_timesteps=332000, episode_reward=417.12 +/- 145.29
Episode length: 52.14 +/- 14.48
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 52.1         |
|    mean_reward          | 417          |
| time/                   |              |
|    total_timesteps      | 332000       |
| train/                  |              |
|    approx_kl            | 0.0017415333 |
|    clip_fraction        | 0.0865       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.412       |
|    explained_variance   | -0.00104     |
|    learning_rate        | 1e-05        |
|    loss                 | 7.09e+03     |
|    n_updates            | 1620         |
|    policy_gradient_loss | 0.00125      |
|    value_loss           | 1.93e+04     |
------------------------------------------
Eval num_timesteps=332500, episode_reward=428.00 +/- 159.26
Episode length: 53.22 +/- 15.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.2     |
|    mean_reward     | 428      |
| time/              |          |
|    total_timesteps | 332500   |
---------------------------------
Eval num_timesteps=333000, episode_reward=459.36 +/- 213.83
Episode length: 56.36 +/- 21.41
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.4     |
|    mean_reward     | 459      |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=333500, episode_reward=454.88 +/- 229.74
Episode length: 55.88 +/- 22.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.9     |
|    mean_reward     | 455      |
| time/              |          |
|    total_timesteps | 333500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 163      |
|    time_elapsed    | 2529     |
|    total_timesteps | 333824   |
---------------------------------
Eval num_timesteps=334000, episode_reward=498.40 +/- 260.70
Episode length: 60.20 +/- 26.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 60.2        |
|    mean_reward          | 498         |
| time/                   |             |
|    total_timesteps      | 334000      |
| train/                  |             |
|    approx_kl            | 0.006036952 |
|    clip_fraction        | 0.0497      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.409      |
|    explained_variance   | 0.0147      |
|    learning_rate        | 1e-05       |
|    loss                 | 4.19e+03    |
|    n_updates            | 1630        |
|    policy_gradient_loss | 0.00128     |
|    value_loss           | 1.71e+04    |
-----------------------------------------
Eval num_timesteps=334500, episode_reward=519.52 +/- 251.62
Episode length: 62.40 +/- 25.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.4     |
|    mean_reward     | 520      |
| time/              |          |
|    total_timesteps | 334500   |
---------------------------------
Eval num_timesteps=335000, episode_reward=516.16 +/- 336.62
Episode length: 61.76 +/- 32.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.8     |
|    mean_reward     | 516      |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=335500, episode_reward=507.36 +/- 198.14
Episode length: 61.14 +/- 19.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.1     |
|    mean_reward     | 507      |
| time/              |          |
|    total_timesteps | 335500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 1.33e+03 |
| time/              |          |
|    fps             | 132      |
|    iterations      | 164      |
|    time_elapsed    | 2543     |
|    total_timesteps | 335872   |
---------------------------------
Eval num_timesteps=336000, episode_reward=604.80 +/- 437.54
Episode length: 70.18 +/- 41.71
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 70.2         |
|    mean_reward          | 605          |
| time/                   |              |
|    total_timesteps      | 336000       |
| train/                  |              |
|    approx_kl            | 0.0053795977 |
|    clip_fraction        | 0.0852       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.457       |
|    explained_variance   | 0.0245       |
|    learning_rate        | 1e-05        |
|    loss                 | 6.31e+03     |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.000346    |
|    value_loss           | 2.01e+04     |
------------------------------------------
Eval num_timesteps=336500, episode_reward=600.64 +/- 425.32
Episode length: 70.26 +/- 41.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.3     |
|    mean_reward     | 601      |
| time/              |          |
|    total_timesteps | 336500   |
---------------------------------
Eval num_timesteps=337000, episode_reward=547.68 +/- 307.77
Episode length: 65.16 +/- 30.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.2     |
|    mean_reward     | 548      |
| time/              |          |
|    total_timesteps | 337000   |
---------------------------------
Eval num_timesteps=337500, episode_reward=596.32 +/- 333.63
Episode length: 70.08 +/- 33.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.1     |
|    mean_reward     | 596      |
| time/              |          |
|    total_timesteps | 337500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 132      |
|    iterations      | 165      |
|    time_elapsed    | 2559     |
|    total_timesteps | 337920   |
---------------------------------
Eval num_timesteps=338000, episode_reward=482.40 +/- 267.73
Episode length: 58.72 +/- 26.77
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 58.7         |
|    mean_reward          | 482          |
| time/                   |              |
|    total_timesteps      | 338000       |
| train/                  |              |
|    approx_kl            | 0.0033554146 |
|    clip_fraction        | 0.061        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.443       |
|    explained_variance   | 0.00983      |
|    learning_rate        | 1e-05        |
|    loss                 | 4.27e+03     |
|    n_updates            | 1650         |
|    policy_gradient_loss | 0.00228      |
|    value_loss           | 1.58e+04     |
------------------------------------------
Eval num_timesteps=338500, episode_reward=522.08 +/- 255.13
Episode length: 62.58 +/- 25.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.6     |
|    mean_reward     | 522      |
| time/              |          |
|    total_timesteps | 338500   |
---------------------------------
Eval num_timesteps=339000, episode_reward=478.56 +/- 227.16
Episode length: 58.28 +/- 22.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.3     |
|    mean_reward     | 479      |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=339500, episode_reward=454.24 +/- 202.43
Episode length: 55.80 +/- 20.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 339500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 134      |
|    ep_rew_mean     | 1.28e+03 |
| time/              |          |
|    fps             | 132      |
|    iterations      | 166      |
|    time_elapsed    | 2573     |
|    total_timesteps | 339968   |
---------------------------------
Eval num_timesteps=340000, episode_reward=510.56 +/- 222.24
Episode length: 61.42 +/- 22.26
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 61.4       |
|    mean_reward          | 511        |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.00578636 |
|    clip_fraction        | 0.051      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.467     |
|    explained_variance   | 0.0138     |
|    learning_rate        | 1e-05      |
|    loss                 | 8.32e+03   |
|    n_updates            | 1660       |
|    policy_gradient_loss | -0.000991  |
|    value_loss           | 2.04e+04   |
----------------------------------------
Eval num_timesteps=340500, episode_reward=435.04 +/- 234.37
Episode length: 53.88 +/- 23.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.9     |
|    mean_reward     | 435      |
| time/              |          |
|    total_timesteps | 340500   |
---------------------------------
Eval num_timesteps=341000, episode_reward=477.92 +/- 253.22
Episode length: 58.08 +/- 25.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.1     |
|    mean_reward     | 478      |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=341500, episode_reward=477.28 +/- 183.49
Episode length: 58.14 +/- 18.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.1     |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 341500   |
---------------------------------
Eval num_timesteps=342000, episode_reward=478.56 +/- 210.02
Episode length: 58.24 +/- 21.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.2     |
|    mean_reward     | 479      |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 132      |
|    iterations      | 167      |
|    time_elapsed    | 2589     |
|    total_timesteps | 342016   |
---------------------------------
Eval num_timesteps=342500, episode_reward=461.92 +/- 237.68
Episode length: 56.58 +/- 23.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.6        |
|    mean_reward          | 462         |
| time/                   |             |
|    total_timesteps      | 342500      |
| train/                  |             |
|    approx_kl            | 0.008245529 |
|    clip_fraction        | 0.0629      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.419      |
|    explained_variance   | 0.0115      |
|    learning_rate        | 1e-05       |
|    loss                 | 9.82e+03    |
|    n_updates            | 1670        |
|    policy_gradient_loss | 0.00129     |
|    value_loss           | 2.13e+04    |
-----------------------------------------
Eval num_timesteps=343000, episode_reward=419.68 +/- 133.57
Episode length: 52.32 +/- 13.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.3     |
|    mean_reward     | 420      |
| time/              |          |
|    total_timesteps | 343000   |
---------------------------------
Eval num_timesteps=343500, episode_reward=502.88 +/- 262.58
Episode length: 60.68 +/- 26.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.7     |
|    mean_reward     | 503      |
| time/              |          |
|    total_timesteps | 343500   |
---------------------------------
Eval num_timesteps=344000, episode_reward=457.44 +/- 178.98
Episode length: 56.08 +/- 17.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.1     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 132      |
|    iterations      | 168      |
|    time_elapsed    | 2603     |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=344500, episode_reward=477.28 +/- 231.64
Episode length: 58.14 +/- 23.23
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 58.1         |
|    mean_reward          | 477          |
| time/                   |              |
|    total_timesteps      | 344500       |
| train/                  |              |
|    approx_kl            | 0.0016183843 |
|    clip_fraction        | 0.0391       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.447       |
|    explained_variance   | 0.0225       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.2e+04      |
|    n_updates            | 1680         |
|    policy_gradient_loss | 0.00137      |
|    value_loss           | 2.21e+04     |
------------------------------------------
Eval num_timesteps=345000, episode_reward=434.40 +/- 140.84
Episode length: 53.80 +/- 14.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.8     |
|    mean_reward     | 434      |
| time/              |          |
|    total_timesteps | 345000   |
---------------------------------
Eval num_timesteps=345500, episode_reward=485.60 +/- 202.51
Episode length: 59.00 +/- 20.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59       |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 345500   |
---------------------------------
Eval num_timesteps=346000, episode_reward=495.20 +/- 228.70
Episode length: 59.92 +/- 22.85
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.9     |
|    mean_reward     | 495      |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 141      |
|    ep_rew_mean     | 1.36e+03 |
| time/              |          |
|    fps             | 132      |
|    iterations      | 169      |
|    time_elapsed    | 2616     |
|    total_timesteps | 346112   |
---------------------------------
Eval num_timesteps=346500, episode_reward=541.92 +/- 290.37
Episode length: 64.60 +/- 29.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 64.6         |
|    mean_reward          | 542          |
| time/                   |              |
|    total_timesteps      | 346500       |
| train/                  |              |
|    approx_kl            | 0.0019267977 |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.434       |
|    explained_variance   | 0.00366      |
|    learning_rate        | 1e-05        |
|    loss                 | 8.28e+03     |
|    n_updates            | 1690         |
|    policy_gradient_loss | 0.000443     |
|    value_loss           | 2.2e+04      |
------------------------------------------
Eval num_timesteps=347000, episode_reward=516.96 +/- 271.46
Episode length: 62.12 +/- 27.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.1     |
|    mean_reward     | 517      |
| time/              |          |
|    total_timesteps | 347000   |
---------------------------------
Eval num_timesteps=347500, episode_reward=509.92 +/- 310.67
Episode length: 61.34 +/- 31.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.3     |
|    mean_reward     | 510      |
| time/              |          |
|    total_timesteps | 347500   |
---------------------------------
Eval num_timesteps=348000, episode_reward=475.36 +/- 208.02
Episode length: 57.92 +/- 20.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.9     |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 1.38e+03 |
| time/              |          |
|    fps             | 132      |
|    iterations      | 170      |
|    time_elapsed    | 2631     |
|    total_timesteps | 348160   |
---------------------------------
Eval num_timesteps=348500, episode_reward=483.68 +/- 267.31
Episode length: 58.74 +/- 26.67
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 58.7         |
|    mean_reward          | 484          |
| time/                   |              |
|    total_timesteps      | 348500       |
| train/                  |              |
|    approx_kl            | 0.0021481237 |
|    clip_fraction        | 0.08         |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.439       |
|    explained_variance   | 0.008        |
|    learning_rate        | 1e-05        |
|    loss                 | 8.78e+03     |
|    n_updates            | 1700         |
|    policy_gradient_loss | 0.00206      |
|    value_loss           | 1.7e+04      |
------------------------------------------
Eval num_timesteps=349000, episode_reward=480.48 +/- 244.21
Episode length: 58.44 +/- 24.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.4     |
|    mean_reward     | 480      |
| time/              |          |
|    total_timesteps | 349000   |
---------------------------------
Eval num_timesteps=349500, episode_reward=490.56 +/- 304.56
Episode length: 59.28 +/- 29.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.3     |
|    mean_reward     | 491      |
| time/              |          |
|    total_timesteps | 349500   |
---------------------------------
Eval num_timesteps=350000, episode_reward=472.80 +/- 167.41
Episode length: 57.66 +/- 16.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.7     |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 147      |
|    ep_rew_mean     | 1.42e+03 |
| time/              |          |
|    fps             | 132      |
|    iterations      | 171      |
|    time_elapsed    | 2644     |
|    total_timesteps | 350208   |
---------------------------------
Eval num_timesteps=350500, episode_reward=543.68 +/- 389.27
Episode length: 64.54 +/- 38.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 64.5        |
|    mean_reward          | 544         |
| time/                   |             |
|    total_timesteps      | 350500      |
| train/                  |             |
|    approx_kl            | 0.004897742 |
|    clip_fraction        | 0.0648      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.43       |
|    explained_variance   | 0.021       |
|    learning_rate        | 1e-05       |
|    loss                 | 1.4e+04     |
|    n_updates            | 1710        |
|    policy_gradient_loss | 0.00467     |
|    value_loss           | 1.82e+04    |
-----------------------------------------
Eval num_timesteps=351000, episode_reward=665.44 +/- 522.59
Episode length: 76.12 +/- 49.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.1     |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 351000   |
---------------------------------
Eval num_timesteps=351500, episode_reward=641.28 +/- 489.31
Episode length: 73.88 +/- 47.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 351500   |
---------------------------------
Eval num_timesteps=352000, episode_reward=617.76 +/- 455.48
Episode length: 71.80 +/- 44.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.8     |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    fps             | 132      |
|    iterations      | 172      |
|    time_elapsed    | 2660     |
|    total_timesteps | 352256   |
---------------------------------
Eval num_timesteps=352500, episode_reward=862.40 +/- 622.34
Episode length: 95.56 +/- 60.15
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 95.6       |
|    mean_reward          | 862        |
| time/                   |            |
|    total_timesteps      | 352500     |
| train/                  |            |
|    approx_kl            | 0.00517347 |
|    clip_fraction        | 0.0648     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.459     |
|    explained_variance   | 0.0127     |
|    learning_rate        | 1e-05      |
|    loss                 | 9.72e+03   |
|    n_updates            | 1720       |
|    policy_gradient_loss | 0.000487   |
|    value_loss           | 1.95e+04   |
----------------------------------------
Eval num_timesteps=353000, episode_reward=762.40 +/- 574.36
Episode length: 86.24 +/- 56.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 86.2     |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 353000   |
---------------------------------
Eval num_timesteps=353500, episode_reward=776.96 +/- 547.79
Episode length: 87.52 +/- 53.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 87.5     |
|    mean_reward     | 777      |
| time/              |          |
|    total_timesteps | 353500   |
---------------------------------
Eval num_timesteps=354000, episode_reward=835.04 +/- 606.88
Episode length: 93.06 +/- 59.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.1     |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    fps             | 132      |
|    iterations      | 173      |
|    time_elapsed    | 2680     |
|    total_timesteps | 354304   |
---------------------------------
Eval num_timesteps=354500, episode_reward=468.32 +/- 234.50
Episode length: 57.24 +/- 23.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 57.2       |
|    mean_reward          | 468        |
| time/                   |            |
|    total_timesteps      | 354500     |
| train/                  |            |
|    approx_kl            | 0.00701996 |
|    clip_fraction        | 0.0454     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.452     |
|    explained_variance   | 0.0135     |
|    learning_rate        | 1e-05      |
|    loss                 | 1.52e+04   |
|    n_updates            | 1730       |
|    policy_gradient_loss | 0.000153   |
|    value_loss           | 1.88e+04   |
----------------------------------------
Eval num_timesteps=355000, episode_reward=483.68 +/- 208.33
Episode length: 58.80 +/- 20.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.8     |
|    mean_reward     | 484      |
| time/              |          |
|    total_timesteps | 355000   |
---------------------------------
Eval num_timesteps=355500, episode_reward=523.20 +/- 375.58
Episode length: 62.48 +/- 36.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.5     |
|    mean_reward     | 523      |
| time/              |          |
|    total_timesteps | 355500   |
---------------------------------
Eval num_timesteps=356000, episode_reward=475.36 +/- 236.41
Episode length: 57.94 +/- 23.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.9     |
|    mean_reward     | 475      |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 146      |
|    ep_rew_mean     | 1.41e+03 |
| time/              |          |
|    fps             | 132      |
|    iterations      | 174      |
|    time_elapsed    | 2694     |
|    total_timesteps | 356352   |
---------------------------------
Eval num_timesteps=356500, episode_reward=499.52 +/- 342.70
Episode length: 60.16 +/- 33.28
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 60.2        |
|    mean_reward          | 500         |
| time/                   |             |
|    total_timesteps      | 356500      |
| train/                  |             |
|    approx_kl            | 0.007503529 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.456      |
|    explained_variance   | 0.033       |
|    learning_rate        | 1e-05       |
|    loss                 | 1.26e+04    |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.00187    |
|    value_loss           | 2.14e+04    |
-----------------------------------------
Eval num_timesteps=357000, episode_reward=625.60 +/- 444.05
Episode length: 72.70 +/- 43.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | 626      |
| time/              |          |
|    total_timesteps | 357000   |
---------------------------------
Eval num_timesteps=357500, episode_reward=583.20 +/- 404.32
Episode length: 68.38 +/- 38.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.4     |
|    mean_reward     | 583      |
| time/              |          |
|    total_timesteps | 357500   |
---------------------------------
Eval num_timesteps=358000, episode_reward=657.60 +/- 422.65
Episode length: 76.00 +/- 41.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | 658      |
| time/              |          |
|    total_timesteps | 358000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    fps             | 132      |
|    iterations      | 175      |
|    time_elapsed    | 2710     |
|    total_timesteps | 358400   |
---------------------------------
Eval num_timesteps=358500, episode_reward=1164.00 +/- 658.02
Episode length: 124.26 +/- 62.33
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 124         |
|    mean_reward          | 1.16e+03    |
| time/                   |             |
|    total_timesteps      | 358500      |
| train/                  |             |
|    approx_kl            | 0.004825171 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.464      |
|    explained_variance   | 0.0198      |
|    learning_rate        | 1e-05       |
|    loss                 | 6.49e+03    |
|    n_updates            | 1750        |
|    policy_gradient_loss | 0.000711    |
|    value_loss           | 2e+04       |
-----------------------------------------
Eval num_timesteps=359000, episode_reward=1096.64 +/- 722.29
Episode length: 117.34 +/- 68.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 117      |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 359000   |
---------------------------------
Eval num_timesteps=359500, episode_reward=881.44 +/- 590.95
Episode length: 97.26 +/- 56.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 97.3     |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 359500   |
---------------------------------
Eval num_timesteps=360000, episode_reward=1038.88 +/- 643.02
Episode length: 112.22 +/- 60.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 158      |
|    ep_rew_mean     | 1.54e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 176      |
|    time_elapsed    | 2734     |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=360500, episode_reward=1043.20 +/- 681.66
Episode length: 112.44 +/- 64.64
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 112          |
|    mean_reward          | 1.04e+03     |
| time/                   |              |
|    total_timesteps      | 360500       |
| train/                  |              |
|    approx_kl            | 0.0077152723 |
|    clip_fraction        | 0.0603       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.473       |
|    explained_variance   | 0.036        |
|    learning_rate        | 1e-05        |
|    loss                 | 7.37e+03     |
|    n_updates            | 1760         |
|    policy_gradient_loss | 0.00159      |
|    value_loss           | 1.48e+04     |
------------------------------------------
Eval num_timesteps=361000, episode_reward=1037.28 +/- 698.10
Episode length: 111.64 +/- 66.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 112      |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 361000   |
---------------------------------
Eval num_timesteps=361500, episode_reward=1266.72 +/- 683.78
Episode length: 134.14 +/- 64.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 134      |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 361500   |
---------------------------------
Eval num_timesteps=362000, episode_reward=1231.52 +/- 683.91
Episode length: 130.62 +/- 64.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 131      |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    fps             | 131      |
|    iterations      | 177      |
|    time_elapsed    | 2759     |
|    total_timesteps | 362496   |
---------------------------------
Eval num_timesteps=362500, episode_reward=999.04 +/- 670.92
Episode length: 108.82 +/- 64.76
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 109          |
|    mean_reward          | 999          |
| time/                   |              |
|    total_timesteps      | 362500       |
| train/                  |              |
|    approx_kl            | 0.0022186837 |
|    clip_fraction        | 0.0599       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.473       |
|    explained_variance   | 0.0267       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.16e+04     |
|    n_updates            | 1770         |
|    policy_gradient_loss | 0.00158      |
|    value_loss           | 1.83e+04     |
------------------------------------------
Eval num_timesteps=363000, episode_reward=969.12 +/- 664.21
Episode length: 106.04 +/- 64.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 969      |
| time/              |          |
|    total_timesteps | 363000   |
---------------------------------
Eval num_timesteps=363500, episode_reward=837.92 +/- 616.51
Episode length: 92.96 +/- 59.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93       |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 363500   |
---------------------------------
Eval num_timesteps=364000, episode_reward=1200.64 +/- 632.62
Episode length: 128.22 +/- 60.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 128      |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=364500, episode_reward=823.52 +/- 564.89
Episode length: 91.94 +/- 54.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.9     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 364500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 161      |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 178      |
|    time_elapsed    | 2787     |
|    total_timesteps | 364544   |
---------------------------------
Eval num_timesteps=365000, episode_reward=613.60 +/- 410.46
Episode length: 71.72 +/- 41.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 71.7        |
|    mean_reward          | 614         |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.003093539 |
|    clip_fraction        | 0.0487      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.456      |
|    explained_variance   | 0.0441      |
|    learning_rate        | 1e-05       |
|    loss                 | 3.05e+03    |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.000997   |
|    value_loss           | 1.62e+04    |
-----------------------------------------
Eval num_timesteps=365500, episode_reward=574.24 +/- 407.34
Episode length: 67.38 +/- 39.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.4     |
|    mean_reward     | 574      |
| time/              |          |
|    total_timesteps | 365500   |
---------------------------------
Eval num_timesteps=366000, episode_reward=542.56 +/- 323.17
Episode length: 64.62 +/- 32.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.6     |
|    mean_reward     | 543      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=366500, episode_reward=484.16 +/- 278.54
Episode length: 58.60 +/- 26.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.6     |
|    mean_reward     | 484      |
| time/              |          |
|    total_timesteps | 366500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 179      |
|    time_elapsed    | 2802     |
|    total_timesteps | 366592   |
---------------------------------
Eval num_timesteps=367000, episode_reward=623.84 +/- 387.50
Episode length: 72.74 +/- 38.69
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 72.7        |
|    mean_reward          | 624         |
| time/                   |             |
|    total_timesteps      | 367000      |
| train/                  |             |
|    approx_kl            | 0.002208998 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.46       |
|    explained_variance   | 0.0096      |
|    learning_rate        | 1e-05       |
|    loss                 | 1.15e+04    |
|    n_updates            | 1790        |
|    policy_gradient_loss | 0.000886    |
|    value_loss           | 2.29e+04    |
-----------------------------------------
Eval num_timesteps=367500, episode_reward=645.44 +/- 413.28
Episode length: 74.80 +/- 40.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.8     |
|    mean_reward     | 645      |
| time/              |          |
|    total_timesteps | 367500   |
---------------------------------
Eval num_timesteps=368000, episode_reward=661.60 +/- 428.27
Episode length: 76.60 +/- 42.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.6     |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=368500, episode_reward=550.88 +/- 319.21
Episode length: 65.46 +/- 31.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.5     |
|    mean_reward     | 551      |
| time/              |          |
|    total_timesteps | 368500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 162      |
|    ep_rew_mean     | 1.58e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 180      |
|    time_elapsed    | 2818     |
|    total_timesteps | 368640   |
---------------------------------
Eval num_timesteps=369000, episode_reward=1012.80 +/- 670.66
Episode length: 109.82 +/- 64.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 110          |
|    mean_reward          | 1.01e+03     |
| time/                   |              |
|    total_timesteps      | 369000       |
| train/                  |              |
|    approx_kl            | 0.0038903733 |
|    clip_fraction        | 0.068        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.447       |
|    explained_variance   | 0.041        |
|    learning_rate        | 1e-05        |
|    loss                 | 9.76e+03     |
|    n_updates            | 1800         |
|    policy_gradient_loss | 0.0026       |
|    value_loss           | 1.99e+04     |
------------------------------------------
Eval num_timesteps=369500, episode_reward=1119.04 +/- 658.39
Episode length: 120.40 +/- 63.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 369500   |
---------------------------------
Eval num_timesteps=370000, episode_reward=1008.48 +/- 684.02
Episode length: 109.64 +/- 65.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=370500, episode_reward=1020.32 +/- 689.09
Episode length: 110.32 +/- 65.76
---------------------------------
| eval/              |          |
|    mean_ep_length  | 110      |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 370500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    fps             | 130      |
|    iterations      | 181      |
|    time_elapsed    | 2841     |
|    total_timesteps | 370688   |
---------------------------------
Eval num_timesteps=371000, episode_reward=1419.52 +/- 709.78
Episode length: 147.56 +/- 66.43
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 148          |
|    mean_reward          | 1.42e+03     |
| time/                   |              |
|    total_timesteps      | 371000       |
| train/                  |              |
|    approx_kl            | 0.0053505157 |
|    clip_fraction        | 0.048        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.497       |
|    explained_variance   | 0.0553       |
|    learning_rate        | 1e-05        |
|    loss                 | 7.94e+03     |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.00126     |
|    value_loss           | 1.65e+04     |
------------------------------------------
Eval num_timesteps=371500, episode_reward=1307.36 +/- 729.42
Episode length: 136.98 +/- 68.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 137      |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 371500   |
---------------------------------
Eval num_timesteps=372000, episode_reward=1205.12 +/- 704.92
Episode length: 127.82 +/- 66.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 128      |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=372500, episode_reward=1123.20 +/- 721.31
Episode length: 119.58 +/- 67.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 372500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 169      |
|    ep_rew_mean     | 1.65e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 182      |
|    time_elapsed    | 2869     |
|    total_timesteps | 372736   |
---------------------------------
Eval num_timesteps=373000, episode_reward=758.56 +/- 535.36
Episode length: 85.86 +/- 52.51
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 85.9        |
|    mean_reward          | 759         |
| time/                   |             |
|    total_timesteps      | 373000      |
| train/                  |             |
|    approx_kl            | 0.006464932 |
|    clip_fraction        | 0.0514      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.473      |
|    explained_variance   | 0.045       |
|    learning_rate        | 1e-05       |
|    loss                 | 5.7e+03     |
|    n_updates            | 1820        |
|    policy_gradient_loss | 0.000299    |
|    value_loss           | 1.71e+04    |
-----------------------------------------
Eval num_timesteps=373500, episode_reward=919.36 +/- 698.07
Episode length: 100.52 +/- 66.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 101      |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 373500   |
---------------------------------
Eval num_timesteps=374000, episode_reward=971.52 +/- 634.97
Episode length: 106.14 +/- 60.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 106      |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=374500, episode_reward=823.52 +/- 658.09
Episode length: 91.06 +/- 62.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.1     |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 374500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 170      |
|    ep_rew_mean     | 1.66e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 183      |
|    time_elapsed    | 2889     |
|    total_timesteps | 374784   |
---------------------------------
Eval num_timesteps=375000, episode_reward=517.60 +/- 245.23
Episode length: 62.18 +/- 24.55
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 62.2         |
|    mean_reward          | 518          |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0015871803 |
|    clip_fraction        | 0.054        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.449       |
|    explained_variance   | 0.0163       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.41e+04     |
|    n_updates            | 1830         |
|    policy_gradient_loss | 0.000582     |
|    value_loss           | 2.29e+04     |
------------------------------------------
Eval num_timesteps=375500, episode_reward=512.48 +/- 305.93
Episode length: 61.62 +/- 30.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.6     |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 375500   |
---------------------------------
Eval num_timesteps=376000, episode_reward=532.96 +/- 311.59
Episode length: 63.70 +/- 31.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.7     |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=376500, episode_reward=515.52 +/- 322.31
Episode length: 61.74 +/- 31.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.7     |
|    mean_reward     | 516      |
| time/              |          |
|    total_timesteps | 376500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 159      |
|    ep_rew_mean     | 1.55e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 184      |
|    time_elapsed    | 2904     |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=377000, episode_reward=461.28 +/- 278.84
Episode length: 56.52 +/- 27.93
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 56.5         |
|    mean_reward          | 461          |
| time/                   |              |
|    total_timesteps      | 377000       |
| train/                  |              |
|    approx_kl            | 0.0046882383 |
|    clip_fraction        | 0.0439       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.423       |
|    explained_variance   | 0.0253       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.23e+04     |
|    n_updates            | 1840         |
|    policy_gradient_loss | -1.84e-05    |
|    value_loss           | 2.54e+04     |
------------------------------------------
Eval num_timesteps=377500, episode_reward=467.04 +/- 241.77
Episode length: 57.08 +/- 24.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.1     |
|    mean_reward     | 467      |
| time/              |          |
|    total_timesteps | 377500   |
---------------------------------
Eval num_timesteps=378000, episode_reward=455.52 +/- 179.29
Episode length: 56.00 +/- 17.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56       |
|    mean_reward     | 456      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=378500, episode_reward=504.80 +/- 286.73
Episode length: 60.92 +/- 28.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.9     |
|    mean_reward     | 505      |
| time/              |          |
|    total_timesteps | 378500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 185      |
|    time_elapsed    | 2917     |
|    total_timesteps | 378880   |
---------------------------------
Eval num_timesteps=379000, episode_reward=461.92 +/- 220.61
Episode length: 56.58 +/- 22.04
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 56.6        |
|    mean_reward          | 462         |
| time/                   |             |
|    total_timesteps      | 379000      |
| train/                  |             |
|    approx_kl            | 0.005325627 |
|    clip_fraction        | 0.0434      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.44       |
|    explained_variance   | 0.0126      |
|    learning_rate        | 1e-05       |
|    loss                 | 1.41e+04    |
|    n_updates            | 1850        |
|    policy_gradient_loss | 0.00108     |
|    value_loss           | 2.51e+04    |
-----------------------------------------
Eval num_timesteps=379500, episode_reward=489.44 +/- 233.06
Episode length: 59.38 +/- 23.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.4     |
|    mean_reward     | 489      |
| time/              |          |
|    total_timesteps | 379500   |
---------------------------------
Eval num_timesteps=380000, episode_reward=497.12 +/- 290.66
Episode length: 60.12 +/- 29.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.1     |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=380500, episode_reward=468.32 +/- 201.33
Episode length: 57.24 +/- 20.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 57.2     |
|    mean_reward     | 468      |
| time/              |          |
|    total_timesteps | 380500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 152      |
|    ep_rew_mean     | 1.47e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 186      |
|    time_elapsed    | 2931     |
|    total_timesteps | 380928   |
---------------------------------
Eval num_timesteps=381000, episode_reward=497.12 +/- 219.62
Episode length: 60.10 +/- 21.95
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 60.1        |
|    mean_reward          | 497         |
| time/                   |             |
|    total_timesteps      | 381000      |
| train/                  |             |
|    approx_kl            | 0.008452347 |
|    clip_fraction        | 0.0929      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.436      |
|    explained_variance   | 0.0405      |
|    learning_rate        | 1e-05       |
|    loss                 | 6.74e+03    |
|    n_updates            | 1860        |
|    policy_gradient_loss | 0.00158     |
|    value_loss           | 1.62e+04    |
-----------------------------------------
Eval num_timesteps=381500, episode_reward=504.80 +/- 232.10
Episode length: 60.92 +/- 23.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.9     |
|    mean_reward     | 505      |
| time/              |          |
|    total_timesteps | 381500   |
---------------------------------
Eval num_timesteps=382000, episode_reward=518.88 +/- 236.25
Episode length: 62.22 +/- 23.66
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.2     |
|    mean_reward     | 519      |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=382500, episode_reward=563.36 +/- 407.28
Episode length: 66.32 +/- 39.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.3     |
|    mean_reward     | 563      |
| time/              |          |
|    total_timesteps | 382500   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 150      |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 187      |
|    time_elapsed    | 2946     |
|    total_timesteps | 382976   |
---------------------------------
Eval num_timesteps=383000, episode_reward=431.84 +/- 140.78
Episode length: 53.56 +/- 14.13
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 53.6         |
|    mean_reward          | 432          |
| time/                   |              |
|    total_timesteps      | 383000       |
| train/                  |              |
|    approx_kl            | 0.0046251337 |
|    clip_fraction        | 0.0379       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.429       |
|    explained_variance   | 0.0372       |
|    learning_rate        | 1e-05        |
|    loss                 | 7.25e+03     |
|    n_updates            | 1870         |
|    policy_gradient_loss | 0.00199      |
|    value_loss           | 1.9e+04      |
------------------------------------------
Eval num_timesteps=383500, episode_reward=495.84 +/- 242.18
Episode length: 59.96 +/- 24.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60       |
|    mean_reward     | 496      |
| time/              |          |
|    total_timesteps | 383500   |
---------------------------------
Eval num_timesteps=384000, episode_reward=511.84 +/- 226.51
Episode length: 61.60 +/- 22.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.6     |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=384500, episode_reward=490.72 +/- 231.04
Episode length: 59.46 +/- 23.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.5     |
|    mean_reward     | 491      |
| time/              |          |
|    total_timesteps | 384500   |
---------------------------------
Eval num_timesteps=385000, episode_reward=454.24 +/- 185.54
Episode length: 55.80 +/- 18.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.8     |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 142      |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 129      |
|    iterations      | 188      |
|    time_elapsed    | 2962     |
|    total_timesteps | 385024   |
---------------------------------
Eval num_timesteps=385500, episode_reward=470.72 +/- 312.53
Episode length: 57.24 +/- 30.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 57.2         |
|    mean_reward          | 471          |
| time/                   |              |
|    total_timesteps      | 385500       |
| train/                  |              |
|    approx_kl            | 0.0030778737 |
|    clip_fraction        | 0.065        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.438       |
|    explained_variance   | 0.0207       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.45e+04     |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.000452    |
|    value_loss           | 2.55e+04     |
------------------------------------------
Eval num_timesteps=386000, episode_reward=456.80 +/- 177.13
Episode length: 56.08 +/- 17.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.1     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 386000   |
---------------------------------
Eval num_timesteps=386500, episode_reward=432.48 +/- 142.21
Episode length: 53.66 +/- 14.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 53.7     |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 386500   |
---------------------------------
Eval num_timesteps=387000, episode_reward=415.20 +/- 172.35
Episode length: 51.88 +/- 17.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 51.9     |
|    mean_reward     | 415      |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 129      |
|    ep_rew_mean     | 1.23e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 189      |
|    time_elapsed    | 2976     |
|    total_timesteps | 387072   |
---------------------------------
Eval num_timesteps=387500, episode_reward=442.72 +/- 161.40
Episode length: 54.68 +/- 16.17
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 54.7         |
|    mean_reward          | 443          |
| time/                   |              |
|    total_timesteps      | 387500       |
| train/                  |              |
|    approx_kl            | 0.0035360882 |
|    clip_fraction        | 0.047        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.441       |
|    explained_variance   | 0.00848      |
|    learning_rate        | 1e-05        |
|    loss                 | 1.83e+04     |
|    n_updates            | 1890         |
|    policy_gradient_loss | 0.00129      |
|    value_loss           | 2.6e+04      |
------------------------------------------
Eval num_timesteps=388000, episode_reward=422.88 +/- 144.62
Episode length: 52.64 +/- 14.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=388500, episode_reward=445.76 +/- 296.58
Episode length: 54.74 +/- 28.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 54.7     |
|    mean_reward     | 446      |
| time/              |          |
|    total_timesteps | 388500   |
---------------------------------
Eval num_timesteps=389000, episode_reward=458.08 +/- 181.71
Episode length: 56.24 +/- 18.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.2     |
|    mean_reward     | 458      |
| time/              |          |
|    total_timesteps | 389000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 1.22e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 190      |
|    time_elapsed    | 2989     |
|    total_timesteps | 389120   |
---------------------------------
Eval num_timesteps=389500, episode_reward=509.92 +/- 268.98
Episode length: 61.46 +/- 26.88
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 61.5         |
|    mean_reward          | 510          |
| time/                   |              |
|    total_timesteps      | 389500       |
| train/                  |              |
|    approx_kl            | 0.0036885012 |
|    clip_fraction        | 0.0712       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.423       |
|    explained_variance   | 0.0194       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.07e+04     |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.00156     |
|    value_loss           | 1.89e+04     |
------------------------------------------
Eval num_timesteps=390000, episode_reward=423.52 +/- 143.22
Episode length: 52.80 +/- 14.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.8     |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=390500, episode_reward=451.68 +/- 158.14
Episode length: 55.58 +/- 15.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 55.6     |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 390500   |
---------------------------------
Eval num_timesteps=391000, episode_reward=491.84 +/- 324.17
Episode length: 59.38 +/- 31.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 59.4     |
|    mean_reward     | 492      |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 133      |
|    ep_rew_mean     | 1.27e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 191      |
|    time_elapsed    | 2999     |
|    total_timesteps | 391168   |
---------------------------------
Eval num_timesteps=391500, episode_reward=546.40 +/- 256.64
Episode length: 65.04 +/- 25.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 65          |
|    mean_reward          | 546         |
| time/                   |             |
|    total_timesteps      | 391500      |
| train/                  |             |
|    approx_kl            | 0.006217229 |
|    clip_fraction        | 0.0742      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.425      |
|    explained_variance   | 0.0291      |
|    learning_rate        | 1e-05       |
|    loss                 | 9.94e+03    |
|    n_updates            | 1910        |
|    policy_gradient_loss | 0.00579     |
|    value_loss           | 1.89e+04    |
-----------------------------------------
Eval num_timesteps=392000, episode_reward=457.44 +/- 188.57
Episode length: 56.08 +/- 18.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.1     |
|    mean_reward     | 457      |
| time/              |          |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=392500, episode_reward=421.60 +/- 135.50
Episode length: 52.56 +/- 13.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.6     |
|    mean_reward     | 422      |
| time/              |          |
|    total_timesteps | 392500   |
---------------------------------
Eval num_timesteps=393000, episode_reward=463.84 +/- 193.69
Episode length: 56.76 +/- 19.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 56.8     |
|    mean_reward     | 464      |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 136      |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 130      |
|    iterations      | 192      |
|    time_elapsed    | 3009     |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=393500, episode_reward=746.40 +/- 533.59
Episode length: 84.70 +/- 52.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 84.7        |
|    mean_reward          | 746         |
| time/                   |             |
|    total_timesteps      | 393500      |
| train/                  |             |
|    approx_kl            | 0.005255153 |
|    clip_fraction        | 0.0314      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.43       |
|    explained_variance   | 0.0414      |
|    learning_rate        | 1e-05       |
|    loss                 | 8.24e+03    |
|    n_updates            | 1920        |
|    policy_gradient_loss | 0.00052     |
|    value_loss           | 1.84e+04    |
-----------------------------------------
Eval num_timesteps=394000, episode_reward=792.48 +/- 592.97
Episode length: 88.36 +/- 56.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 88.4     |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 394000   |
---------------------------------
Eval num_timesteps=394500, episode_reward=647.20 +/- 424.77
Episode length: 74.74 +/- 41.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.7     |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 394500   |
---------------------------------
Eval num_timesteps=395000, episode_reward=872.32 +/- 654.45
Episode length: 96.14 +/- 62.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 96.1     |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 137      |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 193      |
|    time_elapsed    | 3023     |
|    total_timesteps | 395264   |
---------------------------------
Eval num_timesteps=395500, episode_reward=1046.24 +/- 666.18
Episode length: 113.32 +/- 64.07
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 113          |
|    mean_reward          | 1.05e+03     |
| time/                   |              |
|    total_timesteps      | 395500       |
| train/                  |              |
|    approx_kl            | 0.0036644014 |
|    clip_fraction        | 0.042        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.457       |
|    explained_variance   | 0.0214       |
|    learning_rate        | 1e-05        |
|    loss                 | 1.7e+04      |
|    n_updates            | 1930         |
|    policy_gradient_loss | 0.000392     |
|    value_loss           | 2e+04        |
------------------------------------------
Eval num_timesteps=396000, episode_reward=862.08 +/- 644.29
Episode length: 95.22 +/- 61.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 95.2     |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=396500, episode_reward=713.92 +/- 574.82
Episode length: 80.72 +/- 54.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 80.7     |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 396500   |
---------------------------------
Eval num_timesteps=397000, episode_reward=830.72 +/- 629.35
Episode length: 92.02 +/- 60.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92       |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 138      |
|    ep_rew_mean     | 1.32e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 194      |
|    time_elapsed    | 3038     |
|    total_timesteps | 397312   |
---------------------------------
Eval num_timesteps=397500, episode_reward=1143.04 +/- 721.71
Episode length: 121.60 +/- 68.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 122         |
|    mean_reward          | 1.14e+03    |
| time/                   |             |
|    total_timesteps      | 397500      |
| train/                  |             |
|    approx_kl            | 0.004703656 |
|    clip_fraction        | 0.0464      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.455      |
|    explained_variance   | 0.0187      |
|    learning_rate        | 1e-05       |
|    loss                 | 4.55e+03    |
|    n_updates            | 1940        |
|    policy_gradient_loss | 0.000506    |
|    value_loss           | 1.89e+04    |
-----------------------------------------
Eval num_timesteps=398000, episode_reward=1222.88 +/- 730.98
Episode length: 129.36 +/- 69.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 129      |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 398000   |
---------------------------------
Eval num_timesteps=398500, episode_reward=1153.28 +/- 709.74
Episode length: 122.62 +/- 66.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 123      |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 398500   |
---------------------------------
Eval num_timesteps=399000, episode_reward=1278.88 +/- 668.25
Episode length: 135.42 +/- 63.28
---------------------------------
| eval/              |          |
|    mean_ep_length  | 135      |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 143      |
|    ep_rew_mean     | 1.37e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 195      |
|    time_elapsed    | 3058     |
|    total_timesteps | 399360   |
---------------------------------
Eval num_timesteps=399500, episode_reward=962.08 +/- 698.64
Episode length: 104.52 +/- 66.54
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 105          |
|    mean_reward          | 962          |
| time/                   |              |
|    total_timesteps      | 399500       |
| train/                  |              |
|    approx_kl            | 0.0014909332 |
|    clip_fraction        | 0.0642       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.475       |
|    explained_variance   | 0.0405       |
|    learning_rate        | 1e-05        |
|    loss                 | 9.51e+03     |
|    n_updates            | 1950         |
|    policy_gradient_loss | 0.00199      |
|    value_loss           | 1.69e+04     |
------------------------------------------
Eval num_timesteps=400000, episode_reward=952.32 +/- 726.04
Episode length: 103.34 +/- 69.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 103      |
|    mean_reward     | 952      |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=400500, episode_reward=846.72 +/- 617.96
Episode length: 93.64 +/- 58.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 93.6     |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 400500   |
---------------------------------
Eval num_timesteps=401000, episode_reward=833.44 +/- 650.80
Episode length: 92.52 +/- 62.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 92.5     |
|    mean_reward     | 833      |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 151      |
|    ep_rew_mean     | 1.46e+03 |
| time/              |          |
|    fps             | 130      |
|    iterations      | 196      |
|    time_elapsed    | 3074     |
|    total_timesteps | 401408   |
---------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/health-gathering/ppo-1/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'n_steps': 2048, 'batch_size': 64, 'learning_rate': 1e-05, 'gamma': 0.9893, 'gae_lambda': 0.8687}
Training steps: 400000
Frame skip: 10
