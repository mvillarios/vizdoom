/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.
  warnings.warn(
Using cuda device
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 114      |
|    ep_rew_mean      | 6.25     |
|    exploration_rate | 1        |
| time/               |          |
|    episodes         | 4        |
|    fps              | 3442     |
|    time_elapsed     | 0        |
|    total_timesteps  | 456      |
----------------------------------
Eval num_timesteps=500, episode_reward=2.52 +/- 1.96
Episode length: 85.98 +/- 31.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86       |
|    mean_reward      | 2.52     |
| rollout/            |          |
|    exploration_rate | 1        |
| time/               |          |
|    total_timesteps  | 500      |
----------------------------------
New best mean reward!
Eval num_timesteps=1000, episode_reward=3.38 +/- 3.01
Episode length: 102.28 +/- 47.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.38     |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 1000     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 134      |
|    ep_rew_mean      | 8.88     |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 8        |
|    fps              | 187      |
|    time_elapsed     | 5        |
|    total_timesteps  | 1074     |
----------------------------------
Eval num_timesteps=1500, episode_reward=3.02 +/- 2.25
Episode length: 100.18 +/- 34.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 3.02     |
| rollout/            |          |
|    exploration_rate | 0.999    |
| time/               |          |
|    total_timesteps  | 1500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 8.5      |
|    exploration_rate | 0.999    |
| time/               |          |
|    episodes         | 12       |
|    fps              | 190      |
|    time_elapsed     | 8        |
|    total_timesteps  | 1661     |
----------------------------------
Eval num_timesteps=2000, episode_reward=2.72 +/- 2.29
Episode length: 93.06 +/- 36.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.1     |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 2000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 148      |
|    ep_rew_mean      | 9.06     |
|    exploration_rate | 0.998    |
| time/               |          |
|    episodes         | 16       |
|    fps              | 204      |
|    time_elapsed     | 11       |
|    total_timesteps  | 2369     |
----------------------------------
Eval num_timesteps=2500, episode_reward=2.72 +/- 2.50
Episode length: 92.60 +/- 42.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.6     |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.998    |
| time/               |          |
|    total_timesteps  | 2500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 146      |
|    ep_rew_mean      | 8.9      |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 20       |
|    fps              | 203      |
|    time_elapsed     | 14       |
|    total_timesteps  | 2920     |
----------------------------------
Eval num_timesteps=3000, episode_reward=2.26 +/- 1.86
Episode length: 87.26 +/- 29.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 87.3     |
|    mean_reward      | 2.26     |
| rollout/            |          |
|    exploration_rate | 0.997    |
| time/               |          |
|    total_timesteps  | 3000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 8.83     |
|    exploration_rate | 0.997    |
| time/               |          |
|    episodes         | 24       |
|    fps              | 202      |
|    time_elapsed     | 17       |
|    total_timesteps  | 3475     |
----------------------------------
Eval num_timesteps=3500, episode_reward=2.86 +/- 2.09
Episode length: 92.76 +/- 32.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 92.8     |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 3500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 140      |
|    ep_rew_mean      | 8.5      |
|    exploration_rate | 0.996    |
| time/               |          |
|    episodes         | 28       |
|    fps              | 197      |
|    time_elapsed     | 19       |
|    total_timesteps  | 3915     |
----------------------------------
Eval num_timesteps=4000, episode_reward=2.70 +/- 2.09
Episode length: 90.18 +/- 35.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.2     |
|    mean_reward      | 2.7      |
| rollout/            |          |
|    exploration_rate | 0.996    |
| time/               |          |
|    total_timesteps  | 4000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 140      |
|    ep_rew_mean      | 8.62     |
|    exploration_rate | 0.995    |
| time/               |          |
|    episodes         | 32       |
|    fps              | 195      |
|    time_elapsed     | 22       |
|    total_timesteps  | 4473     |
----------------------------------
Eval num_timesteps=4500, episode_reward=2.82 +/- 1.93
Episode length: 93.44 +/- 31.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.4     |
|    mean_reward      | 2.82     |
| rollout/            |          |
|    exploration_rate | 0.995    |
| time/               |          |
|    total_timesteps  | 4500     |
----------------------------------
Eval num_timesteps=5000, episode_reward=2.96 +/- 2.04
Episode length: 93.02 +/- 32.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93       |
|    mean_reward      | 2.96     |
| rollout/            |          |
|    exploration_rate | 0.994    |
| time/               |          |
|    total_timesteps  | 5000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 8.78     |
|    exploration_rate | 0.994    |
| time/               |          |
|    episodes         | 36       |
|    fps              | 181      |
|    time_elapsed     | 28       |
|    total_timesteps  | 5190     |
----------------------------------
Eval num_timesteps=5500, episode_reward=2.74 +/- 2.27
Episode length: 87.98 +/- 29.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88       |
|    mean_reward      | 2.74     |
| rollout/            |          |
|    exploration_rate | 0.993    |
| time/               |          |
|    total_timesteps  | 5500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 147      |
|    ep_rew_mean      | 8.88     |
|    exploration_rate | 0.992    |
| time/               |          |
|    episodes         | 40       |
|    fps              | 186      |
|    time_elapsed     | 31       |
|    total_timesteps  | 5871     |
----------------------------------
Eval num_timesteps=6000, episode_reward=3.32 +/- 2.37
Episode length: 101.76 +/- 40.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 102      |
|    mean_reward      | 3.32     |
| rollout/            |          |
|    exploration_rate | 0.992    |
| time/               |          |
|    total_timesteps  | 6000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 8.73     |
|    exploration_rate | 0.991    |
| time/               |          |
|    episodes         | 44       |
|    fps              | 184      |
|    time_elapsed     | 34       |
|    total_timesteps  | 6370     |
----------------------------------
Eval num_timesteps=6500, episode_reward=3.00 +/- 1.98
Episode length: 97.56 +/- 34.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.6     |
|    mean_reward      | 3        |
| rollout/            |          |
|    exploration_rate | 0.991    |
| time/               |          |
|    total_timesteps  | 6500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 8.71     |
|    exploration_rate | 0.99     |
| time/               |          |
|    episodes         | 48       |
|    fps              | 185      |
|    time_elapsed     | 37       |
|    total_timesteps  | 6923     |
----------------------------------
Eval num_timesteps=7000, episode_reward=2.84 +/- 2.11
Episode length: 91.16 +/- 33.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.2     |
|    mean_reward      | 2.84     |
| rollout/            |          |
|    exploration_rate | 0.99     |
| time/               |          |
|    total_timesteps  | 7000     |
----------------------------------
Eval num_timesteps=7500, episode_reward=2.56 +/- 1.96
Episode length: 86.90 +/- 26.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.9     |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.989    |
| time/               |          |
|    total_timesteps  | 7500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 148      |
|    ep_rew_mean      | 9.06     |
|    exploration_rate | 0.988    |
| time/               |          |
|    episodes         | 52       |
|    fps              | 181      |
|    time_elapsed     | 42       |
|    total_timesteps  | 7719     |
----------------------------------
Eval num_timesteps=8000, episode_reward=2.40 +/- 1.60
Episode length: 86.06 +/- 25.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.1     |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.988    |
| time/               |          |
|    total_timesteps  | 8000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 148      |
|    ep_rew_mean      | 8.88     |
|    exploration_rate | 0.987    |
| time/               |          |
|    episodes         | 56       |
|    fps              | 183      |
|    time_elapsed     | 45       |
|    total_timesteps  | 8283     |
----------------------------------
Eval num_timesteps=8500, episode_reward=2.86 +/- 2.32
Episode length: 97.40 +/- 38.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.4     |
|    mean_reward      | 2.86     |
| rollout/            |          |
|    exploration_rate | 0.987    |
| time/               |          |
|    total_timesteps  | 8500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 146      |
|    ep_rew_mean      | 8.73     |
|    exploration_rate | 0.986    |
| time/               |          |
|    episodes         | 60       |
|    fps              | 182      |
|    time_elapsed     | 48       |
|    total_timesteps  | 8763     |
----------------------------------
Eval num_timesteps=9000, episode_reward=2.52 +/- 1.92
Episode length: 88.84 +/- 29.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88.8     |
|    mean_reward      | 2.52     |
| rollout/            |          |
|    exploration_rate | 0.985    |
| time/               |          |
|    total_timesteps  | 9000     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 8.52     |
|    exploration_rate | 0.985    |
| time/               |          |
|    episodes         | 64       |
|    fps              | 182      |
|    time_elapsed     | 50       |
|    total_timesteps  | 9240     |
----------------------------------
Eval num_timesteps=9500, episode_reward=3.06 +/- 2.49
Episode length: 100.46 +/- 41.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 100      |
|    mean_reward      | 3.06     |
| rollout/            |          |
|    exploration_rate | 0.984    |
| time/               |          |
|    total_timesteps  | 9500     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.41     |
|    exploration_rate | 0.984    |
| time/               |          |
|    episodes         | 68       |
|    fps              | 180      |
|    time_elapsed     | 53       |
|    total_timesteps  | 9679     |
----------------------------------
Eval num_timesteps=10000, episode_reward=3.34 +/- 2.27
Episode length: 103.16 +/- 34.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 3.34     |
| rollout/            |          |
|    exploration_rate | 0.983    |
| time/               |          |
|    total_timesteps  | 10000    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.35     |
|    exploration_rate | 0.982    |
| time/               |          |
|    episodes         | 72       |
|    fps              | 180      |
|    time_elapsed     | 56       |
|    total_timesteps  | 10257    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0155   |
|    n_updates        | 64       |
----------------------------------
Eval num_timesteps=10500, episode_reward=2.56 +/- 2.41
Episode length: 88.02 +/- 37.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 88       |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.982    |
| time/               |          |
|    total_timesteps  | 10500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 124      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 8.36     |
|    exploration_rate | 0.981    |
| time/               |          |
|    episodes         | 76       |
|    fps              | 181      |
|    time_elapsed     | 59       |
|    total_timesteps  | 10840    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0423   |
|    n_updates        | 209      |
----------------------------------
Eval num_timesteps=11000, episode_reward=12.06 +/- 4.23
Episode length: 157.96 +/- 54.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 158      |
|    mean_reward      | 12.1     |
| rollout/            |          |
|    exploration_rate | 0.98     |
| time/               |          |
|    total_timesteps  | 11000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0302   |
|    n_updates        | 249      |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 8.36     |
|    exploration_rate | 0.979    |
| time/               |          |
|    episodes         | 80       |
|    fps              | 177      |
|    time_elapsed     | 64       |
|    total_timesteps  | 11458    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0735   |
|    n_updates        | 364      |
----------------------------------
Eval num_timesteps=11500, episode_reward=2.76 +/- 2.22
Episode length: 94.80 +/- 35.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 94.8     |
|    mean_reward      | 2.76     |
| rollout/            |          |
|    exploration_rate | 0.979    |
| time/               |          |
|    total_timesteps  | 11500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0691   |
|    n_updates        | 374      |
----------------------------------
Eval num_timesteps=12000, episode_reward=5.34 +/- 2.38
Episode length: 119.06 +/- 37.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 5.34     |
| rollout/            |          |
|    exploration_rate | 0.978    |
| time/               |          |
|    total_timesteps  | 12000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0279   |
|    n_updates        | 499      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 8.39     |
|    exploration_rate | 0.977    |
| time/               |          |
|    episodes         | 84       |
|    fps              | 169      |
|    time_elapsed     | 70       |
|    total_timesteps  | 12036    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0433   |
|    n_updates        | 508      |
----------------------------------
Eval num_timesteps=12500, episode_reward=9.46 +/- 2.76
Episode length: 156.32 +/- 42.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 156      |
|    mean_reward      | 9.46     |
| rollout/            |          |
|    exploration_rate | 0.976    |
| time/               |          |
|    total_timesteps  | 12500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0561   |
|    n_updates        | 624      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 8.38     |
|    exploration_rate | 0.976    |
| time/               |          |
|    episodes         | 88       |
|    fps              | 167      |
|    time_elapsed     | 75       |
|    total_timesteps  | 12666    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0566   |
|    n_updates        | 666      |
----------------------------------
Eval num_timesteps=13000, episode_reward=2.40 +/- 2.24
Episode length: 86.58 +/- 35.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 86.6     |
|    mean_reward      | 2.4      |
| rollout/            |          |
|    exploration_rate | 0.975    |
| time/               |          |
|    total_timesteps  | 13000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0545   |
|    n_updates        | 749      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.23     |
|    exploration_rate | 0.975    |
| time/               |          |
|    episodes         | 92       |
|    fps              | 166      |
|    time_elapsed     | 78       |
|    total_timesteps  | 13038    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.068    |
|    n_updates        | 759      |
----------------------------------
Eval num_timesteps=13500, episode_reward=2.56 +/- 2.12
Episode length: 89.96 +/- 33.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90       |
|    mean_reward      | 2.56     |
| rollout/            |          |
|    exploration_rate | 0.973    |
| time/               |          |
|    total_timesteps  | 13500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0151   |
|    n_updates        | 874      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.21     |
|    exploration_rate | 0.973    |
| time/               |          |
|    episodes         | 96       |
|    fps              | 167      |
|    time_elapsed     | 81       |
|    total_timesteps  | 13647    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 911      |
----------------------------------
Eval num_timesteps=14000, episode_reward=2.74 +/- 2.22
Episode length: 91.54 +/- 35.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.5     |
|    mean_reward      | 2.74     |
| rollout/            |          |
|    exploration_rate | 0.972    |
| time/               |          |
|    total_timesteps  | 14000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0263   |
|    n_updates        | 999      |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.26     |
|    exploration_rate | 0.971    |
| time/               |          |
|    episodes         | 100      |
|    fps              | 167      |
|    time_elapsed     | 84       |
|    total_timesteps  | 14158    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.049    |
|    n_updates        | 1039     |
----------------------------------
Eval num_timesteps=14500, episode_reward=12.00 +/- 4.00
Episode length: 149.88 +/- 44.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 12       |
| rollout/            |          |
|    exploration_rate | 0.97     |
| time/               |          |
|    total_timesteps  | 14500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0306   |
|    n_updates        | 1124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.33     |
|    exploration_rate | 0.97     |
| time/               |          |
|    episodes         | 104      |
|    fps              | 165      |
|    time_elapsed     | 88       |
|    total_timesteps  | 14704    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0188   |
|    n_updates        | 1175     |
----------------------------------
Eval num_timesteps=15000, episode_reward=3.54 +/- 2.36
Episode length: 90.40 +/- 29.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 90.4     |
|    mean_reward      | 3.54     |
| rollout/            |          |
|    exploration_rate | 0.969    |
| time/               |          |
|    total_timesteps  | 15000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 1249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 8.21     |
|    exploration_rate | 0.968    |
| time/               |          |
|    episodes         | 108      |
|    fps              | 167      |
|    time_elapsed     | 91       |
|    total_timesteps  | 15365    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0278   |
|    n_updates        | 1341     |
----------------------------------
Eval num_timesteps=15500, episode_reward=2.90 +/- 2.37
Episode length: 91.52 +/- 35.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.5     |
|    mean_reward      | 2.9      |
| rollout/            |          |
|    exploration_rate | 0.967    |
| time/               |          |
|    total_timesteps  | 15500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0706   |
|    n_updates        | 1374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.14     |
|    exploration_rate | 0.966    |
| time/               |          |
|    episodes         | 112      |
|    fps              | 167      |
|    time_elapsed     | 94       |
|    total_timesteps  | 15858    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 1464     |
----------------------------------
Eval num_timesteps=16000, episode_reward=2.72 +/- 1.95
Episode length: 91.86 +/- 28.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.9     |
|    mean_reward      | 2.72     |
| rollout/            |          |
|    exploration_rate | 0.966    |
| time/               |          |
|    total_timesteps  | 16000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0626   |
|    n_updates        | 1499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 140      |
|    ep_rew_mean      | 7.97     |
|    exploration_rate | 0.964    |
| time/               |          |
|    episodes         | 116      |
|    fps              | 167      |
|    time_elapsed     | 97       |
|    total_timesteps  | 16394    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0314   |
|    n_updates        | 1598     |
----------------------------------
Eval num_timesteps=16500, episode_reward=4.90 +/- 2.13
Episode length: 124.62 +/- 34.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 4.9      |
| rollout/            |          |
|    exploration_rate | 0.964    |
| time/               |          |
|    total_timesteps  | 16500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0289   |
|    n_updates        | 1624     |
----------------------------------
Eval num_timesteps=17000, episode_reward=2.70 +/- 2.02
Episode length: 91.28 +/- 33.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 91.3     |
|    mean_reward      | 2.7      |
| rollout/            |          |
|    exploration_rate | 0.962    |
| time/               |          |
|    total_timesteps  | 17000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0476   |
|    n_updates        | 1749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 8.01     |
|    exploration_rate | 0.962    |
| time/               |          |
|    episodes         | 120      |
|    fps              | 162      |
|    time_elapsed     | 104      |
|    total_timesteps  | 17003    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0463   |
|    n_updates        | 1750     |
----------------------------------
Eval num_timesteps=17500, episode_reward=3.28 +/- 2.67
Episode length: 98.16 +/- 35.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 98.2     |
|    mean_reward      | 3.28     |
| rollout/            |          |
|    exploration_rate | 0.961    |
| time/               |          |
|    total_timesteps  | 17500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.012    |
|    n_updates        | 1874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 7.98     |
|    exploration_rate | 0.96     |
| time/               |          |
|    episodes         | 124      |
|    fps              | 162      |
|    time_elapsed     | 107      |
|    total_timesteps  | 17529    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 1882     |
----------------------------------
Eval num_timesteps=18000, episode_reward=3.40 +/- 2.19
Episode length: 105.22 +/- 38.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.959    |
| time/               |          |
|    total_timesteps  | 18000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00258  |
|    n_updates        | 1999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.06     |
|    exploration_rate | 0.959    |
| time/               |          |
|    episodes         | 128      |
|    fps              | 163      |
|    time_elapsed     | 110      |
|    total_timesteps  | 18086    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0254   |
|    n_updates        | 2021     |
----------------------------------
Eval num_timesteps=18500, episode_reward=15.28 +/- 4.89
Episode length: 167.02 +/- 49.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 167      |
|    mean_reward      | 15.3     |
| rollout/            |          |
|    exploration_rate | 0.957    |
| time/               |          |
|    total_timesteps  | 18500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0281   |
|    n_updates        | 2124     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 7.9      |
|    exploration_rate | 0.957    |
| time/               |          |
|    episodes         | 132      |
|    fps              | 159      |
|    time_elapsed     | 116      |
|    total_timesteps  | 18563    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0615   |
|    n_updates        | 2140     |
----------------------------------
Eval num_timesteps=19000, episode_reward=4.70 +/- 2.20
Episode length: 117.40 +/- 37.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 117      |
|    mean_reward      | 4.7      |
| rollout/            |          |
|    exploration_rate | 0.955    |
| time/               |          |
|    total_timesteps  | 19000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0251   |
|    n_updates        | 2249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 7.67     |
|    exploration_rate | 0.955    |
| time/               |          |
|    episodes         | 136      |
|    fps              | 159      |
|    time_elapsed     | 119      |
|    total_timesteps  | 19021    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0282   |
|    n_updates        | 2255     |
----------------------------------
Eval num_timesteps=19500, episode_reward=5.98 +/- 3.66
Episode length: 131.28 +/- 55.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 5.98     |
| rollout/            |          |
|    exploration_rate | 0.954    |
| time/               |          |
|    total_timesteps  | 19500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 2374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 7.56     |
|    exploration_rate | 0.953    |
| time/               |          |
|    episodes         | 140      |
|    fps              | 158      |
|    time_elapsed     | 123      |
|    total_timesteps  | 19546    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00574  |
|    n_updates        | 2386     |
----------------------------------
Eval num_timesteps=20000, episode_reward=13.20 +/- 5.33
Episode length: 156.68 +/- 60.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 157      |
|    mean_reward      | 13.2     |
| rollout/            |          |
|    exploration_rate | 0.952    |
| time/               |          |
|    total_timesteps  | 20000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 2499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 7.57     |
|    exploration_rate | 0.952    |
| time/               |          |
|    episodes         | 144      |
|    fps              | 155      |
|    time_elapsed     | 128      |
|    total_timesteps  | 20008    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 2501     |
----------------------------------
Eval num_timesteps=20500, episode_reward=6.80 +/- 3.01
Episode length: 134.46 +/- 44.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 134      |
|    mean_reward      | 6.8      |
| rollout/            |          |
|    exploration_rate | 0.95     |
| time/               |          |
|    total_timesteps  | 20500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0162   |
|    n_updates        | 2624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 7.72     |
|    exploration_rate | 0.949    |
| time/               |          |
|    episodes         | 148      |
|    fps              | 156      |
|    time_elapsed     | 132      |
|    total_timesteps  | 20752    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0251   |
|    n_updates        | 2687     |
----------------------------------
Eval num_timesteps=21000, episode_reward=4.30 +/- 2.53
Episode length: 111.72 +/- 37.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 4.3      |
| rollout/            |          |
|    exploration_rate | 0.948    |
| time/               |          |
|    total_timesteps  | 21000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 2749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 7.52     |
|    exploration_rate | 0.947    |
| time/               |          |
|    episodes         | 152      |
|    fps              | 156      |
|    time_elapsed     | 136      |
|    total_timesteps  | 21349    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0731   |
|    n_updates        | 2837     |
----------------------------------
Eval num_timesteps=21500, episode_reward=4.78 +/- 2.98
Episode length: 105.86 +/- 40.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 4.78     |
| rollout/            |          |
|    exploration_rate | 0.946    |
| time/               |          |
|    total_timesteps  | 21500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00138  |
|    n_updates        | 2874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 7.58     |
|    exploration_rate | 0.945    |
| time/               |          |
|    episodes         | 156      |
|    fps              | 157      |
|    time_elapsed     | 139      |
|    total_timesteps  | 21925    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 2981     |
----------------------------------
Eval num_timesteps=22000, episode_reward=3.40 +/- 2.31
Episode length: 97.70 +/- 32.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 97.7     |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.944    |
| time/               |          |
|    total_timesteps  | 22000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.027    |
|    n_updates        | 2999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 7.58     |
|    exploration_rate | 0.943    |
| time/               |          |
|    episodes         | 160      |
|    fps              | 156      |
|    time_elapsed     | 142      |
|    total_timesteps  | 22384    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0405   |
|    n_updates        | 3095     |
----------------------------------
Eval num_timesteps=22500, episode_reward=8.12 +/- 4.49
Episode length: 144.80 +/- 58.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 8.12     |
| rollout/            |          |
|    exploration_rate | 0.942    |
| time/               |          |
|    total_timesteps  | 22500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.022    |
|    n_updates        | 3124     |
----------------------------------
Eval num_timesteps=23000, episode_reward=3.40 +/- 2.47
Episode length: 99.42 +/- 36.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 99.4     |
|    mean_reward      | 3.4      |
| rollout/            |          |
|    exploration_rate | 0.941    |
| time/               |          |
|    total_timesteps  | 23000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0424   |
|    n_updates        | 3249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 7.82     |
|    exploration_rate | 0.94     |
| time/               |          |
|    episodes         | 164      |
|    fps              | 153      |
|    time_elapsed     | 150      |
|    total_timesteps  | 23081    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00746  |
|    n_updates        | 3270     |
----------------------------------
Eval num_timesteps=23500, episode_reward=14.32 +/- 6.55
Episode length: 172.14 +/- 63.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 172      |
|    mean_reward      | 14.3     |
| rollout/            |          |
|    exploration_rate | 0.939    |
| time/               |          |
|    total_timesteps  | 23500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 3374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 7.77     |
|    exploration_rate | 0.938    |
| time/               |          |
|    episodes         | 168      |
|    fps              | 151      |
|    time_elapsed     | 155      |
|    total_timesteps  | 23533    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00395  |
|    n_updates        | 3383     |
----------------------------------
Eval num_timesteps=24000, episode_reward=9.38 +/- 6.67
Episode length: 137.46 +/- 67.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 9.38     |
| rollout/            |          |
|    exploration_rate | 0.937    |
| time/               |          |
|    total_timesteps  | 24000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0115   |
|    n_updates        | 3499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 7.84     |
|    exploration_rate | 0.936    |
| time/               |          |
|    episodes         | 172      |
|    fps              | 150      |
|    time_elapsed     | 159      |
|    total_timesteps  | 24042    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0281   |
|    n_updates        | 3510     |
----------------------------------
Eval num_timesteps=24500, episode_reward=8.62 +/- 2.95
Episode length: 136.46 +/- 41.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 136      |
|    mean_reward      | 8.62     |
| rollout/            |          |
|    exploration_rate | 0.935    |
| time/               |          |
|    total_timesteps  | 24500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0498   |
|    n_updates        | 3624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 7.78     |
|    exploration_rate | 0.934    |
| time/               |          |
|    episodes         | 176      |
|    fps              | 150      |
|    time_elapsed     | 163      |
|    total_timesteps  | 24599    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0359   |
|    n_updates        | 3649     |
----------------------------------
Eval num_timesteps=25000, episode_reward=7.00 +/- 5.63
Episode length: 126.46 +/- 56.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 7        |
| rollout/            |          |
|    exploration_rate | 0.933    |
| time/               |          |
|    total_timesteps  | 25000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0381   |
|    n_updates        | 3749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 7.72     |
|    exploration_rate | 0.932    |
| time/               |          |
|    episodes         | 180      |
|    fps              | 149      |
|    time_elapsed     | 167      |
|    total_timesteps  | 25119    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0287   |
|    n_updates        | 3779     |
----------------------------------
Eval num_timesteps=25500, episode_reward=6.12 +/- 4.14
Episode length: 116.26 +/- 44.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 116      |
|    mean_reward      | 6.12     |
| rollout/            |          |
|    exploration_rate | 0.931    |
| time/               |          |
|    total_timesteps  | 25500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0446   |
|    n_updates        | 3874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 136      |
|    ep_rew_mean      | 7.64     |
|    exploration_rate | 0.93     |
| time/               |          |
|    episodes         | 184      |
|    fps              | 149      |
|    time_elapsed     | 171      |
|    total_timesteps  | 25604    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0407   |
|    n_updates        | 3900     |
----------------------------------
Eval num_timesteps=26000, episode_reward=12.56 +/- 6.15
Episode length: 168.74 +/- 66.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 169      |
|    mean_reward      | 12.6     |
| rollout/            |          |
|    exploration_rate | 0.929    |
| time/               |          |
|    total_timesteps  | 26000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0325   |
|    n_updates        | 3999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 135      |
|    ep_rew_mean      | 7.69     |
|    exploration_rate | 0.928    |
| time/               |          |
|    episodes         | 188      |
|    fps              | 148      |
|    time_elapsed     | 176      |
|    total_timesteps  | 26172    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0254   |
|    n_updates        | 4042     |
----------------------------------
Eval num_timesteps=26500, episode_reward=6.16 +/- 3.28
Episode length: 118.48 +/- 44.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 6.16     |
| rollout/            |          |
|    exploration_rate | 0.926    |
| time/               |          |
|    total_timesteps  | 26500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 4124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 7.81     |
|    exploration_rate | 0.926    |
| time/               |          |
|    episodes         | 192      |
|    fps              | 148      |
|    time_elapsed     | 180      |
|    total_timesteps  | 26718    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0782   |
|    n_updates        | 4179     |
----------------------------------
Eval num_timesteps=27000, episode_reward=7.64 +/- 3.34
Episode length: 126.46 +/- 49.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 7.64     |
| rollout/            |          |
|    exploration_rate | 0.924    |
| time/               |          |
|    total_timesteps  | 27000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0454   |
|    n_updates        | 4249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 137      |
|    ep_rew_mean      | 7.86     |
|    exploration_rate | 0.923    |
| time/               |          |
|    episodes         | 196      |
|    fps              | 148      |
|    time_elapsed     | 184      |
|    total_timesteps  | 27366    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00251  |
|    n_updates        | 4341     |
----------------------------------
Eval num_timesteps=27500, episode_reward=16.94 +/- 6.08
Episode length: 180.72 +/- 56.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 16.9     |
| rollout/            |          |
|    exploration_rate | 0.922    |
| time/               |          |
|    total_timesteps  | 27500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0396   |
|    n_updates        | 4374     |
----------------------------------
New best mean reward!
Eval num_timesteps=28000, episode_reward=9.90 +/- 4.80
Episode length: 144.64 +/- 48.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 9.9      |
| rollout/            |          |
|    exploration_rate | 0.92     |
| time/               |          |
|    total_timesteps  | 28000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 4499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 138      |
|    ep_rew_mean      | 7.9      |
|    exploration_rate | 0.92     |
| time/               |          |
|    episodes         | 200      |
|    fps              | 144      |
|    time_elapsed     | 193      |
|    total_timesteps  | 28008    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.025    |
|    n_updates        | 4501     |
----------------------------------
Eval num_timesteps=28500, episode_reward=10.26 +/- 4.04
Episode length: 138.00 +/- 48.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 138      |
|    mean_reward      | 10.3     |
| rollout/            |          |
|    exploration_rate | 0.918    |
| time/               |          |
|    total_timesteps  | 28500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0267   |
|    n_updates        | 4624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 8.02     |
|    exploration_rate | 0.917    |
| time/               |          |
|    episodes         | 204      |
|    fps              | 145      |
|    time_elapsed     | 198      |
|    total_timesteps  | 28782    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0605   |
|    n_updates        | 4695     |
----------------------------------
Eval num_timesteps=29000, episode_reward=14.78 +/- 4.09
Episode length: 165.66 +/- 44.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 166      |
|    mean_reward      | 14.8     |
| rollout/            |          |
|    exploration_rate | 0.916    |
| time/               |          |
|    total_timesteps  | 29000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0845   |
|    n_updates        | 4749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 8.15     |
|    exploration_rate | 0.914    |
| time/               |          |
|    episodes         | 208      |
|    fps              | 144      |
|    time_elapsed     | 203      |
|    total_timesteps  | 29453    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0319   |
|    n_updates        | 4863     |
----------------------------------
Eval num_timesteps=29500, episode_reward=14.06 +/- 5.21
Episode length: 159.00 +/- 51.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 159      |
|    mean_reward      | 14.1     |
| rollout/            |          |
|    exploration_rate | 0.914    |
| time/               |          |
|    total_timesteps  | 29500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00569  |
|    n_updates        | 4874     |
----------------------------------
Eval num_timesteps=30000, episode_reward=12.78 +/- 5.24
Episode length: 171.46 +/- 58.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 171      |
|    mean_reward      | 12.8     |
| rollout/            |          |
|    exploration_rate | 0.911    |
| time/               |          |
|    total_timesteps  | 30000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00304  |
|    n_updates        | 4999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 8.32     |
|    exploration_rate | 0.911    |
| time/               |          |
|    episodes         | 212      |
|    fps              | 141      |
|    time_elapsed     | 213      |
|    total_timesteps  | 30128    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0727   |
|    n_updates        | 5031     |
----------------------------------
Eval num_timesteps=30500, episode_reward=13.92 +/- 6.29
Episode length: 164.88 +/- 58.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 165      |
|    mean_reward      | 13.9     |
| rollout/            |          |
|    exploration_rate | 0.909    |
| time/               |          |
|    total_timesteps  | 30500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 5124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 8.36     |
|    exploration_rate | 0.909    |
| time/               |          |
|    episodes         | 216      |
|    fps              | 140      |
|    time_elapsed     | 218      |
|    total_timesteps  | 30660    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0502   |
|    n_updates        | 5164     |
----------------------------------
Eval num_timesteps=31000, episode_reward=13.64 +/- 4.37
Episode length: 163.78 +/- 46.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 164      |
|    mean_reward      | 13.6     |
| rollout/            |          |
|    exploration_rate | 0.907    |
| time/               |          |
|    total_timesteps  | 31000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0768   |
|    n_updates        | 5249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.37     |
|    exploration_rate | 0.906    |
| time/               |          |
|    episodes         | 220      |
|    fps              | 140      |
|    time_elapsed     | 223      |
|    total_timesteps  | 31252    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00435  |
|    n_updates        | 5312     |
----------------------------------
Eval num_timesteps=31500, episode_reward=4.96 +/- 2.51
Episode length: 113.94 +/- 36.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 114      |
|    mean_reward      | 4.96     |
| rollout/            |          |
|    exploration_rate | 0.905    |
| time/               |          |
|    total_timesteps  | 31500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0156   |
|    n_updates        | 5374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 8.31     |
|    exploration_rate | 0.904    |
| time/               |          |
|    episodes         | 224      |
|    fps              | 139      |
|    time_elapsed     | 226      |
|    total_timesteps  | 31677    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 5419     |
----------------------------------
Eval num_timesteps=32000, episode_reward=10.24 +/- 3.72
Episode length: 147.70 +/- 42.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 10.2     |
| rollout/            |          |
|    exploration_rate | 0.902    |
| time/               |          |
|    total_timesteps  | 32000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0627   |
|    n_updates        | 5499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.41     |
|    exploration_rate | 0.901    |
| time/               |          |
|    episodes         | 228      |
|    fps              | 139      |
|    time_elapsed     | 231      |
|    total_timesteps  | 32308    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0233   |
|    n_updates        | 5576     |
----------------------------------
Eval num_timesteps=32500, episode_reward=5.62 +/- 5.60
Episode length: 106.16 +/- 57.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 106      |
|    mean_reward      | 5.62     |
| rollout/            |          |
|    exploration_rate | 0.9      |
| time/               |          |
|    total_timesteps  | 32500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00578  |
|    n_updates        | 5624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.48     |
|    exploration_rate | 0.899    |
| time/               |          |
|    episodes         | 232      |
|    fps              | 139      |
|    time_elapsed     | 234      |
|    total_timesteps  | 32793    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0284   |
|    n_updates        | 5698     |
----------------------------------
Eval num_timesteps=33000, episode_reward=17.54 +/- 5.04
Episode length: 184.30 +/- 59.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 184      |
|    mean_reward      | 17.5     |
| rollout/            |          |
|    exploration_rate | 0.898    |
| time/               |          |
|    total_timesteps  | 33000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0168   |
|    n_updates        | 5749     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 8.77     |
|    exploration_rate | 0.896    |
| time/               |          |
|    episodes         | 236      |
|    fps              | 139      |
|    time_elapsed     | 240      |
|    total_timesteps  | 33390    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00554  |
|    n_updates        | 5847     |
----------------------------------
Eval num_timesteps=33500, episode_reward=2.92 +/- 2.00
Episode length: 93.80 +/- 31.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 93.8     |
|    mean_reward      | 2.92     |
| rollout/            |          |
|    exploration_rate | 0.896    |
| time/               |          |
|    total_timesteps  | 33500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 5874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 8.9      |
|    exploration_rate | 0.893    |
| time/               |          |
|    episodes         | 240      |
|    fps              | 139      |
|    time_elapsed     | 243      |
|    total_timesteps  | 33943    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00567  |
|    n_updates        | 5985     |
----------------------------------
Eval num_timesteps=34000, episode_reward=6.50 +/- 4.36
Episode length: 125.28 +/- 45.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 125      |
|    mean_reward      | 6.5      |
| rollout/            |          |
|    exploration_rate | 0.893    |
| time/               |          |
|    total_timesteps  | 34000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00959  |
|    n_updates        | 5999     |
----------------------------------
Eval num_timesteps=34500, episode_reward=8.18 +/- 3.04
Episode length: 131.62 +/- 47.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 8.18     |
| rollout/            |          |
|    exploration_rate | 0.891    |
| time/               |          |
|    total_timesteps  | 34500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 6124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 146      |
|    ep_rew_mean      | 9.02     |
|    exploration_rate | 0.89     |
| time/               |          |
|    episodes         | 244      |
|    fps              | 137      |
|    time_elapsed     | 250      |
|    total_timesteps  | 34578    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0337   |
|    n_updates        | 6144     |
----------------------------------
Eval num_timesteps=35000, episode_reward=16.18 +/- 4.56
Episode length: 167.94 +/- 49.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 168      |
|    mean_reward      | 16.2     |
| rollout/            |          |
|    exploration_rate | 0.888    |
| time/               |          |
|    total_timesteps  | 35000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.038    |
|    n_updates        | 6249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 8.99     |
|    exploration_rate | 0.887    |
| time/               |          |
|    episodes         | 248      |
|    fps              | 137      |
|    time_elapsed     | 255      |
|    total_timesteps  | 35205    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 6301     |
----------------------------------
Eval num_timesteps=35500, episode_reward=16.38 +/- 5.28
Episode length: 174.22 +/- 58.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 174      |
|    mean_reward      | 16.4     |
| rollout/            |          |
|    exploration_rate | 0.886    |
| time/               |          |
|    total_timesteps  | 35500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00815  |
|    n_updates        | 6374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 8.92     |
|    exploration_rate | 0.885    |
| time/               |          |
|    episodes         | 252      |
|    fps              | 136      |
|    time_elapsed     | 261      |
|    total_timesteps  | 35646    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0282   |
|    n_updates        | 6411     |
----------------------------------
Eval num_timesteps=36000, episode_reward=18.22 +/- 5.14
Episode length: 190.26 +/- 56.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 18.2     |
| rollout/            |          |
|    exploration_rate | 0.884    |
| time/               |          |
|    total_timesteps  | 36000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0123   |
|    n_updates        | 6499     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.94     |
|    exploration_rate | 0.883    |
| time/               |          |
|    episodes         | 256      |
|    fps              | 135      |
|    time_elapsed     | 266      |
|    total_timesteps  | 36153    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 6538     |
----------------------------------
Eval num_timesteps=36500, episode_reward=15.70 +/- 5.29
Episode length: 172.22 +/- 51.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 172      |
|    mean_reward      | 15.7     |
| rollout/            |          |
|    exploration_rate | 0.881    |
| time/               |          |
|    total_timesteps  | 36500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.026    |
|    n_updates        | 6624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 9.03     |
|    exploration_rate | 0.88     |
| time/               |          |
|    episodes         | 260      |
|    fps              | 135      |
|    time_elapsed     | 272      |
|    total_timesteps  | 36784    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0447   |
|    n_updates        | 6695     |
----------------------------------
Eval num_timesteps=37000, episode_reward=18.08 +/- 5.84
Episode length: 190.50 +/- 62.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 18.1     |
| rollout/            |          |
|    exploration_rate | 0.879    |
| time/               |          |
|    total_timesteps  | 37000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 6749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 8.89     |
|    exploration_rate | 0.877    |
| time/               |          |
|    episodes         | 264      |
|    fps              | 134      |
|    time_elapsed     | 277      |
|    total_timesteps  | 37287    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00854  |
|    n_updates        | 6821     |
----------------------------------
Eval num_timesteps=37500, episode_reward=18.96 +/- 4.64
Episode length: 195.22 +/- 54.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 195      |
|    mean_reward      | 19       |
| rollout/            |          |
|    exploration_rate | 0.876    |
| time/               |          |
|    total_timesteps  | 37500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0459   |
|    n_updates        | 6874     |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 9.02     |
|    exploration_rate | 0.875    |
| time/               |          |
|    episodes         | 268      |
|    fps              | 133      |
|    time_elapsed     | 283      |
|    total_timesteps  | 37836    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0377   |
|    n_updates        | 6958     |
----------------------------------
Eval num_timesteps=38000, episode_reward=15.46 +/- 5.15
Episode length: 166.40 +/- 53.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 166      |
|    mean_reward      | 15.5     |
| rollout/            |          |
|    exploration_rate | 0.874    |
| time/               |          |
|    total_timesteps  | 38000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0515   |
|    n_updates        | 6999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 9.06     |
|    exploration_rate | 0.872    |
| time/               |          |
|    episodes         | 272      |
|    fps              | 132      |
|    time_elapsed     | 289      |
|    total_timesteps  | 38433    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0469   |
|    n_updates        | 7108     |
----------------------------------
Eval num_timesteps=38500, episode_reward=16.82 +/- 4.35
Episode length: 175.46 +/- 44.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 175      |
|    mean_reward      | 16.8     |
| rollout/            |          |
|    exploration_rate | 0.871    |
| time/               |          |
|    total_timesteps  | 38500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0266   |
|    n_updates        | 7124     |
----------------------------------
Eval num_timesteps=39000, episode_reward=15.96 +/- 3.63
Episode length: 176.52 +/- 48.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 177      |
|    mean_reward      | 16       |
| rollout/            |          |
|    exploration_rate | 0.869    |
| time/               |          |
|    total_timesteps  | 39000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0234   |
|    n_updates        | 7249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 9.17     |
|    exploration_rate | 0.868    |
| time/               |          |
|    episodes         | 276      |
|    fps              | 130      |
|    time_elapsed     | 299      |
|    total_timesteps  | 39072    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 7267     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 9.16     |
|    exploration_rate | 0.866    |
| time/               |          |
|    episodes         | 280      |
|    fps              | 131      |
|    time_elapsed     | 299      |
|    total_timesteps  | 39482    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0195   |
|    n_updates        | 7370     |
----------------------------------
Eval num_timesteps=39500, episode_reward=6.02 +/- 4.62
Episode length: 111.50 +/- 46.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 6.02     |
| rollout/            |          |
|    exploration_rate | 0.866    |
| time/               |          |
|    total_timesteps  | 39500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 7374     |
----------------------------------
Eval num_timesteps=40000, episode_reward=7.22 +/- 4.85
Episode length: 125.76 +/- 53.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 126      |
|    mean_reward      | 7.22     |
| rollout/            |          |
|    exploration_rate | 0.864    |
| time/               |          |
|    total_timesteps  | 40000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0119   |
|    n_updates        | 7499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 9.3      |
|    exploration_rate | 0.863    |
| time/               |          |
|    episodes         | 284      |
|    fps              | 130      |
|    time_elapsed     | 307      |
|    total_timesteps  | 40140    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0187   |
|    n_updates        | 7534     |
----------------------------------
Eval num_timesteps=40500, episode_reward=14.48 +/- 5.85
Episode length: 172.52 +/- 60.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 173      |
|    mean_reward      | 14.5     |
| rollout/            |          |
|    exploration_rate | 0.861    |
| time/               |          |
|    total_timesteps  | 40500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0417   |
|    n_updates        | 7624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 9.28     |
|    exploration_rate | 0.86     |
| time/               |          |
|    episodes         | 288      |
|    fps              | 130      |
|    time_elapsed     | 312      |
|    total_timesteps  | 40635    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0654   |
|    n_updates        | 7658     |
----------------------------------
Eval num_timesteps=41000, episode_reward=17.84 +/- 4.80
Episode length: 181.28 +/- 52.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 17.8     |
| rollout/            |          |
|    exploration_rate | 0.859    |
| time/               |          |
|    total_timesteps  | 41000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0493   |
|    n_updates        | 7749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 9.35     |
|    exploration_rate | 0.858    |
| time/               |          |
|    episodes         | 292      |
|    fps              | 129      |
|    time_elapsed     | 317      |
|    total_timesteps  | 41188    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0921   |
|    n_updates        | 7796     |
----------------------------------
Eval num_timesteps=41500, episode_reward=16.14 +/- 5.31
Episode length: 180.96 +/- 52.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 16.1     |
| rollout/            |          |
|    exploration_rate | 0.856    |
| time/               |          |
|    total_timesteps  | 41500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0205   |
|    n_updates        | 7874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 143      |
|    ep_rew_mean      | 9.35     |
|    exploration_rate | 0.855    |
| time/               |          |
|    episodes         | 296      |
|    fps              | 128      |
|    time_elapsed     | 323      |
|    total_timesteps  | 41634    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0252   |
|    n_updates        | 7908     |
----------------------------------
Eval num_timesteps=42000, episode_reward=14.26 +/- 5.66
Episode length: 167.44 +/- 57.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 167      |
|    mean_reward      | 14.3     |
| rollout/            |          |
|    exploration_rate | 0.853    |
| time/               |          |
|    total_timesteps  | 42000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 7999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 142      |
|    ep_rew_mean      | 9.37     |
|    exploration_rate | 0.852    |
| time/               |          |
|    episodes         | 300      |
|    fps              | 128      |
|    time_elapsed     | 328      |
|    total_timesteps  | 42246    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0242   |
|    n_updates        | 8061     |
----------------------------------
Eval num_timesteps=42500, episode_reward=14.56 +/- 4.36
Episode length: 167.90 +/- 46.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 168      |
|    mean_reward      | 14.6     |
| rollout/            |          |
|    exploration_rate | 0.851    |
| time/               |          |
|    total_timesteps  | 42500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 8124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 9.36     |
|    exploration_rate | 0.849    |
| time/               |          |
|    episodes         | 304      |
|    fps              | 128      |
|    time_elapsed     | 333      |
|    total_timesteps  | 42889    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0331   |
|    n_updates        | 8222     |
----------------------------------
Eval num_timesteps=43000, episode_reward=16.00 +/- 5.41
Episode length: 178.62 +/- 52.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 179      |
|    mean_reward      | 16       |
| rollout/            |          |
|    exploration_rate | 0.848    |
| time/               |          |
|    total_timesteps  | 43000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0401   |
|    n_updates        | 8249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 140      |
|    ep_rew_mean      | 9.25     |
|    exploration_rate | 0.846    |
| time/               |          |
|    episodes         | 308      |
|    fps              | 128      |
|    time_elapsed     | 339      |
|    total_timesteps  | 43430    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.015    |
|    n_updates        | 8357     |
----------------------------------
Eval num_timesteps=43500, episode_reward=14.08 +/- 3.75
Episode length: 172.56 +/- 47.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 173      |
|    mean_reward      | 14.1     |
| rollout/            |          |
|    exploration_rate | 0.845    |
| time/               |          |
|    total_timesteps  | 43500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0237   |
|    n_updates        | 8374     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 9.17     |
|    exploration_rate | 0.843    |
| time/               |          |
|    episodes         | 312      |
|    fps              | 127      |
|    time_elapsed     | 344      |
|    total_timesteps  | 43999    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0369   |
|    n_updates        | 8499     |
----------------------------------
Eval num_timesteps=44000, episode_reward=5.96 +/- 3.65
Episode length: 115.52 +/- 39.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 116      |
|    mean_reward     | 5.96     |
| time/              |          |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=44500, episode_reward=4.52 +/- 3.39
Episode length: 105.10 +/- 40.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 105      |
|    mean_reward      | 4.52     |
| rollout/            |          |
|    exploration_rate | 0.84     |
| time/               |          |
|    total_timesteps  | 44500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0323   |
|    n_updates        | 8624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 139      |
|    ep_rew_mean      | 9.26     |
|    exploration_rate | 0.84     |
| time/               |          |
|    episodes         | 316      |
|    fps              | 127      |
|    time_elapsed     | 351      |
|    total_timesteps  | 44598    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0153   |
|    n_updates        | 8649     |
----------------------------------
Eval num_timesteps=45000, episode_reward=16.60 +/- 4.28
Episode length: 173.64 +/- 45.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 174      |
|    mean_reward      | 16.6     |
| rollout/            |          |
|    exploration_rate | 0.837    |
| time/               |          |
|    total_timesteps  | 45000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 8749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 141      |
|    ep_rew_mean      | 9.39     |
|    exploration_rate | 0.835    |
| time/               |          |
|    episodes         | 320      |
|    fps              | 127      |
|    time_elapsed     | 356      |
|    total_timesteps  | 45349    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00594  |
|    n_updates        | 8837     |
----------------------------------
Eval num_timesteps=45500, episode_reward=18.34 +/- 4.84
Episode length: 180.56 +/- 52.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 18.3     |
| rollout/            |          |
|    exploration_rate | 0.835    |
| time/               |          |
|    total_timesteps  | 45500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0417   |
|    n_updates        | 8874     |
----------------------------------
Eval num_timesteps=46000, episode_reward=8.42 +/- 5.51
Episode length: 132.22 +/- 50.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 8.42     |
| rollout/            |          |
|    exploration_rate | 0.832    |
| time/               |          |
|    total_timesteps  | 46000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0542   |
|    n_updates        | 8999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 9.81     |
|    exploration_rate | 0.831    |
| time/               |          |
|    episodes         | 324      |
|    fps              | 125      |
|    time_elapsed     | 365      |
|    total_timesteps  | 46084    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 9020     |
----------------------------------
Eval num_timesteps=46500, episode_reward=12.34 +/- 3.94
Episode length: 154.36 +/- 40.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 154      |
|    mean_reward      | 12.3     |
| rollout/            |          |
|    exploration_rate | 0.829    |
| time/               |          |
|    total_timesteps  | 46500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0275   |
|    n_updates        | 9124     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 9.88     |
|    exploration_rate | 0.827    |
| time/               |          |
|    episodes         | 328      |
|    fps              | 126      |
|    time_elapsed     | 370      |
|    total_timesteps  | 46855    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.017    |
|    n_updates        | 9213     |
----------------------------------
Eval num_timesteps=47000, episode_reward=5.52 +/- 3.22
Episode length: 114.90 +/- 38.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 115      |
|    mean_reward      | 5.52     |
| rollout/            |          |
|    exploration_rate | 0.826    |
| time/               |          |
|    total_timesteps  | 47000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 9249     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 146      |
|    ep_rew_mean      | 9.96     |
|    exploration_rate | 0.824    |
| time/               |          |
|    episodes         | 332      |
|    fps              | 126      |
|    time_elapsed     | 374      |
|    total_timesteps  | 47377    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0355   |
|    n_updates        | 9344     |
----------------------------------
Eval num_timesteps=47500, episode_reward=17.10 +/- 4.45
Episode length: 169.92 +/- 45.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 17.1     |
| rollout/            |          |
|    exploration_rate | 0.824    |
| time/               |          |
|    total_timesteps  | 47500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0313   |
|    n_updates        | 9374     |
----------------------------------
Eval num_timesteps=48000, episode_reward=6.64 +/- 5.16
Episode length: 112.30 +/- 52.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 112      |
|    mean_reward      | 6.64     |
| rollout/            |          |
|    exploration_rate | 0.821    |
| time/               |          |
|    total_timesteps  | 48000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0308   |
|    n_updates        | 9499     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 146      |
|    ep_rew_mean      | 9.96     |
|    exploration_rate | 0.821    |
| time/               |          |
|    episodes         | 336      |
|    fps              | 125      |
|    time_elapsed     | 382      |
|    total_timesteps  | 48004    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.101    |
|    n_updates        | 9500     |
----------------------------------
Eval num_timesteps=48500, episode_reward=16.20 +/- 4.15
Episode length: 167.86 +/- 44.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 168      |
|    mean_reward      | 16.2     |
| rollout/            |          |
|    exploration_rate | 0.818    |
| time/               |          |
|    total_timesteps  | 48500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 9624     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 147      |
|    ep_rew_mean      | 9.94     |
|    exploration_rate | 0.817    |
| time/               |          |
|    episodes         | 340      |
|    fps              | 125      |
|    time_elapsed     | 388      |
|    total_timesteps  | 48599    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0856   |
|    n_updates        | 9649     |
----------------------------------
Eval num_timesteps=49000, episode_reward=17.06 +/- 5.04
Episode length: 175.50 +/- 50.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 176      |
|    mean_reward      | 17.1     |
| rollout/            |          |
|    exploration_rate | 0.815    |
| time/               |          |
|    total_timesteps  | 49000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 9749     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 144      |
|    ep_rew_mean      | 9.8      |
|    exploration_rate | 0.815    |
| time/               |          |
|    episodes         | 344      |
|    fps              | 124      |
|    time_elapsed     | 393      |
|    total_timesteps  | 49023    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0221   |
|    n_updates        | 9755     |
----------------------------------
Eval num_timesteps=49500, episode_reward=15.46 +/- 4.49
Episode length: 164.66 +/- 46.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 165      |
|    mean_reward      | 15.5     |
| rollout/            |          |
|    exploration_rate | 0.812    |
| time/               |          |
|    total_timesteps  | 49500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0365   |
|    n_updates        | 9874     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 145      |
|    ep_rew_mean      | 9.84     |
|    exploration_rate | 0.811    |
| time/               |          |
|    episodes         | 348      |
|    fps              | 124      |
|    time_elapsed     | 398      |
|    total_timesteps  | 49717    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0033   |
|    n_updates        | 9929     |
----------------------------------
Eval num_timesteps=50000, episode_reward=10.06 +/- 6.00
Episode length: 150.20 +/- 64.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 10.1     |
| rollout/            |          |
|    exploration_rate | 0.809    |
| time/               |          |
|    total_timesteps  | 50000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0348   |
|    n_updates        | 9999     |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 147      |
|    ep_rew_mean      | 10       |
|    exploration_rate | 0.807    |
| time/               |          |
|    episodes         | 352      |
|    fps              | 124      |
|    time_elapsed     | 403      |
|    total_timesteps  | 50349    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0412   |
|    n_updates        | 10087    |
----------------------------------
Eval num_timesteps=50500, episode_reward=7.98 +/- 4.71
Episode length: 131.42 +/- 53.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 131      |
|    mean_reward      | 7.98     |
| rollout/            |          |
|    exploration_rate | 0.807    |
| time/               |          |
|    total_timesteps  | 50500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0381   |
|    n_updates        | 10124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 147      |
|    ep_rew_mean      | 10.1     |
|    exploration_rate | 0.804    |
| time/               |          |
|    episodes         | 356      |
|    fps              | 124      |
|    time_elapsed     | 407      |
|    total_timesteps  | 50896    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.043    |
|    n_updates        | 10223    |
----------------------------------
Eval num_timesteps=51000, episode_reward=15.68 +/- 4.28
Episode length: 170.48 +/- 48.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 15.7     |
| rollout/            |          |
|    exploration_rate | 0.804    |
| time/               |          |
|    total_timesteps  | 51000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 10249    |
----------------------------------
Eval num_timesteps=51500, episode_reward=17.28 +/- 4.75
Episode length: 181.14 +/- 50.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 17.3     |
| rollout/            |          |
|    exploration_rate | 0.801    |
| time/               |          |
|    total_timesteps  | 51500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.018    |
|    n_updates        | 10374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 148      |
|    ep_rew_mean      | 10.2     |
|    exploration_rate | 0.8      |
| time/               |          |
|    episodes         | 360      |
|    fps              | 123      |
|    time_elapsed     | 417      |
|    total_timesteps  | 51581    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0225   |
|    n_updates        | 10395    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 147      |
|    ep_rew_mean      | 10.1     |
|    exploration_rate | 0.798    |
| time/               |          |
|    episodes         | 364      |
|    fps              | 124      |
|    time_elapsed     | 418      |
|    total_timesteps  | 51959    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0184   |
|    n_updates        | 10489    |
----------------------------------
Eval num_timesteps=52000, episode_reward=6.76 +/- 3.12
Episode length: 140.14 +/- 43.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 6.76     |
| rollout/            |          |
|    exploration_rate | 0.798    |
| time/               |          |
|    total_timesteps  | 52000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0144   |
|    n_updates        | 10499    |
----------------------------------
Eval num_timesteps=52500, episode_reward=17.48 +/- 4.83
Episode length: 171.18 +/- 47.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 171      |
|    mean_reward      | 17.5     |
| rollout/            |          |
|    exploration_rate | 0.795    |
| time/               |          |
|    total_timesteps  | 52500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 10624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 149      |
|    ep_rew_mean      | 10.2     |
|    exploration_rate | 0.794    |
| time/               |          |
|    episodes         | 368      |
|    fps              | 123      |
|    time_elapsed     | 427      |
|    total_timesteps  | 52706    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00652  |
|    n_updates        | 10676    |
----------------------------------
Eval num_timesteps=53000, episode_reward=19.48 +/- 4.81
Episode length: 186.42 +/- 50.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 186      |
|    mean_reward      | 19.5     |
| rollout/            |          |
|    exploration_rate | 0.792    |
| time/               |          |
|    total_timesteps  | 53000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0277   |
|    n_updates        | 10749    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 149      |
|    ep_rew_mean      | 10.2     |
|    exploration_rate | 0.79     |
| time/               |          |
|    episodes         | 372      |
|    fps              | 123      |
|    time_elapsed     | 433      |
|    total_timesteps  | 53359    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.021    |
|    n_updates        | 10839    |
----------------------------------
Eval num_timesteps=53500, episode_reward=17.30 +/- 5.16
Episode length: 173.68 +/- 54.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 174      |
|    mean_reward      | 17.3     |
| rollout/            |          |
|    exploration_rate | 0.789    |
| time/               |          |
|    total_timesteps  | 53500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 10874    |
----------------------------------
Eval num_timesteps=54000, episode_reward=11.64 +/- 4.92
Episode length: 170.00 +/- 58.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 11.6     |
| rollout/            |          |
|    exploration_rate | 0.786    |
| time/               |          |
|    total_timesteps  | 54000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0288   |
|    n_updates        | 10999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 150      |
|    ep_rew_mean      | 10.5     |
|    exploration_rate | 0.786    |
| time/               |          |
|    episodes         | 376      |
|    fps              | 122      |
|    time_elapsed     | 443      |
|    total_timesteps  | 54101    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.021    |
|    n_updates        | 11025    |
----------------------------------
Eval num_timesteps=54500, episode_reward=14.78 +/- 4.82
Episode length: 167.94 +/- 48.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 168      |
|    mean_reward      | 14.8     |
| rollout/            |          |
|    exploration_rate | 0.783    |
| time/               |          |
|    total_timesteps  | 54500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0279   |
|    n_updates        | 11124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 150      |
|    ep_rew_mean      | 10.5     |
|    exploration_rate | 0.783    |
| time/               |          |
|    episodes         | 380      |
|    fps              | 121      |
|    time_elapsed     | 448      |
|    total_timesteps  | 54524    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0102   |
|    n_updates        | 11130    |
----------------------------------
Eval num_timesteps=55000, episode_reward=13.30 +/- 5.49
Episode length: 160.70 +/- 53.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 161      |
|    mean_reward      | 13.3     |
| rollout/            |          |
|    exploration_rate | 0.78     |
| time/               |          |
|    total_timesteps  | 55000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 11249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 151      |
|    ep_rew_mean      | 10.7     |
|    exploration_rate | 0.778    |
| time/               |          |
|    episodes         | 384      |
|    fps              | 121      |
|    time_elapsed     | 453      |
|    total_timesteps  | 55285    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0189   |
|    n_updates        | 11321    |
----------------------------------
Eval num_timesteps=55500, episode_reward=14.10 +/- 3.73
Episode length: 170.10 +/- 44.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 14.1     |
| rollout/            |          |
|    exploration_rate | 0.777    |
| time/               |          |
|    total_timesteps  | 55500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0267   |
|    n_updates        | 11374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 152      |
|    ep_rew_mean      | 10.7     |
|    exploration_rate | 0.775    |
| time/               |          |
|    episodes         | 388      |
|    fps              | 121      |
|    time_elapsed     | 458      |
|    total_timesteps  | 55883    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00951  |
|    n_updates        | 11470    |
----------------------------------
Eval num_timesteps=56000, episode_reward=10.26 +/- 3.79
Episode length: 160.00 +/- 48.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 160      |
|    mean_reward      | 10.3     |
| rollout/            |          |
|    exploration_rate | 0.774    |
| time/               |          |
|    total_timesteps  | 56000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0475   |
|    n_updates        | 11499    |
----------------------------------
Eval num_timesteps=56500, episode_reward=17.40 +/- 5.57
Episode length: 179.14 +/- 55.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 179      |
|    mean_reward      | 17.4     |
| rollout/            |          |
|    exploration_rate | 0.771    |
| time/               |          |
|    total_timesteps  | 56500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0334   |
|    n_updates        | 11624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 156      |
|    ep_rew_mean      | 11       |
|    exploration_rate | 0.769    |
| time/               |          |
|    episodes         | 392      |
|    fps              | 121      |
|    time_elapsed     | 469      |
|    total_timesteps  | 56772    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0346   |
|    n_updates        | 11692    |
----------------------------------
Eval num_timesteps=57000, episode_reward=9.26 +/- 4.45
Episode length: 131.66 +/- 41.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 132      |
|    mean_reward      | 9.26     |
| rollout/            |          |
|    exploration_rate | 0.768    |
| time/               |          |
|    total_timesteps  | 57000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0394   |
|    n_updates        | 11749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 158      |
|    ep_rew_mean      | 11.1     |
|    exploration_rate | 0.765    |
| time/               |          |
|    episodes         | 396      |
|    fps              | 121      |
|    time_elapsed     | 473      |
|    total_timesteps  | 57428    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0192   |
|    n_updates        | 11856    |
----------------------------------
Eval num_timesteps=57500, episode_reward=17.96 +/- 4.83
Episode length: 178.94 +/- 50.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 179      |
|    mean_reward      | 18       |
| rollout/            |          |
|    exploration_rate | 0.765    |
| time/               |          |
|    total_timesteps  | 57500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0158   |
|    n_updates        | 11874    |
----------------------------------
Eval num_timesteps=58000, episode_reward=20.46 +/- 6.45
Episode length: 195.94 +/- 61.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 196      |
|    mean_reward      | 20.5     |
| rollout/            |          |
|    exploration_rate | 0.762    |
| time/               |          |
|    total_timesteps  | 58000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0319   |
|    n_updates        | 11999    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 158      |
|    ep_rew_mean      | 11.2     |
|    exploration_rate | 0.761    |
| time/               |          |
|    episodes         | 400      |
|    fps              | 119      |
|    time_elapsed     | 484      |
|    total_timesteps  | 58091    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0492   |
|    n_updates        | 12022    |
----------------------------------
Eval num_timesteps=58500, episode_reward=20.32 +/- 4.87
Episode length: 191.42 +/- 45.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 191      |
|    mean_reward      | 20.3     |
| rollout/            |          |
|    exploration_rate | 0.759    |
| time/               |          |
|    total_timesteps  | 58500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0212   |
|    n_updates        | 12124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 157      |
|    ep_rew_mean      | 11.2     |
|    exploration_rate | 0.759    |
| time/               |          |
|    episodes         | 404      |
|    fps              | 119      |
|    time_elapsed     | 490      |
|    total_timesteps  | 58548    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0451   |
|    n_updates        | 12136    |
----------------------------------
Eval num_timesteps=59000, episode_reward=11.22 +/- 3.64
Episode length: 150.72 +/- 44.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 151      |
|    mean_reward      | 11.2     |
| rollout/            |          |
|    exploration_rate | 0.756    |
| time/               |          |
|    total_timesteps  | 59000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0467   |
|    n_updates        | 12249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 157      |
|    ep_rew_mean      | 11.2     |
|    exploration_rate | 0.755    |
| time/               |          |
|    episodes         | 408      |
|    fps              | 119      |
|    time_elapsed     | 494      |
|    total_timesteps  | 59135    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0186   |
|    n_updates        | 12283    |
----------------------------------
Eval num_timesteps=59500, episode_reward=11.58 +/- 4.54
Episode length: 148.16 +/- 43.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 148      |
|    mean_reward      | 11.6     |
| rollout/            |          |
|    exploration_rate | 0.753    |
| time/               |          |
|    total_timesteps  | 59500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0218   |
|    n_updates        | 12374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 158      |
|    ep_rew_mean      | 11.4     |
|    exploration_rate | 0.751    |
| time/               |          |
|    episodes         | 412      |
|    fps              | 119      |
|    time_elapsed     | 499      |
|    total_timesteps  | 59813    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0549   |
|    n_updates        | 12453    |
----------------------------------
Eval num_timesteps=60000, episode_reward=20.20 +/- 7.05
Episode length: 200.18 +/- 68.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 200      |
|    mean_reward      | 20.2     |
| rollout/            |          |
|    exploration_rate | 0.75     |
| time/               |          |
|    total_timesteps  | 60000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0352   |
|    n_updates        | 12499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 158      |
|    ep_rew_mean      | 11.5     |
|    exploration_rate | 0.747    |
| time/               |          |
|    episodes         | 416      |
|    fps              | 119      |
|    time_elapsed     | 505      |
|    total_timesteps  | 60398    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 12599    |
----------------------------------
Eval num_timesteps=60500, episode_reward=13.42 +/- 5.94
Episode length: 160.60 +/- 57.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 161      |
|    mean_reward      | 13.4     |
| rollout/            |          |
|    exploration_rate | 0.746    |
| time/               |          |
|    total_timesteps  | 60500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0641   |
|    n_updates        | 12624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 155      |
|    ep_rew_mean      | 11.3     |
|    exploration_rate | 0.744    |
| time/               |          |
|    episodes         | 420      |
|    fps              | 119      |
|    time_elapsed     | 510      |
|    total_timesteps  | 60830    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 12707    |
----------------------------------
Eval num_timesteps=61000, episode_reward=9.00 +/- 3.67
Episode length: 137.36 +/- 40.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 137      |
|    mean_reward      | 9        |
| rollout/            |          |
|    exploration_rate | 0.743    |
| time/               |          |
|    total_timesteps  | 61000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0383   |
|    n_updates        | 12749    |
----------------------------------
Eval num_timesteps=61500, episode_reward=21.86 +/- 5.83
Episode length: 203.98 +/- 59.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 21.9     |
| rollout/            |          |
|    exploration_rate | 0.74     |
| time/               |          |
|    total_timesteps  | 61500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0272   |
|    n_updates        | 12874    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 156      |
|    ep_rew_mean      | 11.2     |
|    exploration_rate | 0.739    |
| time/               |          |
|    episodes         | 424      |
|    fps              | 118      |
|    time_elapsed     | 520      |
|    total_timesteps  | 61688    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.018    |
|    n_updates        | 12921    |
----------------------------------
Eval num_timesteps=62000, episode_reward=20.62 +/- 4.74
Episode length: 193.14 +/- 48.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 193      |
|    mean_reward      | 20.6     |
| rollout/            |          |
|    exploration_rate | 0.737    |
| time/               |          |
|    total_timesteps  | 62000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 12999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 156      |
|    ep_rew_mean      | 11.2     |
|    exploration_rate | 0.734    |
| time/               |          |
|    episodes         | 428      |
|    fps              | 118      |
|    time_elapsed     | 526      |
|    total_timesteps  | 62412    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0175   |
|    n_updates        | 13102    |
----------------------------------
Eval num_timesteps=62500, episode_reward=18.06 +/- 5.03
Episode length: 196.94 +/- 60.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 197      |
|    mean_reward      | 18.1     |
| rollout/            |          |
|    exploration_rate | 0.734    |
| time/               |          |
|    total_timesteps  | 62500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.035    |
|    n_updates        | 13124    |
----------------------------------
Eval num_timesteps=63000, episode_reward=12.98 +/- 4.88
Episode length: 160.32 +/- 48.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 160      |
|    mean_reward      | 13       |
| rollout/            |          |
|    exploration_rate | 0.731    |
| time/               |          |
|    total_timesteps  | 63000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0366   |
|    n_updates        | 13249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 157      |
|    ep_rew_mean      | 11.2     |
|    exploration_rate | 0.73     |
| time/               |          |
|    episodes         | 432      |
|    fps              | 117      |
|    time_elapsed     | 537      |
|    total_timesteps  | 63035    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0184   |
|    n_updates        | 13258    |
----------------------------------
Eval num_timesteps=63500, episode_reward=10.52 +/- 5.12
Episode length: 150.46 +/- 52.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 150      |
|    mean_reward      | 10.5     |
| rollout/            |          |
|    exploration_rate | 0.727    |
| time/               |          |
|    total_timesteps  | 63500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0239   |
|    n_updates        | 13374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 158      |
|    ep_rew_mean      | 11.3     |
|    exploration_rate | 0.726    |
| time/               |          |
|    episodes         | 436      |
|    fps              | 117      |
|    time_elapsed     | 542      |
|    total_timesteps  | 63772    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0156   |
|    n_updates        | 13442    |
----------------------------------
Eval num_timesteps=64000, episode_reward=13.22 +/- 5.45
Episode length: 174.60 +/- 60.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 175      |
|    mean_reward      | 13.2     |
| rollout/            |          |
|    exploration_rate | 0.724    |
| time/               |          |
|    total_timesteps  | 64000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0127   |
|    n_updates        | 13499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 158      |
|    ep_rew_mean      | 11.4     |
|    exploration_rate | 0.722    |
| time/               |          |
|    episodes         | 440      |
|    fps              | 117      |
|    time_elapsed     | 547      |
|    total_timesteps  | 64386    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0216   |
|    n_updates        | 13596    |
----------------------------------
Eval num_timesteps=64500, episode_reward=6.98 +/- 5.17
Episode length: 118.42 +/- 51.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 118      |
|    mean_reward      | 6.98     |
| rollout/            |          |
|    exploration_rate | 0.721    |
| time/               |          |
|    total_timesteps  | 64500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0145   |
|    n_updates        | 13624    |
----------------------------------
Eval num_timesteps=65000, episode_reward=18.42 +/- 6.35
Episode length: 199.18 +/- 69.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 199      |
|    mean_reward      | 18.4     |
| rollout/            |          |
|    exploration_rate | 0.718    |
| time/               |          |
|    total_timesteps  | 65000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0202   |
|    n_updates        | 13749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 162      |
|    ep_rew_mean      | 11.8     |
|    exploration_rate | 0.716    |
| time/               |          |
|    episodes         | 444      |
|    fps              | 117      |
|    time_elapsed     | 557      |
|    total_timesteps  | 65210    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0324   |
|    n_updates        | 13802    |
----------------------------------
Eval num_timesteps=65500, episode_reward=18.68 +/- 6.03
Episode length: 191.22 +/- 63.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 191      |
|    mean_reward      | 18.7     |
| rollout/            |          |
|    exploration_rate | 0.714    |
| time/               |          |
|    total_timesteps  | 65500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.02     |
|    n_updates        | 13874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 161      |
|    ep_rew_mean      | 11.8     |
|    exploration_rate | 0.712    |
| time/               |          |
|    episodes         | 448      |
|    fps              | 116      |
|    time_elapsed     | 563      |
|    total_timesteps  | 65835    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0427   |
|    n_updates        | 13958    |
----------------------------------
Eval num_timesteps=66000, episode_reward=21.48 +/- 5.46
Episode length: 206.82 +/- 55.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | 21.5     |
| rollout/            |          |
|    exploration_rate | 0.711    |
| time/               |          |
|    total_timesteps  | 66000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0323   |
|    n_updates        | 13999    |
----------------------------------
Eval num_timesteps=66500, episode_reward=19.08 +/- 4.25
Episode length: 192.62 +/- 48.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 193      |
|    mean_reward      | 19.1     |
| rollout/            |          |
|    exploration_rate | 0.708    |
| time/               |          |
|    total_timesteps  | 66500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0219   |
|    n_updates        | 14124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 162      |
|    ep_rew_mean      | 11.8     |
|    exploration_rate | 0.708    |
| time/               |          |
|    episodes         | 452      |
|    fps              | 115      |
|    time_elapsed     | 574      |
|    total_timesteps  | 66537    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0191   |
|    n_updates        | 14134    |
----------------------------------
Eval num_timesteps=67000, episode_reward=19.58 +/- 5.36
Episode length: 197.14 +/- 57.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 197      |
|    mean_reward      | 19.6     |
| rollout/            |          |
|    exploration_rate | 0.704    |
| time/               |          |
|    total_timesteps  | 67000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 14249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 163      |
|    ep_rew_mean      | 12       |
|    exploration_rate | 0.703    |
| time/               |          |
|    episodes         | 456      |
|    fps              | 115      |
|    time_elapsed     | 580      |
|    total_timesteps  | 67220    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 14304    |
----------------------------------
Eval num_timesteps=67500, episode_reward=9.42 +/- 5.21
Episode length: 139.88 +/- 49.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 9.42     |
| rollout/            |          |
|    exploration_rate | 0.701    |
| time/               |          |
|    total_timesteps  | 67500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 14374    |
----------------------------------
Eval num_timesteps=68000, episode_reward=18.32 +/- 6.44
Episode length: 195.64 +/- 65.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 196      |
|    mean_reward      | 18.3     |
| rollout/            |          |
|    exploration_rate | 0.698    |
| time/               |          |
|    total_timesteps  | 68000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.021    |
|    n_updates        | 14499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 165      |
|    ep_rew_mean      | 12.2     |
|    exploration_rate | 0.697    |
| time/               |          |
|    episodes         | 460      |
|    fps              | 115      |
|    time_elapsed     | 591      |
|    total_timesteps  | 68114    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 14528    |
----------------------------------
Eval num_timesteps=68500, episode_reward=18.54 +/- 6.09
Episode length: 186.28 +/- 59.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 186      |
|    mean_reward      | 18.5     |
| rollout/            |          |
|    exploration_rate | 0.694    |
| time/               |          |
|    total_timesteps  | 68500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0349   |
|    n_updates        | 14624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 169      |
|    ep_rew_mean      | 12.7     |
|    exploration_rate | 0.692    |
| time/               |          |
|    episodes         | 464      |
|    fps              | 115      |
|    time_elapsed     | 596      |
|    total_timesteps  | 68886    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0351   |
|    n_updates        | 14721    |
----------------------------------
Eval num_timesteps=69000, episode_reward=11.10 +/- 4.50
Episode length: 159.00 +/- 54.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 159      |
|    mean_reward      | 11.1     |
| rollout/            |          |
|    exploration_rate | 0.691    |
| time/               |          |
|    total_timesteps  | 69000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0157   |
|    n_updates        | 14749    |
----------------------------------
Eval num_timesteps=69500, episode_reward=19.06 +/- 4.53
Episode length: 194.38 +/- 52.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | 19.1     |
| rollout/            |          |
|    exploration_rate | 0.688    |
| time/               |          |
|    total_timesteps  | 69500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 14874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 171      |
|    ep_rew_mean      | 12.9     |
|    exploration_rate | 0.686    |
| time/               |          |
|    episodes         | 468      |
|    fps              | 114      |
|    time_elapsed     | 607      |
|    total_timesteps  | 69786    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 14946    |
----------------------------------
Eval num_timesteps=70000, episode_reward=14.52 +/- 5.60
Episode length: 178.04 +/- 62.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 178      |
|    mean_reward      | 14.5     |
| rollout/            |          |
|    exploration_rate | 0.684    |
| time/               |          |
|    total_timesteps  | 70000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0679   |
|    n_updates        | 14999    |
----------------------------------
Eval num_timesteps=70500, episode_reward=6.24 +/- 2.86
Episode length: 127.84 +/- 41.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 128      |
|    mean_reward      | 6.24     |
| rollout/            |          |
|    exploration_rate | 0.681    |
| time/               |          |
|    total_timesteps  | 70500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0441   |
|    n_updates        | 15124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 172      |
|    ep_rew_mean      | 13.1     |
|    exploration_rate | 0.681    |
| time/               |          |
|    episodes         | 472      |
|    fps              | 114      |
|    time_elapsed     | 616      |
|    total_timesteps  | 70526    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 15131    |
----------------------------------
Eval num_timesteps=71000, episode_reward=9.48 +/- 4.79
Episode length: 138.58 +/- 52.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 9.48     |
| rollout/            |          |
|    exploration_rate | 0.678    |
| time/               |          |
|    total_timesteps  | 71000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0638   |
|    n_updates        | 15249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 171      |
|    ep_rew_mean      | 13.1     |
|    exploration_rate | 0.676    |
| time/               |          |
|    episodes         | 476      |
|    fps              | 114      |
|    time_elapsed     | 621      |
|    total_timesteps  | 71218    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 15304    |
----------------------------------
Eval num_timesteps=71500, episode_reward=6.16 +/- 3.72
Episode length: 119.18 +/- 44.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 6.16     |
| rollout/            |          |
|    exploration_rate | 0.674    |
| time/               |          |
|    total_timesteps  | 71500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.042    |
|    n_updates        | 15374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 174      |
|    ep_rew_mean      | 13.3     |
|    exploration_rate | 0.671    |
| time/               |          |
|    episodes         | 480      |
|    fps              | 115      |
|    time_elapsed     | 625      |
|    total_timesteps  | 71929    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.03     |
|    n_updates        | 15482    |
----------------------------------
Eval num_timesteps=72000, episode_reward=20.18 +/- 6.40
Episode length: 197.82 +/- 61.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | 20.2     |
| rollout/            |          |
|    exploration_rate | 0.671    |
| time/               |          |
|    total_timesteps  | 72000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0118   |
|    n_updates        | 15499    |
----------------------------------
Eval num_timesteps=72500, episode_reward=4.10 +/- 3.02
Episode length: 107.30 +/- 44.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 107      |
|    mean_reward      | 4.1      |
| rollout/            |          |
|    exploration_rate | 0.667    |
| time/               |          |
|    total_timesteps  | 72500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0538   |
|    n_updates        | 15624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 174      |
|    ep_rew_mean      | 13.2     |
|    exploration_rate | 0.666    |
| time/               |          |
|    episodes         | 484      |
|    fps              | 114      |
|    time_elapsed     | 634      |
|    total_timesteps  | 72670    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.019    |
|    n_updates        | 15667    |
----------------------------------
Eval num_timesteps=73000, episode_reward=13.60 +/- 5.60
Episode length: 174.42 +/- 63.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 174      |
|    mean_reward      | 13.6     |
| rollout/            |          |
|    exploration_rate | 0.664    |
| time/               |          |
|    total_timesteps  | 73000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 15749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 174      |
|    ep_rew_mean      | 13.3     |
|    exploration_rate | 0.662    |
| time/               |          |
|    episodes         | 488      |
|    fps              | 114      |
|    time_elapsed     | 639      |
|    total_timesteps  | 73264    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0808   |
|    n_updates        | 15815    |
----------------------------------
Eval num_timesteps=73500, episode_reward=13.12 +/- 5.46
Episode length: 161.52 +/- 58.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 162      |
|    mean_reward      | 13.1     |
| rollout/            |          |
|    exploration_rate | 0.66     |
| time/               |          |
|    total_timesteps  | 73500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0343   |
|    n_updates        | 15874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 171      |
|    ep_rew_mean      | 13.1     |
|    exploration_rate | 0.658    |
| time/               |          |
|    episodes         | 492      |
|    fps              | 114      |
|    time_elapsed     | 644      |
|    total_timesteps  | 73908    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0449   |
|    n_updates        | 15976    |
----------------------------------
Eval num_timesteps=74000, episode_reward=20.62 +/- 4.57
Episode length: 190.96 +/- 47.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 191      |
|    mean_reward      | 20.6     |
| rollout/            |          |
|    exploration_rate | 0.657    |
| time/               |          |
|    total_timesteps  | 74000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0453   |
|    n_updates        | 15999    |
----------------------------------
Eval num_timesteps=74500, episode_reward=5.78 +/- 3.07
Episode length: 118.70 +/- 37.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 119      |
|    mean_reward      | 5.78     |
| rollout/            |          |
|    exploration_rate | 0.653    |
| time/               |          |
|    total_timesteps  | 74500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 16124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 171      |
|    ep_rew_mean      | 13.1     |
|    exploration_rate | 0.653    |
| time/               |          |
|    episodes         | 496      |
|    fps              | 113      |
|    time_elapsed     | 654      |
|    total_timesteps  | 74531    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0664   |
|    n_updates        | 16132    |
----------------------------------
Eval num_timesteps=75000, episode_reward=17.76 +/- 5.11
Episode length: 180.52 +/- 58.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 17.8     |
| rollout/            |          |
|    exploration_rate | 0.65     |
| time/               |          |
|    total_timesteps  | 75000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0258   |
|    n_updates        | 16249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 171      |
|    ep_rew_mean      | 13.1     |
|    exploration_rate | 0.649    |
| time/               |          |
|    episodes         | 500      |
|    fps              | 113      |
|    time_elapsed     | 659      |
|    total_timesteps  | 75203    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0093   |
|    n_updates        | 16300    |
----------------------------------
Eval num_timesteps=75500, episode_reward=11.70 +/- 4.84
Episode length: 147.22 +/- 46.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 147      |
|    mean_reward      | 11.7     |
| rollout/            |          |
|    exploration_rate | 0.646    |
| time/               |          |
|    total_timesteps  | 75500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.052    |
|    n_updates        | 16374    |
----------------------------------
Eval num_timesteps=76000, episode_reward=16.56 +/- 4.57
Episode length: 181.10 +/- 48.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 16.6     |
| rollout/            |          |
|    exploration_rate | 0.643    |
| time/               |          |
|    total_timesteps  | 76000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0135   |
|    n_updates        | 16499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 176      |
|    ep_rew_mean      | 13.4     |
|    exploration_rate | 0.642    |
| time/               |          |
|    episodes         | 504      |
|    fps              | 113      |
|    time_elapsed     | 669      |
|    total_timesteps  | 76126    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 16531    |
----------------------------------
Eval num_timesteps=76500, episode_reward=4.20 +/- 3.54
Episode length: 103.14 +/- 44.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 103      |
|    mean_reward      | 4.2      |
| rollout/            |          |
|    exploration_rate | 0.639    |
| time/               |          |
|    total_timesteps  | 76500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0217   |
|    n_updates        | 16624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 176      |
|    ep_rew_mean      | 13.5     |
|    exploration_rate | 0.638    |
| time/               |          |
|    episodes         | 508      |
|    fps              | 113      |
|    time_elapsed     | 673      |
|    total_timesteps  | 76748    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0469   |
|    n_updates        | 16686    |
----------------------------------
Eval num_timesteps=77000, episode_reward=16.52 +/- 5.35
Episode length: 171.04 +/- 54.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 171      |
|    mean_reward      | 16.5     |
| rollout/            |          |
|    exploration_rate | 0.636    |
| time/               |          |
|    total_timesteps  | 77000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0457   |
|    n_updates        | 16749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 174      |
|    ep_rew_mean      | 13.3     |
|    exploration_rate | 0.634    |
| time/               |          |
|    episodes         | 512      |
|    fps              | 113      |
|    time_elapsed     | 678      |
|    total_timesteps  | 77213    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 16803    |
----------------------------------
Eval num_timesteps=77500, episode_reward=12.46 +/- 6.97
Episode length: 160.92 +/- 67.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 161      |
|    mean_reward      | 12.5     |
| rollout/            |          |
|    exploration_rate | 0.632    |
| time/               |          |
|    total_timesteps  | 77500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 16874    |
----------------------------------
Eval num_timesteps=78000, episode_reward=9.52 +/- 3.63
Episode length: 139.12 +/- 39.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 139      |
|    mean_reward      | 9.52     |
| rollout/            |          |
|    exploration_rate | 0.629    |
| time/               |          |
|    total_timesteps  | 78000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0113   |
|    n_updates        | 16999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 176      |
|    ep_rew_mean      | 13.5     |
|    exploration_rate | 0.629    |
| time/               |          |
|    episodes         | 516      |
|    fps              | 113      |
|    time_elapsed     | 687      |
|    total_timesteps  | 78033    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0165   |
|    n_updates        | 17008    |
----------------------------------
Eval num_timesteps=78500, episode_reward=17.02 +/- 5.83
Episode length: 183.52 +/- 55.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 184      |
|    mean_reward      | 17       |
| rollout/            |          |
|    exploration_rate | 0.625    |
| time/               |          |
|    total_timesteps  | 78500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.015    |
|    n_updates        | 17124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 13.9     |
|    exploration_rate | 0.623    |
| time/               |          |
|    episodes         | 520      |
|    fps              | 113      |
|    time_elapsed     | 693      |
|    total_timesteps  | 78851    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0313   |
|    n_updates        | 17212    |
----------------------------------
Eval num_timesteps=79000, episode_reward=20.96 +/- 5.01
Episode length: 194.40 +/- 51.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | 21       |
| rollout/            |          |
|    exploration_rate | 0.622    |
| time/               |          |
|    total_timesteps  | 79000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 17249    |
----------------------------------
Eval num_timesteps=79500, episode_reward=12.14 +/- 4.99
Episode length: 146.40 +/- 48.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 146      |
|    mean_reward      | 12.1     |
| rollout/            |          |
|    exploration_rate | 0.618    |
| time/               |          |
|    total_timesteps  | 79500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0269   |
|    n_updates        | 17374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 179      |
|    ep_rew_mean      | 13.8     |
|    exploration_rate | 0.618    |
| time/               |          |
|    episodes         | 524      |
|    fps              | 113      |
|    time_elapsed     | 703      |
|    total_timesteps  | 79542    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00881  |
|    n_updates        | 17385    |
----------------------------------
Eval num_timesteps=80000, episode_reward=17.04 +/- 5.31
Episode length: 184.18 +/- 51.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 184      |
|    mean_reward      | 17       |
| rollout/            |          |
|    exploration_rate | 0.614    |
| time/               |          |
|    total_timesteps  | 80000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.02     |
|    n_updates        | 17499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 177      |
|    ep_rew_mean      | 13.9     |
|    exploration_rate | 0.613    |
| time/               |          |
|    episodes         | 528      |
|    fps              | 112      |
|    time_elapsed     | 709      |
|    total_timesteps  | 80153    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0248   |
|    n_updates        | 17538    |
----------------------------------
Eval num_timesteps=80500, episode_reward=17.74 +/- 6.14
Episode length: 180.10 +/- 63.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 180      |
|    mean_reward      | 17.7     |
| rollout/            |          |
|    exploration_rate | 0.611    |
| time/               |          |
|    total_timesteps  | 80500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0315   |
|    n_updates        | 17624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 179      |
|    ep_rew_mean      | 14.1     |
|    exploration_rate | 0.607    |
| time/               |          |
|    episodes         | 532      |
|    fps              | 113      |
|    time_elapsed     | 715      |
|    total_timesteps  | 80968    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0428   |
|    n_updates        | 17741    |
----------------------------------
Eval num_timesteps=81000, episode_reward=19.76 +/- 4.74
Episode length: 190.14 +/- 52.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 19.8     |
| rollout/            |          |
|    exploration_rate | 0.607    |
| time/               |          |
|    total_timesteps  | 81000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.027    |
|    n_updates        | 17749    |
----------------------------------
Eval num_timesteps=81500, episode_reward=22.32 +/- 4.90
Episode length: 206.42 +/- 47.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | 22.3     |
| rollout/            |          |
|    exploration_rate | 0.604    |
| time/               |          |
|    total_timesteps  | 81500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 17874    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 14.3     |
|    exploration_rate | 0.601    |
| time/               |          |
|    episodes         | 536      |
|    fps              | 112      |
|    time_elapsed     | 726      |
|    total_timesteps  | 81788    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.037    |
|    n_updates        | 17946    |
----------------------------------
Eval num_timesteps=82000, episode_reward=19.54 +/- 5.37
Episode length: 191.36 +/- 53.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 191      |
|    mean_reward      | 19.5     |
| rollout/            |          |
|    exploration_rate | 0.6      |
| time/               |          |
|    total_timesteps  | 82000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0201   |
|    n_updates        | 17999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 181      |
|    ep_rew_mean      | 14.4     |
|    exploration_rate | 0.596    |
| time/               |          |
|    episodes         | 540      |
|    fps              | 112      |
|    time_elapsed     | 732      |
|    total_timesteps  | 82479    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 18119    |
----------------------------------
Eval num_timesteps=82500, episode_reward=20.90 +/- 4.80
Episode length: 194.64 +/- 47.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 195      |
|    mean_reward      | 20.9     |
| rollout/            |          |
|    exploration_rate | 0.596    |
| time/               |          |
|    total_timesteps  | 82500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 18124    |
----------------------------------
Eval num_timesteps=83000, episode_reward=21.06 +/- 6.01
Episode length: 198.54 +/- 60.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 199      |
|    mean_reward      | 21.1     |
| rollout/            |          |
|    exploration_rate | 0.593    |
| time/               |          |
|    total_timesteps  | 83000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 18249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 181      |
|    ep_rew_mean      | 14.6     |
|    exploration_rate | 0.59     |
| time/               |          |
|    episodes         | 544      |
|    fps              | 111      |
|    time_elapsed     | 744      |
|    total_timesteps  | 83357    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0252   |
|    n_updates        | 18339    |
----------------------------------
Eval num_timesteps=83500, episode_reward=21.04 +/- 5.30
Episode length: 192.22 +/- 49.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 192      |
|    mean_reward      | 21       |
| rollout/            |          |
|    exploration_rate | 0.589    |
| time/               |          |
|    total_timesteps  | 83500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0107   |
|    n_updates        | 18374    |
----------------------------------
Eval num_timesteps=84000, episode_reward=19.68 +/- 5.03
Episode length: 187.54 +/- 51.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 188      |
|    mean_reward      | 19.7     |
| rollout/            |          |
|    exploration_rate | 0.585    |
| time/               |          |
|    total_timesteps  | 84000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0294   |
|    n_updates        | 18499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 183      |
|    ep_rew_mean      | 14.7     |
|    exploration_rate | 0.584    |
| time/               |          |
|    episodes         | 548      |
|    fps              | 111      |
|    time_elapsed     | 755      |
|    total_timesteps  | 84095    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0839   |
|    n_updates        | 18523    |
----------------------------------
Eval num_timesteps=84500, episode_reward=22.22 +/- 5.41
Episode length: 206.32 +/- 50.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | 22.2     |
| rollout/            |          |
|    exploration_rate | 0.581    |
| time/               |          |
|    total_timesteps  | 84500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0126   |
|    n_updates        | 18624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 181      |
|    ep_rew_mean      | 14.6     |
|    exploration_rate | 0.581    |
| time/               |          |
|    episodes         | 552      |
|    fps              | 111      |
|    time_elapsed     | 762      |
|    total_timesteps  | 84610    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0391   |
|    n_updates        | 18652    |
----------------------------------
Eval num_timesteps=85000, episode_reward=21.54 +/- 5.27
Episode length: 201.42 +/- 53.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 201      |
|    mean_reward      | 21.5     |
| rollout/            |          |
|    exploration_rate | 0.578    |
| time/               |          |
|    total_timesteps  | 85000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0213   |
|    n_updates        | 18749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 182      |
|    ep_rew_mean      | 14.7     |
|    exploration_rate | 0.575    |
| time/               |          |
|    episodes         | 556      |
|    fps              | 111      |
|    time_elapsed     | 768      |
|    total_timesteps  | 85379    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.018    |
|    n_updates        | 18844    |
----------------------------------
Eval num_timesteps=85500, episode_reward=18.64 +/- 6.07
Episode length: 201.06 +/- 66.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 201      |
|    mean_reward      | 18.6     |
| rollout/            |          |
|    exploration_rate | 0.574    |
| time/               |          |
|    total_timesteps  | 85500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.016    |
|    n_updates        | 18874    |
----------------------------------
Eval num_timesteps=86000, episode_reward=21.04 +/- 5.90
Episode length: 203.88 +/- 60.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 21       |
| rollout/            |          |
|    exploration_rate | 0.57     |
| time/               |          |
|    total_timesteps  | 86000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0181   |
|    n_updates        | 18999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 14.6     |
|    exploration_rate | 0.57     |
| time/               |          |
|    episodes         | 560      |
|    fps              | 110      |
|    time_elapsed     | 780      |
|    total_timesteps  | 86066    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0217   |
|    n_updates        | 19016    |
----------------------------------
Eval num_timesteps=86500, episode_reward=23.08 +/- 5.56
Episode length: 219.74 +/- 60.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 23.1     |
| rollout/            |          |
|    exploration_rate | 0.566    |
| time/               |          |
|    total_timesteps  | 86500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0254   |
|    n_updates        | 19124    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 179      |
|    ep_rew_mean      | 14.4     |
|    exploration_rate | 0.565    |
| time/               |          |
|    episodes         | 564      |
|    fps              | 110      |
|    time_elapsed     | 787      |
|    total_timesteps  | 86743    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 19185    |
----------------------------------
Eval num_timesteps=87000, episode_reward=21.84 +/- 4.60
Episode length: 207.82 +/- 48.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 21.8     |
| rollout/            |          |
|    exploration_rate | 0.563    |
| time/               |          |
|    total_timesteps  | 87000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0234   |
|    n_updates        | 19249    |
----------------------------------
Eval num_timesteps=87500, episode_reward=19.00 +/- 4.97
Episode length: 184.80 +/- 51.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 185      |
|    mean_reward      | 19       |
| rollout/            |          |
|    exploration_rate | 0.559    |
| time/               |          |
|    total_timesteps  | 87500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0245   |
|    n_updates        | 19374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 14.5     |
|    exploration_rate | 0.558    |
| time/               |          |
|    episodes         | 568      |
|    fps              | 109      |
|    time_elapsed     | 798      |
|    total_timesteps  | 87583    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0267   |
|    n_updates        | 19395    |
----------------------------------
Eval num_timesteps=88000, episode_reward=21.22 +/- 5.72
Episode length: 203.36 +/- 60.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | 21.2     |
| rollout/            |          |
|    exploration_rate | 0.555    |
| time/               |          |
|    total_timesteps  | 88000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0195   |
|    n_updates        | 19499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 14.6     |
|    exploration_rate | 0.553    |
| time/               |          |
|    episodes         | 572      |
|    fps              | 109      |
|    time_elapsed     | 805      |
|    total_timesteps  | 88329    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0244   |
|    n_updates        | 19582    |
----------------------------------
Eval num_timesteps=88500, episode_reward=18.74 +/- 5.46
Episode length: 189.28 +/- 55.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 189      |
|    mean_reward      | 18.7     |
| rollout/            |          |
|    exploration_rate | 0.551    |
| time/               |          |
|    total_timesteps  | 88500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 19624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 177      |
|    ep_rew_mean      | 14.6     |
|    exploration_rate | 0.548    |
| time/               |          |
|    episodes         | 576      |
|    fps              | 109      |
|    time_elapsed     | 811      |
|    total_timesteps  | 88944    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0206   |
|    n_updates        | 19735    |
----------------------------------
Eval num_timesteps=89000, episode_reward=20.80 +/- 5.20
Episode length: 202.58 +/- 52.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | 20.8     |
| rollout/            |          |
|    exploration_rate | 0.548    |
| time/               |          |
|    total_timesteps  | 89000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0264   |
|    n_updates        | 19749    |
----------------------------------
Eval num_timesteps=89500, episode_reward=18.80 +/- 5.89
Episode length: 192.78 +/- 52.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 193      |
|    mean_reward      | 18.8     |
| rollout/            |          |
|    exploration_rate | 0.544    |
| time/               |          |
|    total_timesteps  | 89500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0258   |
|    n_updates        | 19874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 179      |
|    ep_rew_mean      | 14.8     |
|    exploration_rate | 0.541    |
| time/               |          |
|    episodes         | 580      |
|    fps              | 109      |
|    time_elapsed     | 822      |
|    total_timesteps  | 89828    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0205   |
|    n_updates        | 19956    |
----------------------------------
Eval num_timesteps=90000, episode_reward=18.72 +/- 4.69
Episode length: 183.16 +/- 51.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 183      |
|    mean_reward      | 18.7     |
| rollout/            |          |
|    exploration_rate | 0.54     |
| time/               |          |
|    total_timesteps  | 90000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0382   |
|    n_updates        | 19999    |
----------------------------------
Eval num_timesteps=90500, episode_reward=18.86 +/- 5.74
Episode length: 192.26 +/- 59.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 192      |
|    mean_reward      | 18.9     |
| rollout/            |          |
|    exploration_rate | 0.536    |
| time/               |          |
|    total_timesteps  | 90500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0306   |
|    n_updates        | 20124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 179      |
|    ep_rew_mean      | 15       |
|    exploration_rate | 0.536    |
| time/               |          |
|    episodes         | 584      |
|    fps              | 108      |
|    time_elapsed     | 834      |
|    total_timesteps  | 90565    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0222   |
|    n_updates        | 20141    |
----------------------------------
Eval num_timesteps=91000, episode_reward=21.72 +/- 4.87
Episode length: 201.50 +/- 48.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | 21.7     |
| rollout/            |          |
|    exploration_rate | 0.532    |
| time/               |          |
|    total_timesteps  | 91000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 20249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 179      |
|    ep_rew_mean      | 15.1     |
|    exploration_rate | 0.531    |
| time/               |          |
|    episodes         | 588      |
|    fps              | 108      |
|    time_elapsed     | 840      |
|    total_timesteps  | 91156    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0292   |
|    n_updates        | 20288    |
----------------------------------
Eval num_timesteps=91500, episode_reward=16.98 +/- 6.25
Episode length: 187.38 +/- 60.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 187      |
|    mean_reward      | 17       |
| rollout/            |          |
|    exploration_rate | 0.528    |
| time/               |          |
|    total_timesteps  | 91500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 20374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 15.2     |
|    exploration_rate | 0.526    |
| time/               |          |
|    episodes         | 592      |
|    fps              | 108      |
|    time_elapsed     | 846      |
|    total_timesteps  | 91745    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0335   |
|    n_updates        | 20436    |
----------------------------------
Eval num_timesteps=92000, episode_reward=24.14 +/- 6.12
Episode length: 219.26 +/- 62.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24.1     |
| rollout/            |          |
|    exploration_rate | 0.524    |
| time/               |          |
|    total_timesteps  | 92000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0278   |
|    n_updates        | 20499    |
----------------------------------
New best mean reward!
Eval num_timesteps=92500, episode_reward=23.30 +/- 5.68
Episode length: 211.48 +/- 53.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 23.3     |
| rollout/            |          |
|    exploration_rate | 0.521    |
| time/               |          |
|    total_timesteps  | 92500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0404   |
|    n_updates        | 20624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 15.3     |
|    exploration_rate | 0.52     |
| time/               |          |
|    episodes         | 596      |
|    fps              | 107      |
|    time_elapsed     | 859      |
|    total_timesteps  | 92547    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0394   |
|    n_updates        | 20636    |
----------------------------------
Eval num_timesteps=93000, episode_reward=22.08 +/- 4.87
Episode length: 216.22 +/- 49.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 22.1     |
| rollout/            |          |
|    exploration_rate | 0.517    |
| time/               |          |
|    total_timesteps  | 93000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0413   |
|    n_updates        | 20749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 181      |
|    ep_rew_mean      | 15.5     |
|    exploration_rate | 0.514    |
| time/               |          |
|    episodes         | 600      |
|    fps              | 107      |
|    time_elapsed     | 865      |
|    total_timesteps  | 93307    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0189   |
|    n_updates        | 20826    |
----------------------------------
Eval num_timesteps=93500, episode_reward=24.38 +/- 6.65
Episode length: 221.38 +/- 67.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 24.4     |
| rollout/            |          |
|    exploration_rate | 0.513    |
| time/               |          |
|    total_timesteps  | 93500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0337   |
|    n_updates        | 20874    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 178      |
|    ep_rew_mean      | 15.3     |
|    exploration_rate | 0.509    |
| time/               |          |
|    episodes         | 604      |
|    fps              | 107      |
|    time_elapsed     | 872      |
|    total_timesteps  | 93957    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0236   |
|    n_updates        | 20989    |
----------------------------------
Eval num_timesteps=94000, episode_reward=20.72 +/- 6.24
Episode length: 208.66 +/- 66.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 20.7     |
| rollout/            |          |
|    exploration_rate | 0.509    |
| time/               |          |
|    total_timesteps  | 94000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0281   |
|    n_updates        | 20999    |
----------------------------------
Eval num_timesteps=94500, episode_reward=22.04 +/- 5.59
Episode length: 209.48 +/- 57.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 22       |
| rollout/            |          |
|    exploration_rate | 0.505    |
| time/               |          |
|    total_timesteps  | 94500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0882   |
|    n_updates        | 21124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 180      |
|    ep_rew_mean      | 15.5     |
|    exploration_rate | 0.503    |
| time/               |          |
|    episodes         | 608      |
|    fps              | 106      |
|    time_elapsed     | 885      |
|    total_timesteps  | 94705    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0446   |
|    n_updates        | 21176    |
----------------------------------
Eval num_timesteps=95000, episode_reward=18.14 +/- 6.41
Episode length: 189.12 +/- 61.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 189      |
|    mean_reward      | 18.1     |
| rollout/            |          |
|    exploration_rate | 0.501    |
| time/               |          |
|    total_timesteps  | 95000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0182   |
|    n_updates        | 21249    |
----------------------------------
Eval num_timesteps=95500, episode_reward=21.74 +/- 5.25
Episode length: 205.26 +/- 56.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 21.7     |
| rollout/            |          |
|    exploration_rate | 0.497    |
| time/               |          |
|    total_timesteps  | 95500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0435   |
|    n_updates        | 21374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 184      |
|    ep_rew_mean      | 15.9     |
|    exploration_rate | 0.496    |
| time/               |          |
|    episodes         | 612      |
|    fps              | 106      |
|    time_elapsed     | 897      |
|    total_timesteps  | 95605    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.038    |
|    n_updates        | 21401    |
----------------------------------
Eval num_timesteps=96000, episode_reward=23.68 +/- 5.02
Episode length: 215.12 +/- 53.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 215      |
|    mean_reward      | 23.7     |
| rollout/            |          |
|    exploration_rate | 0.493    |
| time/               |          |
|    total_timesteps  | 96000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0362   |
|    n_updates        | 21499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 184      |
|    ep_rew_mean      | 16       |
|    exploration_rate | 0.49     |
| time/               |          |
|    episodes         | 616      |
|    fps              | 106      |
|    time_elapsed     | 903      |
|    total_timesteps  | 96446    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0603   |
|    n_updates        | 21611    |
----------------------------------
Eval num_timesteps=96500, episode_reward=21.48 +/- 5.50
Episode length: 207.08 +/- 55.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | 21.5     |
| rollout/            |          |
|    exploration_rate | 0.489    |
| time/               |          |
|    total_timesteps  | 96500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0233   |
|    n_updates        | 21624    |
----------------------------------
Eval num_timesteps=97000, episode_reward=23.58 +/- 5.51
Episode length: 217.06 +/- 57.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 23.6     |
| rollout/            |          |
|    exploration_rate | 0.485    |
| time/               |          |
|    total_timesteps  | 97000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0245   |
|    n_updates        | 21749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 185      |
|    ep_rew_mean      | 16.1     |
|    exploration_rate | 0.482    |
| time/               |          |
|    episodes         | 620      |
|    fps              | 106      |
|    time_elapsed     | 916      |
|    total_timesteps  | 97347    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 21836    |
----------------------------------
Eval num_timesteps=97500, episode_reward=20.68 +/- 5.69
Episode length: 197.74 +/- 57.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | 20.7     |
| rollout/            |          |
|    exploration_rate | 0.481    |
| time/               |          |
|    total_timesteps  | 97500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0462   |
|    n_updates        | 21874    |
----------------------------------
Eval num_timesteps=98000, episode_reward=21.78 +/- 6.29
Episode length: 201.24 +/- 60.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 201      |
|    mean_reward      | 21.8     |
| rollout/            |          |
|    exploration_rate | 0.477    |
| time/               |          |
|    total_timesteps  | 98000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0433   |
|    n_updates        | 21999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 16.4     |
|    exploration_rate | 0.476    |
| time/               |          |
|    episodes         | 624      |
|    fps              | 105      |
|    time_elapsed     | 928      |
|    total_timesteps  | 98198    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0216   |
|    n_updates        | 22049    |
----------------------------------
Eval num_timesteps=98500, episode_reward=22.38 +/- 6.04
Episode length: 205.32 +/- 61.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 22.4     |
| rollout/            |          |
|    exploration_rate | 0.473    |
| time/               |          |
|    total_timesteps  | 98500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0243   |
|    n_updates        | 22124    |
----------------------------------
Eval num_timesteps=99000, episode_reward=20.54 +/- 5.39
Episode length: 204.82 +/- 62.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 20.5     |
| rollout/            |          |
|    exploration_rate | 0.469    |
| time/               |          |
|    total_timesteps  | 99000    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0407   |
|    n_updates        | 22249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 190      |
|    ep_rew_mean      | 16.7     |
|    exploration_rate | 0.468    |
| time/               |          |
|    episodes         | 628      |
|    fps              | 105      |
|    time_elapsed     | 940      |
|    total_timesteps  | 99136    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00465  |
|    n_updates        | 22283    |
----------------------------------
Eval num_timesteps=99500, episode_reward=22.08 +/- 5.36
Episode length: 216.34 +/- 58.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 22.1     |
| rollout/            |          |
|    exploration_rate | 0.465    |
| time/               |          |
|    total_timesteps  | 99500    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0661   |
|    n_updates        | 22374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 190      |
|    ep_rew_mean      | 16.7     |
|    exploration_rate | 0.462    |
| time/               |          |
|    episodes         | 632      |
|    fps              | 105      |
|    time_elapsed     | 947      |
|    total_timesteps  | 99923    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0258   |
|    n_updates        | 22480    |
----------------------------------
Eval num_timesteps=100000, episode_reward=21.96 +/- 4.89
Episode length: 211.72 +/- 51.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 22       |
| rollout/            |          |
|    exploration_rate | 0.461    |
| time/               |          |
|    total_timesteps  | 100000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0522   |
|    n_updates        | 22499    |
----------------------------------
Eval num_timesteps=100500, episode_reward=20.32 +/- 5.93
Episode length: 218.04 +/- 65.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 20.3     |
| rollout/            |          |
|    exploration_rate | 0.457    |
| time/               |          |
|    total_timesteps  | 100500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0367   |
|    n_updates        | 22624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 16.7     |
|    exploration_rate | 0.456    |
| time/               |          |
|    episodes         | 636      |
|    fps              | 104      |
|    time_elapsed     | 960      |
|    total_timesteps  | 100644   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 22660    |
----------------------------------
Eval num_timesteps=101000, episode_reward=18.80 +/- 5.39
Episode length: 198.76 +/- 59.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 199      |
|    mean_reward      | 18.8     |
| rollout/            |          |
|    exploration_rate | 0.453    |
| time/               |          |
|    total_timesteps  | 101000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0273   |
|    n_updates        | 22749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 16.7     |
|    exploration_rate | 0.45     |
| time/               |          |
|    episodes         | 640      |
|    fps              | 104      |
|    time_elapsed     | 966      |
|    total_timesteps  | 101336   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 22833    |
----------------------------------
Eval num_timesteps=101500, episode_reward=22.68 +/- 6.66
Episode length: 228.68 +/- 74.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 22.7     |
| rollout/            |          |
|    exploration_rate | 0.449    |
| time/               |          |
|    total_timesteps  | 101500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0402   |
|    n_updates        | 22874    |
----------------------------------
Eval num_timesteps=102000, episode_reward=7.96 +/- 4.92
Episode length: 144.74 +/- 60.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 145      |
|    mean_reward      | 7.96     |
| rollout/            |          |
|    exploration_rate | 0.445    |
| time/               |          |
|    total_timesteps  | 102000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 22999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 16.5     |
|    exploration_rate | 0.444    |
| time/               |          |
|    episodes         | 644      |
|    fps              | 104      |
|    time_elapsed     | 977      |
|    total_timesteps  | 102047   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0324   |
|    n_updates        | 23011    |
----------------------------------
Eval num_timesteps=102500, episode_reward=17.84 +/- 5.78
Episode length: 207.98 +/- 67.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 17.8     |
| rollout/            |          |
|    exploration_rate | 0.441    |
| time/               |          |
|    total_timesteps  | 102500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0458   |
|    n_updates        | 23124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 16.6     |
|    exploration_rate | 0.438    |
| time/               |          |
|    episodes         | 648      |
|    fps              | 104      |
|    time_elapsed     | 984      |
|    total_timesteps  | 102784   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0139   |
|    n_updates        | 23195    |
----------------------------------
Eval num_timesteps=103000, episode_reward=21.22 +/- 5.20
Episode length: 212.72 +/- 54.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 213      |
|    mean_reward      | 21.2     |
| rollout/            |          |
|    exploration_rate | 0.437    |
| time/               |          |
|    total_timesteps  | 103000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0537   |
|    n_updates        | 23249    |
----------------------------------
Eval num_timesteps=103500, episode_reward=20.38 +/- 6.26
Episode length: 212.32 +/- 68.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 20.4     |
| rollout/            |          |
|    exploration_rate | 0.433    |
| time/               |          |
|    total_timesteps  | 103500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.012    |
|    n_updates        | 23374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 16.9     |
|    exploration_rate | 0.432    |
| time/               |          |
|    episodes         | 652      |
|    fps              | 103      |
|    time_elapsed     | 997      |
|    total_timesteps  | 103546   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.032    |
|    n_updates        | 23386    |
----------------------------------
Eval num_timesteps=104000, episode_reward=13.94 +/- 5.50
Episode length: 167.90 +/- 59.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 168      |
|    mean_reward      | 13.9     |
| rollout/            |          |
|    exploration_rate | 0.428    |
| time/               |          |
|    total_timesteps  | 104000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0244   |
|    n_updates        | 23499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 16.9     |
|    exploration_rate | 0.427    |
| time/               |          |
|    episodes         | 656      |
|    fps              | 103      |
|    time_elapsed     | 1002     |
|    total_timesteps  | 104142   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0307   |
|    n_updates        | 23535    |
----------------------------------
Eval num_timesteps=104500, episode_reward=23.52 +/- 5.31
Episode length: 224.26 +/- 58.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 23.5     |
| rollout/            |          |
|    exploration_rate | 0.424    |
| time/               |          |
|    total_timesteps  | 104500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0407   |
|    n_updates        | 23624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 17       |
|    exploration_rate | 0.42     |
| time/               |          |
|    episodes         | 660      |
|    fps              | 103      |
|    time_elapsed     | 1010     |
|    total_timesteps  | 104978   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.024    |
|    n_updates        | 23744    |
----------------------------------
Eval num_timesteps=105000, episode_reward=21.06 +/- 5.10
Episode length: 198.12 +/- 50.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 198      |
|    mean_reward      | 21.1     |
| rollout/            |          |
|    exploration_rate | 0.42     |
| time/               |          |
|    total_timesteps  | 105000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0174   |
|    n_updates        | 23749    |
----------------------------------
Eval num_timesteps=105500, episode_reward=20.58 +/- 5.45
Episode length: 206.62 +/- 54.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | 20.6     |
| rollout/            |          |
|    exploration_rate | 0.416    |
| time/               |          |
|    total_timesteps  | 105500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0487   |
|    n_updates        | 23874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 17.3     |
|    exploration_rate | 0.413    |
| time/               |          |
|    episodes         | 664      |
|    fps              | 103      |
|    time_elapsed     | 1022     |
|    total_timesteps  | 105889   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.032    |
|    n_updates        | 23972    |
----------------------------------
Eval num_timesteps=106000, episode_reward=21.54 +/- 5.33
Episode length: 212.92 +/- 59.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 213      |
|    mean_reward      | 21.5     |
| rollout/            |          |
|    exploration_rate | 0.412    |
| time/               |          |
|    total_timesteps  | 106000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0281   |
|    n_updates        | 23999    |
----------------------------------
Eval num_timesteps=106500, episode_reward=16.36 +/- 5.77
Episode length: 181.80 +/- 63.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 182      |
|    mean_reward      | 16.4     |
| rollout/            |          |
|    exploration_rate | 0.408    |
| time/               |          |
|    total_timesteps  | 106500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0127   |
|    n_updates        | 24124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 17.4     |
|    exploration_rate | 0.405    |
| time/               |          |
|    episodes         | 668      |
|    fps              | 103      |
|    time_elapsed     | 1034     |
|    total_timesteps  | 106822   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0187   |
|    n_updates        | 24205    |
----------------------------------
Eval num_timesteps=107000, episode_reward=19.38 +/- 4.81
Episode length: 195.22 +/- 48.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 195      |
|    mean_reward      | 19.4     |
| rollout/            |          |
|    exploration_rate | 0.404    |
| time/               |          |
|    total_timesteps  | 107000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.025    |
|    n_updates        | 24249    |
----------------------------------
Eval num_timesteps=107500, episode_reward=18.58 +/- 5.22
Episode length: 199.46 +/- 55.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 199      |
|    mean_reward      | 18.6     |
| rollout/            |          |
|    exploration_rate | 0.399    |
| time/               |          |
|    total_timesteps  | 107500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 24374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 17.3     |
|    exploration_rate | 0.399    |
| time/               |          |
|    episodes         | 672      |
|    fps              | 102      |
|    time_elapsed     | 1045     |
|    total_timesteps  | 107509   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0353   |
|    n_updates        | 24377    |
----------------------------------
Eval num_timesteps=108000, episode_reward=14.16 +/- 5.79
Episode length: 181.64 +/- 70.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 182      |
|    mean_reward      | 14.2     |
| rollout/            |          |
|    exploration_rate | 0.395    |
| time/               |          |
|    total_timesteps  | 108000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0337   |
|    n_updates        | 24499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 17.3     |
|    exploration_rate | 0.394    |
| time/               |          |
|    episodes         | 676      |
|    fps              | 102      |
|    time_elapsed     | 1051     |
|    total_timesteps  | 108185   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 24546    |
----------------------------------
Eval num_timesteps=108500, episode_reward=20.46 +/- 5.31
Episode length: 201.84 +/- 54.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | 20.5     |
| rollout/            |          |
|    exploration_rate | 0.391    |
| time/               |          |
|    total_timesteps  | 108500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0468   |
|    n_updates        | 24624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 17       |
|    exploration_rate | 0.39     |
| time/               |          |
|    episodes         | 680      |
|    fps              | 102      |
|    time_elapsed     | 1057     |
|    total_timesteps  | 108656   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.011    |
|    n_updates        | 24663    |
----------------------------------
Eval num_timesteps=109000, episode_reward=21.36 +/- 3.76
Episode length: 202.28 +/- 39.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | 21.4     |
| rollout/            |          |
|    exploration_rate | 0.387    |
| time/               |          |
|    total_timesteps  | 109000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0192   |
|    n_updates        | 24749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 17       |
|    exploration_rate | 0.383    |
| time/               |          |
|    episodes         | 684      |
|    fps              | 102      |
|    time_elapsed     | 1064     |
|    total_timesteps  | 109476   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0291   |
|    n_updates        | 24868    |
----------------------------------
Eval num_timesteps=109500, episode_reward=21.54 +/- 5.15
Episode length: 214.44 +/- 57.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 21.5     |
| rollout/            |          |
|    exploration_rate | 0.383    |
| time/               |          |
|    total_timesteps  | 109500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0316   |
|    n_updates        | 24874    |
----------------------------------
Eval num_timesteps=110000, episode_reward=18.26 +/- 5.07
Episode length: 195.30 +/- 57.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 195      |
|    mean_reward      | 18.3     |
| rollout/            |          |
|    exploration_rate | 0.378    |
| time/               |          |
|    total_timesteps  | 110000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0158   |
|    n_updates        | 24999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 17.1     |
|    exploration_rate | 0.376    |
| time/               |          |
|    episodes         | 688      |
|    fps              | 102      |
|    time_elapsed     | 1076     |
|    total_timesteps  | 110309   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0687   |
|    n_updates        | 25077    |
----------------------------------
Eval num_timesteps=110500, episode_reward=21.88 +/- 5.81
Episode length: 204.16 +/- 56.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 21.9     |
| rollout/            |          |
|    exploration_rate | 0.374    |
| time/               |          |
|    total_timesteps  | 110500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0269   |
|    n_updates        | 25124    |
----------------------------------
Eval num_timesteps=111000, episode_reward=20.80 +/- 5.77
Episode length: 201.56 +/- 59.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | 20.8     |
| rollout/            |          |
|    exploration_rate | 0.37     |
| time/               |          |
|    total_timesteps  | 111000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0629   |
|    n_updates        | 25249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 17.3     |
|    exploration_rate | 0.369    |
| time/               |          |
|    episodes         | 692      |
|    fps              | 102      |
|    time_elapsed     | 1088     |
|    total_timesteps  | 111056   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 25263    |
----------------------------------
Eval num_timesteps=111500, episode_reward=15.92 +/- 5.31
Episode length: 179.10 +/- 61.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 179      |
|    mean_reward      | 15.9     |
| rollout/            |          |
|    exploration_rate | 0.366    |
| time/               |          |
|    total_timesteps  | 111500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 25374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 17.2     |
|    exploration_rate | 0.364    |
| time/               |          |
|    episodes         | 696      |
|    fps              | 102      |
|    time_elapsed     | 1093     |
|    total_timesteps  | 111702   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.026    |
|    n_updates        | 25425    |
----------------------------------
Eval num_timesteps=112000, episode_reward=16.32 +/- 5.40
Episode length: 181.86 +/- 55.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 182      |
|    mean_reward      | 16.3     |
| rollout/            |          |
|    exploration_rate | 0.361    |
| time/               |          |
|    total_timesteps  | 112000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0403   |
|    n_updates        | 25499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 17.1     |
|    exploration_rate | 0.359    |
| time/               |          |
|    episodes         | 700      |
|    fps              | 102      |
|    time_elapsed     | 1099     |
|    total_timesteps  | 112256   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 25563    |
----------------------------------
Eval num_timesteps=112500, episode_reward=20.34 +/- 5.60
Episode length: 202.56 +/- 61.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | 20.3     |
| rollout/            |          |
|    exploration_rate | 0.357    |
| time/               |          |
|    total_timesteps  | 112500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0192   |
|    n_updates        | 25624    |
----------------------------------
Eval num_timesteps=113000, episode_reward=14.74 +/- 4.84
Episode length: 184.72 +/- 53.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 185      |
|    mean_reward      | 14.7     |
| rollout/            |          |
|    exploration_rate | 0.353    |
| time/               |          |
|    total_timesteps  | 113000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0189   |
|    n_updates        | 25749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 17.3     |
|    exploration_rate | 0.351    |
| time/               |          |
|    episodes         | 704      |
|    fps              | 101      |
|    time_elapsed     | 1111     |
|    total_timesteps  | 113152   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 25787    |
----------------------------------
Eval num_timesteps=113500, episode_reward=20.22 +/- 6.07
Episode length: 202.12 +/- 63.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | 20.2     |
| rollout/            |          |
|    exploration_rate | 0.348    |
| time/               |          |
|    total_timesteps  | 113500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0676   |
|    n_updates        | 25874    |
----------------------------------
Eval num_timesteps=114000, episode_reward=19.18 +/- 4.95
Episode length: 198.72 +/- 51.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 199      |
|    mean_reward      | 19.2     |
| rollout/            |          |
|    exploration_rate | 0.344    |
| time/               |          |
|    total_timesteps  | 114000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00939  |
|    n_updates        | 25999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 17.6     |
|    exploration_rate | 0.344    |
| time/               |          |
|    episodes         | 708      |
|    fps              | 101      |
|    time_elapsed     | 1123     |
|    total_timesteps  | 114048   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0214   |
|    n_updates        | 26011    |
----------------------------------
Eval num_timesteps=114500, episode_reward=21.54 +/- 5.85
Episode length: 197.02 +/- 57.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 197      |
|    mean_reward      | 21.5     |
| rollout/            |          |
|    exploration_rate | 0.34     |
| time/               |          |
|    total_timesteps  | 114500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0311   |
|    n_updates        | 26124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 17.5     |
|    exploration_rate | 0.337    |
| time/               |          |
|    episodes         | 712      |
|    fps              | 101      |
|    time_elapsed     | 1129     |
|    total_timesteps  | 114825   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0156   |
|    n_updates        | 26206    |
----------------------------------
Eval num_timesteps=115000, episode_reward=18.66 +/- 5.82
Episode length: 185.32 +/- 58.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 185      |
|    mean_reward      | 18.7     |
| rollout/            |          |
|    exploration_rate | 0.335    |
| time/               |          |
|    total_timesteps  | 115000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0213   |
|    n_updates        | 26249    |
----------------------------------
Eval num_timesteps=115500, episode_reward=5.14 +/- 3.16
Episode length: 113.16 +/- 39.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 113      |
|    mean_reward      | 5.14     |
| rollout/            |          |
|    exploration_rate | 0.331    |
| time/               |          |
|    total_timesteps  | 115500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0396   |
|    n_updates        | 26374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 17.5     |
|    exploration_rate | 0.33     |
| time/               |          |
|    episodes         | 716      |
|    fps              | 101      |
|    time_elapsed     | 1138     |
|    total_timesteps  | 115636   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0126   |
|    n_updates        | 26408    |
----------------------------------
Eval num_timesteps=116000, episode_reward=13.50 +/- 5.79
Episode length: 155.26 +/- 61.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 155      |
|    mean_reward      | 13.5     |
| rollout/            |          |
|    exploration_rate | 0.327    |
| time/               |          |
|    total_timesteps  | 116000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 26499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 17.3     |
|    exploration_rate | 0.323    |
| time/               |          |
|    episodes         | 720      |
|    fps              | 101      |
|    time_elapsed     | 1143     |
|    total_timesteps  | 116404   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0258   |
|    n_updates        | 26600    |
----------------------------------
Eval num_timesteps=116500, episode_reward=10.96 +/- 4.56
Episode length: 139.82 +/- 42.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 140      |
|    mean_reward      | 11       |
| rollout/            |          |
|    exploration_rate | 0.322    |
| time/               |          |
|    total_timesteps  | 116500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.028    |
|    n_updates        | 26624    |
----------------------------------
Eval num_timesteps=117000, episode_reward=19.68 +/- 4.75
Episode length: 200.00 +/- 50.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 200      |
|    mean_reward      | 19.7     |
| rollout/            |          |
|    exploration_rate | 0.318    |
| time/               |          |
|    total_timesteps  | 117000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 26749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 190      |
|    ep_rew_mean      | 17.2     |
|    exploration_rate | 0.317    |
| time/               |          |
|    episodes         | 724      |
|    fps              | 101      |
|    time_elapsed     | 1153     |
|    total_timesteps  | 117150   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0151   |
|    n_updates        | 26787    |
----------------------------------
Eval num_timesteps=117500, episode_reward=18.30 +/- 4.77
Episode length: 180.14 +/- 52.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 180      |
|    mean_reward      | 18.3     |
| rollout/            |          |
|    exploration_rate | 0.314    |
| time/               |          |
|    total_timesteps  | 117500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0219   |
|    n_updates        | 26874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 17.1     |
|    exploration_rate | 0.31     |
| time/               |          |
|    episodes         | 728      |
|    fps              | 101      |
|    time_elapsed     | 1159     |
|    total_timesteps  | 117930   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0418   |
|    n_updates        | 26982    |
----------------------------------
Eval num_timesteps=118000, episode_reward=13.82 +/- 4.74
Episode length: 167.22 +/- 46.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 167      |
|    mean_reward      | 13.8     |
| rollout/            |          |
|    exploration_rate | 0.309    |
| time/               |          |
|    total_timesteps  | 118000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0166   |
|    n_updates        | 26999    |
----------------------------------
Eval num_timesteps=118500, episode_reward=20.74 +/- 5.04
Episode length: 208.12 +/- 53.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 20.7     |
| rollout/            |          |
|    exploration_rate | 0.305    |
| time/               |          |
|    total_timesteps  | 118500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0162   |
|    n_updates        | 27124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 17       |
|    exploration_rate | 0.304    |
| time/               |          |
|    episodes         | 732      |
|    fps              | 101      |
|    time_elapsed     | 1170     |
|    total_timesteps  | 118595   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0298   |
|    n_updates        | 27148    |
----------------------------------
Eval num_timesteps=119000, episode_reward=16.60 +/- 5.26
Episode length: 169.80 +/- 45.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 170      |
|    mean_reward      | 16.6     |
| rollout/            |          |
|    exploration_rate | 0.3      |
| time/               |          |
|    total_timesteps  | 119000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0278   |
|    n_updates        | 27249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 17.1     |
|    exploration_rate | 0.297    |
| time/               |          |
|    episodes         | 736      |
|    fps              | 101      |
|    time_elapsed     | 1176     |
|    total_timesteps  | 119378   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.029    |
|    n_updates        | 27344    |
----------------------------------
Eval num_timesteps=119500, episode_reward=20.14 +/- 5.41
Episode length: 189.58 +/- 51.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 190      |
|    mean_reward      | 20.1     |
| rollout/            |          |
|    exploration_rate | 0.296    |
| time/               |          |
|    total_timesteps  | 119500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0218   |
|    n_updates        | 27374    |
----------------------------------
Eval num_timesteps=120000, episode_reward=13.54 +/- 4.04
Episode length: 153.86 +/- 44.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 154      |
|    mean_reward      | 13.5     |
| rollout/            |          |
|    exploration_rate | 0.292    |
| time/               |          |
|    total_timesteps  | 120000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0264   |
|    n_updates        | 27499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 17.1     |
|    exploration_rate | 0.292    |
| time/               |          |
|    episodes         | 740      |
|    fps              | 101      |
|    time_elapsed     | 1186     |
|    total_timesteps  | 120008   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0296   |
|    n_updates        | 27501    |
----------------------------------
Eval num_timesteps=120500, episode_reward=14.84 +/- 5.33
Episode length: 166.12 +/- 51.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 166      |
|    mean_reward      | 14.8     |
| rollout/            |          |
|    exploration_rate | 0.287    |
| time/               |          |
|    total_timesteps  | 120500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.039    |
|    n_updates        | 27624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 17.2     |
|    exploration_rate | 0.285    |
| time/               |          |
|    episodes         | 744      |
|    fps              | 101      |
|    time_elapsed     | 1191     |
|    total_timesteps  | 120704   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0355   |
|    n_updates        | 27675    |
----------------------------------
Eval num_timesteps=121000, episode_reward=19.64 +/- 4.96
Episode length: 193.10 +/- 51.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 193      |
|    mean_reward      | 19.6     |
| rollout/            |          |
|    exploration_rate | 0.283    |
| time/               |          |
|    total_timesteps  | 121000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00715  |
|    n_updates        | 27749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 186      |
|    ep_rew_mean      | 17.2     |
|    exploration_rate | 0.279    |
| time/               |          |
|    episodes         | 748      |
|    fps              | 101      |
|    time_elapsed     | 1197     |
|    total_timesteps  | 121424   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0205   |
|    n_updates        | 27855    |
----------------------------------
Eval num_timesteps=121500, episode_reward=23.56 +/- 5.70
Episode length: 222.30 +/- 60.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 23.6     |
| rollout/            |          |
|    exploration_rate | 0.278    |
| time/               |          |
|    total_timesteps  | 121500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0387   |
|    n_updates        | 27874    |
----------------------------------
Eval num_timesteps=122000, episode_reward=21.84 +/- 4.88
Episode length: 205.06 +/- 49.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 21.8     |
| rollout/            |          |
|    exploration_rate | 0.274    |
| time/               |          |
|    total_timesteps  | 122000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0226   |
|    n_updates        | 27999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 17.4     |
|    exploration_rate | 0.272    |
| time/               |          |
|    episodes         | 752      |
|    fps              | 100      |
|    time_elapsed     | 1210     |
|    total_timesteps  | 122254   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0297   |
|    n_updates        | 28063    |
----------------------------------
Eval num_timesteps=122500, episode_reward=22.40 +/- 6.22
Episode length: 213.62 +/- 65.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 22.4     |
| rollout/            |          |
|    exploration_rate | 0.269    |
| time/               |          |
|    total_timesteps  | 122500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.02     |
|    n_updates        | 28124    |
----------------------------------
Eval num_timesteps=123000, episode_reward=17.66 +/- 6.11
Episode length: 180.52 +/- 58.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 181      |
|    mean_reward      | 17.7     |
| rollout/            |          |
|    exploration_rate | 0.265    |
| time/               |          |
|    total_timesteps  | 123000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0194   |
|    n_updates        | 28249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 190      |
|    ep_rew_mean      | 17.6     |
|    exploration_rate | 0.264    |
| time/               |          |
|    episodes         | 756      |
|    fps              | 100      |
|    time_elapsed     | 1222     |
|    total_timesteps  | 123134   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0142   |
|    n_updates        | 28283    |
----------------------------------
Eval num_timesteps=123500, episode_reward=20.76 +/- 5.54
Episode length: 204.34 +/- 64.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 20.8     |
| rollout/            |          |
|    exploration_rate | 0.26     |
| time/               |          |
|    total_timesteps  | 123500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00969  |
|    n_updates        | 28374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 17.6     |
|    exploration_rate | 0.258    |
| time/               |          |
|    episodes         | 760      |
|    fps              | 100      |
|    time_elapsed     | 1228     |
|    total_timesteps  | 123787   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.018    |
|    n_updates        | 28446    |
----------------------------------
Eval num_timesteps=124000, episode_reward=21.74 +/- 4.36
Episode length: 201.56 +/- 43.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | 21.7     |
| rollout/            |          |
|    exploration_rate | 0.256    |
| time/               |          |
|    total_timesteps  | 124000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0396   |
|    n_updates        | 28499    |
----------------------------------
Eval num_timesteps=124500, episode_reward=22.94 +/- 5.40
Episode length: 212.16 +/- 53.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 22.9     |
| rollout/            |          |
|    exploration_rate | 0.251    |
| time/               |          |
|    total_timesteps  | 124500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0284   |
|    n_updates        | 28624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 17.6     |
|    exploration_rate | 0.25     |
| time/               |          |
|    episodes         | 764      |
|    fps              | 100      |
|    time_elapsed     | 1241     |
|    total_timesteps  | 124664   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0136   |
|    n_updates        | 28665    |
----------------------------------
Eval num_timesteps=125000, episode_reward=20.98 +/- 4.93
Episode length: 203.34 +/- 52.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | 21       |
| rollout/            |          |
|    exploration_rate | 0.247    |
| time/               |          |
|    total_timesteps  | 125000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 28749    |
----------------------------------
Eval num_timesteps=125500, episode_reward=23.48 +/- 4.92
Episode length: 208.54 +/- 50.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 23.5     |
| rollout/            |          |
|    exploration_rate | 0.242    |
| time/               |          |
|    total_timesteps  | 125500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0241   |
|    n_updates        | 28874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 17.7     |
|    exploration_rate | 0.241    |
| time/               |          |
|    episodes         | 768      |
|    fps              | 100      |
|    time_elapsed     | 1253     |
|    total_timesteps  | 125620   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 28904    |
----------------------------------
Eval num_timesteps=126000, episode_reward=23.34 +/- 5.12
Episode length: 216.32 +/- 52.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 23.3     |
| rollout/            |          |
|    exploration_rate | 0.238    |
| time/               |          |
|    total_timesteps  | 126000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0611   |
|    n_updates        | 28999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 187      |
|    ep_rew_mean      | 17.7     |
|    exploration_rate | 0.236    |
| time/               |          |
|    episodes         | 772      |
|    fps              | 100      |
|    time_elapsed     | 1260     |
|    total_timesteps  | 126207   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0815   |
|    n_updates        | 29051    |
----------------------------------
Eval num_timesteps=126500, episode_reward=20.54 +/- 5.47
Episode length: 201.86 +/- 60.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | 20.5     |
| rollout/            |          |
|    exploration_rate | 0.233    |
| time/               |          |
|    total_timesteps  | 126500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0217   |
|    n_updates        | 29124    |
----------------------------------
Eval num_timesteps=127000, episode_reward=20.48 +/- 5.84
Episode length: 198.88 +/- 57.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 199      |
|    mean_reward      | 20.5     |
| rollout/            |          |
|    exploration_rate | 0.229    |
| time/               |          |
|    total_timesteps  | 127000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0277   |
|    n_updates        | 29249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 188      |
|    ep_rew_mean      | 17.9     |
|    exploration_rate | 0.229    |
| time/               |          |
|    episodes         | 776      |
|    fps              | 99       |
|    time_elapsed     | 1272     |
|    total_timesteps  | 127011   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.025    |
|    n_updates        | 29252    |
----------------------------------
Eval num_timesteps=127500, episode_reward=23.96 +/- 5.41
Episode length: 221.88 +/- 55.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 24       |
| rollout/            |          |
|    exploration_rate | 0.224    |
| time/               |          |
|    total_timesteps  | 127500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0394   |
|    n_updates        | 29374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 18.2     |
|    exploration_rate | 0.222    |
| time/               |          |
|    episodes         | 780      |
|    fps              | 99       |
|    time_elapsed     | 1279     |
|    total_timesteps  | 127726   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0378   |
|    n_updates        | 29431    |
----------------------------------
Eval num_timesteps=128000, episode_reward=21.52 +/- 5.76
Episode length: 215.52 +/- 63.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 21.5     |
| rollout/            |          |
|    exploration_rate | 0.22     |
| time/               |          |
|    total_timesteps  | 128000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.019    |
|    n_updates        | 29499    |
----------------------------------
Eval num_timesteps=128500, episode_reward=21.62 +/- 4.75
Episode length: 210.24 +/- 49.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 21.6     |
| rollout/            |          |
|    exploration_rate | 0.215    |
| time/               |          |
|    total_timesteps  | 128500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0188   |
|    n_updates        | 29624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 18.2     |
|    exploration_rate | 0.215    |
| time/               |          |
|    episodes         | 784      |
|    fps              | 99       |
|    time_elapsed     | 1291     |
|    total_timesteps  | 128534   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 29633    |
----------------------------------
Eval num_timesteps=129000, episode_reward=23.88 +/- 5.48
Episode length: 210.38 +/- 56.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 23.9     |
| rollout/            |          |
|    exploration_rate | 0.21     |
| time/               |          |
|    total_timesteps  | 129000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0323   |
|    n_updates        | 29749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 18.2     |
|    exploration_rate | 0.209    |
| time/               |          |
|    episodes         | 788      |
|    fps              | 99       |
|    time_elapsed     | 1298     |
|    total_timesteps  | 129200   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0279   |
|    n_updates        | 29799    |
----------------------------------
Eval num_timesteps=129500, episode_reward=21.00 +/- 5.39
Episode length: 203.68 +/- 55.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 21       |
| rollout/            |          |
|    exploration_rate | 0.206    |
| time/               |          |
|    total_timesteps  | 129500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0208   |
|    n_updates        | 29874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 189      |
|    ep_rew_mean      | 18.3     |
|    exploration_rate | 0.201    |
| time/               |          |
|    episodes         | 792      |
|    fps              | 99       |
|    time_elapsed     | 1305     |
|    total_timesteps  | 129993   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0246   |
|    n_updates        | 29998    |
----------------------------------
Eval num_timesteps=130000, episode_reward=21.50 +/- 4.56
Episode length: 198.80 +/- 42.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 199      |
|    mean_reward      | 21.5     |
| rollout/            |          |
|    exploration_rate | 0.201    |
| time/               |          |
|    total_timesteps  | 130000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0212   |
|    n_updates        | 29999    |
----------------------------------
Eval num_timesteps=130500, episode_reward=21.78 +/- 5.17
Episode length: 202.42 +/- 54.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 202      |
|    mean_reward      | 21.8     |
| rollout/            |          |
|    exploration_rate | 0.197    |
| time/               |          |
|    total_timesteps  | 130500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0264   |
|    n_updates        | 30124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 18.6     |
|    exploration_rate | 0.193    |
| time/               |          |
|    episodes         | 796      |
|    fps              | 99       |
|    time_elapsed     | 1317     |
|    total_timesteps  | 130928   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0344   |
|    n_updates        | 30231    |
----------------------------------
Eval num_timesteps=131000, episode_reward=24.48 +/- 5.57
Episode length: 222.40 +/- 54.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 24.5     |
| rollout/            |          |
|    exploration_rate | 0.192    |
| time/               |          |
|    total_timesteps  | 131000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.02     |
|    n_updates        | 30249    |
----------------------------------
New best mean reward!
Eval num_timesteps=131500, episode_reward=24.46 +/- 5.31
Episode length: 230.10 +/- 52.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 24.5     |
| rollout/            |          |
|    exploration_rate | 0.187    |
| time/               |          |
|    total_timesteps  | 131500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0654   |
|    n_updates        | 30374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 193      |
|    ep_rew_mean      | 18.8     |
|    exploration_rate | 0.187    |
| time/               |          |
|    episodes         | 800      |
|    fps              | 98       |
|    time_elapsed     | 1330     |
|    total_timesteps  | 131591   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0668   |
|    n_updates        | 30397    |
----------------------------------
Eval num_timesteps=132000, episode_reward=22.58 +/- 5.81
Episode length: 204.48 +/- 57.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 22.6     |
| rollout/            |          |
|    exploration_rate | 0.183    |
| time/               |          |
|    total_timesteps  | 132000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0359   |
|    n_updates        | 30499    |
----------------------------------
Eval num_timesteps=132500, episode_reward=24.58 +/- 5.36
Episode length: 222.70 +/- 53.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.178    |
| time/               |          |
|    total_timesteps  | 132500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0234   |
|    n_updates        | 30624    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | 18.9     |
|    exploration_rate | 0.178    |
| time/               |          |
|    episodes         | 804      |
|    fps              | 98       |
|    time_elapsed     | 1343     |
|    total_timesteps  | 132530   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0292   |
|    n_updates        | 30632    |
----------------------------------
Eval num_timesteps=133000, episode_reward=16.74 +/- 5.80
Episode length: 162.40 +/- 58.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 162      |
|    mean_reward      | 16.7     |
| rollout/            |          |
|    exploration_rate | 0.173    |
| time/               |          |
|    total_timesteps  | 133000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0302   |
|    n_updates        | 30749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 192      |
|    ep_rew_mean      | 18.8     |
|    exploration_rate | 0.171    |
| time/               |          |
|    episodes         | 808      |
|    fps              | 98       |
|    time_elapsed     | 1348     |
|    total_timesteps  | 133217   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0287   |
|    n_updates        | 30804    |
----------------------------------
Eval num_timesteps=133500, episode_reward=25.58 +/- 5.87
Episode length: 231.24 +/- 58.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 25.6     |
| rollout/            |          |
|    exploration_rate | 0.169    |
| time/               |          |
|    total_timesteps  | 133500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0434   |
|    n_updates        | 30874    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 191      |
|    ep_rew_mean      | 18.7     |
|    exploration_rate | 0.165    |
| time/               |          |
|    episodes         | 812      |
|    fps              | 98       |
|    time_elapsed     | 1355     |
|    total_timesteps  | 133948   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0194   |
|    n_updates        | 30986    |
----------------------------------
Eval num_timesteps=134000, episode_reward=20.22 +/- 6.18
Episode length: 200.84 +/- 62.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 201      |
|    mean_reward      | 20.2     |
| rollout/            |          |
|    exploration_rate | 0.164    |
| time/               |          |
|    total_timesteps  | 134000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0445   |
|    n_updates        | 30999    |
----------------------------------
Eval num_timesteps=134500, episode_reward=22.48 +/- 4.64
Episode length: 207.50 +/- 45.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 22.5     |
| rollout/            |          |
|    exploration_rate | 0.159    |
| time/               |          |
|    total_timesteps  | 134500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0246   |
|    n_updates        | 31124    |
----------------------------------
Eval num_timesteps=135000, episode_reward=22.90 +/- 5.01
Episode length: 215.92 +/- 51.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 22.9     |
| rollout/            |          |
|    exploration_rate | 0.155    |
| time/               |          |
|    total_timesteps  | 135000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0273   |
|    n_updates        | 31249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | 19.1     |
|    exploration_rate | 0.154    |
| time/               |          |
|    episodes         | 816      |
|    fps              | 98       |
|    time_elapsed     | 1374     |
|    total_timesteps  | 135024   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0152   |
|    n_updates        | 31255    |
----------------------------------
Eval num_timesteps=135500, episode_reward=24.52 +/- 6.60
Episode length: 230.54 +/- 71.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 24.5     |
| rollout/            |          |
|    exploration_rate | 0.15     |
| time/               |          |
|    total_timesteps  | 135500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0226   |
|    n_updates        | 31374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | 19.4     |
|    exploration_rate | 0.147    |
| time/               |          |
|    episodes         | 820      |
|    fps              | 98       |
|    time_elapsed     | 1381     |
|    total_timesteps  | 135866   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0252   |
|    n_updates        | 31466    |
----------------------------------
Eval num_timesteps=136000, episode_reward=24.56 +/- 6.22
Episode length: 232.30 +/- 65.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.145    |
| time/               |          |
|    total_timesteps  | 136000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0384   |
|    n_updates        | 31499    |
----------------------------------
Eval num_timesteps=136500, episode_reward=24.60 +/- 5.94
Episode length: 218.76 +/- 56.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.141    |
| time/               |          |
|    total_timesteps  | 136500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 31624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 194      |
|    ep_rew_mean      | 19.5     |
|    exploration_rate | 0.14     |
| time/               |          |
|    episodes         | 824      |
|    fps              | 97       |
|    time_elapsed     | 1396     |
|    total_timesteps  | 136581   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0715   |
|    n_updates        | 31645    |
----------------------------------
Eval num_timesteps=137000, episode_reward=25.14 +/- 6.15
Episode length: 223.38 +/- 58.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.136    |
| time/               |          |
|    total_timesteps  | 137000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0607   |
|    n_updates        | 31749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 195      |
|    ep_rew_mean      | 19.6     |
|    exploration_rate | 0.132    |
| time/               |          |
|    episodes         | 828      |
|    fps              | 97       |
|    time_elapsed     | 1403     |
|    total_timesteps  | 137440   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.05     |
|    n_updates        | 31859    |
----------------------------------
Eval num_timesteps=137500, episode_reward=23.84 +/- 5.72
Episode length: 225.18 +/- 56.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 23.8     |
| rollout/            |          |
|    exploration_rate | 0.131    |
| time/               |          |
|    total_timesteps  | 137500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 31874    |
----------------------------------
Eval num_timesteps=138000, episode_reward=24.20 +/- 5.75
Episode length: 221.06 +/- 62.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 24.2     |
| rollout/            |          |
|    exploration_rate | 0.126    |
| time/               |          |
|    total_timesteps  | 138000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0208   |
|    n_updates        | 31999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 197      |
|    ep_rew_mean      | 19.8     |
|    exploration_rate | 0.124    |
| time/               |          |
|    episodes         | 832      |
|    fps              | 97       |
|    time_elapsed     | 1416     |
|    total_timesteps  | 138289   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0293   |
|    n_updates        | 32072    |
----------------------------------
Eval num_timesteps=138500, episode_reward=20.48 +/- 5.44
Episode length: 207.50 +/- 59.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 20.5     |
| rollout/            |          |
|    exploration_rate | 0.122    |
| time/               |          |
|    total_timesteps  | 138500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0184   |
|    n_updates        | 32124    |
----------------------------------
Eval num_timesteps=139000, episode_reward=19.26 +/- 5.47
Episode length: 186.36 +/- 49.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 186      |
|    mean_reward      | 19.3     |
| rollout/            |          |
|    exploration_rate | 0.117    |
| time/               |          |
|    total_timesteps  | 139000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0167   |
|    n_updates        | 32249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 198      |
|    ep_rew_mean      | 19.9     |
|    exploration_rate | 0.115    |
| time/               |          |
|    episodes         | 836      |
|    fps              | 97       |
|    time_elapsed     | 1428     |
|    total_timesteps  | 139182   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.014    |
|    n_updates        | 32295    |
----------------------------------
Eval num_timesteps=139500, episode_reward=22.04 +/- 5.72
Episode length: 216.10 +/- 64.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 22       |
| rollout/            |          |
|    exploration_rate | 0.112    |
| time/               |          |
|    total_timesteps  | 139500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0389   |
|    n_updates        | 32374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 199      |
|    ep_rew_mean      | 20.1     |
|    exploration_rate | 0.108    |
| time/               |          |
|    episodes         | 840      |
|    fps              | 97       |
|    time_elapsed     | 1435     |
|    total_timesteps  | 139919   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0403   |
|    n_updates        | 32479    |
----------------------------------
Eval num_timesteps=140000, episode_reward=22.72 +/- 6.04
Episode length: 214.68 +/- 56.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 215      |
|    mean_reward      | 22.7     |
| rollout/            |          |
|    exploration_rate | 0.107    |
| time/               |          |
|    total_timesteps  | 140000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0194   |
|    n_updates        | 32499    |
----------------------------------
Eval num_timesteps=140500, episode_reward=22.86 +/- 5.46
Episode length: 209.88 +/- 55.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 22.9     |
| rollout/            |          |
|    exploration_rate | 0.103    |
| time/               |          |
|    total_timesteps  | 140500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0347   |
|    n_updates        | 32624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 202      |
|    ep_rew_mean      | 20.4     |
|    exploration_rate | 0.0988   |
| time/               |          |
|    episodes         | 844      |
|    fps              | 97       |
|    time_elapsed     | 1448     |
|    total_timesteps  | 140894   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0297   |
|    n_updates        | 32723    |
----------------------------------
Eval num_timesteps=141000, episode_reward=22.32 +/- 4.78
Episode length: 217.66 +/- 49.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 22.3     |
| rollout/            |          |
|    exploration_rate | 0.0978   |
| time/               |          |
|    total_timesteps  | 141000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0411   |
|    n_updates        | 32749    |
----------------------------------
Eval num_timesteps=141500, episode_reward=23.02 +/- 6.78
Episode length: 220.80 +/- 66.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 23       |
| rollout/            |          |
|    exploration_rate | 0.093    |
| time/               |          |
|    total_timesteps  | 141500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 32874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 203      |
|    ep_rew_mean      | 20.5     |
|    exploration_rate | 0.0907   |
| time/               |          |
|    episodes         | 848      |
|    fps              | 96       |
|    time_elapsed     | 1461     |
|    total_timesteps  | 141730   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.02     |
|    n_updates        | 32932    |
----------------------------------
Eval num_timesteps=142000, episode_reward=23.40 +/- 6.02
Episode length: 214.38 +/- 66.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 23.4     |
| rollout/            |          |
|    exploration_rate | 0.0881   |
| time/               |          |
|    total_timesteps  | 142000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 32999    |
----------------------------------
Eval num_timesteps=142500, episode_reward=22.48 +/- 5.85
Episode length: 210.86 +/- 59.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 22.5     |
| rollout/            |          |
|    exploration_rate | 0.0833   |
| time/               |          |
|    total_timesteps  | 142500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0333   |
|    n_updates        | 33124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | 20.6     |
|    exploration_rate | 0.0819   |
| time/               |          |
|    episodes         | 852      |
|    fps              | 96       |
|    time_elapsed     | 1474     |
|    total_timesteps  | 142644   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0243   |
|    n_updates        | 33160    |
----------------------------------
Eval num_timesteps=143000, episode_reward=21.46 +/- 7.86
Episode length: 203.98 +/- 72.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 21.5     |
| rollout/            |          |
|    exploration_rate | 0.0785   |
| time/               |          |
|    total_timesteps  | 143000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0301   |
|    n_updates        | 33249    |
----------------------------------
Eval num_timesteps=143500, episode_reward=22.82 +/- 5.51
Episode length: 214.72 +/- 60.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 215      |
|    mean_reward      | 22.8     |
| rollout/            |          |
|    exploration_rate | 0.0737   |
| time/               |          |
|    total_timesteps  | 143500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0145   |
|    n_updates        | 33374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 204      |
|    ep_rew_mean      | 20.7     |
|    exploration_rate | 0.0732   |
| time/               |          |
|    episodes         | 856      |
|    fps              | 96       |
|    time_elapsed     | 1486     |
|    total_timesteps  | 143550   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0277   |
|    n_updates        | 33387    |
----------------------------------
Eval num_timesteps=144000, episode_reward=21.24 +/- 6.13
Episode length: 212.00 +/- 65.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 21.2     |
| rollout/            |          |
|    exploration_rate | 0.0688   |
| time/               |          |
|    total_timesteps  | 144000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0222   |
|    n_updates        | 33499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 20.9     |
|    exploration_rate | 0.0649   |
| time/               |          |
|    episodes         | 860      |
|    fps              | 96       |
|    time_elapsed     | 1493     |
|    total_timesteps  | 144407   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0387   |
|    n_updates        | 33601    |
----------------------------------
Eval num_timesteps=144500, episode_reward=24.74 +/- 5.86
Episode length: 224.90 +/- 60.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.064    |
| time/               |          |
|    total_timesteps  | 144500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0224   |
|    n_updates        | 33624    |
----------------------------------
Eval num_timesteps=145000, episode_reward=22.72 +/- 4.87
Episode length: 209.16 +/- 54.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 22.7     |
| rollout/            |          |
|    exploration_rate | 0.0591   |
| time/               |          |
|    total_timesteps  | 145000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0306   |
|    n_updates        | 33749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 21.1     |
|    exploration_rate | 0.0561   |
| time/               |          |
|    episodes         | 864      |
|    fps              | 96       |
|    time_elapsed     | 1506     |
|    total_timesteps  | 145304   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0256   |
|    n_updates        | 33825    |
----------------------------------
Eval num_timesteps=145500, episode_reward=23.42 +/- 5.43
Episode length: 215.60 +/- 55.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 23.4     |
| rollout/            |          |
|    exploration_rate | 0.0542   |
| time/               |          |
|    total_timesteps  | 145500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0338   |
|    n_updates        | 33874    |
----------------------------------
Eval num_timesteps=146000, episode_reward=21.98 +/- 4.67
Episode length: 215.62 +/- 51.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 22       |
| rollout/            |          |
|    exploration_rate | 0.0493   |
| time/               |          |
|    total_timesteps  | 146000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0211   |
|    n_updates        | 33999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 206      |
|    ep_rew_mean      | 21       |
|    exploration_rate | 0.047    |
| time/               |          |
|    episodes         | 868      |
|    fps              | 96       |
|    time_elapsed     | 1519     |
|    total_timesteps  | 146243   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 34060    |
----------------------------------
Eval num_timesteps=146500, episode_reward=19.98 +/- 5.26
Episode length: 204.96 +/- 54.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 20       |
| rollout/            |          |
|    exploration_rate | 0.0445   |
| time/               |          |
|    total_timesteps  | 146500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0177   |
|    n_updates        | 34124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | 21.2     |
|    exploration_rate | 0.0402   |
| time/               |          |
|    episodes         | 872      |
|    fps              | 96       |
|    time_elapsed     | 1526     |
|    total_timesteps  | 146930   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0173   |
|    n_updates        | 34232    |
----------------------------------
Eval num_timesteps=147000, episode_reward=21.74 +/- 4.98
Episode length: 206.32 +/- 50.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | 21.7     |
| rollout/            |          |
|    exploration_rate | 0.0396   |
| time/               |          |
|    total_timesteps  | 147000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0531   |
|    n_updates        | 34249    |
----------------------------------
Eval num_timesteps=147500, episode_reward=21.02 +/- 5.81
Episode length: 192.40 +/- 52.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 192      |
|    mean_reward      | 21       |
| rollout/            |          |
|    exploration_rate | 0.0347   |
| time/               |          |
|    total_timesteps  | 147500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 34374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | 21.3     |
|    exploration_rate | 0.0323   |
| time/               |          |
|    episodes         | 876      |
|    fps              | 96       |
|    time_elapsed     | 1538     |
|    total_timesteps  | 147738   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 34434    |
----------------------------------
Eval num_timesteps=148000, episode_reward=22.10 +/- 5.70
Episode length: 209.28 +/- 63.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 22.1     |
| rollout/            |          |
|    exploration_rate | 0.0297   |
| time/               |          |
|    total_timesteps  | 148000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 34499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | 21.3     |
|    exploration_rate | 0.0249   |
| time/               |          |
|    episodes         | 880      |
|    fps              | 96       |
|    time_elapsed     | 1544     |
|    total_timesteps  | 148493   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0205   |
|    n_updates        | 34623    |
----------------------------------
Eval num_timesteps=148500, episode_reward=16.10 +/- 5.72
Episode length: 173.62 +/- 51.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 174      |
|    mean_reward      | 16.1     |
| rollout/            |          |
|    exploration_rate | 0.0248   |
| time/               |          |
|    total_timesteps  | 148500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0261   |
|    n_updates        | 34624    |
----------------------------------
Eval num_timesteps=149000, episode_reward=23.12 +/- 5.90
Episode length: 213.32 +/- 61.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 213      |
|    mean_reward      | 23.1     |
| rollout/            |          |
|    exploration_rate | 0.0199   |
| time/               |          |
|    total_timesteps  | 149000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0129   |
|    n_updates        | 34749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 207      |
|    ep_rew_mean      | 21.2     |
|    exploration_rate | 0.0173   |
| time/               |          |
|    episodes         | 884      |
|    fps              | 95       |
|    time_elapsed     | 1556     |
|    total_timesteps  | 149264   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 34815    |
----------------------------------
Eval num_timesteps=149500, episode_reward=23.48 +/- 4.44
Episode length: 214.04 +/- 45.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 23.5     |
| rollout/            |          |
|    exploration_rate | 0.015    |
| time/               |          |
|    total_timesteps  | 149500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0275   |
|    n_updates        | 34874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | 21.3     |
|    exploration_rate | 0.0101   |
| time/               |          |
|    episodes         | 888      |
|    fps              | 95       |
|    time_elapsed     | 1563     |
|    total_timesteps  | 149989   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0245   |
|    n_updates        | 34997    |
----------------------------------
Eval num_timesteps=150000, episode_reward=23.44 +/- 6.08
Episode length: 225.14 +/- 60.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 23.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 150000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0444   |
|    n_updates        | 34999    |
----------------------------------
Eval num_timesteps=150500, episode_reward=22.80 +/- 6.86
Episode length: 218.40 +/- 73.31
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 22.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 150500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0219   |
|    n_updates        | 35124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | 21.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 892      |
|    fps              | 95       |
|    time_elapsed     | 1576     |
|    total_timesteps  | 150962   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0443   |
|    n_updates        | 35240    |
----------------------------------
Eval num_timesteps=151000, episode_reward=23.04 +/- 4.69
Episode length: 217.74 +/- 50.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 23       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 151000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0616   |
|    n_updates        | 35249    |
----------------------------------
Eval num_timesteps=151500, episode_reward=24.80 +/- 5.71
Episode length: 223.10 +/- 57.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 151500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 35374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 208      |
|    ep_rew_mean      | 21.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 896      |
|    fps              | 95       |
|    time_elapsed     | 1589     |
|    total_timesteps  | 151717   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0206   |
|    n_updates        | 35429    |
----------------------------------
Eval num_timesteps=152000, episode_reward=22.12 +/- 5.58
Episode length: 203.88 +/- 55.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 22.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 152000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0282   |
|    n_updates        | 35499    |
----------------------------------
Eval num_timesteps=152500, episode_reward=14.72 +/- 5.69
Episode length: 164.50 +/- 59.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 164      |
|    mean_reward      | 14.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 152500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 35624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 209      |
|    ep_rew_mean      | 21.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 900      |
|    fps              | 95       |
|    time_elapsed     | 1600     |
|    total_timesteps  | 152528   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0147   |
|    n_updates        | 35631    |
----------------------------------
Eval num_timesteps=153000, episode_reward=23.32 +/- 6.37
Episode length: 220.34 +/- 68.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 23.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 153000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0298   |
|    n_updates        | 35749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 210      |
|    ep_rew_mean      | 21.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 904      |
|    fps              | 95       |
|    time_elapsed     | 1607     |
|    total_timesteps  | 153497   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0251   |
|    n_updates        | 35874    |
----------------------------------
Eval num_timesteps=153500, episode_reward=21.42 +/- 7.25
Episode length: 220.00 +/- 73.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 21.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 153500   |
----------------------------------
Eval num_timesteps=154000, episode_reward=24.84 +/- 5.57
Episode length: 232.24 +/- 61.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 154000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0208   |
|    n_updates        | 35999    |
----------------------------------
Eval num_timesteps=154500, episode_reward=24.52 +/- 6.59
Episode length: 229.42 +/- 70.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 24.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 154500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.019    |
|    n_updates        | 36124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 215      |
|    ep_rew_mean      | 22.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 908      |
|    fps              | 94       |
|    time_elapsed     | 1628     |
|    total_timesteps  | 154671   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.029    |
|    n_updates        | 36167    |
----------------------------------
Eval num_timesteps=155000, episode_reward=23.24 +/- 6.72
Episode length: 236.34 +/- 74.76
----------------------------------
| eval/               |          |
|    mean_ep_length   | 236      |
|    mean_reward      | 23.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 155000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0195   |
|    n_updates        | 36249    |
----------------------------------
Eval num_timesteps=155500, episode_reward=27.56 +/- 6.03
Episode length: 255.14 +/- 64.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 255      |
|    mean_reward      | 27.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 155500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0129   |
|    n_updates        | 36374    |
----------------------------------
New best mean reward!
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 22.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 912      |
|    fps              | 94       |
|    time_elapsed     | 1643     |
|    total_timesteps  | 155705   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0252   |
|    n_updates        | 36426    |
----------------------------------
Eval num_timesteps=156000, episode_reward=23.48 +/- 6.79
Episode length: 220.32 +/- 72.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 23.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 156000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.024    |
|    n_updates        | 36499    |
----------------------------------
Eval num_timesteps=156500, episode_reward=26.32 +/- 6.32
Episode length: 246.18 +/- 71.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 246      |
|    mean_reward      | 26.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 156500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0259   |
|    n_updates        | 36624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 22.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 916      |
|    fps              | 94       |
|    time_elapsed     | 1657     |
|    total_timesteps  | 156623   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0186   |
|    n_updates        | 36655    |
----------------------------------
Eval num_timesteps=157000, episode_reward=25.26 +/- 6.30
Episode length: 242.18 +/- 68.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 242      |
|    mean_reward      | 25.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 157000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0338   |
|    n_updates        | 36749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 22.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 920      |
|    fps              | 94       |
|    time_elapsed     | 1664     |
|    total_timesteps  | 157482   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.03     |
|    n_updates        | 36870    |
----------------------------------
Eval num_timesteps=157500, episode_reward=25.14 +/- 5.47
Episode length: 234.46 +/- 59.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 234      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 157500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0373   |
|    n_updates        | 36874    |
----------------------------------
Eval num_timesteps=158000, episode_reward=23.62 +/- 5.54
Episode length: 223.40 +/- 62.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 23.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 158000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0208   |
|    n_updates        | 36999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 215      |
|    ep_rew_mean      | 22.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 924      |
|    fps              | 94       |
|    time_elapsed     | 1678     |
|    total_timesteps  | 158120   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 37029    |
----------------------------------
Eval num_timesteps=158500, episode_reward=24.06 +/- 5.75
Episode length: 218.84 +/- 63.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 158500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0297   |
|    n_updates        | 37124    |
----------------------------------
Eval num_timesteps=159000, episode_reward=23.60 +/- 6.09
Episode length: 222.96 +/- 66.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 23.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 159000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.025    |
|    n_updates        | 37249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 22.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 928      |
|    fps              | 93       |
|    time_elapsed     | 1692     |
|    total_timesteps  | 159015   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0196   |
|    n_updates        | 37253    |
----------------------------------
Eval num_timesteps=159500, episode_reward=24.36 +/- 5.30
Episode length: 228.32 +/- 54.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 24.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 159500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0372   |
|    n_updates        | 37374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 215      |
|    ep_rew_mean      | 22.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 932      |
|    fps              | 93       |
|    time_elapsed     | 1700     |
|    total_timesteps  | 159775   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0349   |
|    n_updates        | 37443    |
----------------------------------
Eval num_timesteps=160000, episode_reward=25.32 +/- 6.06
Episode length: 229.50 +/- 61.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 25.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 160000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0224   |
|    n_updates        | 37499    |
----------------------------------
Eval num_timesteps=160500, episode_reward=21.02 +/- 6.10
Episode length: 205.94 +/- 59.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | 21       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 160500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0677   |
|    n_updates        | 37624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 22.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 936      |
|    fps              | 93       |
|    time_elapsed     | 1714     |
|    total_timesteps  | 160850   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0269   |
|    n_updates        | 37712    |
----------------------------------
Eval num_timesteps=161000, episode_reward=23.12 +/- 6.21
Episode length: 218.62 +/- 58.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 23.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 161000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0305   |
|    n_updates        | 37749    |
----------------------------------
Eval num_timesteps=161500, episode_reward=22.64 +/- 5.53
Episode length: 207.84 +/- 56.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 22.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 161500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0233   |
|    n_updates        | 37874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 22.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 940      |
|    fps              | 93       |
|    time_elapsed     | 1728     |
|    total_timesteps  | 161752   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0225   |
|    n_updates        | 37937    |
----------------------------------
Eval num_timesteps=162000, episode_reward=23.32 +/- 5.88
Episode length: 218.44 +/- 63.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 23.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 162000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0354   |
|    n_updates        | 37999    |
----------------------------------
Eval num_timesteps=162500, episode_reward=24.88 +/- 5.97
Episode length: 225.26 +/- 65.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 24.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 162500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0201   |
|    n_updates        | 38124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 22.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 944      |
|    fps              | 93       |
|    time_elapsed     | 1741     |
|    total_timesteps  | 162673   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0181   |
|    n_updates        | 38168    |
----------------------------------
Eval num_timesteps=163000, episode_reward=24.86 +/- 4.71
Episode length: 229.16 +/- 50.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 24.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 163000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 38249    |
----------------------------------
Eval num_timesteps=163500, episode_reward=23.78 +/- 5.81
Episode length: 231.04 +/- 65.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 23.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 163500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0214   |
|    n_updates        | 38374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 22.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 948      |
|    fps              | 93       |
|    time_elapsed     | 1755     |
|    total_timesteps  | 163651   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0195   |
|    n_updates        | 38412    |
----------------------------------
Eval num_timesteps=164000, episode_reward=23.80 +/- 6.17
Episode length: 231.12 +/- 62.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 23.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 164000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0276   |
|    n_updates        | 38499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 22.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 952      |
|    fps              | 93       |
|    time_elapsed     | 1762     |
|    total_timesteps  | 164393   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0226   |
|    n_updates        | 38598    |
----------------------------------
Eval num_timesteps=164500, episode_reward=24.30 +/- 5.88
Episode length: 227.82 +/- 64.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 24.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 164500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0246   |
|    n_updates        | 38624    |
----------------------------------
Eval num_timesteps=165000, episode_reward=22.78 +/- 5.11
Episode length: 212.48 +/- 47.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 22.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 165000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0226   |
|    n_updates        | 38749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 22.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 956      |
|    fps              | 93       |
|    time_elapsed     | 1776     |
|    total_timesteps  | 165317   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0323   |
|    n_updates        | 38829    |
----------------------------------
Eval num_timesteps=165500, episode_reward=25.76 +/- 6.67
Episode length: 250.26 +/- 72.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 250      |
|    mean_reward      | 25.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 165500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0154   |
|    n_updates        | 38874    |
----------------------------------
Eval num_timesteps=166000, episode_reward=24.84 +/- 5.72
Episode length: 232.62 +/- 61.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 233      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 166000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 38999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 22.5     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 960      |
|    fps              | 92       |
|    time_elapsed     | 1790     |
|    total_timesteps  | 166135   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0433   |
|    n_updates        | 39033    |
----------------------------------
Eval num_timesteps=166500, episode_reward=26.24 +/- 6.32
Episode length: 236.16 +/- 66.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 236      |
|    mean_reward      | 26.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 166500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0288   |
|    n_updates        | 39124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 215      |
|    ep_rew_mean      | 22.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 964      |
|    fps              | 92       |
|    time_elapsed     | 1798     |
|    total_timesteps  | 166828   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0218   |
|    n_updates        | 39206    |
----------------------------------
Eval num_timesteps=167000, episode_reward=22.86 +/- 5.13
Episode length: 222.16 +/- 55.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 22.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 167000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 39249    |
----------------------------------
Eval num_timesteps=167500, episode_reward=23.56 +/- 6.39
Episode length: 240.28 +/- 66.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 240      |
|    mean_reward      | 23.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 167500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.033    |
|    n_updates        | 39374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 22.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 968      |
|    fps              | 92       |
|    time_elapsed     | 1812     |
|    total_timesteps  | 167793   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0291   |
|    n_updates        | 39448    |
----------------------------------
Eval num_timesteps=168000, episode_reward=21.22 +/- 4.31
Episode length: 210.68 +/- 50.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 21.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 168000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 39499    |
----------------------------------
Eval num_timesteps=168500, episode_reward=22.42 +/- 6.83
Episode length: 231.82 +/- 73.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 22.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 168500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0294   |
|    n_updates        | 39624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 22.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 972      |
|    fps              | 92       |
|    time_elapsed     | 1825     |
|    total_timesteps  | 168652   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.025    |
|    n_updates        | 39662    |
----------------------------------
Eval num_timesteps=169000, episode_reward=25.94 +/- 5.66
Episode length: 234.36 +/- 59.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 234      |
|    mean_reward      | 25.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 169000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0198   |
|    n_updates        | 39749    |
----------------------------------
Eval num_timesteps=169500, episode_reward=25.06 +/- 6.15
Episode length: 235.12 +/- 65.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 235      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 169500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0224   |
|    n_updates        | 39874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 22.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 976      |
|    fps              | 92       |
|    time_elapsed     | 1840     |
|    total_timesteps  | 169631   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0342   |
|    n_updates        | 39907    |
----------------------------------
Eval num_timesteps=170000, episode_reward=24.76 +/- 5.81
Episode length: 244.58 +/- 70.89
----------------------------------
| eval/               |          |
|    mean_ep_length   | 245      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 170000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 39999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 22.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 980      |
|    fps              | 92       |
|    time_elapsed     | 1848     |
|    total_timesteps  | 170414   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0187   |
|    n_updates        | 40103    |
----------------------------------
Eval num_timesteps=170500, episode_reward=26.24 +/- 4.91
Episode length: 236.84 +/- 51.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 237      |
|    mean_reward      | 26.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 170500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0191   |
|    n_updates        | 40124    |
----------------------------------
Eval num_timesteps=171000, episode_reward=25.36 +/- 5.95
Episode length: 235.60 +/- 64.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 236      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 171000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0131   |
|    n_updates        | 40249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 23       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 984      |
|    fps              | 91       |
|    time_elapsed     | 1862     |
|    total_timesteps  | 171328   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0223   |
|    n_updates        | 40331    |
----------------------------------
Eval num_timesteps=171500, episode_reward=25.30 +/- 6.41
Episode length: 234.04 +/- 67.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 234      |
|    mean_reward      | 25.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 171500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 40374    |
----------------------------------
Eval num_timesteps=172000, episode_reward=24.58 +/- 5.87
Episode length: 238.62 +/- 66.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 239      |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 172000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0296   |
|    n_updates        | 40499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 23.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 988      |
|    fps              | 91       |
|    time_elapsed     | 1876     |
|    total_timesteps  | 172351   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0174   |
|    n_updates        | 40587    |
----------------------------------
Eval num_timesteps=172500, episode_reward=26.38 +/- 5.60
Episode length: 234.78 +/- 61.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 235      |
|    mean_reward      | 26.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 172500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0287   |
|    n_updates        | 40624    |
----------------------------------
Eval num_timesteps=173000, episode_reward=25.52 +/- 5.92
Episode length: 230.16 +/- 64.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 25.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 173000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0142   |
|    n_updates        | 40749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 23.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 992      |
|    fps              | 91       |
|    time_elapsed     | 1890     |
|    total_timesteps  | 173471   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0275   |
|    n_updates        | 40867    |
----------------------------------
Eval num_timesteps=173500, episode_reward=24.88 +/- 5.80
Episode length: 231.24 +/- 57.19
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 24.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 173500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0252   |
|    n_updates        | 40874    |
----------------------------------
Eval num_timesteps=174000, episode_reward=22.48 +/- 7.03
Episode length: 211.48 +/- 65.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 22.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 174000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0129   |
|    n_updates        | 40999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 227      |
|    ep_rew_mean      | 23.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 996      |
|    fps              | 91       |
|    time_elapsed     | 1904     |
|    total_timesteps  | 174446   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 41111    |
----------------------------------
Eval num_timesteps=174500, episode_reward=25.02 +/- 6.09
Episode length: 222.28 +/- 61.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 174500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0349   |
|    n_updates        | 41124    |
----------------------------------
Eval num_timesteps=175000, episode_reward=25.92 +/- 5.28
Episode length: 232.06 +/- 54.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 25.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 175000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0749   |
|    n_updates        | 41249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 227      |
|    ep_rew_mean      | 23.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1000     |
|    fps              | 91       |
|    time_elapsed     | 1917     |
|    total_timesteps  | 175277   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.025    |
|    n_updates        | 41319    |
----------------------------------
Eval num_timesteps=175500, episode_reward=25.38 +/- 5.19
Episode length: 233.84 +/- 55.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 234      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 175500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0263   |
|    n_updates        | 41374    |
----------------------------------
Eval num_timesteps=176000, episode_reward=24.64 +/- 6.41
Episode length: 224.26 +/- 69.38
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 176000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.02     |
|    n_updates        | 41499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 226      |
|    ep_rew_mean      | 23.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1004     |
|    fps              | 91       |
|    time_elapsed     | 1931     |
|    total_timesteps  | 176127   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.025    |
|    n_updates        | 41531    |
----------------------------------
Eval num_timesteps=176500, episode_reward=24.82 +/- 4.59
Episode length: 218.58 +/- 48.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 176500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0132   |
|    n_updates        | 41624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 23.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1008     |
|    fps              | 91       |
|    time_elapsed     | 1938     |
|    total_timesteps  | 176761   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0222   |
|    n_updates        | 41690    |
----------------------------------
Eval num_timesteps=177000, episode_reward=25.34 +/- 5.82
Episode length: 231.56 +/- 61.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 25.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 177000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0353   |
|    n_updates        | 41749    |
----------------------------------
Eval num_timesteps=177500, episode_reward=26.30 +/- 5.26
Episode length: 233.82 +/- 51.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 234      |
|    mean_reward      | 26.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 177500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0277   |
|    n_updates        | 41874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 23.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1012     |
|    fps              | 91       |
|    time_elapsed     | 1952     |
|    total_timesteps  | 177713   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0245   |
|    n_updates        | 41928    |
----------------------------------
Eval num_timesteps=178000, episode_reward=26.08 +/- 4.60
Episode length: 232.30 +/- 50.40
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 26.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 178000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0396   |
|    n_updates        | 41999    |
----------------------------------
Eval num_timesteps=178500, episode_reward=23.56 +/- 4.65
Episode length: 217.92 +/- 50.26
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 23.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 178500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0181   |
|    n_updates        | 42124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 23.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1016     |
|    fps              | 90       |
|    time_elapsed     | 1965     |
|    total_timesteps  | 178508   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0287   |
|    n_updates        | 42126    |
----------------------------------
Eval num_timesteps=179000, episode_reward=25.86 +/- 6.28
Episode length: 229.38 +/- 58.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 25.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 179000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0177   |
|    n_updates        | 42249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 23.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1020     |
|    fps              | 90       |
|    time_elapsed     | 1973     |
|    total_timesteps  | 179373   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0263   |
|    n_updates        | 42343    |
----------------------------------
Eval num_timesteps=179500, episode_reward=24.66 +/- 4.96
Episode length: 229.74 +/- 57.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 179500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0113   |
|    n_updates        | 42374    |
----------------------------------
Eval num_timesteps=180000, episode_reward=25.36 +/- 4.54
Episode length: 230.04 +/- 52.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 180000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0782   |
|    n_updates        | 42499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 23.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1024     |
|    fps              | 90       |
|    time_elapsed     | 1987     |
|    total_timesteps  | 180431   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0439   |
|    n_updates        | 42607    |
----------------------------------
Eval num_timesteps=180500, episode_reward=24.74 +/- 5.80
Episode length: 214.02 +/- 56.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 180500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0645   |
|    n_updates        | 42624    |
----------------------------------
Eval num_timesteps=181000, episode_reward=22.62 +/- 5.52
Episode length: 209.40 +/- 57.60
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 22.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 181000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0332   |
|    n_updates        | 42749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 23.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1028     |
|    fps              | 90       |
|    time_elapsed     | 1999     |
|    total_timesteps  | 181253   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0583   |
|    n_updates        | 42813    |
----------------------------------
Eval num_timesteps=181500, episode_reward=24.06 +/- 6.10
Episode length: 212.48 +/- 59.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 24.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 181500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0316   |
|    n_updates        | 42874    |
----------------------------------
Eval num_timesteps=182000, episode_reward=26.84 +/- 5.28
Episode length: 239.84 +/- 57.98
----------------------------------
| eval/               |          |
|    mean_ep_length   | 240      |
|    mean_reward      | 26.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 182000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0273   |
|    n_updates        | 42999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 23.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1032     |
|    fps              | 90       |
|    time_elapsed     | 2013     |
|    total_timesteps  | 182003   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0401   |
|    n_updates        | 43000    |
----------------------------------
Eval num_timesteps=182500, episode_reward=26.82 +/- 6.13
Episode length: 236.24 +/- 71.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 236      |
|    mean_reward      | 26.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 182500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0177   |
|    n_updates        | 43124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 23.5     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1036     |
|    fps              | 90       |
|    time_elapsed     | 2020     |
|    total_timesteps  | 182711   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0234   |
|    n_updates        | 43177    |
----------------------------------
Eval num_timesteps=183000, episode_reward=25.76 +/- 5.50
Episode length: 226.42 +/- 54.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 25.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 183000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0202   |
|    n_updates        | 43249    |
----------------------------------
Eval num_timesteps=183500, episode_reward=25.42 +/- 5.85
Episode length: 218.00 +/- 56.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 183500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0138   |
|    n_updates        | 43374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 23.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1040     |
|    fps              | 90       |
|    time_elapsed     | 2034     |
|    total_timesteps  | 183633   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 43408    |
----------------------------------
Eval num_timesteps=184000, episode_reward=24.52 +/- 5.19
Episode length: 209.44 +/- 49.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 24.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 184000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0347   |
|    n_updates        | 43499    |
----------------------------------
Eval num_timesteps=184500, episode_reward=23.68 +/- 5.28
Episode length: 218.20 +/- 48.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 23.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 184500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0301   |
|    n_updates        | 43624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 23.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1044     |
|    fps              | 90       |
|    time_elapsed     | 2047     |
|    total_timesteps  | 184615   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0393   |
|    n_updates        | 43653    |
----------------------------------
Eval num_timesteps=185000, episode_reward=24.00 +/- 5.62
Episode length: 211.16 +/- 56.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 24       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 185000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0886   |
|    n_updates        | 43749    |
----------------------------------
Eval num_timesteps=185500, episode_reward=25.50 +/- 5.60
Episode length: 224.62 +/- 56.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 25.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 185500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.019    |
|    n_updates        | 43874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 23.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1048     |
|    fps              | 90       |
|    time_elapsed     | 2060     |
|    total_timesteps  | 185555   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.013    |
|    n_updates        | 43888    |
----------------------------------
Eval num_timesteps=186000, episode_reward=24.16 +/- 5.73
Episode length: 209.50 +/- 61.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 24.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 186000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0266   |
|    n_updates        | 43999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 23.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1052     |
|    fps              | 90       |
|    time_elapsed     | 2067     |
|    total_timesteps  | 186426   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0214   |
|    n_updates        | 44106    |
----------------------------------
Eval num_timesteps=186500, episode_reward=25.14 +/- 5.28
Episode length: 218.42 +/- 49.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 186500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0196   |
|    n_updates        | 44124    |
----------------------------------
Eval num_timesteps=187000, episode_reward=25.04 +/- 6.40
Episode length: 223.98 +/- 64.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 187000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0246   |
|    n_updates        | 44249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 24       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1056     |
|    fps              | 90       |
|    time_elapsed     | 2080     |
|    total_timesteps  | 187422   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0323   |
|    n_updates        | 44355    |
----------------------------------
Eval num_timesteps=187500, episode_reward=26.86 +/- 5.43
Episode length: 235.74 +/- 51.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 236      |
|    mean_reward      | 26.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 187500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0259   |
|    n_updates        | 44374    |
----------------------------------
Eval num_timesteps=188000, episode_reward=25.42 +/- 6.89
Episode length: 229.68 +/- 76.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 188000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0344   |
|    n_updates        | 44499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 24.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1060     |
|    fps              | 89       |
|    time_elapsed     | 2094     |
|    total_timesteps  | 188451   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0332   |
|    n_updates        | 44612    |
----------------------------------
Eval num_timesteps=188500, episode_reward=24.88 +/- 5.23
Episode length: 218.22 +/- 55.84
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 24.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 188500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0136   |
|    n_updates        | 44624    |
----------------------------------
Eval num_timesteps=189000, episode_reward=25.58 +/- 4.27
Episode length: 225.70 +/- 45.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 25.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 189000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0446   |
|    n_updates        | 44749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 24.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1064     |
|    fps              | 89       |
|    time_elapsed     | 2107     |
|    total_timesteps  | 189367   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0601   |
|    n_updates        | 44841    |
----------------------------------
Eval num_timesteps=189500, episode_reward=24.82 +/- 6.71
Episode length: 219.86 +/- 61.15
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 189500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0209   |
|    n_updates        | 44874    |
----------------------------------
Eval num_timesteps=190000, episode_reward=25.66 +/- 5.81
Episode length: 226.46 +/- 57.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 25.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 190000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0128   |
|    n_updates        | 44999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 24.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1068     |
|    fps              | 89       |
|    time_elapsed     | 2121     |
|    total_timesteps  | 190182   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0431   |
|    n_updates        | 45045    |
----------------------------------
Eval num_timesteps=190500, episode_reward=23.34 +/- 6.05
Episode length: 214.42 +/- 57.92
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 23.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 190500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0305   |
|    n_updates        | 45124    |
----------------------------------
Eval num_timesteps=191000, episode_reward=26.22 +/- 6.35
Episode length: 234.42 +/- 65.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 234      |
|    mean_reward      | 26.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 191000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0243   |
|    n_updates        | 45249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 226      |
|    ep_rew_mean      | 24.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1072     |
|    fps              | 89       |
|    time_elapsed     | 2135     |
|    total_timesteps  | 191220   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 45304    |
----------------------------------
Eval num_timesteps=191500, episode_reward=24.06 +/- 6.09
Episode length: 213.52 +/- 62.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 24.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 191500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0245   |
|    n_updates        | 45374    |
----------------------------------
Eval num_timesteps=192000, episode_reward=25.40 +/- 5.45
Episode length: 221.88 +/- 54.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 192000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0361   |
|    n_updates        | 45499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 24.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1076     |
|    fps              | 89       |
|    time_elapsed     | 2148     |
|    total_timesteps  | 192101   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0218   |
|    n_updates        | 45525    |
----------------------------------
Eval num_timesteps=192500, episode_reward=24.32 +/- 5.17
Episode length: 213.94 +/- 51.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 24.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 192500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0122   |
|    n_updates        | 45624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 24.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1080     |
|    fps              | 89       |
|    time_elapsed     | 2154     |
|    total_timesteps  | 192753   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0259   |
|    n_updates        | 45688    |
----------------------------------
Eval num_timesteps=193000, episode_reward=23.02 +/- 4.93
Episode length: 211.94 +/- 46.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 23       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 193000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0274   |
|    n_updates        | 45749    |
----------------------------------
Eval num_timesteps=193500, episode_reward=25.36 +/- 6.34
Episode length: 226.24 +/- 64.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 193500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0412   |
|    n_updates        | 45874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 24.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1084     |
|    fps              | 89       |
|    time_elapsed     | 2167     |
|    total_timesteps  | 193659   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 45914    |
----------------------------------
Eval num_timesteps=194000, episode_reward=23.24 +/- 5.44
Episode length: 208.50 +/- 53.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 23.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 194000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0201   |
|    n_updates        | 45999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 24.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1088     |
|    fps              | 89       |
|    time_elapsed     | 2174     |
|    total_timesteps  | 194399   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0148   |
|    n_updates        | 46099    |
----------------------------------
Eval num_timesteps=194500, episode_reward=24.68 +/- 5.16
Episode length: 218.66 +/- 46.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 194500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.024    |
|    n_updates        | 46124    |
----------------------------------
Eval num_timesteps=195000, episode_reward=25.06 +/- 5.75
Episode length: 217.14 +/- 58.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 195000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0329   |
|    n_updates        | 46249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 24.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1092     |
|    fps              | 89       |
|    time_elapsed     | 2187     |
|    total_timesteps  | 195141   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0261   |
|    n_updates        | 46285    |
----------------------------------
Eval num_timesteps=195500, episode_reward=25.40 +/- 5.77
Episode length: 228.06 +/- 59.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 195500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0337   |
|    n_updates        | 46374    |
----------------------------------
Eval num_timesteps=196000, episode_reward=26.00 +/- 5.09
Episode length: 227.48 +/- 46.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 227      |
|    mean_reward      | 26       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 196000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0305   |
|    n_updates        | 46499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 24.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1096     |
|    fps              | 89       |
|    time_elapsed     | 2201     |
|    total_timesteps  | 196006   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0297   |
|    n_updates        | 46501    |
----------------------------------
Eval num_timesteps=196500, episode_reward=25.04 +/- 5.18
Episode length: 219.06 +/- 49.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 196500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0319   |
|    n_updates        | 46624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 24.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1100     |
|    fps              | 89       |
|    time_elapsed     | 2208     |
|    total_timesteps  | 196878   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0438   |
|    n_updates        | 46719    |
----------------------------------
Eval num_timesteps=197000, episode_reward=24.74 +/- 5.12
Episode length: 224.50 +/- 53.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 197000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.019    |
|    n_updates        | 46749    |
----------------------------------
Eval num_timesteps=197500, episode_reward=25.58 +/- 4.68
Episode length: 223.92 +/- 45.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 25.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 197500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0192   |
|    n_updates        | 46874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 24.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1104     |
|    fps              | 89       |
|    time_elapsed     | 2221     |
|    total_timesteps  | 197739   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 46934    |
----------------------------------
Eval num_timesteps=198000, episode_reward=22.82 +/- 4.88
Episode length: 210.46 +/- 52.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 22.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 198000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0164   |
|    n_updates        | 46999    |
----------------------------------
Eval num_timesteps=198500, episode_reward=25.16 +/- 4.49
Episode length: 224.42 +/- 42.46
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 25.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 198500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0192   |
|    n_updates        | 47124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 24.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1108     |
|    fps              | 88       |
|    time_elapsed     | 2234     |
|    total_timesteps  | 198509   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0329   |
|    n_updates        | 47127    |
----------------------------------
Eval num_timesteps=199000, episode_reward=25.66 +/- 4.64
Episode length: 219.20 +/- 46.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 25.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 199000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0296   |
|    n_updates        | 47249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 215      |
|    ep_rew_mean      | 24.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1112     |
|    fps              | 88       |
|    time_elapsed     | 2241     |
|    total_timesteps  | 199221   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0755   |
|    n_updates        | 47305    |
----------------------------------
Eval num_timesteps=199500, episode_reward=24.38 +/- 5.77
Episode length: 217.90 +/- 55.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 24.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 199500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0352   |
|    n_updates        | 47374    |
----------------------------------
Eval num_timesteps=200000, episode_reward=25.12 +/- 5.27
Episode length: 230.46 +/- 57.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 200000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0483   |
|    n_updates        | 47499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 24.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1116     |
|    fps              | 88       |
|    time_elapsed     | 2255     |
|    total_timesteps  | 200163   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0442   |
|    n_updates        | 47540    |
----------------------------------
Eval num_timesteps=200500, episode_reward=24.64 +/- 4.81
Episode length: 212.46 +/- 46.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 200500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 47624    |
----------------------------------
Eval num_timesteps=201000, episode_reward=25.56 +/- 5.28
Episode length: 223.56 +/- 53.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 25.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 201000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0262   |
|    n_updates        | 47749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 24.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1120     |
|    fps              | 88       |
|    time_elapsed     | 2268     |
|    total_timesteps  | 201116   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0406   |
|    n_updates        | 47778    |
----------------------------------
Eval num_timesteps=201500, episode_reward=22.22 +/- 5.12
Episode length: 201.04 +/- 53.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 201      |
|    mean_reward      | 22.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 201500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0464   |
|    n_updates        | 47874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 24.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1124     |
|    fps              | 88       |
|    time_elapsed     | 2274     |
|    total_timesteps  | 201996   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0293   |
|    n_updates        | 47998    |
----------------------------------
Eval num_timesteps=202000, episode_reward=24.14 +/- 5.47
Episode length: 215.32 +/- 54.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 215      |
|    mean_reward      | 24.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 202000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0567   |
|    n_updates        | 47999    |
----------------------------------
Eval num_timesteps=202500, episode_reward=24.02 +/- 4.68
Episode length: 218.60 +/- 48.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 202500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0237   |
|    n_updates        | 48124    |
----------------------------------
Eval num_timesteps=203000, episode_reward=23.66 +/- 6.58
Episode length: 208.92 +/- 64.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 23.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 203000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0251   |
|    n_updates        | 48249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 24.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1128     |
|    fps              | 88       |
|    time_elapsed     | 2293     |
|    total_timesteps  | 203059   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.046    |
|    n_updates        | 48264    |
----------------------------------
Eval num_timesteps=203500, episode_reward=22.94 +/- 5.28
Episode length: 212.56 +/- 58.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 213      |
|    mean_reward      | 22.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 203500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0726   |
|    n_updates        | 48374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 24.5     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1132     |
|    fps              | 88       |
|    time_elapsed     | 2300     |
|    total_timesteps  | 203852   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0924   |
|    n_updates        | 48462    |
----------------------------------
Eval num_timesteps=204000, episode_reward=22.82 +/- 5.42
Episode length: 208.12 +/- 55.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 22.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 204000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0151   |
|    n_updates        | 48499    |
----------------------------------
Eval num_timesteps=204500, episode_reward=25.30 +/- 5.88
Episode length: 227.90 +/- 60.34
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 25.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 204500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0272   |
|    n_updates        | 48624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 24.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1136     |
|    fps              | 88       |
|    time_elapsed     | 2313     |
|    total_timesteps  | 204740   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0403   |
|    n_updates        | 48684    |
----------------------------------
Eval num_timesteps=205000, episode_reward=21.74 +/- 5.24
Episode length: 196.64 +/- 53.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 197      |
|    mean_reward      | 21.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 205000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.029    |
|    n_updates        | 48749    |
----------------------------------
Eval num_timesteps=205500, episode_reward=21.06 +/- 5.83
Episode length: 195.30 +/- 57.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 195      |
|    mean_reward      | 21.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 205500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0304   |
|    n_updates        | 48874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 24.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1140     |
|    fps              | 88       |
|    time_elapsed     | 2325     |
|    total_timesteps  | 205757   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0371   |
|    n_updates        | 48939    |
----------------------------------
Eval num_timesteps=206000, episode_reward=24.64 +/- 4.80
Episode length: 220.42 +/- 52.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 206000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 48999    |
----------------------------------
Eval num_timesteps=206500, episode_reward=22.28 +/- 5.42
Episode length: 204.70 +/- 58.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 22.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 206500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0315   |
|    n_updates        | 49124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 24.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1144     |
|    fps              | 88       |
|    time_elapsed     | 2338     |
|    total_timesteps  | 206624   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0223   |
|    n_updates        | 49155    |
----------------------------------
Eval num_timesteps=207000, episode_reward=22.78 +/- 7.17
Episode length: 210.72 +/- 66.10
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 22.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 207000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 49249    |
----------------------------------
Eval num_timesteps=207500, episode_reward=24.90 +/- 5.68
Episode length: 221.04 +/- 52.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 24.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 207500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0353   |
|    n_updates        | 49374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 24.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1148     |
|    fps              | 88       |
|    time_elapsed     | 2351     |
|    total_timesteps  | 207740   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0206   |
|    n_updates        | 49434    |
----------------------------------
Eval num_timesteps=208000, episode_reward=25.42 +/- 4.24
Episode length: 226.96 +/- 44.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 227      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 208000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0219   |
|    n_updates        | 49499    |
----------------------------------
Eval num_timesteps=208500, episode_reward=25.98 +/- 5.43
Episode length: 225.12 +/- 54.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 26       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 208500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0322   |
|    n_updates        | 49624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 24.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1152     |
|    fps              | 88       |
|    time_elapsed     | 2365     |
|    total_timesteps  | 208609   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0176   |
|    n_updates        | 49652    |
----------------------------------
Eval num_timesteps=209000, episode_reward=25.06 +/- 4.80
Episode length: 226.08 +/- 49.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 209000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.018    |
|    n_updates        | 49749    |
----------------------------------
Eval num_timesteps=209500, episode_reward=25.04 +/- 5.66
Episode length: 221.28 +/- 55.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 209500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 49874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 24.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1156     |
|    fps              | 88       |
|    time_elapsed     | 2378     |
|    total_timesteps  | 209555   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 49888    |
----------------------------------
Eval num_timesteps=210000, episode_reward=23.36 +/- 5.58
Episode length: 211.20 +/- 54.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 23.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 210000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0331   |
|    n_updates        | 49999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 24.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1160     |
|    fps              | 88       |
|    time_elapsed     | 2385     |
|    total_timesteps  | 210455   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0341   |
|    n_updates        | 50113    |
----------------------------------
Eval num_timesteps=210500, episode_reward=23.44 +/- 5.39
Episode length: 210.92 +/- 52.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 23.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 210500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0242   |
|    n_updates        | 50124    |
----------------------------------
Eval num_timesteps=211000, episode_reward=24.38 +/- 6.01
Episode length: 212.28 +/- 61.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 24.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 211000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0229   |
|    n_updates        | 50249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 24.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1164     |
|    fps              | 88       |
|    time_elapsed     | 2398     |
|    total_timesteps  | 211364   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0356   |
|    n_updates        | 50340    |
----------------------------------
Eval num_timesteps=211500, episode_reward=23.50 +/- 5.88
Episode length: 212.52 +/- 53.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 213      |
|    mean_reward      | 23.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 211500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0327   |
|    n_updates        | 50374    |
----------------------------------
Eval num_timesteps=212000, episode_reward=23.90 +/- 5.81
Episode length: 216.16 +/- 55.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 23.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 212000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0241   |
|    n_updates        | 50499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 24.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1168     |
|    fps              | 87       |
|    time_elapsed     | 2411     |
|    total_timesteps  | 212170   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 50542    |
----------------------------------
Eval num_timesteps=212500, episode_reward=23.36 +/- 5.02
Episode length: 212.06 +/- 47.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 23.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 212500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 50624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 23.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1172     |
|    fps              | 88       |
|    time_elapsed     | 2418     |
|    total_timesteps  | 212862   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.037    |
|    n_updates        | 50715    |
----------------------------------
Eval num_timesteps=213000, episode_reward=22.64 +/- 5.75
Episode length: 205.58 +/- 52.07
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | 22.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 213000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0212   |
|    n_updates        | 50749    |
----------------------------------
Eval num_timesteps=213500, episode_reward=25.40 +/- 5.08
Episode length: 228.54 +/- 50.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 213500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0296   |
|    n_updates        | 50874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 23.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1176     |
|    fps              | 87       |
|    time_elapsed     | 2431     |
|    total_timesteps  | 213753   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0262   |
|    n_updates        | 50938    |
----------------------------------
Eval num_timesteps=214000, episode_reward=23.92 +/- 6.32
Episode length: 216.80 +/- 63.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 23.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 214000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0752   |
|    n_updates        | 50999    |
----------------------------------
Eval num_timesteps=214500, episode_reward=24.56 +/- 5.71
Episode length: 215.10 +/- 54.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 215      |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 214500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 51124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 24.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1180     |
|    fps              | 87       |
|    time_elapsed     | 2444     |
|    total_timesteps  | 214808   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0397   |
|    n_updates        | 51201    |
----------------------------------
Eval num_timesteps=215000, episode_reward=26.22 +/- 4.95
Episode length: 229.44 +/- 48.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 26.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 215000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0155   |
|    n_updates        | 51249    |
----------------------------------
Eval num_timesteps=215500, episode_reward=25.50 +/- 5.17
Episode length: 221.82 +/- 51.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 25.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 215500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0234   |
|    n_updates        | 51374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 24.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1184     |
|    fps              | 87       |
|    time_elapsed     | 2458     |
|    total_timesteps  | 215822   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0323   |
|    n_updates        | 51455    |
----------------------------------
Eval num_timesteps=216000, episode_reward=25.84 +/- 4.77
Episode length: 221.76 +/- 49.36
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 25.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 216000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.03     |
|    n_updates        | 51499    |
----------------------------------
Eval num_timesteps=216500, episode_reward=25.02 +/- 5.90
Episode length: 214.00 +/- 57.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 216500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0236   |
|    n_updates        | 51624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 24.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1188     |
|    fps              | 87       |
|    time_elapsed     | 2471     |
|    total_timesteps  | 216845   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0351   |
|    n_updates        | 51711    |
----------------------------------
Eval num_timesteps=217000, episode_reward=24.40 +/- 5.45
Episode length: 218.84 +/- 55.64
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 217000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0143   |
|    n_updates        | 51749    |
----------------------------------
Eval num_timesteps=217500, episode_reward=25.54 +/- 5.43
Episode length: 227.46 +/- 57.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 227      |
|    mean_reward      | 25.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 217500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 51874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 227      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1192     |
|    fps              | 87       |
|    time_elapsed     | 2484     |
|    total_timesteps  | 217836   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0276   |
|    n_updates        | 51958    |
----------------------------------
Eval num_timesteps=218000, episode_reward=26.40 +/- 5.24
Episode length: 232.54 +/- 51.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 233      |
|    mean_reward      | 26.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 218000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0231   |
|    n_updates        | 51999    |
----------------------------------
Eval num_timesteps=218500, episode_reward=21.96 +/- 6.85
Episode length: 195.64 +/- 48.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 196      |
|    mean_reward      | 22       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 218500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0465   |
|    n_updates        | 52124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 228      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1196     |
|    fps              | 87       |
|    time_elapsed     | 2497     |
|    total_timesteps  | 218788   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0356   |
|    n_updates        | 52196    |
----------------------------------
Eval num_timesteps=219000, episode_reward=26.42 +/- 5.83
Episode length: 233.74 +/- 58.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 234      |
|    mean_reward      | 26.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 219000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0539   |
|    n_updates        | 52249    |
----------------------------------
Eval num_timesteps=219500, episode_reward=23.62 +/- 5.74
Episode length: 213.88 +/- 56.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 23.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 219500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0282   |
|    n_updates        | 52374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 228      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1200     |
|    fps              | 87       |
|    time_elapsed     | 2511     |
|    total_timesteps  | 219688   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0378   |
|    n_updates        | 52421    |
----------------------------------
Eval num_timesteps=220000, episode_reward=26.30 +/- 5.21
Episode length: 228.10 +/- 51.14
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 26.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 220000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0361   |
|    n_updates        | 52499    |
----------------------------------
Eval num_timesteps=220500, episode_reward=23.90 +/- 5.87
Episode length: 210.54 +/- 61.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 23.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 220500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0218   |
|    n_updates        | 52624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 230      |
|    ep_rew_mean      | 25.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1204     |
|    fps              | 87       |
|    time_elapsed     | 2524     |
|    total_timesteps  | 220719   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0277   |
|    n_updates        | 52679    |
----------------------------------
Eval num_timesteps=221000, episode_reward=25.32 +/- 4.73
Episode length: 226.44 +/- 50.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 25.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 221000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0644   |
|    n_updates        | 52749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 230      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1208     |
|    fps              | 87       |
|    time_elapsed     | 2531     |
|    total_timesteps  | 221465   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 52866    |
----------------------------------
Eval num_timesteps=221500, episode_reward=20.52 +/- 6.02
Episode length: 197.06 +/- 57.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 197      |
|    mean_reward      | 20.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 221500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0146   |
|    n_updates        | 52874    |
----------------------------------
Eval num_timesteps=222000, episode_reward=23.70 +/- 5.73
Episode length: 205.86 +/- 49.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | 23.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 222000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0146   |
|    n_updates        | 52999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 230      |
|    ep_rew_mean      | 25.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1212     |
|    fps              | 87       |
|    time_elapsed     | 2543     |
|    total_timesteps  | 222224   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0243   |
|    n_updates        | 53055    |
----------------------------------
Eval num_timesteps=222500, episode_reward=23.78 +/- 5.55
Episode length: 205.26 +/- 51.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 23.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 222500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0325   |
|    n_updates        | 53124    |
----------------------------------
Eval num_timesteps=223000, episode_reward=24.64 +/- 5.37
Episode length: 218.12 +/- 56.91
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 223000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.042    |
|    n_updates        | 53249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 231      |
|    ep_rew_mean      | 25.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1216     |
|    fps              | 87       |
|    time_elapsed     | 2556     |
|    total_timesteps  | 223257   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 53314    |
----------------------------------
Eval num_timesteps=223500, episode_reward=24.66 +/- 5.81
Episode length: 222.70 +/- 53.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 223500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0269   |
|    n_updates        | 53374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 228      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1220     |
|    fps              | 87       |
|    time_elapsed     | 2563     |
|    total_timesteps  | 223949   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0363   |
|    n_updates        | 53487    |
----------------------------------
Eval num_timesteps=224000, episode_reward=26.20 +/- 5.32
Episode length: 228.78 +/- 56.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 26.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 224000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0362   |
|    n_updates        | 53499    |
----------------------------------
Eval num_timesteps=224500, episode_reward=25.84 +/- 5.64
Episode length: 229.42 +/- 55.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 25.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 224500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0427   |
|    n_updates        | 53624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 228      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1224     |
|    fps              | 87       |
|    time_elapsed     | 2577     |
|    total_timesteps  | 224781   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0259   |
|    n_updates        | 53695    |
----------------------------------
Eval num_timesteps=225000, episode_reward=24.36 +/- 5.44
Episode length: 214.58 +/- 55.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 215      |
|    mean_reward      | 24.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 225000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0347   |
|    n_updates        | 53749    |
----------------------------------
Eval num_timesteps=225500, episode_reward=23.32 +/- 4.50
Episode length: 204.76 +/- 44.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 23.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 225500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.037    |
|    n_updates        | 53874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1228     |
|    fps              | 87       |
|    time_elapsed     | 2590     |
|    total_timesteps  | 225503   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0324   |
|    n_updates        | 53875    |
----------------------------------
Eval num_timesteps=226000, episode_reward=24.06 +/- 5.61
Episode length: 209.04 +/- 48.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 24.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 226000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 53999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1232     |
|    fps              | 87       |
|    time_elapsed     | 2596     |
|    total_timesteps  | 226220   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0218   |
|    n_updates        | 54054    |
----------------------------------
Eval num_timesteps=226500, episode_reward=23.26 +/- 4.99
Episode length: 212.20 +/- 49.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 23.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 226500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0218   |
|    n_updates        | 54124    |
----------------------------------
Eval num_timesteps=227000, episode_reward=24.52 +/- 5.68
Episode length: 212.34 +/- 52.93
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 24.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 227000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0145   |
|    n_updates        | 54249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1236     |
|    fps              | 87       |
|    time_elapsed     | 2609     |
|    total_timesteps  | 227202   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0496   |
|    n_updates        | 54300    |
----------------------------------
Eval num_timesteps=227500, episode_reward=25.32 +/- 6.83
Episode length: 225.12 +/- 66.04
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 25.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 227500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0187   |
|    n_updates        | 54374    |
----------------------------------
Eval num_timesteps=228000, episode_reward=25.76 +/- 5.45
Episode length: 230.00 +/- 54.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 25.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 228000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0266   |
|    n_updates        | 54499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1240     |
|    fps              | 86       |
|    time_elapsed     | 2623     |
|    total_timesteps  | 228224   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0449   |
|    n_updates        | 54555    |
----------------------------------
Eval num_timesteps=228500, episode_reward=25.88 +/- 5.84
Episode length: 232.10 +/- 58.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 25.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 228500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0237   |
|    n_updates        | 54624    |
----------------------------------
Eval num_timesteps=229000, episode_reward=24.80 +/- 4.89
Episode length: 218.80 +/- 48.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 229000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0236   |
|    n_updates        | 54749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 227      |
|    ep_rew_mean      | 25.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1244     |
|    fps              | 86       |
|    time_elapsed     | 2637     |
|    total_timesteps  | 229347   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.052    |
|    n_updates        | 54836    |
----------------------------------
Eval num_timesteps=229500, episode_reward=25.12 +/- 5.94
Episode length: 223.24 +/- 63.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 229500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0121   |
|    n_updates        | 54874    |
----------------------------------
Eval num_timesteps=230000, episode_reward=26.16 +/- 5.74
Episode length: 229.46 +/- 55.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 26.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 230000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.028    |
|    n_updates        | 54999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1248     |
|    fps              | 86       |
|    time_elapsed     | 2650     |
|    total_timesteps  | 230179   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0367   |
|    n_updates        | 55044    |
----------------------------------
Eval num_timesteps=230500, episode_reward=24.98 +/- 5.24
Episode length: 220.28 +/- 56.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 230500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0253   |
|    n_updates        | 55124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1252     |
|    fps              | 86       |
|    time_elapsed     | 2657     |
|    total_timesteps  | 230973   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0387   |
|    n_updates        | 55243    |
----------------------------------
Eval num_timesteps=231000, episode_reward=24.40 +/- 5.39
Episode length: 219.40 +/- 53.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 231000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0329   |
|    n_updates        | 55249    |
----------------------------------
Eval num_timesteps=231500, episode_reward=26.08 +/- 4.53
Episode length: 230.26 +/- 45.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 26.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 231500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0814   |
|    n_updates        | 55374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1256     |
|    fps              | 86       |
|    time_elapsed     | 2671     |
|    total_timesteps  | 231992   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0722   |
|    n_updates        | 55497    |
----------------------------------
Eval num_timesteps=232000, episode_reward=24.32 +/- 5.16
Episode length: 219.30 +/- 50.27
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 232000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0285   |
|    n_updates        | 55499    |
----------------------------------
Eval num_timesteps=232500, episode_reward=26.82 +/- 4.65
Episode length: 244.16 +/- 54.12
----------------------------------
| eval/               |          |
|    mean_ep_length   | 244      |
|    mean_reward      | 26.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 232500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0329   |
|    n_updates        | 55624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 25.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1260     |
|    fps              | 86       |
|    time_elapsed     | 2685     |
|    total_timesteps  | 232928   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0157   |
|    n_updates        | 55731    |
----------------------------------
Eval num_timesteps=233000, episode_reward=26.10 +/- 4.72
Episode length: 229.86 +/- 46.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 26.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 233000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 55749    |
----------------------------------
Eval num_timesteps=233500, episode_reward=25.04 +/- 5.01
Episode length: 232.08 +/- 56.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 233500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0162   |
|    n_updates        | 55874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 25.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1264     |
|    fps              | 86       |
|    time_elapsed     | 2699     |
|    total_timesteps  | 233717   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0368   |
|    n_updates        | 55929    |
----------------------------------
Eval num_timesteps=234000, episode_reward=24.76 +/- 5.30
Episode length: 223.32 +/- 57.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 234000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0427   |
|    n_updates        | 55999    |
----------------------------------
Eval num_timesteps=234500, episode_reward=26.00 +/- 6.01
Episode length: 246.10 +/- 69.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 246      |
|    mean_reward      | 26       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 234500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0151   |
|    n_updates        | 56124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 25.5     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1268     |
|    fps              | 86       |
|    time_elapsed     | 2713     |
|    total_timesteps  | 234684   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0409   |
|    n_updates        | 56170    |
----------------------------------
Eval num_timesteps=235000, episode_reward=24.28 +/- 5.25
Episode length: 211.38 +/- 52.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 24.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 235000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.032    |
|    n_updates        | 56249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 226      |
|    ep_rew_mean      | 25.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1272     |
|    fps              | 86       |
|    time_elapsed     | 2720     |
|    total_timesteps  | 235459   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0259   |
|    n_updates        | 56364    |
----------------------------------
Eval num_timesteps=235500, episode_reward=24.78 +/- 4.87
Episode length: 226.06 +/- 47.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 235500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.00894  |
|    n_updates        | 56374    |
----------------------------------
Eval num_timesteps=236000, episode_reward=26.34 +/- 5.41
Episode length: 229.94 +/- 52.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 26.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 236000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0548   |
|    n_updates        | 56499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 25.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1276     |
|    fps              | 86       |
|    time_elapsed     | 2733     |
|    total_timesteps  | 236228   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0686   |
|    n_updates        | 56556    |
----------------------------------
Eval num_timesteps=236500, episode_reward=25.84 +/- 4.52
Episode length: 229.70 +/- 50.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 25.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 236500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 56624    |
----------------------------------
Eval num_timesteps=237000, episode_reward=25.88 +/- 5.51
Episode length: 228.54 +/- 54.49
----------------------------------
| eval/               |          |
|    mean_ep_length   | 229      |
|    mean_reward      | 25.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 237000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0544   |
|    n_updates        | 56749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 25.5     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1280     |
|    fps              | 86       |
|    time_elapsed     | 2747     |
|    total_timesteps  | 237177   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0339   |
|    n_updates        | 56794    |
----------------------------------
Eval num_timesteps=237500, episode_reward=25.06 +/- 6.40
Episode length: 228.30 +/- 64.57
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 237500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.033    |
|    n_updates        | 56874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1284     |
|    fps              | 86       |
|    time_elapsed     | 2754     |
|    total_timesteps  | 237868   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0202   |
|    n_updates        | 56966    |
----------------------------------
Eval num_timesteps=238000, episode_reward=25.84 +/- 6.58
Episode length: 235.20 +/- 63.96
----------------------------------
| eval/               |          |
|    mean_ep_length   | 235      |
|    mean_reward      | 25.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 238000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0386   |
|    n_updates        | 56999    |
----------------------------------
Eval num_timesteps=238500, episode_reward=24.30 +/- 5.82
Episode length: 215.72 +/- 55.75
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 24.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 238500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0212   |
|    n_updates        | 57124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1288     |
|    fps              | 86       |
|    time_elapsed     | 2768     |
|    total_timesteps  | 238776   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 57193    |
----------------------------------
Eval num_timesteps=239000, episode_reward=25.86 +/- 5.48
Episode length: 225.94 +/- 58.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 25.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 239000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0456   |
|    n_updates        | 57249    |
----------------------------------
Eval num_timesteps=239500, episode_reward=26.56 +/- 6.30
Episode length: 237.90 +/- 63.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 238      |
|    mean_reward      | 26.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 239500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 57374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1292     |
|    fps              | 86       |
|    time_elapsed     | 2782     |
|    total_timesteps  | 239717   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0289   |
|    n_updates        | 57429    |
----------------------------------
Eval num_timesteps=240000, episode_reward=25.90 +/- 4.92
Episode length: 225.96 +/- 46.97
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 25.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 240000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0502   |
|    n_updates        | 57499    |
----------------------------------
Eval num_timesteps=240500, episode_reward=25.46 +/- 4.81
Episode length: 223.70 +/- 51.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 25.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 240500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0477   |
|    n_updates        | 57624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 24.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1296     |
|    fps              | 86       |
|    time_elapsed     | 2795     |
|    total_timesteps  | 240564   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0269   |
|    n_updates        | 57640    |
----------------------------------
Eval num_timesteps=241000, episode_reward=27.40 +/- 4.53
Episode length: 240.58 +/- 48.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 241      |
|    mean_reward      | 27.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 241000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0476   |
|    n_updates        | 57749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 24.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1300     |
|    fps              | 86       |
|    time_elapsed     | 2803     |
|    total_timesteps  | 241399   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 57849    |
----------------------------------
Eval num_timesteps=241500, episode_reward=26.02 +/- 5.62
Episode length: 230.72 +/- 56.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 26       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 241500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0212   |
|    n_updates        | 57874    |
----------------------------------
Eval num_timesteps=242000, episode_reward=25.60 +/- 5.66
Episode length: 222.20 +/- 59.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 25.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 242000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0141   |
|    n_updates        | 57999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 24.4     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1304     |
|    fps              | 86       |
|    time_elapsed     | 2817     |
|    total_timesteps  | 242284   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0304   |
|    n_updates        | 58070    |
----------------------------------
Eval num_timesteps=242500, episode_reward=27.46 +/- 5.20
Episode length: 238.44 +/- 55.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 238      |
|    mean_reward      | 27.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 242500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0194   |
|    n_updates        | 58124    |
----------------------------------
Eval num_timesteps=243000, episode_reward=25.70 +/- 5.10
Episode length: 222.88 +/- 51.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 25.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 243000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0228   |
|    n_updates        | 58249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 24.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1308     |
|    fps              | 85       |
|    time_elapsed     | 2831     |
|    total_timesteps  | 243182   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0323   |
|    n_updates        | 58295    |
----------------------------------
Eval num_timesteps=243500, episode_reward=24.18 +/- 5.74
Episode length: 214.22 +/- 57.28
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 24.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 243500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0159   |
|    n_updates        | 58374    |
----------------------------------
Eval num_timesteps=244000, episode_reward=26.48 +/- 5.84
Episode length: 233.24 +/- 62.44
----------------------------------
| eval/               |          |
|    mean_ep_length   | 233      |
|    mean_reward      | 26.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 244000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.029    |
|    n_updates        | 58499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1312     |
|    fps              | 85       |
|    time_elapsed     | 2844     |
|    total_timesteps  | 244056   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0202   |
|    n_updates        | 58513    |
----------------------------------
Eval num_timesteps=244500, episode_reward=23.42 +/- 5.54
Episode length: 205.44 +/- 56.94
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 23.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 244500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0408   |
|    n_updates        | 58624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 216      |
|    ep_rew_mean      | 24.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1316     |
|    fps              | 85       |
|    time_elapsed     | 2851     |
|    total_timesteps  | 244841   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0118   |
|    n_updates        | 58710    |
----------------------------------
Eval num_timesteps=245000, episode_reward=26.60 +/- 5.50
Episode length: 238.44 +/- 61.69
----------------------------------
| eval/               |          |
|    mean_ep_length   | 238      |
|    mean_reward      | 26.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 245000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0304   |
|    n_updates        | 58749    |
----------------------------------
Eval num_timesteps=245500, episode_reward=25.44 +/- 5.40
Episode length: 220.96 +/- 53.68
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 245500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0313   |
|    n_updates        | 58874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1320     |
|    fps              | 85       |
|    time_elapsed     | 2865     |
|    total_timesteps  | 245803   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0206   |
|    n_updates        | 58950    |
----------------------------------
Eval num_timesteps=246000, episode_reward=24.58 +/- 4.79
Episode length: 212.60 +/- 45.50
----------------------------------
| eval/               |          |
|    mean_ep_length   | 213      |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 246000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0197   |
|    n_updates        | 58999    |
----------------------------------
Eval num_timesteps=246500, episode_reward=24.26 +/- 4.58
Episode length: 207.20 +/- 43.55
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | 24.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 246500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 59124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 217      |
|    ep_rew_mean      | 24.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1324     |
|    fps              | 85       |
|    time_elapsed     | 2877     |
|    total_timesteps  | 246524   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0239   |
|    n_updates        | 59130    |
----------------------------------
Eval num_timesteps=247000, episode_reward=25.98 +/- 5.35
Episode length: 224.82 +/- 55.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 26       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 247000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0189   |
|    n_updates        | 59249    |
----------------------------------
Eval num_timesteps=247500, episode_reward=25.38 +/- 5.26
Episode length: 222.42 +/- 53.11
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 247500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 59374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1328     |
|    fps              | 85       |
|    time_elapsed     | 2891     |
|    total_timesteps  | 247552   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0473   |
|    n_updates        | 59387    |
----------------------------------
Eval num_timesteps=248000, episode_reward=24.48 +/- 5.58
Episode length: 214.00 +/- 53.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 24.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 248000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0136   |
|    n_updates        | 59499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1332     |
|    fps              | 85       |
|    time_elapsed     | 2898     |
|    total_timesteps  | 248321   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0161   |
|    n_updates        | 59580    |
----------------------------------
Eval num_timesteps=248500, episode_reward=23.90 +/- 5.28
Episode length: 218.92 +/- 54.58
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 23.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 248500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 59624    |
----------------------------------
Eval num_timesteps=249000, episode_reward=26.30 +/- 4.65
Episode length: 230.04 +/- 47.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 26.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 249000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0527   |
|    n_updates        | 59749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1336     |
|    fps              | 85       |
|    time_elapsed     | 2911     |
|    total_timesteps  | 249258   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0325   |
|    n_updates        | 59814    |
----------------------------------
Eval num_timesteps=249500, episode_reward=24.42 +/- 5.28
Episode length: 217.24 +/- 56.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 24.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 249500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0473   |
|    n_updates        | 59874    |
----------------------------------
Eval num_timesteps=250000, episode_reward=24.24 +/- 5.58
Episode length: 217.30 +/- 57.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 24.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 250000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0419   |
|    n_updates        | 59999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1340     |
|    fps              | 85       |
|    time_elapsed     | 2924     |
|    total_timesteps  | 250257   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0587   |
|    n_updates        | 60064    |
----------------------------------
Eval num_timesteps=250500, episode_reward=26.02 +/- 6.43
Episode length: 228.12 +/- 63.71
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 26       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 250500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0218   |
|    n_updates        | 60124    |
----------------------------------
Eval num_timesteps=251000, episode_reward=27.20 +/- 6.30
Episode length: 236.26 +/- 64.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 236      |
|    mean_reward      | 27.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 251000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.024    |
|    n_updates        | 60249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1344     |
|    fps              | 85       |
|    time_elapsed     | 2939     |
|    total_timesteps  | 251264   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0342   |
|    n_updates        | 60315    |
----------------------------------
Eval num_timesteps=251500, episode_reward=25.48 +/- 5.58
Episode length: 223.90 +/- 49.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 25.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 251500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0303   |
|    n_updates        | 60374    |
----------------------------------
Eval num_timesteps=252000, episode_reward=25.28 +/- 6.30
Episode length: 224.76 +/- 63.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 25.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 252000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0317   |
|    n_updates        | 60499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1348     |
|    fps              | 85       |
|    time_elapsed     | 2952     |
|    total_timesteps  | 252106   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0239   |
|    n_updates        | 60526    |
----------------------------------
Eval num_timesteps=252500, episode_reward=25.10 +/- 5.70
Episode length: 220.90 +/- 59.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 252500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0299   |
|    n_updates        | 60624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1352     |
|    fps              | 85       |
|    time_elapsed     | 2959     |
|    total_timesteps  | 252870   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.04     |
|    n_updates        | 60717    |
----------------------------------
Eval num_timesteps=253000, episode_reward=25.30 +/- 4.67
Episode length: 224.38 +/- 45.62
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 25.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 253000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0267   |
|    n_updates        | 60749    |
----------------------------------
Eval num_timesteps=253500, episode_reward=24.76 +/- 5.49
Episode length: 211.48 +/- 51.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 253500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0297   |
|    n_updates        | 60874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1356     |
|    fps              | 85       |
|    time_elapsed     | 2972     |
|    total_timesteps  | 253850   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0357   |
|    n_updates        | 60962    |
----------------------------------
Eval num_timesteps=254000, episode_reward=26.12 +/- 6.48
Episode length: 231.38 +/- 63.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 26.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 254000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0722   |
|    n_updates        | 60999    |
----------------------------------
Eval num_timesteps=254500, episode_reward=24.66 +/- 5.38
Episode length: 215.72 +/- 53.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 254500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0317   |
|    n_updates        | 61124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1360     |
|    fps              | 85       |
|    time_elapsed     | 2986     |
|    total_timesteps  | 254761   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0236   |
|    n_updates        | 61190    |
----------------------------------
Eval num_timesteps=255000, episode_reward=26.80 +/- 6.21
Episode length: 240.56 +/- 70.03
----------------------------------
| eval/               |          |
|    mean_ep_length   | 241      |
|    mean_reward      | 26.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 255000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0352   |
|    n_updates        | 61249    |
----------------------------------
Eval num_timesteps=255500, episode_reward=25.64 +/- 5.80
Episode length: 225.78 +/- 61.80
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 25.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 255500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0262   |
|    n_updates        | 61374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1364     |
|    fps              | 85       |
|    time_elapsed     | 3000     |
|    total_timesteps  | 255567   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0433   |
|    n_updates        | 61391    |
----------------------------------
Eval num_timesteps=256000, episode_reward=25.66 +/- 5.78
Episode length: 223.58 +/- 60.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 25.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 256000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0139   |
|    n_updates        | 61499    |
----------------------------------
Eval num_timesteps=256500, episode_reward=24.40 +/- 5.17
Episode length: 219.96 +/- 51.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 24.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 256500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0178   |
|    n_updates        | 61624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 24.8     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1368     |
|    fps              | 85       |
|    time_elapsed     | 3013     |
|    total_timesteps  | 256575   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 61643    |
----------------------------------
Eval num_timesteps=257000, episode_reward=24.04 +/- 6.17
Episode length: 213.92 +/- 60.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 24       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 257000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0334   |
|    n_updates        | 61749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1372     |
|    fps              | 85       |
|    time_elapsed     | 3020     |
|    total_timesteps  | 257475   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0296   |
|    n_updates        | 61868    |
----------------------------------
Eval num_timesteps=257500, episode_reward=24.68 +/- 5.80
Episode length: 220.48 +/- 58.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 257500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0394   |
|    n_updates        | 61874    |
----------------------------------
Eval num_timesteps=258000, episode_reward=24.68 +/- 5.55
Episode length: 217.00 +/- 55.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 258000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0242   |
|    n_updates        | 61999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1376     |
|    fps              | 85       |
|    time_elapsed     | 3033     |
|    total_timesteps  | 258360   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0345   |
|    n_updates        | 62089    |
----------------------------------
Eval num_timesteps=258500, episode_reward=25.36 +/- 5.09
Episode length: 217.72 +/- 49.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 258500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0281   |
|    n_updates        | 62124    |
----------------------------------
Eval num_timesteps=259000, episode_reward=24.44 +/- 6.26
Episode length: 217.14 +/- 62.22
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 24.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 259000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0218   |
|    n_updates        | 62249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1380     |
|    fps              | 85       |
|    time_elapsed     | 3047     |
|    total_timesteps  | 259407   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0329   |
|    n_updates        | 62351    |
----------------------------------
Eval num_timesteps=259500, episode_reward=22.98 +/- 5.21
Episode length: 211.44 +/- 56.17
----------------------------------
| eval/               |          |
|    mean_ep_length   | 211      |
|    mean_reward      | 23       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 259500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.018    |
|    n_updates        | 62374    |
----------------------------------
Eval num_timesteps=260000, episode_reward=24.34 +/- 5.69
Episode length: 212.68 +/- 55.09
----------------------------------
| eval/               |          |
|    mean_ep_length   | 213      |
|    mean_reward      | 24.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 260000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0398   |
|    n_updates        | 62499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1384     |
|    fps              | 85       |
|    time_elapsed     | 3059     |
|    total_timesteps  | 260164   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0441   |
|    n_updates        | 62540    |
----------------------------------
Eval num_timesteps=260500, episode_reward=23.60 +/- 5.44
Episode length: 216.64 +/- 57.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 23.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 260500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0491   |
|    n_updates        | 62624    |
----------------------------------
Eval num_timesteps=261000, episode_reward=25.02 +/- 5.71
Episode length: 223.24 +/- 56.86
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 261000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0153   |
|    n_updates        | 62749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1388     |
|    fps              | 84       |
|    time_elapsed     | 3072     |
|    total_timesteps  | 261020   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0215   |
|    n_updates        | 62754    |
----------------------------------
Eval num_timesteps=261500, episode_reward=25.02 +/- 5.32
Episode length: 217.98 +/- 50.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 261500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0322   |
|    n_updates        | 62874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1392     |
|    fps              | 85       |
|    time_elapsed     | 3079     |
|    total_timesteps  | 261874   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0309   |
|    n_updates        | 62968    |
----------------------------------
Eval num_timesteps=262000, episode_reward=22.92 +/- 6.23
Episode length: 214.18 +/- 66.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 22.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 262000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0316   |
|    n_updates        | 62999    |
----------------------------------
Eval num_timesteps=262500, episode_reward=21.96 +/- 5.75
Episode length: 202.76 +/- 61.29
----------------------------------
| eval/               |          |
|    mean_ep_length   | 203      |
|    mean_reward      | 22       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 262500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0457   |
|    n_updates        | 63124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1396     |
|    fps              | 84       |
|    time_elapsed     | 3093     |
|    total_timesteps  | 262689   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0608   |
|    n_updates        | 63172    |
----------------------------------
Eval num_timesteps=263000, episode_reward=22.56 +/- 5.76
Episode length: 217.74 +/- 61.67
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 22.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 263000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.015    |
|    n_updates        | 63249    |
----------------------------------
Eval num_timesteps=263500, episode_reward=24.84 +/- 4.57
Episode length: 217.76 +/- 43.01
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 263500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0382   |
|    n_updates        | 63374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1400     |
|    fps              | 84       |
|    time_elapsed     | 3106     |
|    total_timesteps  | 263635   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0244   |
|    n_updates        | 63408    |
----------------------------------
Eval num_timesteps=264000, episode_reward=25.00 +/- 5.36
Episode length: 225.10 +/- 57.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 264000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0433   |
|    n_updates        | 63499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1404     |
|    fps              | 84       |
|    time_elapsed     | 3113     |
|    total_timesteps  | 264488   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0192   |
|    n_updates        | 63621    |
----------------------------------
Eval num_timesteps=264500, episode_reward=24.74 +/- 5.92
Episode length: 215.88 +/- 58.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 264500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.029    |
|    n_updates        | 63624    |
----------------------------------
Eval num_timesteps=265000, episode_reward=23.68 +/- 5.80
Episode length: 209.12 +/- 57.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 23.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 265000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0344   |
|    n_updates        | 63749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 24.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1408     |
|    fps              | 84       |
|    time_elapsed     | 3126     |
|    total_timesteps  | 265069   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0284   |
|    n_updates        | 63767    |
----------------------------------
Eval num_timesteps=265500, episode_reward=21.68 +/- 4.76
Episode length: 193.68 +/- 47.20
----------------------------------
| eval/               |          |
|    mean_ep_length   | 194      |
|    mean_reward      | 21.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 265500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0224   |
|    n_updates        | 63874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 218      |
|    ep_rew_mean      | 24.6     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1412     |
|    fps              | 84       |
|    time_elapsed     | 3132     |
|    total_timesteps  | 265837   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0477   |
|    n_updates        | 63959    |
----------------------------------
Eval num_timesteps=266000, episode_reward=25.54 +/- 5.36
Episode length: 229.92 +/- 52.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 25.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 266000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.022    |
|    n_updates        | 63999    |
----------------------------------
Eval num_timesteps=266500, episode_reward=24.66 +/- 5.21
Episode length: 220.70 +/- 46.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 266500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0389   |
|    n_updates        | 64124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 24.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1416     |
|    fps              | 84       |
|    time_elapsed     | 3146     |
|    total_timesteps  | 266755   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0308   |
|    n_updates        | 64188    |
----------------------------------
Eval num_timesteps=267000, episode_reward=24.20 +/- 5.28
Episode length: 214.64 +/- 50.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 215      |
|    mean_reward      | 24.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 267000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0281   |
|    n_updates        | 64249    |
----------------------------------
Eval num_timesteps=267500, episode_reward=24.44 +/- 5.26
Episode length: 220.28 +/- 52.73
----------------------------------
| eval/               |          |
|    mean_ep_length   | 220      |
|    mean_reward      | 24.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 267500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0136   |
|    n_updates        | 64374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 219      |
|    ep_rew_mean      | 24.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1420     |
|    fps              | 84       |
|    time_elapsed     | 3159     |
|    total_timesteps  | 267733   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0247   |
|    n_updates        | 64433    |
----------------------------------
Eval num_timesteps=268000, episode_reward=23.18 +/- 4.55
Episode length: 207.66 +/- 44.16
----------------------------------
| eval/               |          |
|    mean_ep_length   | 208      |
|    mean_reward      | 23.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 268000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0207   |
|    n_updates        | 64499    |
----------------------------------
Eval num_timesteps=268500, episode_reward=22.64 +/- 4.33
Episode length: 204.34 +/- 42.02
----------------------------------
| eval/               |          |
|    mean_ep_length   | 204      |
|    mean_reward      | 22.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 268500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0318   |
|    n_updates        | 64624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1424     |
|    fps              | 84       |
|    time_elapsed     | 3172     |
|    total_timesteps  | 268547   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0389   |
|    n_updates        | 64636    |
----------------------------------
Eval num_timesteps=269000, episode_reward=22.58 +/- 5.58
Episode length: 206.04 +/- 54.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 206      |
|    mean_reward      | 22.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 269000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0441   |
|    n_updates        | 64749    |
----------------------------------
Eval num_timesteps=269500, episode_reward=25.30 +/- 5.83
Episode length: 223.90 +/- 60.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 25.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 269500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0196   |
|    n_updates        | 64874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 220      |
|    ep_rew_mean      | 24.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1428     |
|    fps              | 84       |
|    time_elapsed     | 3185     |
|    total_timesteps  | 269532   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0473   |
|    n_updates        | 64882    |
----------------------------------
Eval num_timesteps=270000, episode_reward=24.00 +/- 5.11
Episode length: 217.10 +/- 55.52
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 24       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 270000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0309   |
|    n_updates        | 64999    |
----------------------------------
Eval num_timesteps=270500, episode_reward=22.98 +/- 5.93
Episode length: 211.64 +/- 61.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 23       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 270500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0451   |
|    n_updates        | 65124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1432     |
|    fps              | 84       |
|    time_elapsed     | 3198     |
|    total_timesteps  | 270501   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.033    |
|    n_updates        | 65125    |
----------------------------------
Eval num_timesteps=271000, episode_reward=25.22 +/- 5.41
Episode length: 225.68 +/- 57.74
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 25.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 271000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0271   |
|    n_updates        | 65249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1436     |
|    fps              | 84       |
|    time_elapsed     | 3205     |
|    total_timesteps  | 271435   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0396   |
|    n_updates        | 65358    |
----------------------------------
Eval num_timesteps=271500, episode_reward=24.96 +/- 5.25
Episode length: 223.76 +/- 47.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 271500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0351   |
|    n_updates        | 65374    |
----------------------------------
Eval num_timesteps=272000, episode_reward=24.04 +/- 6.25
Episode length: 218.60 +/- 62.79
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 272000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0229   |
|    n_updates        | 65499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1440     |
|    fps              | 84       |
|    time_elapsed     | 3218     |
|    total_timesteps  | 272394   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.027    |
|    n_updates        | 65598    |
----------------------------------
Eval num_timesteps=272500, episode_reward=24.10 +/- 4.93
Episode length: 211.72 +/- 51.25
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 24.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 272500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0315   |
|    n_updates        | 65624    |
----------------------------------
Eval num_timesteps=273000, episode_reward=23.74 +/- 6.43
Episode length: 212.26 +/- 63.30
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 23.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 273000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0449   |
|    n_updates        | 65749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1444     |
|    fps              | 84       |
|    time_elapsed     | 3231     |
|    total_timesteps  | 273364   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0233   |
|    n_updates        | 65840    |
----------------------------------
Eval num_timesteps=273500, episode_reward=25.06 +/- 5.12
Episode length: 227.10 +/- 55.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 227      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 273500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0206   |
|    n_updates        | 65874    |
----------------------------------
Eval num_timesteps=274000, episode_reward=26.40 +/- 5.77
Episode length: 235.34 +/- 58.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 235      |
|    mean_reward      | 26.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 274000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0127   |
|    n_updates        | 65999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1448     |
|    fps              | 84       |
|    time_elapsed     | 3245     |
|    total_timesteps  | 274416   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0292   |
|    n_updates        | 66103    |
----------------------------------
Eval num_timesteps=274500, episode_reward=23.40 +/- 5.20
Episode length: 207.28 +/- 52.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | 23.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 274500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0244   |
|    n_updates        | 66124    |
----------------------------------
Eval num_timesteps=275000, episode_reward=24.92 +/- 5.08
Episode length: 221.90 +/- 50.43
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 24.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 275000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0457   |
|    n_updates        | 66249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 25.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1452     |
|    fps              | 84       |
|    time_elapsed     | 3258     |
|    total_timesteps  | 275405   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0296   |
|    n_updates        | 66351    |
----------------------------------
Eval num_timesteps=275500, episode_reward=25.78 +/- 4.89
Episode length: 231.22 +/- 49.47
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 25.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 275500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0361   |
|    n_updates        | 66374    |
----------------------------------
Eval num_timesteps=276000, episode_reward=23.32 +/- 6.18
Episode length: 216.94 +/- 65.18
----------------------------------
| eval/               |          |
|    mean_ep_length   | 217      |
|    mean_reward      | 23.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 276000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0383   |
|    n_updates        | 66499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1456     |
|    fps              | 84       |
|    time_elapsed     | 3272     |
|    total_timesteps  | 276229   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0408   |
|    n_updates        | 66557    |
----------------------------------
Eval num_timesteps=276500, episode_reward=25.22 +/- 5.27
Episode length: 219.34 +/- 49.82
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 25.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 276500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0278   |
|    n_updates        | 66624    |
----------------------------------
Eval num_timesteps=277000, episode_reward=22.84 +/- 5.45
Episode length: 206.80 +/- 51.45
----------------------------------
| eval/               |          |
|    mean_ep_length   | 207      |
|    mean_reward      | 22.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 277000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0249   |
|    n_updates        | 66749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1460     |
|    fps              | 84       |
|    time_elapsed     | 3285     |
|    total_timesteps  | 277303   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0341   |
|    n_updates        | 66825    |
----------------------------------
Eval num_timesteps=277500, episode_reward=23.72 +/- 5.61
Episode length: 211.84 +/- 54.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 23.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 277500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0285   |
|    n_updates        | 66874    |
----------------------------------
Eval num_timesteps=278000, episode_reward=24.96 +/- 4.48
Episode length: 223.04 +/- 42.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 278000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0124   |
|    n_updates        | 66999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 25.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1464     |
|    fps              | 84       |
|    time_elapsed     | 3298     |
|    total_timesteps  | 278111   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0313   |
|    n_updates        | 67027    |
----------------------------------
Eval num_timesteps=278500, episode_reward=24.68 +/- 6.07
Episode length: 218.94 +/- 59.37
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 278500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0481   |
|    n_updates        | 67124    |
----------------------------------
Eval num_timesteps=279000, episode_reward=24.84 +/- 4.81
Episode length: 216.50 +/- 46.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 216      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 279000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0299   |
|    n_updates        | 67249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 226      |
|    ep_rew_mean      | 25.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1468     |
|    fps              | 84       |
|    time_elapsed     | 3311     |
|    total_timesteps  | 279172   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0334   |
|    n_updates        | 67292    |
----------------------------------
Eval num_timesteps=279500, episode_reward=25.04 +/- 5.23
Episode length: 220.74 +/- 51.35
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 25       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 279500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0431   |
|    n_updates        | 67374    |
----------------------------------
Eval num_timesteps=280000, episode_reward=24.24 +/- 5.92
Episode length: 209.36 +/- 56.32
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 24.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 280000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0248   |
|    n_updates        | 67499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1472     |
|    fps              | 84       |
|    time_elapsed     | 3324     |
|    total_timesteps  | 280006   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0503   |
|    n_updates        | 67501    |
----------------------------------
Eval num_timesteps=280500, episode_reward=26.48 +/- 5.81
Episode length: 239.22 +/- 60.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 239      |
|    mean_reward      | 26.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 280500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0172   |
|    n_updates        | 67624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1476     |
|    fps              | 84       |
|    time_elapsed     | 3332     |
|    total_timesteps  | 280803   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0183   |
|    n_updates        | 67700    |
----------------------------------
Eval num_timesteps=281000, episode_reward=22.12 +/- 6.04
Episode length: 199.38 +/- 58.78
----------------------------------
| eval/               |          |
|    mean_ep_length   | 199      |
|    mean_reward      | 22.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 281000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0357   |
|    n_updates        | 67749    |
----------------------------------
Eval num_timesteps=281500, episode_reward=24.34 +/- 5.71
Episode length: 214.68 +/- 55.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 215      |
|    mean_reward      | 24.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 281500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0286   |
|    n_updates        | 67874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1480     |
|    fps              | 84       |
|    time_elapsed     | 3344     |
|    total_timesteps  | 281672   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0259   |
|    n_updates        | 67917    |
----------------------------------
Eval num_timesteps=282000, episode_reward=25.88 +/- 5.77
Episode length: 231.06 +/- 57.65
----------------------------------
| eval/               |          |
|    mean_ep_length   | 231      |
|    mean_reward      | 25.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 282000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0803   |
|    n_updates        | 67999    |
----------------------------------
Eval num_timesteps=282500, episode_reward=25.52 +/- 6.25
Episode length: 224.30 +/- 63.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 25.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 282500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0304   |
|    n_updates        | 68124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1484     |
|    fps              | 84       |
|    time_elapsed     | 3358     |
|    total_timesteps  | 282564   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0198   |
|    n_updates        | 68140    |
----------------------------------
Eval num_timesteps=283000, episode_reward=25.28 +/- 5.12
Episode length: 227.10 +/- 57.33
----------------------------------
| eval/               |          |
|    mean_ep_length   | 227      |
|    mean_reward      | 25.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 283000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0332   |
|    n_updates        | 68249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1488     |
|    fps              | 84       |
|    time_elapsed     | 3365     |
|    total_timesteps  | 283297   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0206   |
|    n_updates        | 68324    |
----------------------------------
Eval num_timesteps=283500, episode_reward=25.52 +/- 5.70
Episode length: 221.26 +/- 55.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 221      |
|    mean_reward      | 25.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 283500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0386   |
|    n_updates        | 68374    |
----------------------------------
Eval num_timesteps=284000, episode_reward=27.02 +/- 4.51
Episode length: 236.76 +/- 51.70
----------------------------------
| eval/               |          |
|    mean_ep_length   | 237      |
|    mean_reward      | 27       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 284000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0134   |
|    n_updates        | 68499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 222      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1492     |
|    fps              | 84       |
|    time_elapsed     | 3378     |
|    total_timesteps  | 284085   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0307   |
|    n_updates        | 68521    |
----------------------------------
Eval num_timesteps=284500, episode_reward=24.38 +/- 4.98
Episode length: 219.10 +/- 50.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 284500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0302   |
|    n_updates        | 68624    |
----------------------------------
Eval num_timesteps=285000, episode_reward=25.44 +/- 6.39
Episode length: 224.22 +/- 60.88
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 25.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 285000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0647   |
|    n_updates        | 68749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1496     |
|    fps              | 84       |
|    time_elapsed     | 3392     |
|    total_timesteps  | 285013   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.035    |
|    n_updates        | 68753    |
----------------------------------
Eval num_timesteps=285500, episode_reward=26.16 +/- 5.06
Episode length: 226.94 +/- 57.24
----------------------------------
| eval/               |          |
|    mean_ep_length   | 227      |
|    mean_reward      | 26.2     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 285500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0378   |
|    n_updates        | 68874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 221      |
|    ep_rew_mean      | 24.7     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1500     |
|    fps              | 84       |
|    time_elapsed     | 3399     |
|    total_timesteps  | 285780   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0339   |
|    n_updates        | 68944    |
----------------------------------
Eval num_timesteps=286000, episode_reward=22.72 +/- 5.92
Episode length: 213.76 +/- 61.66
----------------------------------
| eval/               |          |
|    mean_ep_length   | 214      |
|    mean_reward      | 22.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 286000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0241   |
|    n_updates        | 68999    |
----------------------------------
Eval num_timesteps=286500, episode_reward=25.52 +/- 5.19
Episode length: 227.64 +/- 49.99
----------------------------------
| eval/               |          |
|    mean_ep_length   | 228      |
|    mean_reward      | 25.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 286500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.04     |
|    n_updates        | 69124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1504     |
|    fps              | 84       |
|    time_elapsed     | 3413     |
|    total_timesteps  | 286779   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0502   |
|    n_updates        | 69194    |
----------------------------------
Eval num_timesteps=287000, episode_reward=24.62 +/- 4.90
Episode length: 225.52 +/- 50.48
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 287000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0933   |
|    n_updates        | 69249    |
----------------------------------
Eval num_timesteps=287500, episode_reward=25.58 +/- 4.69
Episode length: 229.72 +/- 49.08
----------------------------------
| eval/               |          |
|    mean_ep_length   | 230      |
|    mean_reward      | 25.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 287500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0283   |
|    n_updates        | 69374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1508     |
|    fps              | 83       |
|    time_elapsed     | 3427     |
|    total_timesteps  | 287552   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0298   |
|    n_updates        | 69387    |
----------------------------------
Eval num_timesteps=288000, episode_reward=23.94 +/- 6.17
Episode length: 222.20 +/- 61.54
----------------------------------
| eval/               |          |
|    mean_ep_length   | 222      |
|    mean_reward      | 23.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 288000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0333   |
|    n_updates        | 69499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 226      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1512     |
|    fps              | 83       |
|    time_elapsed     | 3435     |
|    total_timesteps  | 288457   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0222   |
|    n_updates        | 69614    |
----------------------------------
Eval num_timesteps=288500, episode_reward=26.56 +/- 6.48
Episode length: 239.76 +/- 65.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 240      |
|    mean_reward      | 26.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 288500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0257   |
|    n_updates        | 69624    |
----------------------------------
Eval num_timesteps=289000, episode_reward=23.60 +/- 5.11
Episode length: 208.78 +/- 48.83
----------------------------------
| eval/               |          |
|    mean_ep_length   | 209      |
|    mean_reward      | 23.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 289000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0184   |
|    n_updates        | 69749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1516     |
|    fps              | 83       |
|    time_elapsed     | 3448     |
|    total_timesteps  | 289187   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.032    |
|    n_updates        | 69796    |
----------------------------------
Eval num_timesteps=289500, episode_reward=26.62 +/- 5.85
Episode length: 236.74 +/- 60.72
----------------------------------
| eval/               |          |
|    mean_ep_length   | 237      |
|    mean_reward      | 26.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 289500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.045    |
|    n_updates        | 69874    |
----------------------------------
Eval num_timesteps=290000, episode_reward=24.58 +/- 4.94
Episode length: 218.30 +/- 52.21
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 24.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 290000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0263   |
|    n_updates        | 69999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1520     |
|    fps              | 83       |
|    time_elapsed     | 3462     |
|    total_timesteps  | 290114   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0309   |
|    n_updates        | 70028    |
----------------------------------
Eval num_timesteps=290500, episode_reward=25.94 +/- 5.40
Episode length: 233.18 +/- 53.51
----------------------------------
| eval/               |          |
|    mean_ep_length   | 233      |
|    mean_reward      | 25.9     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 290500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0315   |
|    n_updates        | 70124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1524     |
|    fps              | 83       |
|    time_elapsed     | 3469     |
|    total_timesteps  | 290887   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0332   |
|    n_updates        | 70221    |
----------------------------------
Eval num_timesteps=291000, episode_reward=26.80 +/- 5.99
Episode length: 238.20 +/- 65.87
----------------------------------
| eval/               |          |
|    mean_ep_length   | 238      |
|    mean_reward      | 26.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 291000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0235   |
|    n_updates        | 70249    |
----------------------------------
Eval num_timesteps=291500, episode_reward=22.38 +/- 5.23
Episode length: 209.98 +/- 53.95
----------------------------------
| eval/               |          |
|    mean_ep_length   | 210      |
|    mean_reward      | 22.4     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 291500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0212   |
|    n_updates        | 70374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1528     |
|    fps              | 83       |
|    time_elapsed     | 3483     |
|    total_timesteps  | 291864   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0323   |
|    n_updates        | 70465    |
----------------------------------
Eval num_timesteps=292000, episode_reward=27.28 +/- 5.37
Episode length: 236.80 +/- 54.56
----------------------------------
| eval/               |          |
|    mean_ep_length   | 237      |
|    mean_reward      | 27.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 292000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0236   |
|    n_updates        | 70499    |
----------------------------------
Eval num_timesteps=292500, episode_reward=25.12 +/- 5.16
Episode length: 217.90 +/- 46.90
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 292500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.034    |
|    n_updates        | 70624    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 223      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1532     |
|    fps              | 83       |
|    time_elapsed     | 3496     |
|    total_timesteps  | 292790   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0435   |
|    n_updates        | 70697    |
----------------------------------
Eval num_timesteps=293000, episode_reward=26.02 +/- 5.45
Episode length: 224.44 +/- 55.61
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 26       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 293000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0203   |
|    n_updates        | 70749    |
----------------------------------
Eval num_timesteps=293500, episode_reward=24.72 +/- 5.90
Episode length: 223.92 +/- 59.77
----------------------------------
| eval/               |          |
|    mean_ep_length   | 224      |
|    mean_reward      | 24.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 293500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0332   |
|    n_updates        | 70874    |
----------------------------------
Eval num_timesteps=294000, episode_reward=27.50 +/- 6.03
Episode length: 241.24 +/- 60.23
----------------------------------
| eval/               |          |
|    mean_ep_length   | 241      |
|    mean_reward      | 27.5     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 294000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0275   |
|    n_updates        | 70999    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 226      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1536     |
|    fps              | 83       |
|    time_elapsed     | 3517     |
|    total_timesteps  | 294057   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0369   |
|    n_updates        | 71014    |
----------------------------------
Eval num_timesteps=294500, episode_reward=25.68 +/- 5.10
Episode length: 231.82 +/- 53.06
----------------------------------
| eval/               |          |
|    mean_ep_length   | 232      |
|    mean_reward      | 25.7     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 294500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0189   |
|    n_updates        | 71124    |
----------------------------------
Eval num_timesteps=295000, episode_reward=23.64 +/- 5.41
Episode length: 212.02 +/- 54.00
----------------------------------
| eval/               |          |
|    mean_ep_length   | 212      |
|    mean_reward      | 23.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 295000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0358   |
|    n_updates        | 71249    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 227      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1540     |
|    fps              | 83       |
|    time_elapsed     | 3530     |
|    total_timesteps  | 295047   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0167   |
|    n_updates        | 71261    |
----------------------------------
Eval num_timesteps=295500, episode_reward=25.06 +/- 6.21
Episode length: 218.30 +/- 60.85
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 25.1     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 295500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0173   |
|    n_updates        | 71374    |
----------------------------------
Eval num_timesteps=296000, episode_reward=26.02 +/- 5.97
Episode length: 233.72 +/- 61.41
----------------------------------
| eval/               |          |
|    mean_ep_length   | 234      |
|    mean_reward      | 26       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 296000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0109   |
|    n_updates        | 71499    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 227      |
|    ep_rew_mean      | 25.3     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1544     |
|    fps              | 83       |
|    time_elapsed     | 3544     |
|    total_timesteps  | 296082   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0169   |
|    n_updates        | 71520    |
----------------------------------
Eval num_timesteps=296500, episode_reward=22.58 +/- 4.82
Episode length: 204.78 +/- 46.42
----------------------------------
| eval/               |          |
|    mean_ep_length   | 205      |
|    mean_reward      | 22.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 296500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0361   |
|    n_updates        | 71624    |
----------------------------------
Eval num_timesteps=297000, episode_reward=26.62 +/- 5.17
Episode length: 234.64 +/- 57.05
----------------------------------
| eval/               |          |
|    mean_ep_length   | 235      |
|    mean_reward      | 26.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 297000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0225   |
|    n_updates        | 71749    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 226      |
|    ep_rew_mean      | 25.1     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1548     |
|    fps              | 83       |
|    time_elapsed     | 3558     |
|    total_timesteps  | 297053   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0255   |
|    n_updates        | 71763    |
----------------------------------
Eval num_timesteps=297500, episode_reward=23.82 +/- 6.39
Episode length: 223.20 +/- 68.13
----------------------------------
| eval/               |          |
|    mean_ep_length   | 223      |
|    mean_reward      | 23.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 297500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0288   |
|    n_updates        | 71874    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 225      |
|    ep_rew_mean      | 25       |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1552     |
|    fps              | 83       |
|    time_elapsed     | 3565     |
|    total_timesteps  | 297919   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0201   |
|    n_updates        | 71979    |
----------------------------------
Eval num_timesteps=298000, episode_reward=20.60 +/- 5.47
Episode length: 194.58 +/- 55.63
----------------------------------
| eval/               |          |
|    mean_ep_length   | 195      |
|    mean_reward      | 20.6     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 298000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0306   |
|    n_updates        | 71999    |
----------------------------------
Eval num_timesteps=298500, episode_reward=23.80 +/- 5.08
Episode length: 217.60 +/- 45.53
----------------------------------
| eval/               |          |
|    mean_ep_length   | 218      |
|    mean_reward      | 23.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 298500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0214   |
|    n_updates        | 72124    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 226      |
|    ep_rew_mean      | 25.2     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1556     |
|    fps              | 83       |
|    time_elapsed     | 3578     |
|    total_timesteps  | 298848   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0468   |
|    n_updates        | 72211    |
----------------------------------
Eval num_timesteps=299000, episode_reward=23.98 +/- 6.15
Episode length: 219.36 +/- 62.39
----------------------------------
| eval/               |          |
|    mean_ep_length   | 219      |
|    mean_reward      | 24       |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 299000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0239   |
|    n_updates        | 72249    |
----------------------------------
Eval num_timesteps=299500, episode_reward=25.26 +/- 5.95
Episode length: 225.66 +/- 57.81
----------------------------------
| eval/               |          |
|    mean_ep_length   | 226      |
|    mean_reward      | 25.3     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 299500   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.028    |
|    n_updates        | 72374    |
----------------------------------
----------------------------------
| rollout/            |          |
|    ep_len_mean      | 224      |
|    ep_rew_mean      | 24.9     |
|    exploration_rate | 0.01     |
| time/               |          |
|    episodes         | 1560     |
|    fps              | 83       |
|    time_elapsed     | 3591     |
|    total_timesteps  | 299730   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0294   |
|    n_updates        | 72432    |
----------------------------------
Eval num_timesteps=300000, episode_reward=24.82 +/- 5.93
Episode length: 225.46 +/- 57.59
----------------------------------
| eval/               |          |
|    mean_ep_length   | 225      |
|    mean_reward      | 24.8     |
| rollout/            |          |
|    exploration_rate | 0.01     |
| time/               |          |
|    total_timesteps  | 300000   |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.0171   |
|    n_updates        | 72499    |
----------------------------------
/home/miguelvilla/anaconda3/envs/doom/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'trains/defend-line/dqn-1/saves' does not exist. Will create it.
  warnings.warn(f"Path '{path.parent}' does not exist. Will create it.")
Parameters: {'batch_size': 32, 'learning_rate': 0.0001, 'buffer_size': 10000, 'gamma': 0.97, 'exploration_fraction': 0.15, 'exploration_final_eps': 0.01, 'learning_starts': 10000.0, 'decay_start_steps': 0, 'decay_end_steps': 150000.0}
Training steps: 300000
Frame skip: 4
